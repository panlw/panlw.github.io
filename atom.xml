<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Junkman]]></title>
  <link href="http://panlw.github.io/atom.xml" rel="self"/>
  <link href="http://panlw.github.io/"/>
  <updated>2018-06-29T17:29:55+08:00</updated>
  <id>http://panlw.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[架构设计：负载均衡层设计方案（9）——负载均衡层总结下篇]]></title>
    <link href="http://panlw.github.io/15301813999205.html"/>
    <updated>2018-06-28T18:23:19+08:00</updated>
    <id>http://panlw.github.io/15301813999205.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://blog.csdn.net/yinwenjie/article/details/48101869">https://blog.csdn.net/yinwenjie/article/details/48101869</a></p>
</blockquote>

<p>(接上一篇<a href="http://blog.csdn.net/yinwenjie/article/details/47211641%20%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%B1%82%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88%EF%BC%888%EF%BC%89%E2%80%94%E2%80%94%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%B1%82%E6%80%BB%E7%BB%93%E4%B8%8B%E7%AF%87">《架构设计：负载均衡层设计方案（8）——负载均衡层总结上篇》</a>)</p>

<h1 id="toc_0"><a></a>3、负载均衡层技术汇总</h1>

<h2 id="toc_1"><a></a>3-4、Keepalived 技术</h2>

<p>Keepalived 在我的博客文章《架构设计：负载均衡层设计方案（7）》（<a href="http://blog.csdn.net/yinwenjie/article/details/47211551">http://blog.csdn.net/yinwenjie/article/details/47211551</a>）、《架构设计：负载均衡层设计方案（6）》（<a href="http://blog.csdn.net/yinwenjie/article/details/47130609">http://blog.csdn.net/yinwenjie/article/details/47130609</a>）中都有介绍。大家可能注意到，在这些文章中从来没有单独介绍 Keepalived。这是因为 Keepalived 是为了监控集群节点的工作状态，在因为某种原因不能正常提供服务的前提下，完成备机的切换。这里面有两个关键点：<strong>监控节点上提供的服务</strong>、<strong>完成网络切换</strong>。keepalived 本身是不提供业务服务的，只是监控提供的服务是否正常工作，那么既然都没有可以监控的服务，那么 Keepalived 有什么独立使用的必要呢？</p>

<p>下图是曾经在博文中展现过的 Nginx + Keepalived 的工作结构和 LVS + Keepalived 的工作结构：</p>

<p><img src="http://img.blog.csdn.net/20150730143806220" alt=""/><br/>
Nginx + Keepalived 的工作方式</p>

<p><img src="http://img.blog.csdn.net/20150822113121644" alt=""/><br/>
LVS + Keepalived + Nginx 的工作方式</p>

<ul>
<li>  <strong>相关技术还有</strong>：
Heartbeat 是 Linux-HA 计划中的一个重要项目，它的功能比 Keepalived 更强大，安装和管理也相对复杂。网络上有很多资料介绍 Heartbeat 和 Keepalived 的优缺点和使用对比。但就我自己的使用经验来说，个人更喜欢使用 Keepalived，原因很简单：Keepalived 安装和配置更简单，而且够用。另外 Redhat Rhcs 套件也可以搭建类似的 HA 集群，但是说实话本人没有尝试过。</li>
</ul>

<h2 id="toc_2"><a></a>3-5、DNS 轮询和智能 DNS</h2>

<p>//TODO DNS 技术还没有介绍</p>

<h2 id="toc_3"><a></a>3-6、硬件负载</h2>

<p>在这个系列的 “负载均衡层设计方案” 博文中，我们所提到的诸如 Nginx、LVS 等技术，没有详细讲述的 Haproxy、Squid 等技术，都是基于软件的负载技术。F5 是一家公司，它的 BIG-IP LTM 技术是基于硬件负载的。硬件负载方案提供了软件负载技术无法提供了性能空间，并且集成了 NAT 映射功能、SSL 加速、Cookie 加密、高速缓存、攻击过滤、包过滤、动态 Session 保持等等很多软件负载无法提供的功能（或者需要多个软件组合使用才能提供的功能）。</p>

<p>但是硬件负载方案也有其缺点，主要就是建设费用比较高昂，它不像软负载可以根据系统的吞吐量的持续增加进行持续扩展。当然您可以根据系统的吞吐量需求，在前期采用软负载，后期采用硬件负载的方案。除了 F5 公司提供的硬件负载技术，还有 Citrix 公司的硬负载方案、A10 公司的硬件负载方案。</p>

<p><img src="http://img.blog.csdn.net/20150902093134033" alt=""/></p>

<h1 id="toc_4"><a></a>4、常见负载均衡技术组合</h1>

<p>这里我们在重新回顾一下这个系列博文中，提到的目前常用的负载均衡技术的组合方式。</p>

<h2 id="toc_5"><a></a>4-1、独立的 Nginx/Haproxy</h2>

<p><img src="http://img.blog.csdn.net/20150630091204501" alt=""/><br/>
一般的 WEB 系统，前段假设一个 Nginx 或者 Haproxy 服务器，基本上可以解决包括负载分发在内的很多问题了。</p>

<h2 id="toc_6"><a></a>4-2、Nginx + Keepalived 或 Haproxy + Keepalived 或 + Heartbeat</h2>

<p><img src="http://img.blog.csdn.net/20150630091331246" alt=""/></p>

<p>为了保证 Nginx 或者 HaProxy 服务器的稳定性，可以使用 Keepalived 或者 Heartbeat 做一个简单的热备方案。</p>

<h2 id="toc_7"><a></a>4-3、LVS + (Keepalived | Heartbeat) + (Nginx | Haproxy)</h2>

<p><img src="http://img.blog.csdn.net/20150630091527570" alt=""/></p>

<p>随着访问压力的增大，我们开始采用多层负载方案，在 Nginx 或者 Haproxy 的前段架设 LVS 服务，并通过 Keepalived 或者 Heartbeat 保证 Keepalived 的持续工作。</p>

<h2 id="toc_8"><a></a>4-4、加如 DNS 轮询技术或者智能 DNS 路由技术</h2>

<p><img src="http://img.blog.csdn.net/20150627232428537" alt=""/></p>

<p>技术方案扩展到这一步，日千万级 PV 是完全可以支撑了。前提条件是：程序没有问题 <sup>_<sup>。</sup></sup></p>

<p>如果您站点的流量还要大甚至高出几个数量级，那么恭喜您，您肯定是全球排名前 100 位互联网公司工作；但是从另一个角度来说，您遇到的问题可能只能根据贵公司的业务特点，自己寻求解决方案了。这样的例子有很多，例如 YouTube 发现市面上的商用 CDN 网络无法满足他们对视频加速的需求，于是 YouTube 的工程师们自己动手写了一专门针对自己业务的 CDN 加速技术；再例如，淘宝发现市面上已经没有一款分布式文件系统能够满足他们对小文件存储的需求，于是动手写了一个 TFS。</p>

<h1 id="toc_9"><a></a>5、负载均衡技术的其他运用</h1>

<p>在这个系列的文章中，我们全在将用于客户端使用 HTTP 协议请求服务器端进行处理的情况，这里的客户端可以使最终用户，也可以是某个一第三方系统。但实际上负载均衡技术在信息处理领域内，不是只有这样的请求响应层才使用，在其它的技术领域也大量使用，这个小节，我们就来梳理这些技术，作为一个扩展话题。</p>

<h2 id="toc_10"><a></a>5-1、关系型数据库系统的负载均衡</h2>

<p>一说到关系型数据库，大家自然会联想到 Oracle、MS SQL、DB2 和 Mysql。在移动互联网领域，通常很多公司在走去 OEI 的路程。这里我们不去讨论去 OEI 是否是正确的，也不去讨论怎样走去 OEI 这条路才合理，一个不可争辩的事实是，目前很多移动互联网公司在使用 Mysql 数据库。</p>

<p>单台 Mysql 数据库的处理能力确实赶不上 Oracle，甚至赶不上 MS SQL 这些商用数据库，但是我们可以为 Mysql 做集群来提高整个数据服务的性能。Mysql 从 5.1.X 版本开始，就已经支持单数据节点的 “表分区” 功能了，但这个支持仅限于每个节点的配置，提高单个 Mysql 上的读写性能（还要配合底层的块存储选型，例如 DAS）。而想要实现整个 Mysql 集群性能，就需要从更高级别实现读写分离了。</p>

<p>其中有一种成熟的 Mysql 集群读写分离的做法，是一台写节点做成 Master 节点（Master 节点单机性能可以做得较高，后端可以使用 DAS 系统）；然后多台读节点做成 Salve 节点，并接受来源于 Master 节点的同步日志（MySQL Replication 技术），并通过另一个 LVS 进行读请求的负载，而且可以再配合单个节点上的 “表分区” 功能。这个做法在 80% 以上都是读请求的任何系统上，都可以大大增强数据库系统的整体性能，如下图所示：</p>

<p><img src="http://img.blog.csdn.net/20150902154444273" alt=""/></p>

<p>从上图可以看到，来源于程序的 “写” 操作通过一个数据源提交给了 Mysql Master，而所有的读操作则通过 LVS-DR 模式分发给 3 个 Mysql Salve。这里要说明几个问题：</p>

<ul>
<li><p>Mysql Master 和 Mysql Salve 的数据同步是通过 MySQL Replication 同步技术来实现的，这是一种基于操作日志的异步同步，虽然响应时间不能达到 “毫秒” 级，但是基本上还是很快很快的。如果不是银行系统、或者 “秒杀系统” 基本上可以满足事实性</p></li>
<li><p>MySQL Replication 会降低 Mysql Master 节点的 20% 的工作性能，但是转移了原来 Mysql Master 负责的所有读操作。当然，我们以后介绍 “多主” 方式和使用 HiveDB 横向切分的时候，还会重点介绍如何提高 Mysql 的写性能。</p></li>
<li><p>事实上正式的开发架构中，我们不会给程序员两个数据源，这样既不利于代码的管理，也增加了开发难度。我们会采用类似 Mysql-Proxy、Amoeba 之类的软件实现数据源的整个。</p></li>
<li><p>后面在介绍数据存储层架构的时候，我还会介绍多种成熟的可靠的 Mysql 集群、Mysql 读写分离、Mysql 横向扩展方式，和读者讨论如何实现几十台 Mysql 节点的运行和管理。</p></li>
</ul>

<h2 id="toc_11"><a></a>5-2、分布式存储系统的负载均衡</h2>

<p>分布式存储系统目前有很多，Ceph、Swift、MFS、HDFS。他们有的是基于对象存储的，有的是基于快存储的（在<a href="http://blog.csdn.net/yinwenjie/article/details/46480485%20%E6%A0%87%E5%87%86Web%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%9E%B6%E6%9E%84%E5%88%86%E5%B1%82">《标准 Web 系统的架构分层》</a>这篇博文中，我对块存储、文件存储和对象存储做了较详细的介绍，后文我们还将详细介绍存储系统）。但是他们有一个或者多个主控节点（有的叫 namenode、有的叫 master、有的叫 Metadata），无论怎么叫，他们都有一些相同的功能：</p>

<ul>
<li>  计算 “数据该存储在哪里” 的问题</li>
<li>  协调控制 “数据是否正确存储” 的问题</li>
<li>  监控 “数据节点” 的健康状态</li>
<li>  转移数据</li>
<li>  回答客户端 “到哪里取数据” 的问题</li>
<li>  。。。。。</li>
</ul>

<p>在处理问题的过程中，这些控制节点实际上起到的就是负载分发的作用，他们的基本原理都是通过 “一致性 hash 算法”，对“数据该存储在” 哪里的问题进行分析（用来做 hash 的属性依据不同而已）：</p>

<p><img src="http://img.blog.csdn.net/20150703172857890" alt=""/></p>

<h2 id="toc_12"><a></a>5-3、更广义的负载均衡系统</h2>

<p>相同的客流量下，银行多个窗口排队的等待时间肯定比一个窗口排队的时间短；同样的车流量，8 车道肯定比 6 车道的通过率高；把一个任务拆分成多个任务由多个人负责处理其中的一部分，肯定比一个人做一个大任务的时间短；</p>

<p><strong>负载均衡的核心思想在于分流、关键问题在于如何分流、评价标准在于分流后的吞吐量。</strong></p>

<h1 id="toc_13"><a></a>6、后文介绍</h1>

<p>终于，负载均衡层设计方案算是告一段落了。在这个过程中有很多网友给我提了建议，帮助我进行讲解改进和知识点梳理，在此感谢了。我知道在这几天文章我，我留了很多扣子，无奈本人写作时间有限，能力也不高，所以待到后面的空余时间，我们再进行相关话题的整理。</p>

<p>从下篇文章开始，我们将开始介绍 “业务系统间通信” 的相关技术、原理和方案，当然也会形成一个系列博文。欢迎各位继续关注我的博客。</p>

<p>明天就是 70 周年胜利日了，祝我大中华！</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[架构设计：负载均衡层设计方案（8）——负载均衡层总结上篇]]></title>
    <link href="http://panlw.github.io/15301813109409.html"/>
    <updated>2018-06-28T18:21:50+08:00</updated>
    <id>http://panlw.github.io/15301813109409.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://blog.csdn.net/yinwenjie/article/details/47211641">https://blog.csdn.net/yinwenjie/article/details/47211641</a></p>
</blockquote>

<h1 id="toc_0"><a></a>1、概述</h1>

<p>很明显通过前面的八篇文章的介绍，并不能覆盖负载均衡层的所有技术，但是可以作为一个引子，告诉各位读者一个学习和使用负载均衡技术的思路。虽然后面我们将转向 “业务层” 和“业务通信”层的介绍，但是对负载均衡层的介绍也不会停止。在后续的时间我们将穿插进行负载均衡层的新文章的发布，包括 Nginx 技术的再介绍、HaProxy、LVS 新的使用场景等等。</p>

<p>这篇文章我们对前面的知识点进行总结，并有意进行一些扩展，以便于各位读者找到新的学习思路。</p>

<h1 id="toc_1"><a></a>2、负载均衡层的核心思想</h1>

<h2 id="toc_2"><a></a>2-1、一致性哈希与 Key 的选取</h2>

<p><img src="https://img-blog.csdn.net/20150703172857890" alt=""/></p>

<p>在<a href="http://blog.csdn.net/yinwenjie/article/details/46620711%20%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%EF%BC%9A%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%B1%82%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88%EF%BC%882%EF%BC%89%E2%80%94%E2%80%94Nginx%E5%AE%89%E8%A3%85">《架构设计：负载均衡层设计方案（2）——Nginx 安装》</a> 文章中我们详细介绍了一致性哈希算法。并且强调了一致性 Hash 算法是现代系统架构中的最关键算法之一，在分布式计算系统、分布式存储系统、数据分析等众多领域中广泛应用。针对我的博文，在负载均衡层、业务通信层、数据存储层都会有它的身影。</p>

<p>一致性算法的核心是：</p>

<ul>
<li>  使用对象的某一个属性（这个属性可以是服务器的 IP 地址、开放端口 还可以是用户名、某种加密串。凡是你可以想到的有散列意义的属性），算出一个整数，让其分布在 0 至 2 的 32 次方 范围内。</li>
<li>  一台服务器的某个或者某一些属性当然也可以进行 hash 计算，并且根据计算分布在这个圆环上的某一个点，也就是图中圆环上的蓝色点。</li>
<li>  一个处理请求到来时，根据这个请求的某一个或者某一些属性进行 hash 计算，并且根据计算记过分布在这个圆环上的某一个点上。也就是上图圆环上的黄色点。</li>
<li>  我们约定落在某一个蓝点 A 左侧和蓝点 B 右侧的黄色点所代表的请求，都有蓝点 A 所代表的服务器进行处理，这样就完成解决了 “谁来处理” 的问题。在蓝色点稳定存在的前提下，来自于同一个 Hash 约定的请求所落在的位置都是一样的，这就保证了服务处理映射的稳定性。</li>
<li>  当某一个蓝色点由于某种原因下线，其所影响到的黄色点也是有限的。即下一次客户端的请求将由其他的蓝色点所代表的服务器进行处理。</li>
</ul>

<h2 id="toc_3"><a></a>2-2、轮询与权</h2>

<p><img src="https://img-blog.csdn.net/20150705093244323" alt=""/></p>

<ul>
<li><p>不加权轮询，就是主控节点（任务来源点）在<strong>不考虑目标节点的任何因素的情况下</strong>（例如 CPU 性能、磁盘性能、网络性能），按照目标节点的列表顺序将任务依次分配下去。这是最简单的轮询，也是对主控节点实现复杂性要求最低的轮询。我之前的博文<a href="http://blog.csdn.net/yinwenjie/article/details/46620711%20%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%EF%BC%9A%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%B1%82%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88%EF%BC%882%EF%BC%89%E2%80%94%E2%80%94Nginx%E5%AE%89%E8%A3%85">《架构设计：负载均衡层设计方案（2）——Nginx 安装》</a>、<a href="http://blog.csdn.net/yinwenjie/article/details/46845997%20%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%EF%BC%9A%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%B1%82%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88%EF%BC%884%EF%BC%89%E2%80%94%E2%80%94LVS%E5%8E%9F%E7%90%86">《架构设计：负载均衡层设计方案（4）——LVS 原理》</a> 都对这种最简轮询进行了介绍：例如 LVS 中的 “rr” 参数。</p></li>
<li><p>加权轮询中的 “权”，您可以看成是“轮询” 依据的意思。“权”可以是很多种可能，可以是目标机器的性能量化值、可以是一个固定的数字（按照固定数字加权）、可以是目标节点的网络速度。例如 LVS 中的 “lc” 参数，就是指按照目标机器，现在已有的 “连接” 数量进行加权：连接数量越少，越有更大的几率获得这个任务的处理权。</p></li>
</ul>

<h2 id="toc_4"><a></a>2-3、租约与健康检查</h2>

<p><img src="https://img-blog.csdn.net/20150830090139148" alt=""/></p>

<p>租约协议主要为了保证一个事实：如果服务器对客户端的检查操作在 “最迟时间” 失败后，那么服务器端肯定会注销客户端的登录信息，同时客户端上服务器的连接信息也会消失（并且不在向下提供服务）。每一次检查成功，这个 “最迟时间” 都会向后推移。</p>

<p>租约协议和我们提到的哈希算法一下一样，也是系统架构设计中最基本的设计思想，并且大量运用在各类型的系统中，它的工作原理是每一位架构师都需要掌握的。例如：zookeeper 使用这个协议保证 Flow 节点和 Leader 节点的链路是正常的；分布式存储系统用这个协议保证 datanode 和 namenode 的连接是正常的；</p>

<h1 id="toc_5"><a></a>3、负载均衡层技术汇总</h1>

<p>在前面的博文中，我重点介绍了 Nginx、LVS、Keepalived 技术。由于时间有限，这里我们对博文中提到的几种技术进行一个总结，然后再扩展介绍一下 DNS 技术、CDN 技术和硬件负载技术。</p>

<h2 id="toc_6"><a></a>3-1、Nginx 技术</h2>

<p>在负载均衡层这个大的章节中，我有三篇文章都在直接介绍 Nginx 的原理和使用。但是之后有朋友给我反映还想了解更多的 Nginx 知识，特别点名要求我再做一篇文章介绍 Nginx 的动态缓存。是的，我在后面的时间里是有计划介绍 Nginx 的动态缓存技术，还会介绍 Nginx 和多款主流的反向代理软件的性能对比。但这需要时间，特别是我不想去网上找一些已有的性能对比图，还是自己一边做这样的性能测试，一边做性能报告比较靠谱。</p>

<p>下面这些技术是我在博文中已经重点介绍过得，我们再做一下总结：</p>

<ul>
<li>  <strong>Nginx 中的连接数限制问题</strong></li>
</ul>

<p>重要的配置项包括：worker_processes、worker_connections。但是光是配置这些属性是不够的，最关键的是我们要打开操作系统级别的 “最大文件数” 限制问题。使用 “ulimit -n 65535” 设置本次会话的 “最大文件数” 限制；还要使用 “vim /etc/security/limits.conf” 命令，修改内核的配置信息。主要是以下两项：</p>

<pre><code>* soft nofile 65535 
* hard nofile 65535
</code></pre>

<p>另外，还要注意和 nginx 配置项中的 “worker_rlimit_nofile” 属性共同使用：</p>

<pre><code>user root root; 
worker_processes 4; 
worker_rlimit_nofile 65535;

#error_log logs/error.log; 
#error_log logs/error.log notice; 
#error_log logs/error.log info;

#pid logs/nginx.pid; 
events { 
    use epoll; 
    worker_connections 65535; 
}
</code></pre>

<ul>
<li>  <strong>Nginx 中的 Gzip 技术</strong></li>
</ul>

<p>gzip 是 Nginx 进行 HTTP Body 数据压缩的技术。下面这段 Nginx 配置信息是启用 gzip 压缩的实例：</p>

<pre><code>#开启gzip压缩服务， 
gzip on;

#gzip压缩是要申请临时内存空间的，假设前提是压缩后大小是小于等于压缩前的。例如，如果原始文件大小为10K，那么它超过了8K，所以分配的内存是8 * 2 = 16K;再例如，原始文件大小为18K，很明显16K也是不够的，那么按照 8 * 2 * 2 = 32K的大小申请内存。如果没有设置，默认值是申请跟原始数据相同大小的内存空间去存储gzip压缩结果。 
gzip_buffers 2 8k;

#进行压缩的原始文件的最小大小值，也就是说如果原始文件小于5K，那么就不会进行压缩了 
gzip_min_length 5K;

#gzip压缩基于的http协议版本，默认就是HTTP 1.1 
gzip_http_version 1.1;

# gzip压缩级别1-9，级别越高压缩率越大，压缩时间也就越长CPU越高 
gzip_comp_level 5;

#需要进行gzip压缩的Content-Type的Header的类型。建议js、text、css、xml、json都要进行压缩；图片就没必要了，gif、jpge文件已经压缩得很好了，就算再压，效果也不好，而且还耗费cpu。 
gzip_types text/HTML text/plain application/x-javascript text/css application/xml;
</code></pre>

<p>http 返回数据进行压缩的功能在很多场景下都实用：</p>

<p>a、 如果浏览器使用的是 3G/4G 网络，那么流量对于用户来说就是 money。</p>

<p>b、 压缩可节约服务器机房的对外带宽，为更多用户服务。按照目前的市场价良好的机房带宽资源的一般在 200RMB/Mbps，而服务器方案的压力往往也来自于机房带宽。</p>

<p>c、 不是 Nginx 开启了 gzip 功能，HTTP 响应的数据就一定会被压缩，除了满足 Nginx 设置的 “需要压缩的 http 格式” 以外，客户端（浏览器）也需要支持 gzip（不然它怎么解压呢），一个好消息是，目前大多数浏览器和 API 都支持 http 压缩。</p>

<ul>
<li>  <strong>Nginx 中的 rewrite（重写）技术</strong></li>
</ul>

<p>Nginx 的强大在于其对 URL 请求的重写（重定位）。Nginx 的 rewrite 功能依赖于 PCRE Lib，请一定在 Nginx 编译安装时，安装 Pcre lib。</p>

<p>Nginx 的 rewrite 功能在我<a href="http://blog.csdn.net/yinwenjie/article/details/46742661#t2%20%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%EF%BC%9A%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%B1%82%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88%EF%BC%883%EF%BC%89%E2%80%94%E2%80%94Nginx%E8%BF%9B%E9%98%B6">《架构设计：负载均衡层设计方案（3）——Nginx 进阶》</a> 这边博客中进行了讲解。</p>

<p>下面是一段 rewrite 的示例：</p>

<pre><code>#示例1：
location ~* ^/(.+)/(.+)\.(jpg|gif|png|jpeg)$ {
    rewrite ^/orderinfo/(.+)\.(jpg|gif|png|jpeg)$   /img/$1.$2   break;
    root   /cephclient;
}

#location在不进行大小写区分的情况下利用正则表达式对$url进行匹配。当匹配成功后进行rewrite重定位。
#rewrite进行重写url的规则是：regex表达式第一个括号中的内容对应$1，regex表达式第二个括号中的内容对应$2，以此类推。
#这样重定位的意义就很明确了：将任何目录下的文件名重定位到img目录下的对应文件名，
#并且马上在这个location中（注意是Nginx，而不是客户端）执行这个重写后的URL定位。

#示例2：
server {
    。。。。
    。。。。
    location ~* ^/orderinfo/(.+)\.(jpg|gif|png|jpeg)$ {
        rewrite ^/orderinfo/(.+)\.(.+)$  /img/$1.$2   last;
    }

    location / {
        root   /cephclient;
    }
}

#在server中，有两个location位置，当url需要访问orderinfo目录下的某一个图片时，rewrite将重写这个url，
#并且重新带入这个url到server执行，这样“location /”这个location就会执行了，并找到图片存储的目录。
</code></pre>

<ul>
<li>  <strong>Nginx 的图片处理模块</strong></li>
</ul>

<p>http_image_filter_module 是 nginx 的图片处理模块，是使用 nginx 进行静态资源和动态资源分开管理的关键引用技术。通过这个模块可以对静态资源进行缩放、旋转、验证。</p>

<p>需要注意的是，http_image_filter_module 模块所处理的缩率图片是不进行保存的，完全使用节点的 CPU 性能进行计算，使用节点的内存进行临时存储。<strong>所以如果要使用 http_image_filter_module 进行图片处理，一定要根据客户端的请求规模进行 nginx 节点的调整。并且当站点的 PV 达到一定的规模时，一定要使用 CDN 技术进行访问加速、对图片的访问处理手段进行规划。</strong></p>

<p>由于我们在之前涉及 Nginx 的文章中，并没有详细讲解 Nginx 的图片处理模块，只是说了要进行介绍，所以这里我给出一个较为详细的安装和配置示例：</p>

<p>nginx 的 http_image_filter_module 模块由 GD library 进行支持，所以要使用这个图片处理模块，就必须进行第三方依赖包的安装：</p>

<pre><code>yum install gd-devel
</code></pre>

<p>然后，Nginx 要进行重新编译：</p>

<pre><code>configure --with-http_image_filter_module
make &amp;&amp; make install
</code></pre>

<p>使用图片处理模块的配置示例：</p>

<pre><code>location ~* /(.+)_(\d+)_(\d+)\.(jpg|gif|png|ioc|jpeg)$ {
    set $h $3;
    set $w $2;
    rewrite /(.+)_(\d+)_(\d+)\.(jpg|gif|png|ioc|jpeg)$ /$1.$4 break;

    image_filter resize $w $h;
    image_filter_buffer 2M;
}
</code></pre>

<p>其中关于正则表达式的语法和已经介绍过的 rewrite 的语法就不再进行介绍了，主要看 http_image_filter_module 相关的属性设置：</p>

<p>image_filter test：测试图片文件合法性<br/>
image_filter rotate：进行图片旋转，只能按照 90 | 180 | 270 进行旋转<br/>
image_filter size：返回图片的 JSON 数据<br/>
image_filter resize width height：按比例进行图片的等比例缩小，注意，是只能缩小，第二缩小是等比例的。<br/>
image_filter_buffer：限制图片最大读取大小，没有设置就是 1M；根据不同的系统最好设置为 2M—3M<br/>
image_filter_jpeg_quality：设置 jpeg 图片的压缩比例（1-99，越高越好）<br/>
image_filter_transparency：禁用 gif 和 png 图片的透明度。</p>

<ul>
<li>  <strong>和 Nginx 类似的其他技术 / 软件</strong></li>
</ul>

<p>目前行业内也有很多与 Nginx 解决同类问题的软件，他们分别是 Apache 基金会的 Apache HTTP Server、淘宝开源的 Tengine、Haproxy、包括 Windows 下运行的 IIS，也支持反向代理 。</p>

<p>这里笔者再次重点提到 Tengine，建议各位读者有时间的时候可以使用一下，这个对 Nginx 进行了深度再开发的软件。</p>

<h2 id="toc_7"><a></a>3-2、LVS 技术</h2>

<p>LVS 是 Linux Virtual Server 的简写，意即 Linux 虚拟服务器，是一个虚拟的服务器集群系统。本项目在 1998 年 5 月由章文嵩博士成立。</p>

<p>LVS 集群采用 IP 负载均衡技术和基于内容请求分发技术。调度器具有很好的吞吐率，将请求均衡地转移到不同的服务器上执行，且调度器自动屏蔽掉服务器的故障，从而将一组服务器构成一个高性能的、高可用的虚拟服务器。整个服务器集群的结构对客户是透明的，而且无需修改客户端和服务器端的程序。</p>

<p>在我的系列文章中，<a href="http://blog.csdn.net/yinwenjie/article/details/46845997%20%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%EF%BC%9A%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%B1%82%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88%EF%BC%884%EF%BC%89%E2%80%94%E2%80%94LVS%E5%8E%9F%E7%90%86">《架构设计：负载均衡层设计方案（4）——LVS 原理》</a> 、<a href="http://blog.csdn.net/yinwenjie/article/details/47010569%20%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%EF%BC%9A%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%B1%82%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88%EF%BC%885%EF%BC%89%E2%80%94%E2%80%94LVS%E5%8D%95%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85">《架构设计：负载均衡层设计方案（5）——LVS 单节点安装》</a> 、<a href="http://blog.csdn.net/yinwenjie/article/details/47211551%20%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%B1%82%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88%EF%BC%887%EF%BC%89%E2%80%94%E2%80%94LVS%20+%20Keepalived%20+%20Nginx%E5%AE%89%E8%A3%85%E5%8F%8A%E9%85%8D%E7%BD%AE">《负载均衡层设计方案（7）——LVS + Keepalived + Nginx 安装及配置》</a> 都涉及到 LVS 的讲解。</p>

<p>这里我们再总结一下 LVS 中的三种工作模式：</p>

<h3 id="toc_8"><a></a>3-2-1、NAT 模式</h3>

<p>NAT 方式是一种由 LVS Master 服务节点收到数据报，然后转给下层的 Real Server 节点，当 Real Server 处理完成后回发给 LVS Master 节点然后又由 LVS Master 节点转发出去的工作方式。LVS 的管理程序 IPVSADMIN 负责绑定转发规则，并完成 IP 数据报文和 TCP 数据报文中属性的重写。</p>

<p><img src="https://img-blog.csdn.net/20150716170216002" alt=""/></p>

<p>LVS-NAT 模式的优点在于：</p>

<ul>
<li><p>配置管理简单。LVS-NAT 的工作方式是 LVS 三种工作模式中最容易理解、最容易配置、最容易管理的工作模式。</p></li>
<li><p>节省外网 IP 资源，一般机房分配给使用者的 IP 数量是有限的，特别是您购买的机架的数量不多时。LVS-NAT 工作方式将您的系统架构封装在局域网中，只需要 LVS 有一个外网地址或外网地址映射就可以实现访问了。</p></li>
<li><p>系统架构相对封闭。在内网环境下我们对防火墙的设置要求不会很高，也相对容易进行物理服务器的运维。您可以设置来源于外网的请求需要进行防火墙过滤，而对内网请求开放访问。</p></li>
<li><p>另外改写后转给 Real Server 的数据报文，Real Server 并不会关心它的真实性，只要 TCP 校验和 IP 校验都能通过，Real Server 就可以进行处理。所以 LVS-NAT 工作模式下 Real Server 可以是任何操作系统，只要它支持 TCP/IP 协议即可。</p></li>
</ul>

<h3 id="toc_9"><a></a>3-2-2、DR 模式</h3>

<p>LVS 的 DR 工作模式，是目前生产环境中最常用的一种工作模式，网上的资料也是最多的，有的文章对 DR 工作模式的讲解还是比较透彻的：</p>

<p><img src="https://img-blog.csdn.net/20150716210428765" alt=""/></p>

<p>LVS-DR 模式的优点在于：</p>

<ul>
<li><p>解决了 LVS-NAT 工作模式中的转发瓶颈问题，能够支撑规模更大的负载均衡场景。</p></li>
<li><p>比较耗费网外 IP 资源，机房的外网 IP 资源都是有限的，如果在正式生产环境中确实存在这个问题，可以采用 LVS-NAT 和 LVS-DR 混合使用的方式来缓解。</p></li>
</ul>

<p>LVS-DR 当然也有缺点：</p>

<ul>
<li><p>配置工作较 LVS-NAT 方式稍微麻烦一点，您至少需要了解 LVS-DR 模式的基本工作方式才能更好的指导自己进行 LVS-DR 模式的配置和运行过程中问题的解决。</p></li>
<li><p>由于 LVS-DR 模式的报文改写规则，导致 LVS 节点和 Real Server 节点必须在一个网段，因为二层交换是没法跨子网的。但是这个问题针对大多数系统架构方案来说，实际上并没有本质限制。</p></li>
</ul>

<h3 id="toc_10"><a></a>3-2-3、TUN 模式</h3>

<p>LVS-DR 模式和 LVS-TUN 模式的工作原理完全不一样，工作场景完全不一样。DR 基于数据报文重写，TUN 模式基于 IP 隧道，后者是对数据报文的重新封装：</p>

<p><img src="https://img-blog.csdn.net/20150717160306570" alt=""/></p>

<p>IPIP 隧道。将一个完整的 IP 报文封装成另一个新的 IP 报文的数据部分，并通过路由器传送到指定的地点。在这个过程中路由器并不在意被封装的原始协议的内容。到达目的地点后，由目的地方依靠自己的计算能力和对 IPIP 隧道协议的支持，打开封装协议，取得原始协议：</p>

<p><img src="https://img-blog.csdn.net/20150717135916686" alt=""/></p>

<p>可以说 LVS-TUN 方式基本上具有 LVS-DR 的优点。在此基础上又支持跨子网间穿透。</p>

<h2 id="toc_11"><a></a>3-3、CDN 技术</h2>

<p>CDN 技术 Content Delivery Network：内容分发网络。为什么有时我们访问互联网上的视频资源、图片资源会比较慢，甚至访问失败。其中有一个重要的原因，是资源的物理位置离客户端太远了，可能其中有 4 层 NAT 设备（相当于使用网通的线路访问电信服务器上的资源）。</p>

<p>我们试想一下，如果将我们要访问的资源放到离我们客户端最近的一个服务上（例如在广州的客户端访问的资源就在广州的机房）。那么是不是就解决了这个问题（这个点称为 “边缘节点”）。这就是 CDN 网络解决的问题，如下图所示：</p>

<p><img src="https://img-blog.csdn.net/20150829132418544" alt=""/></p>

<p>目前 CDN 服务不需要我们进行开发，市面上有很多公司都提供免费的 / 付费的 CDN 服务（请直接在 google 或者百度上面输入：CDN，就会有很多 “推广” 信息了，在我的博文中不打广告 <sup>_<sup>）。当然如果您想自行搭建</sup></sup> CDN 网络，可以参考以下技术方案：</p>

<p>Squid：Squid 是一个缓存 internet 数据的一个软件，它接收用户的下载申请，并自动处理所下载的数据。目前，国内很多 CDN 服务商的网络都是基于 Squid 搭建的</p>

<p>利用 Nginx 的 proxy_cache 搭建：Nginx 中的 rewrite 技术实际上就可以实现 URL 请求重写，实现请求转发。而 Nginx 中的 proxy_cache 组件可以使得从远端请求的源数据保存在本地，从而实现一个 CDN 网络的搭建。</p>

<p>自己写：CDN 网络没有特别复杂的技术门槛，如果您有特别的需求，可以自己写一个。当然上图中所介绍的 CDN 网络属于<strong>第一代 CDN 网络</strong>，将<strong>第二代 / 第三代 P2P 技术</strong>加入到 CDN 原理中，可以形成<strong>第二代 CDN 网络</strong>：如下图所示：</p>

<p><img src="https://img-blog.csdn.net/20150829134043600" alt=""/></p>

<p>第三代 P2P 技术又被称为混合型 P2P 技术主要是为了解决元数据服务器的处理压力，加速资源的本地化速度。关于 P2P 技术我会在讲完 “业务系统设计”、“业务通信系统设计” 后，专门做一个新的专题进行介绍。另外提一下，YouTube 的 P2P 网络就是自己做的。</p>

<h1 id="toc_12"><a></a>4、后文介绍</h1>

<p>要总结的内容实在太多了，我决定再开一篇文章《架构设计：负载均衡层设计方案（9）——负载均衡层总结下篇》，继续进行负载均衡层技术的总结。我们将总结 Keepalived、DNS 技术、硬件负载，并且向大家介绍更广义的负载均衡技术。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[架构设计：负载均衡层设计方案（7）——LVS + Keepalived + Nginx安装及配置]]></title>
    <link href="http://panlw.github.io/15301812452579.html"/>
    <updated>2018-06-28T18:20:45+08:00</updated>
    <id>http://panlw.github.io/15301812452579.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://blog.csdn.net/yinwenjie/article/details/47211551">https://blog.csdn.net/yinwenjie/article/details/47211551</a></p>
</blockquote>

<h2 id="toc_0"><a></a>1、概述</h2>

<p>上篇文章《架构设计：负载均衡层设计方案（6）——Nginx + Keepalived 构建高可用的负载层》(<a href="http://blog.csdn.net/yinwenjie/article/details/47130609">http://blog.csdn.net/yinwenjie/article/details/47130609</a>) 我们讲解了 Nginx 的故障切换，并且承诺各位读者会尽快讲解 LVS + Keepalived + Nginx 的安装和配置。在中间由于工作的原因，我又插写了三篇关于 zookeeper 的原理使用的文章。今天这边文章我们回归主题，为各位读者讲解 LVS + Keepalived + Nginx 的安装及配置。</p>

<h2 id="toc_1"><a></a>2、安装计划和准备工作</h2>

<p>下图，我们表示了本篇文章要搭建的整个集成架构的抽象结构：</p>

<p><img src="https://img-blog.csdn.net/20150822113121644" alt=""/></p>

<p>我们采用两个 LVS 节点（141 和 142），但是一个时间工作的只有一个 LVS 节点，另一个始终处于热备 standby 状态，由 keepalived 监控这两个节点的工作状态并完成切换。</p>

<p>在 LVS 节点下，我们采用 LVS-DR 工作模式挂载了两个 Nginx 节点（131、132）。并最终将外网请求交由这两个节点进行处理。<strong>注意：在实际工作中，Nginx 下面一般就是访问静态资源、动态资源的配置了。</strong></p>

<h2 id="toc_2"><a></a>2-1、准备两个 keepalived 节点</h2>

<p>首先我们在将要安装 LVS 的两个节点上，先安装 keepalived，并保证这两个 keepalived 节点能够正常工作（监控批次的状态）。当然，您也可以先准备 LVS，在准备 keepalived。</p>

<p>我想准备 keepalived 节点，大家应该轻车熟路了吧，在<a href="http://blog.csdn.net/yinwenjie/article/details/47130609#t5%20%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%EF%BC%9A%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%B1%82%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88%EF%BC%886%EF%BC%89%E2%80%94%E2%80%94Nginx%20+%20Keepalived%E6%9E%84%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84%E8%B4%9F%E8%BD%BD%E5%B1%82">《架构设计：负载均衡层设计方案（6）——Nginx + Keepalived 构建高可用的负载层》</a>这篇文章中详细介绍了 keepalived 的最简配置方式。为了大家阅读方便，我们在这里再进行依次简要说明。准备 keepalived 的整个过程包括：</p>

<ol>
<li> 安装必要的支撑组件，源码安装 keepalived</li>
<li> 将 keepalived 注册成节点的服务，以便保证 keepalived 在节点启动时就开始工作</li>
<li> 更改 keepalived 的配置文件，让其可以正常工作</li>
<li> 验证准备工作</li>
</ol>

<p>============= 安装 keepalived</p>

<pre><code>[root@lvs1 ~]# yum install -y zlib zlib-devel gcc gcc-c++ openssl openssl-devel openssh
[root@lvs1 ~]# tar -zxvf keepalived-1.2.17.tar.gz
[root@lvs1 ~]# cd keepalived-1.2.17
[root@lvs1 ~]# ./configure --perfix=/usr/keepalived-1.2.17
[root@lvs1 ~]# make &amp; make install 
</code></pre>

<p>============= 将 keepalived 注册成服务（如果您使用的默认路径安装，就不需要 cp 命令了）</p>

<pre><code>[root@lvs1 ~]# cp /usr/keepalived-1.2.17/etc/sysconfig/keepalived  /etc/sysconfig/keepalived 
[root@lvs1 ~]# cp /usr/keepalived-1.2.17/sbin/keepalived /usr/sbin/keepalived
[root@lvs1 ~]# cp /usr/keepalived-1.2.17/etc/rc.d/init.d/keepalived  /etc/rc.d/init.d/keepalived
[root@lvs1 ~]# mkdir /etc/keepalived
[root@lvs1 ~]# cp /usr/keepalived-1.2.17/etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf
[root@lvs1 ~]# 可以做成服务了（不要拷贝，没用的）
[root@lvs1 ~]# chkconfig keepalived on
</code></pre>

<p>做成服务后，先不要急着启动，因为配置文件还没有改好。<br/>
======== 配置 keepalived（配置文件在：/etc/keepalived/keepalived.conf）</p>

<pre><code>! Configuration File for keepalived

global_defs {
   #notification_email {
   #  acassen@firewall.loc
   #  failover@firewall.loc
   #  sysadmin@firewall.loc
   #}
   #notification_email_from Alexandre.Cassen@firewall.loc
   #smtp_server 192.168.200.1
   #smtp_connect_timeout 30
   router_id LVS_DEVEL
}

vrrp_instance VI_1 {
    #141节点设置为MASTER，142或者还有其他的节点设置为BACKUP
    #还记得我们前面文章讲到的无抢占设置吗？这里也可以用哦。
    state MASTER
    #网络适配器名称
    interface eth0
    virtual_router_id 51
    #所有的SLAVE节点的优先级都要比这个设置值低
    priority 120
    advert_int 1
    #真实ip，142要改成相应的lvs节点真实ip
    mcast_src_ip=192.168.220.141
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    #虚拟/浮动IP
    virtual_ipaddress {
        192.168.220.140
    }
}
</code></pre>

<p>以上配置还是最简单的 keepalived 配置，因为我们还没有加上配合 LVS 使用的虚拟 ip 监测设置和下层真实 ip 监测的设置。最简配置主要是为了保证 keepalived 节点是工作正常的。</p>

<p><strong>将以上的配置分别对应到 LVS 的两个节点（注意要改动的地方哦）</strong></p>

<p>========== 进行 keepalived 工作状态的检查：</p>

<pre><code>[root@lvs1 ~]# /etc/init.d/keepalived start
</code></pre>

<p>现在设置为 MASTER 的 keepalived 节点（或者在非抢占模式下，优先级最高的那个节点），已经绑定了 140 这个虚拟 ip 了：</p>

<pre><code>[root@lvs2 ~]# ip addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 16436 qdisc noqueue state UNKNOWN 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 00:0c:29:39:75:9f brd ff:ff:ff:ff:ff:ff
    inet 192.168.220.141/24 brd 192.168.220.255 scope global eth0
    inet 192.168.220.140/32 scope global eth0
    inet6 fe80::20c:29ff:fe39:759f/64 scope link 
       valid_lft forever preferred_lft forever
</code></pre>

<p>当然您也可以通过 /var/log/message 的日志进行 keepalived 是否正常工作的验证。</p>

<h2 id="toc_3"><a></a>2-2、继续两个 keepalived 节点上准备 LVS</h2>

<p>准备 lvs 的工作就太简单了，因为 centos6.4、6.5、6.6 都已经集成了 LVS 的核心，我们只需要安装 LVS 的管理工具就行了：</p>

<p>两个 LVS 节点都执行：</p>

<pre><code>yum -y install ipvsadm
</code></pre>

<p>还记得 lvs 管理工具怎么使用吗？请参见我介绍 LVS 单节点安装的博文：<a href="http://blog.csdn.net/yinwenjie/article/details/47010569%20%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%EF%BC%9A%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%B1%82%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88%EF%BC%885%EF%BC%89%E2%80%94%E2%80%94LVS%E5%8D%95%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85">《架构设计：负载均衡层设计方案（5）——LVS 单节点安装》</a>。这里为了方便阅读，给出主要参数的含义列表：</p>

<pre><code>-A –add-service 在内核的虚拟服务器表中添加一条新的虚拟服务器记录。也就是增加一台新的虚拟服务器。
-E –edit-service 编辑内核虚拟服务器表中的一条虚拟服务器记录。
-D –delete-service 删除内核虚拟服务器表中的一条虚拟服务器记录。
-C –clear 清除内核虚拟服务器表中的所有记录。
-R –restore 恢复虚拟服务器规则
-S –save 保存虚拟服务器规则，输出为-R 选项可读的格式
-a –add-server 在内核虚拟服务器表的一条记录里添加一条新的真实服务器记录。也就是在一个虚拟服务器中增加一台新的真实服务器
-e –edit-server 编辑一条虚拟服务器记录中的某条真实服务器记录
-d –delete-server 删除一条虚拟服务器记录中的某条真实服务器记录
-L –list 显示内核虚拟服务器表
-Z –zero 虚拟服务表计数器清零（清空当前的连接数量等）
–set tcp tcpfin udp 设置连接超时值
–start-daemon 启动同步守护进程。他后面可以是master 或backup，用来说明LVS Router 是master 或是backup。在这个功能上也可以采用keepalived 的VRRP 功能。
–stop-daemon 停止同步守护进程
-t –tcp-service service-address 说明虚拟服务器提供的是tcp 的服务[vip:port] or [real-server-ip:port]
-u –udp-service service-address 说明虚拟服务器提供的是udp 的服务[vip:port] or [real-server-ip:port]
-f –fwmark-service fwmark 说明是经过iptables 标记过的服务类型。
-s –scheduler scheduler 使用的调度算法，选项：rr|wrr|lc|wlc|lblc|lblcr|dh|sh|sed|nq, 默认的调度算法是： wlc.
-p –persistent [timeout] 持久稳固的服务。这个选项的意思是来自同一个客户的多次请求，将被同一台真实的服务器处理。timeout 的默认值为300 秒。
-M –netmask netmask persistent granularity mask
-r –real-server server-address 真实的服务器[Real-Server:port]
-g –gatewaying 指定LVS 的工作模式为直接路由模式DR模式（也是LVS默认的模式）
-i –ipip 指定LVS 的工作模式为隧道模式
-m –masquerading 指定LVS 的工作模式为NAT 模式
-w –weight weight 真实服务器的权值
–mcast-interface interface 指定组播的同步接口
–connection 显示LVS 目前的连接 如：ipvsadm -L -c
–timeout 显示tcp tcpfin udp 的timeout 值 如：ipvsadm -L –timeout
–daemon 显示同步守护进程状态
–stats 显示统计信息
–rate 显示速率信息
–sort 对虚拟服务器和真实服务器排序输出
–numeric -n 输出IP 地址和端口的数字形式
</code></pre>

<p>到后面正式启动 LVS 的时候，就不要问我参数含义咯。<sup>_^</sup></p>

<h2 id="toc_4"><a></a>2-3、准备两个 Nginx 节点并保证可用</h2>

<p>在《架构设计：负载均衡层设计方案（5）——LVS 单节点安装》(<a href="http://blog.csdn.net/yinwenjie/article/details/47010569">http://blog.csdn.net/yinwenjie/article/details/47010569</a>) 这篇文章中，我们详细讲解了 Nginx 节点的准备工作，但是为了方便各位读者阅读，这里我们大致再讲一下。</p>

<p>Nginx 节点的准备工作主要由以下步骤构成（这个不是本文的重点，点到即可）：</p>

<ol>
<li> 安装 Nginx（当然，正式系统中，还涉及到 Nginx 的参数调优，可以参见<a href="http://blog.csdn.net/yinwenjie/article/details/46620711#t7%20%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%EF%BC%9A%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%B1%82%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88%EF%BC%882%EF%BC%89%E2%80%94%E2%80%94Nginx%E5%AE%89%E8%A3%85">《架构设计：负载均衡层设计方案（2）——Nginx 安装》这篇文章</a>）</li>
<li> 打开 Nginx 所在服务器的 “路由” 功能、关闭 “ARP 查询” 功能</li>
<li> 将 VIP：192.168.220.140 设置成 Nginx 所在节点的回环 IP</li>
</ol>

<p>============= 安装 Nginx</p>

<pre><code>[root@vm1 ~]# yum -y install make zlib zlib-devel gcc gcc-c++ ssh libtool
[root@vm1 ~]# 下载nginx（别拷贝，不能执行的）
[root@vm1 ~]# 解压nginx（别拷贝，不能执行的）
[root@vm1 ~]# ./configure –prefix=/usr/nginx-1.8.0 
[root@vm1 ~]# make &amp;&amp; make install 
[root@vm1 ~]# 设置环境变量（别拷贝，不能执行的）
[root@vm1 ~]# 启动nginx
</code></pre>

<p>============= 打开 Nginx 所在服务器的 “路由” 功能、关闭 “ARP 查询” 功能</p>

<pre><code>[root@vm1 ~]# echo &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore
[root@vm1 ~]# echo &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_announce
[root@vm1 ~]# echo &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_ignore
[root@vm1 ~]# echo &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_announce
</code></pre>

<p>============= 设置回环 IP</p>

<pre><code>[root@vm1 ~]# ifconfig lo:0 192.168.220.140 broadcast 192.168.220.140 netmask 255.255.255.255 up
[root@vm1 ~]# route add -host 192.168.220.140 dev lo:0
</code></pre>

<p>两台 Nginx 的节点都按照这样的方法去设置。然后使用浏览器，看看这两个节点的 Nginx 是否工作正常：</p>

<p><img src="https://img-blog.csdn.net/20150822115708460" alt=""/></p>

<h1 id="toc_5"></h1>

<p><img src="https://img-blog.csdn.net/20150822115814724" alt=""/></p>

<h2 id="toc_6"><a></a>2-4、其他说明</h2>

<p>keepalived 和 LVS 是天生配合完美的一对，<strong>LVS 负责进行请求转发不负责任何节点的健康监测；keepalived 负责监控整个环境中，包括虚拟 ip，真实 ip 对应的下层节点的健康状态监测</strong>。</p>

<h2 id="toc_7"><a></a>3、开始配置：LVS-DR 工作模式</h2>

<h2 id="toc_8"><a></a>3-1、keepalived 的更改——健康监测</h2>

<p>首先我们要更改之前配置的 “最简 keepalived” 配置，让 keepalived 能够对结构中所有节点和虚拟 ip 的健康状态进行监测。更改配置和含义如下：</p>

<pre><code>! Configuration File for keepalived

global_defs {
   #notification_email {
   #  acassen@firewall.loc
   #  failover@firewall.loc
   #  sysadmin@firewall.loc
   #}
   #notification_email_from Alexandre.Cassen@firewall.loc
   #smtp_server 192.168.200.1
   #smtp_connect_timeout 30
   router_id LVS_DEVEL
}

vrrp_instance VI_1 {
    #141节点设置为MASTER，142或者还有其他的节点设置为BACKUP
    #还记得我们前面文章讲到的无抢占设置吗？这里也可以用哦。
    state MASTER
    #网络适配器名称
    interface eth0
    virtual_router_id 51
    #所有的SLAVE节点的优先级都要比这个设置值低
    priority 120
    advert_int 1
    #真实ip，142要改成相应的lvs节点真实ip
    mcast_src_ip=192.168.220.141
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    #虚拟/浮动IP
    virtual_ipaddress {
        192.168.220.140
    }
}

virtual_server 192.168.220.140 80 {
    #健康时间检查，单位秒
    delay_loop 6
    #负载均衡调度算法wlc|rr，和您将使用的LVS的调度算法保持原则一致
    lb_algo rr
    #负载均衡转发规则 DR NAT TUN。和您将启动的LVS的工作模式设置一致
    lb_kind DR
    #虚拟地址的子网掩码
    nat_mask 255.255.255.0
    #会话保持时间，因为我们经常使用的是无状态的集群架构，所以这个设置可有可无
    #persistence_timeout 50
    #转发协议，当然是TCP
    protocol TCP

    #真实的下层Nginx节点的健康监测
    real_server 192.168.220.131 80 {
        #节点权重，
        weight 10
        #设置检查方式，可以设置HTTP_GET | SSL_GET
        HTTP_GET {
            url {
              path /
              digest ff20ad2481f97b1754ef3e12ecd3a9cc
            }
            #超时时间，秒。如果在这个时间内没有返回，则说明一次监测失败
            connect_timeout 3
            #设置多少次监测失败，就认为这个真实节点死掉了
            nb_get_retry 3
            #重试间隔
            delay_before_retry 3
        }
    }

    real_server 192.168.220.132 80 {
        weight 10
        HTTP_GET {
            url {
              path /
              digest 640205b7b0fc66c1ea91c463fac6334d
            }
            connect_timeout 3
            nb_get_retry 3
            delay_before_retry 3
        }
    }
}
</code></pre>

<p>这样一来，keepalived 就可以检查整个架构中的所有节点状态了。<strong>另外要说明的是，这个检查过程并不是必须的，您使用 keepalived 的最简配置也是可以的，不过您就需要自己写监测脚本了（道理是一样的）</strong>。</p>

<h2 id="toc_9"><a></a>3-2、启动两个 LVS 节点</h2>

<p>启动 LVS 的过程就太简单了（两个节点都是一样的启动方式）：</p>

<pre><code>[root@lvs2 ~]# ipvsadm -C
[root@lvs2 ~]# ipvsadm -At 192.168.220.140:80 -s rr 
[root@lvs2 ~]# ipvsadm -at 192.168.220.140:80 -r 192.168.220.131 -g
[root@lvs2 ~]# ipvsadm -at 192.168.220.140:80 -r 192.168.220.132 -g
[root@lvs2 ~]# 然后我们可以使用ipvsadm 监控目前LVS的状态
[root@lvs2 ~]# ipvsadm
</code></pre>

<p><strong>注意：处于 standby 的 lvs1 节点也要这样进行设置。</strong><br/>
<strong>还有，以上的 LVS 的设置，和 real server 上的设置，在重启后都会消失，所以一定要做成脚本哦。</strong></p>

<h2 id="toc_10"><a></a>4、验证工作</h2>

<p>这样 LVS + Keepalived + Nginx 方式的配置就做完了。现在我们进行搭建效果的监测：</p>

<h2 id="toc_11"><a></a>4-1、验证 Master-LVS 节点的工作</h2>

<p>我们使用两个不同的浏览器，验证 Master-LVS 节点的工作：</p>

<p>========= 浏览器 1：<br/>
<img src="https://img-blog.csdn.net/20150822160827352" alt=""/></p>

<p>========= 浏览器 2：<br/>
<img src="https://img-blog.csdn.net/20150822160919601" alt=""/></p>

<p>看来 140 这个 VIP 下的 LVS 工作是正常的。</p>

<h2 id="toc_12"><a></a>4-2、验证 Master-LVS 节点停止后的效果</h2>

<p>下面我们停止 Master-LVS1：</p>

<pre><code>[root@lvs2 ~]# service keepalived stop
</code></pre>

<p>在经历了一些访问停顿后，浏览器 1 显示的效果如下（这就是为什么 keepalived 最好设置为非抢占模式）：</p>

<p><img src="https://img-blog.csdn.net/20150822161511700" alt=""/></p>

<h2 id="toc_13"><a></a>5、后文介绍</h2>

<p>好了，到这里负载均衡层所使用的几个标准工具就介绍完了。下一篇文章我们将进行总结，然后进入架构设计：业务层设计方案 的系列文章。在下一个系列文章中，我们将介绍至少两套 SOA 的实现、至少两套系统间通信使用的消息队列。哦，应我朋友的要求，我会专门写几篇文章，介绍 java 中线程的基础知识和进阶知识。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[架构设计：负载均衡层设计方案（6）——Nginx + Keepalived构建高可用的负载层]]></title>
    <link href="http://panlw.github.io/15301811967181.html"/>
    <updated>2018-06-28T18:19:56+08:00</updated>
    <id>http://panlw.github.io/15301811967181.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://blog.csdn.net/yinwenjie/article/details/47130609">https://blog.csdn.net/yinwenjie/article/details/47130609</a></p>
</blockquote>

<h1 id="toc_0"><a></a>1、概述</h1>

<p>前两遍文章中，我们一直在说后文要介绍 Nginx + Keepalived 的搭建方式。这篇文章开始，我们就来兑现前文的承诺，后续的两篇文章我们将介绍 Nginx + Keepalived 和 LVS + Keepalived 搭建高可用的负载层系统。如果你还不了解 Nginx 和 LVS 的相关知识，请参见我之前的两篇文章《架构设计：负载均衡层设计方案（2）——Nginx 安装》（<a href="http://blog.csdn.net/yinwenjie/article/details/46620711">http://blog.csdn.net/yinwenjie/article/details/46620711</a>）、《架构设计：负载均衡层设计方案（4）——LVS 原理》（<a href="http://blog.csdn.net/yinwenjie/article/details/46845997">http://blog.csdn.net/yinwenjie/article/details/46845997</a>）</p>

<h1 id="toc_1"><a></a>2、准备工作</h1>

<h2 id="toc_2"><a></a>2.1、准备两台独立工作的 Nginx 系统</h2>

<p>准备两台 Nginx 的主机，如果您不知道为什么需要准备两台，没关系，准备就行。保证两台 Nginx 主机能够被外网访问。在我这里，安装两台 Nginx 的虚拟机 IP 地址分别是：</p>

<ul>
<li>  Nginx VM1：192.168.61.129:80</li>
<li>  Nginx VM2：192.168.61.130:80</li>
</ul>

<p>访问相关的地址，确保两台 Nginx 都是可用的：</p>

<ul>
<li><p>VM1：<br/>
<img src="https://img-blog.csdn.net/20150730141119869" alt=""/></p></li>
<li><p>VM2：<br/>
<img src="https://img-blog.csdn.net/20150730141305112" alt=""/></p></li>
</ul>

<p>Nginx 的安装在我的前文《架构设计：负载均衡层设计方案（2）——Nginx 安装》（<a href="http://blog.csdn.net/yinwenjie/article/details/46620711">http://blog.csdn.net/yinwenjie/article/details/46620711</a>）中已经进行了详细的讲解，所以这里的讲解就一笔带过。</p>

<h2 id="toc_3"><a></a>2.2、再分别独立安装 Keepalived 系统</h2>

<p>我们的目标是 “<strong>在一台工作的 Nginx 崩溃的情况下，系统能够检测到，并自动将请求切换到另外一台备份的 Nginx 服务器上</strong>”。所以，之前安装的两台 Nginx，一台是 Master 服务器是主要的工作服务器，另一台是备份服务器，在 Master 服务器出现问题后，由后者接替其工作。如下图所示（外网的请求使用一个由 keepalived 控制的虚拟的浮动 IP 进行访问）：</p>

<p><img src="https://img-blog.csdn.net/20150730143806220" alt=""/></p>

<ul>
<li><p>请到 <a href="http://www.keepalived.org">www.keepalived.org</a> 下载 keepalived 的稳定版本，我下载的是 1.2.17 版本。</p></li>
<li><p>解压，并且安装。注意，我在这里制定了 perfix 参数，指定安装位置，这是为了我自己便于管理。您在安装的时候，可以根据自己的情况来决定是不是加这个参数：</p>

<pre><code>tar -zxvf keepalived-1.2.17.tar.gz

./configure --perfix=/usr/keepalived-1.2.17

make &amp; make install 

</code></pre></li>
<li><p>如果您不是安装到默认路径，那么为了将 keepalived 做成系统服务，您需要拷贝一些文件到指定的路径下，如下：</p>

<pre><code>cp /usr/keepalived-1.2.17/etc/sysconfig/keepalived  /etc/sysconfig/keepalived

cp /usr/keepalived-1.2.17/sbin/keepalived /usr/sbin/keepalived

cp /usr/keepalived-1.2.17/etc/rc.d/init.d/keepalived  /etc/rc.d/init.d/keepalived

mkdir /etc/keepalived

cp /usr/keepalived-1.2.17/etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf

</code></pre></li>
<li><p>然后您可以将 keepalived 做成服务了：</p>

<pre><code>/etc/rc.d/init.d/keepalived

chkconfig keepalived on

</code></pre></li>
</ul>

<h1 id="toc_4"><a></a>3、检查 Nginx 状态</h1>

<p>在正式介绍 Nginx + Keepalived 的配置前，我们首先介绍一下如何检查 Nginx 的状态。是的，这是为下一小节做准备。<strong>我们只有能够正确检查 Nginx 的状态，才说得上在 Nginx 节点出现问题的情况下，切换到另一台 Nginx 上接替其工作</strong>。</p>

<p>Nginx 为什么会停止响应呢？在我的工作经验中，无非有以下几种情况：</p>

<ul>
<li><p>Nginx 的所有进程被强行终止（或管理进程）。<br/>
这种情况，是我们需要检查和切换的。无论什么情况下进程被终止了，如果它不能重启，我们就要切换到备机。</p></li>
<li><p>Nginx 日志盘的挂载点崩溃或者磁盘写满。<br/>
这个也是我们需要检查和切换的。</p></li>
<li><p>Nginx 已经达到设置的最大连接数，暂时停止响应。<br/>
这种情况下，我们不能进行备机切换，因为通过 VIP:192.168.61.100 连接过来的用户请求比较多（在我们优化参数后，可以达到 65535 / 4 的数量），一旦我们进行备机切换，这些用户请求将全部异常。这个问题的解决需要靠增加负载机器，而不是主备切换。</p></li>
<li><p>Nginx 物理机异常关机。<br/>
这个肯定是需要进行检查和切换的。</p></li>
</ul>

<p>我们来看一段 Linux 脚本：</p>

<pre><code>#!/bin/sh
if [ $(ps -C nginx --no-header | wc -l) -eq 0 ]; then
    /usr/nginx-1.6.2/sbin/nginx
fi

sleep 2
if [ $(ps -C nginx --no-header | wc -l) -eq 0 ]; then
    service keepalived stop
fi

</code></pre>

<p>我们大致讲解一下 “ps -C nginx –no-header | wc -l” 这个命令：<br/>
- ps 这个命令用来进行 Linux 中进程相关的查询，-C 意思是按照进程名称进行查询。查询出来后的结果如下：</p>

<pre><code>[root@vm2 ~]# ps -C nginx
  PID TTY          TIME CMD
 3374 ?        00:00:00 nginx
 3375 ?        00:00:01 nginx

</code></pre>

<ul>
<li><p>如果要去掉统计出来的结果表的头部，那么要使用 –no-header 参数，加上参数后，查询结果如下：</p>

<pre><code>[root@vm2 ~]# ps -C nginx  --no-header
 3374 ?        00:00:00 nginx
 3375 ?        00:00:01 nginx

</code></pre></li>
<li><p>“|”，这是 Linux 中的管道流命令，将上一个命令的输出结果作为下一个命令的输入。</p></li>
<li><p>wc 统计命令，-l 参数，代表按行数进行统计。所以整个命令的输出结果为：</p>

<pre><code>[root@vm2 ~]# ps -C nginx --no-header | wc -l
2

</code></pre></li>
</ul>

<p>清楚了其中最关键的命令，我们再来讲解一下整个脚本的含义：<br/>
第一个判断说明的是，如果当前 nginx 的进程数量 == 0，那么执行 nginx 的启动命令，试图重新启动 nginx；接下来等待 2 秒（这是为了给 nginx 一定的启动时间），然后再次查看 Nginx 的进程数量，如果仍然 == 0，那么停止这台机器的 keepalived 服务，以便备用的 Keepalived 节点检查到 Keepalived 已经停止这个事件，并将浮动 IP 切换到备用服务器上。</p>

<p><strong>注意，这段脚本是和我机器上的 Nginx 安装路径、Keepalived 服务的状态有关的，您如果要用的话，请进行相应的更改</strong>。</p>

<h1 id="toc_5"><a></a>4、Nginx + Keepalived 最简配置</h1>

<h2 id="toc_6"><a></a>4.1、请再次确认前提</h2>

<p>（首先，为了保证不会出现额外的问题，请首先关闭防火墙，当然正式环境里面，防火墙不能关闭）</p>

<p>外网进行 Nginx 访问的浮动 IP：192.168.61.100</p>

<ul>
<li><p>我们将 192.168.61.129 这台服务器上运行的 Nginx 作为主要的 Nginx，其上的 keepalived 服务我们设置成 Master 方式。</p></li>
<li><p>我们将 192.168.61.129 这台服务器上运行的 Nginx 作为备用的 Nginx 服务，其上的 keepalived 服务我们设置为 Backup 方式。</p></li>
</ul>

<h2 id="toc_7"><a></a>4.2、正式开始设置</h2>

<p>注意，经过安装，您的 keepalived 配置文件的位置在 “/etc/keepalived/keepalived.conf”（如果没有，请创建一个，但是经过之前的步骤，这个位置肯定是有文件的，如果没有可能是之前您的步骤出现了什么问题）。</p>

<h2 id="toc_8"><a></a>4.2.1、设置 192.168.61.129 上的 MASTER</h2>

<p>我们先来看看 192.168.61.129 上的原始 ip 信息：</p>

<p><img src="https://img-blog.csdn.net/20150730181017823" alt=""/></p>

<p>注意，这个 129 机器上的网卡设备号是 eth1，而不是 eth0，这个参数我们将在配置 keepalived 的时候使用到。</p>

<p>下面是 129 上 keepalived 的最简配置：</p>

<pre><code>! Configuration File for keepalived
# global setting , notify email setting
global_defs {
   #存在于同一个网段中，一组keepalived的各个节点都有不同的名字
   #在全局设置中，我们还可以设置管理员的email信息等。
   router_id LVS_V1
}

#这个是我们在上一小结讲到的nginx检查脚本，我们保存在这个文件中（注意文件权限）
vrrp_script chknginx {
    script &quot;/usr/keepalived-1.2.17/bin/checknginx.sh&quot;
    #每10秒钟，检查一次
    interval 10
}

#keepalived实例设置，是最重要的设置信息
vrrp_instance VI_1 {
    #state状态MASTER表示是主要工作节点。
    #一个keepalived组中，最多只有一个MASTER节点，当然也可以没有
    state MASTER
    #实例所绑定的网卡设备，我的网卡设备是eth1。您按照您自己的来
    interface eth1
    #同一个keepalived组，节点的设置必须一样，这样才会被识别
    virtual_router_id 52
    #节点优先级，BACKUP的优先级一定要比MASTER的优先级低
    priority 100
    #组播信息发送间隔，两个节点设置必须一样
    advert_int 1
    #实际的eth1上的固定ip地址
    mcast_src_ip=192.168.61.129
    #验证信息，只有验证信息相同，才能被加入到一个组中。
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    #虚拟地址和绑定的端口，如果有多个，就绑定多个
    #dev 是指定浮动IP要绑定的网卡设备号
    virtual_ipaddress {
        192.168.61.100 dev eth1
    }

    #设置的检查脚本
    #关联上方的“vrrp_script chknginx”
    track_script {
        chknginx
    }
}

</code></pre>

<h2 id="toc_9"><a></a>4.2.2、设置 192.168.61.130 上的 BACKUP</h2>

<p>再来看看 192.168.61.130 这个备用节点上 keepalived 的设置：</p>

<pre><code>! Configuration File for keepalived
# global setting , notify email setting
global_defs {
   #这里和master节点不同
   router_id LVS_V2
}

#check nginx
vrrp_script chknginx {
    script &quot;/usr/keepalived-1.2.17/bin/checknginx.sh&quot;
    interval 10
}

# instance setting
vrrp_instance VI_1 {
    # 这里和Master节点不一样
    state BACKUP
    interface eth1
    # 这里一定是一样的
    virtual_router_id 52
    # 这里的优先级比Master节点低
    priority 99
    advert_int 1
    # 这里和Master节点不一样
    mcast_src_ip=192.168.61.130
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.61.100 dev eth1
    }

    track_script {
        chknginx
    }
}

</code></pre>

<h2 id="toc_10"><a></a>4.3、启动主节点和备用节点</h2>

<p>以上配置中请注意几个关键点：</p>

<ul>
<li>  注意 nginx 状态检查的脚本的位置，根据自己创建文件的位置不一样，脚本检查的指定位置也不一样</li>
<li>  注意优先级，MASTER 节点的优先级一定要高于所有的 BACKUP 节点。</li>
<li>  <strong>注意局域网的组播地址，一定要可用</strong>。局域网内所有 keepalived 节点都是利用组播方式寻找对方。</li>
<li>  谁说 BACKUP 节点只能有一个！？</li>
<li>  最后，keepalived 一定要注册成服务形式，您可以想象上面所有脚本、配置、命令如果重启后再来一次，会是什么情况。</li>
</ul>

<p>接下来，我们要开始启动 Master 节点和 Backup 节点了，为了准确的查看日志状态，您需要观察系统日志。系统日志所在的位置：</p>

<pre><code>tail -f /var/log/messages

</code></pre>

<p>先启动 Master 节点：</p>

<pre><code>service keepalived start

</code></pre>

<p>再启动 Backup 节点：</p>

<pre><code>service keepalived start

</code></pre>

<p><strong>如果设置和启动都是成功的，您不会在日志信息中收到任何的 keepalived 报错信息</strong>。接下来您就可以使用 192.168.61.100 这个 IP 访问 Nginx 了：</p>

<p><img src="https://img-blog.csdn.net/20150730210724952" alt=""/></p>

<p>另外，这个绑定在 192.168.61.129 上的浮动 ip：192.168.61.100，您通过 ipconfig 命令一般是看不到的，要使用 ip addr 命令进行查看：</p>

<p><img src="https://img-blog.csdn.net/20150730211138410" alt=""/></p>

<p>为了试验，我们主动停止 Master 节点上的 keepalived 服务（注意，杀 Nginx 进程不起作用，因为我们的检查脚本会试图重新启动 Nginx 进程），接下来我们可以看到浮动 IP 漂移到了 130 备机上：</p>

<p><img src="https://img-blog.csdn.net/20150730211530195" alt=""/></p>

<h1 id="toc_11"><a></a>5、Nginx + Keepalived 非抢占模式</h1>

<p>通过第 4 节的详细介绍，相信您对 Nginx + Keepalived 的安装方式有了一个明确的理解。keepalived 的切换可以是自动的，但是却做不到毫秒级别，他怎么都需要几秒钟的时间进行切换。</p>

<p>这就有一个问题，虽然在主节点出现问题我们转向备份节点时，这个延时无可避免，但是在我们修复主节点后，实际上并没有必要再马上做一次切换，所以 Keepalived 提供了一种非抢占模式，来满足这个要求。</p>

<p>下面我们就来介绍一下 Keepalived 的非抢占模式的配置（无 MASTER 节点，全部依据优先级确定哪个节点进行工作）：</p>

<h2 id="toc_12"><a></a>5.1、原来主节点的配置改动</h2>

<pre><code>! Configuration File for keepalived
# global setting , notify email setting
global_defs {
   router_id LVS_V1
}

vrrp_script chknginx {
    script &quot;/usr/keepalived-1.2.17/bin/checknginx.sh&quot;
    interval 10
    # 一旦节点失效，节点的优先级就减少2
    # 有多少个keepalived节点，就填写多少数量。
    # 这样保证这个节点的优先级比其他节点都低
    weight -2
    # fall 表示多少次检查失败，就算节点失效。默认1
    #fall 1
}

vrrp_instance VI_1 {
    #state状态都是BACKUP表示是主要工作节点。
    state BACKUP
    interface eth1
    virtual_router_id 52
    # 这个关键配置项，设置为“非抢占”模式
    nopreempt
    # 每个节点的优先级一定要不一样
    priority 100
    advert_int 1
    mcast_src_ip=192.168.61.129
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    #虚拟地址和绑定的端口，如果有多个，就绑定多个
    #dev 是指定浮动IP要绑定的网卡设备号
    virtual_ipaddress {
        192.168.61.100 dev eth1
    }

    #设置的检查脚本
    #关联上方的“vrrp_script chknginx”
    track_script {
        chknginx
    }
}

</code></pre>

<p>原来的主节点设置更改完成。</p>

<h2 id="toc_13"><a></a>5.2、原来备份节点的配置改动</h2>

<p>加入 “非抢占” 模式的关键字、更改一个确定的优先级，设置检查失败后优先级的递减量，就行了。</p>

<h1 id="toc_14"><a></a>6、后文介绍</h1>

<p>这是我 8 月份的首篇文章，后文我们将介绍 LVS + Keepalived + Nginx 的安装和配置方式。注意，LVS 被 Keepalived 后，就没有必要在对 Nginx 做 Keepalived 了。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[架构设计：负载均衡层设计方案（5）——LVS单节点安装]]></title>
    <link href="http://panlw.github.io/15301811361753.html"/>
    <updated>2018-06-28T18:18:56+08:00</updated>
    <id>http://panlw.github.io/15301811361753.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://blog.csdn.net/yinwenjie/article/details/47010569">https://blog.csdn.net/yinwenjie/article/details/47010569</a></p>
</blockquote>

<h1 id="toc_0"><a></a>1、概述</h1>

<p>上篇文章《架构设计：负载均衡层设计方案（4）——LVS 原理》（<a href="http://blog.csdn.net/yinwenjie/article/details/46845997">http://blog.csdn.net/yinwenjie/article/details/46845997</a>），我们介绍了 LVS 的工作模式，和每一种模式的具体工作过程。这篇文章中，我们将介绍单一 LVS 节点的安装方式。比起上一篇文章，这一片要提到的安装和配置就是非常简单的了，只要您了解原理，实践就是从容的事情。</p>

<p>您可以在您的电脑上使用 VMware 虚拟机，按照下面介绍的过程一步一步实践。我们将采用两台虚拟机，一台作为 LVS 节点，另外一台安装了 Nginx 的服务器作为 Real Server 节点。</p>

<h1 id="toc_1"><a></a>2、LVS-NAT 模式安装</h1>

<h2 id="toc_2"><a></a>2.1、准备工作——LVS Server：</h2>

<p>LVS Server：LSV Server 有两张网卡。</p>

<ul>
<li>  eth0：192.168.100.10：这张网卡对应一个封闭的内网，不能访问外网资源，外网也不能直接通过这个 IP 访问这台主机</li>
<li><p>eth1：192.168.220.100：这张网卡设置的 IP 可以访问外网，也可以被外网访问。eth1 的网关：192.168.220.1。</p>

<pre><code>以下是设置的eth0    ip信息，
[root@lvs1 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0
DEVICE=&quot;eth0&quot;
BOOTPROTO=&quot;static&quot;
HWADDR=&quot;00:0C:29:3E:4A:4F&quot;
ONBOOT=&quot;yes&quot;
TYPE=&quot;Ethernet&quot;
IPADDR=&quot;192.168.100.10&quot;
NETMASK=&quot;255.255.255.0&quot;
====================================
以下是设置的eth1  ip信息
[root@lvs1 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth1
DEVICE=&quot;eth1&quot;
BOOTPROTO=&quot;static&quot;
HWADDR=&quot;00:0C:29:3E:4A:59&quot;
ONBOOT=&quot;yes&quot;
TYPE=&quot;Ethernet&quot;
IPADDR=&quot;192.168.220.100&quot;
NETMASK=&quot;255.255.255.0&quot;
GATEWAY=&quot;192.168.220.1&quot;

</code></pre></li>
</ul>

<p>记得设置完成后，要重启 network 服务：</p>

<pre><code>[root@lvs1 ~]# service network restart

</code></pre>

<p>ping ping 更健康（说明到外网的网管工作是正常的）：</p>

<pre><code>[root@lvs1 ~]# ping 192.168.220.1
PING 192.168.220.1 (192.168.220.1) 56(84) bytes of data.
64 bytes from 192.168.220.1: icmp_seq=1 ttl=128 time=0.447 ms
64 bytes from 192.168.220.1: icmp_seq=2 ttl=128 time=0.154 ms

</code></pre>

<p>另外还可以通过 route 命令检查：</p>

<pre><code>[root@lvs1 ~]# route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
192.168.100.0   *               255.255.255.0   U     1      0        0 eth0
192.168.220.0   *               255.255.255.0   U     1      0        0 eth1
default         192.168.220.1   0.0.0.0         UG    0      0        0 eth1

</code></pre>

<p>注意，route 表中有一项 eth1 的 default 的信息，指向 192.168.220.1。说明路由配置是正确的。</p>

<h2 id="toc_3"><a></a>2.2、准备工作——Real Server：</h2>

<p>Real Server：Real Server 有一张网卡，在一个封闭的内网环境中。</p>

<ul>
<li><p>eth0:192.168.100.11：这样 LVS Server 和 Real Server 就组成了一个相对封闭的局域网络。注意按照我们介绍的 NAT 原理，Real Server 的 eth0 的默认网关要设置成 Lvs Server：192.168.100.10.</p></li>
<li><p>在 Real Server 上运行了一个 Nginx 程序，在 80 端口上。这样以便在后续的过程中，测试 LVS-NAT 的工作是否正常。</p></li>
</ul>

<p>以下是设置的 Real Server eth0 的 IP 信息：</p>

<pre><code>[root@vm1 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0 
DEVICE=&quot;eth0&quot;
BOOTPROTO=&quot;static&quot;
HWADDR=&quot;00:0C:29:45:04:32&quot;
ONBOOT=&quot;yes&quot;
TYPE=&quot;Ethernet&quot;
IPADDR=192.168.100.11
NETMASK=255.255.255.0
GATEWAY=&quot;192.168.100.10&quot;

</code></pre>

<p>一定注意 Real Server 的网管要设置到 LVS 的 IP：192.168.100.10。接着，看看 Nginx 是不是工作正常的：</p>

<p><img src="https://img-blog.csdn.net/20150725111742203" alt=""/></p>

<p>当然，同样的 ping ping 更健康：</p>

<pre><code>[root@vm1 ~]# ping 192.168.100.10
PING 192.168.100.10 (192.168.100.10) 56(84) bytes of data.
64 bytes from 192.168.100.10: icmp_seq=1 ttl=64 time=0.259 ms
64 bytes from 192.168.100.10: icmp_seq=2 ttl=64 time=0.215 ms
64 bytes from 192.168.100.10: icmp_seq=3 ttl=64 time=0.227 ms

</code></pre>

<p>另外一个检查方式，也可以通过 route 命令：</p>

<pre><code>[root@vm1 ~]# route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
192.168.100.0   *               255.255.255.0   U     1      0        0 eth0
default         192.168.100.10  0.0.0.0         UG    0      0        0 eth0

</code></pre>

<p>注意，默认的路由指向 192.168.100.10。<br/>
<strong>完成准备工作后，我们可以开始安装和配置 LVS 了。</strong></p>

<h2 id="toc_4"><a></a>2.3、开始安装和配置 LVS-NAT 模式：</h2>

<p>ipvsadm 是一个 LVS 的管理程序。我们队 LVS 的配置都是通过这个管理程序进行实现的。首先我们要安装 ipvsadm：</p>

<pre><code>yum -y install ipvsadm

</code></pre>

<p>然后开始配置。首先我们要设置 LVS 机器支持 IP 转发功能。注意默认 IP 转发功能是关闭的，重启机器后，又会关闭：</p>

<pre><code>[root@lvs1 ~]# echo 1 &gt;&gt; /proc/sys/net/ipv4/ip_forward

</code></pre>

<p>然后我们查看一下，是否改写成功：</p>

<pre><code>[root@lvs1 ~]# cat /proc/sys/net/ipv4/ip_forward 
1

</code></pre>

<p>注意，如果您使用 vim 或者 vi 命令，改写文件，是不会成功的。因为这个文件存在于内存。不在硬盘上。所以只能通过 echo 这样的命令改写。</p>

<p>接下来执行如下的命令：</p>

<pre><code>[root@lvs1 ~]# ipvsadm -At 192.168.220.100:80 -s rr
[root@lvs1 ~]# ipvsadm -at 192.168.220.100:80 -r 192.168.100.11 -m

</code></pre>

<p>我们来解释一下其中的参数：</p>

<pre><code>-A --add-service 在内核的虚拟服务器表中添加一条新的虚拟服务器记录。也就是增加一台新的虚拟服务器。
-t --tcp-service service-address 说明虚拟服务器提供的是tcp 的服务。
-s --scheduler scheduler 使用的调度算法，可选项包括：rr|wrr|lc|wlc|lblc|lblcr|dh|sh|sed|nq（关于调度算法我们在上篇文章中已经详细介绍了）
-r --real-server server-address 真实的服务器[Real-Server:port]。
-m --masquerading 指定LVS 的工作模式为NAT 模式。

</code></pre>

<p>当然 ipvsadm 还包括很多参数，后面慢慢讲，然后再本文章的最后部分，会给出一个比较完整的参数汇总。最后我们测试一下：</p>

<p><img src="https://img-blog.csdn.net/20150725115900158" alt=""/></p>

<p>我们在相对于 Real Server 的外网，通过 192.168.220.100 的 LVS Server IP 访问到了 Real Server 上的 Nginx 服务。安装和配置成功。</p>

<h2 id="toc_5"><a></a>2.4、关于 iptables 和重启服务的说明</h2>

<p>在配置 LVA-NAT 过程中，建议关闭 LVS 和 Real Server 的防火墙服务。这样可以避免不必要的错误发生，增加一次配置成功的几率。但是正常生产环境中，LVS 的防火墙根据实际情况最好还是要打开。</p>

<p><strong>请注意，刚才使用 ipvsadm 配置的信息，在 LVS 服务器重启后，就会失效。包括 ip_forward 的配置</strong>。所以，最好制作一个脚本文件，并加入到 / etc/profile 中：</p>

<pre><code>vim /usr/lvsshell.sh

#!/bin/bash
echo 1 &gt; /proc/sys/net/ipv4/ip_forward 
ipvsadm -C
ipvsadm -At 192.168.220.100:80 -s rr
ipvsadm -at 192.168.220.100:80 -r 192.168.100.11 -m

</code></pre>

<h1 id="toc_6"><a></a>3、LVS-DR 模式安装</h1>

<h2 id="toc_7"><a></a>3.1、准备工作——LVS Server</h2>

<p>为了让您了解 LVS 的另外设置方式，本次我们使用 VIP 的方式，而不是两张网卡的方式（当然您也可以用两张网卡的方式）。VIP 的方式是后面我们将讲到的 LVS + Keepalived 组合工作模式的常用方式。所谓 VIP 就死虚拟 IP，是指这个 IP 不会固定捆绑到某一个网卡设备，而是通过 ifconfig 命令绑定，并在 “适当时候” 这种绑定关系会随之变化：</p>

<p>DIP：192.168.220.137<br/>
VIP：192.168.220.100</p>

<p>首先我们看看在没有设置 VIP 之前，LVS 主机上的 IP 信息：</p>

<pre><code>[root@lvs1 ~]# ifconfig 
eth1      Link encap:Ethernet  HWaddr 00:0C:29:3E:4A:59  
          inet addr:192.168.220.137  Bcast:192.168.220.255  Mask:255.255.255.0
          inet6 addr: fe80::20c:29ff:fe3e:4a59/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:2612 errors:0 dropped:0 overruns:0 frame:0
          TX packets:117 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:159165 (155.4 KiB)  TX bytes:8761 (8.5 KiB)

lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:16436  Metric:1
          RX packets:210 errors:0 dropped:0 overruns:0 frame:0
          TX packets:210 errors:0 dropped:0 overruns:0 carrier:0 
          collisions:0 txqueuelen:0 
          RX bytes:16944 (16.5 KiB)  TX bytes:16944 (16.5 KiB)

</code></pre>

<p>然后我们来设置 VIP 信息：</p>

<pre><code>[root@lvs1 ~]# ifconfig eth1:0 192.168.220.100 broadcast 192.168.220.100 netmask 255.255.255.255 up
[root@lvs1 ~]# route add -host 192.168.220.100 dev eth1:0

</code></pre>

<p>路由表会有新的路由信息：</p>

<pre><code>[root@lvs1 ~]# route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
192.168.220.100 *               255.255.255.255 UH    0      0        0 eth1
192.168.100.0   *               255.255.255.0   U     1      0        0 eth0
192.168.220.0   *               255.255.255.0   U     1      0        0 eth1
default         192.168.220.1   0.0.0.0         UG    0      0        0 eth1

</code></pre>

<p>这时，通过一个外网 IP，可以 ping 通这个 VIP（下面这个 DOS 系统是 VM 的母机）：</p>

<pre><code>C:\Users\yinwenjie&gt;  ping 192.168.220.100
正在 Ping 192.168.220.100 具有 32 字节的数据:
来自 192.168.220.100 的回复: 字节=32 时间&lt;1ms TTL=64
来自 192.168.220.100 的回复: 字节=32 时间&lt;1ms TTL=64

</code></pre>

<p>以上，我们完成了 LVS 主机设置 LVS-DR 工作模式的准备工作。注意：</p>

<ul>
<li>  <strong>在您设置过程中，防火前最好关闭，但在正式生产环境中，LVS 的防火强最好打开。</strong></li>
<li>  <strong>VIP 信息在 LVS 主机重启后，会消失。所以您最好将设置 VIP 的命令做成一个脚本。</strong></li>
</ul>

<h2 id="toc_8"><a></a>3.2、准备工作——Real Server</h2>

<p>RIP：192.168.220.132</p>

<p>真实服务器的准备工作，需要保证真实服务器能够访问外网网关，并且保证由 LVS 改写的报文能够被 Real Server 处理（请参见我的上一篇介绍 LVS 原理的博文），这就需要做一个回环 IP。首先我们看看没有开始设置之前的 IP 信息：</p>

<pre><code>[root@localhost ~]# ifconfig 
eth0      Link encap:Ethernet  HWaddr 00:0C:29:FC:91:FC  
          inet addr:192.168.220.132  Bcast:192.168.220.255  Mask:255.255.255.0
          inet6 addr: fe80::20c:29ff:fefc:91fc/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:2384 errors:0 dropped:0 overruns:0 frame:0
          TX packets:1564 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:1551652 (1.4 MiB)  TX bytes:144642 (141.2 KiB)

lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:44 errors:0 dropped:0 overruns:0 frame:0
          TX packets:44 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:3361 (3.2 KiB)  TX bytes:3361 (3.2 KiB)

</code></pre>

<p>使用 route 命令，查看原始 route 信息：</p>

<pre><code>[root@localhost ~]# route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
192.168.220.0   *               255.255.255.0   U     1      0        0 eth0
default         192.168.220.1   0.0.0.0         UG    0      0        0 eth0

</code></pre>

<p>另外，这台 Real Server 上有一个 Nginx，以保证我们观测 LVS-DR 的运行情况。我们在外网使用 192.168.220.132 这个 IP，能够访问到 Nginx 的页面：</p>

<p><img src="https://img-blog.csdn.net/20150726085859705" alt=""/></p>

<p>接下来，我们开始设置 Real Server 上的回环 IP，首先关闭这台机器进行 ARP 查询的功能，否则，Real Server 会在路由器或者交换机上去查询 192.168.220.100 这个 IP 对应的 MAC 地址（注意，以下的信息都会被还原）：</p>

<pre><code>echo &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore
echo &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_announce
echo &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_ignore
echo &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_announce

</code></pre>

<p>然后可以设置回环 IP 了：</p>

<pre><code>[root@vm3 ~]# ifconfig lo:0 192.168.220.100 broadcast 192.168.220.100 netmask 255.255.255.255 up
[root@vm3 ~]# route add -host 192.168.220.100 dev lo:0

</code></pre>

<p>检查新的路由信息：</p>

<pre><code>[root@vm3 ~]# route 
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
192.168.220.100 *               255.255.255.255 UH    0      0        0 lo
192.168.220.0   *               255.255.255.0   U     1      0        0 eth0
default         192.168.220.1   0.0.0.0         UG    0      0        0 eth0

</code></pre>

<p>以上设置完成。<strong>完成后，您需要通过 ping 命令，检查一下网关是否可用（最好可以瓶 ping 一下外网的某个地址，例如 163.com）。LVS-DR 模式下，Real Server 是直接向请求方返回结果，所以一定要保证网关时可用的。</strong></p>

<h2 id="toc_9"><a></a>3.3、开始安装和配置 LVS-DR 模式</h2>

<p>在验证了 LVS Server 和 Real Server 都准备好以后，就可以进行 LVS-DR 模式的设置了。LVS 管理软件 ipvsadm 的安装我们就不敷述了，在 LVS-NAT 的小结中有关于 LVS 管理软件安装的介绍。</p>

<pre><code>[root@lvs1 ~]# echo 1 &gt; /proc/sys/net/ipv4/ip_forward
[root@lvs1 ~]# cat /proc/sys/net/ipv4/ip_forward
1
[root@lvs1 ~]# ipvsadm -C
[root@lvs1 ~]# ipvsadm -At 192.168.220.100:81 -s rr 
[root@lvs1 ~]# ipvsadm -at 192.168.220.100:81 -r 192.168.220.132 -g

</code></pre>

<p>介绍一下新出现的参数：</p>

<ul>
<li>  -g –gatewaying 指定 LVS 的工作模式为直接路由模式 DR 模式（也是 LVS 默认的模式）</li>
</ul>

<p>配置完成，是不是很简单。接下来，我们就可以在外网，通过 192.168.220.100 这个 IP 访问 132 这台 Real Server 上的 Nginx 服务了：</p>

<p><img src="https://img-blog.csdn.net/20150726091844324" alt=""/></p>

<h1 id="toc_10"><a></a>4、LVS-TUN 模式安装</h1>

<p>LVS-TUN 模式的安装我不打算再花很大的篇幅讲了，在您通过上一篇文章了解了 DR 和 TUN 的不同以后，其配置的过程都是差不多的，只不过一个可以跨越子网，一个不能。所以 LVS-TUN 模式的安装和配置请首先参考 LVS-DR 模式。</p>

<h1 id="toc_11"><a></a>5、IPVSADM 参数汇总</h1>

<p>-A –add-service 在内核的虚拟服务器表中添加一条新的虚拟服务器记录。也就是增加一台新的虚拟服务器。</p>

<p>-E –edit-service 编辑内核虚拟服务器表中的一条虚拟服务器记录。</p>

<p>-D –delete-service 删除内核虚拟服务器表中的一条虚拟服务器记录。</p>

<p>-C –clear 清除内核虚拟服务器表中的所有记录。</p>

<p>-R –restore 恢复虚拟服务器规则</p>

<p>-S –save 保存虚拟服务器规则，输出为 - R 选项可读的格式</p>

<p>-a –add-server 在内核虚拟服务器表的一条记录里添加一条新的真实服务器记录。也就是在一个虚拟服务器中增加一台新的真实服务器</p>

<p>-e –edit-server 编辑一条虚拟服务器记录中的某条真实服务器记录</p>

<p>-d –delete-server 删除一条虚拟服务器记录中的某条真实服务器记录</p>

<p>-L –list 显示内核虚拟服务器表</p>

<p>-Z –zero 虚拟服务表计数器清零（清空当前的连接数量等）</p>

<p>–set tcp tcpfin udp 设置连接超时值</p>

<p>–start-daemon 启动同步守护进程。他后面可以是 master 或 backup，用来说明 LVS Router 是 master 或是 backup。在这个功能上也可以采用 keepalived 的 VRRP 功能。</p>

<p>–stop-daemon 停止同步守护进程</p>

<p>-t –tcp-service service-address 说明虚拟服务器提供的是 tcp 的服务 [vip:port] or [real-server-ip:port]</p>

<p>-u –udp-service service-address 说明虚拟服务器提供的是 udp 的服务 [vip:port] or [real-server-ip:port]</p>

<p>-f –fwmark-service fwmark 说明是经过 iptables 标记过的服务类型。</p>

<p>-s –scheduler scheduler 使用的调度算法，选项：rr|wrr|lc|wlc|lblc|lblcr|dh|sh|sed|nq, 默认的调度算法是： wlc.</p>

<p>-p –persistent [timeout] 持久稳固的服务。这个选项的意思是来自同一个客户的多次请求，将被同一台真实的服务器处理。timeout 的默认值为 300 秒。</p>

<p>-M –netmask netmask persistent granularity mask</p>

<p>-r –real-server server-address 真实的服务器 [Real-Server:port]</p>

<p>-g –gatewaying 指定 LVS 的工作模式为直接路由模式 DR 模式（也是 LVS 默认的模式）</p>

<p>-i –ipip 指定 LVS 的工作模式为隧道模式</p>

<p>-m –masquerading 指定 LVS 的工作模式为 NAT 模式</p>

<p>-w –weight weight 真实服务器的权值</p>

<p>–mcast-interface interface 指定组播的同步接口</p>

<p>-c</p>

<p>–connection 显示 LVS 目前的连接 如：ipvsadm -L -c</p>

<p>–timeout 显示 tcp tcpfin udp 的 timeout 值 如：ipvsadm -L –timeout</p>

<p>–daemon 显示同步守护进程状态</p>

<p>–stats 显示统计信息</p>

<p>–rate 显示速率信息</p>

<p>–sort 对虚拟服务器和真实服务器排序输出</p>

<p>–numeric -n 输出 IP 地址和端口的数字形式</p>

<p>好吧，这个就是我网上抄的。</p>

<h1 id="toc_12"><a></a>6、后文介绍</h1>

<p><strong>LVS 的设置实际上没有什么难度，充其量是一个高级玩意儿，是解决特定问题的特定方法</strong>。您需要的基础知识涉及到 IP 协议、子网分割、IP 映射、Linux 脚本等基本知识。打通任督二脉才是您的根本。</p>

<p>LVS 的设置过程中肯定会遇到实际问题，特别是您头几次配置。别慌，别怕，透过现象猜测本质，一个一个的解决问题。就我的经验总结，无非几种问题分类：</p>

<ul>
<li>  网关不通</li>
<li>  回环 IP 设置问题</li>
<li>  防火墙问题</li>
<li>  VIP 设置问题</li>
<li>  网段或者子网问题</li>
</ul>

<p>通过包括这篇博客在内的这几篇文章，您已经分别了解了 Nginx、LVS 的功能、原理、特 点和工作模式。后面一篇文章，我们将把这些负载层的技术混合在一起，介绍 Nginx + Keepalived、LVS + Keepalived、LVS + Keepalived + Nginx 的安装和配置方式。</p>

<p>（本人这个月 “持之以恒” 奖已经拿到了，笑一个：哈哈哈）</p>

<p>补充一下：由于工作的原因，期间可能还会推出一篇关于 Apache Camel 的文章，就算提前进入业务通信层的讨论吧。不过这个看当时的时间安排。目前情况下还是以负载均衡层的讨论为主线。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[架构设计：负载均衡层设计方案（4）——LVS原理]]></title>
    <link href="http://panlw.github.io/15301810578621.html"/>
    <updated>2018-06-28T18:17:37+08:00</updated>
    <id>http://panlw.github.io/15301810578621.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://blog.csdn.net/yinwenjie/article/details/46845997">https://blog.csdn.net/yinwenjie/article/details/46845997</a></p>
</blockquote>

<p>之前我们花了两篇文章的篇幅，详细讲解了 Nginx 的原理、安装和特性组件。请参看《负载均衡层设计方案（2）——Nginx 安装》（<a href="http://blog.csdn.net/yinwenjie/article/details/46620711">http://blog.csdn.net/yinwenjie/article/details/46620711</a>）和《架构设计：负载均衡层设计方案（3）——Nginx 进阶》（<a href="http://blog.csdn.net/yinwenjie/article/details/46742661">http://blog.csdn.net/yinwenjie/article/details/46742661</a>）两篇文章。虽然不包括 Nginx 的所有知识（也不可能全部包括），但是足够读者将 Nginx 应用到实际的生产中，并进行重要特性的优化，后面有时间我们还会重新回到 Nginx 的讲解上。从本篇文章开始，我们将开始介绍 LVS 技术，包括基本概念、简单使用和进阶使用。</p>

<h1 id="toc_0"><a></a>1、LVS 介绍</h1>

<p>请自行 Google 或者百度。</p>

<h1 id="toc_1"><a></a>2、网络协议基础知识</h1>

<p>根据官方文档 LVS 支持三种负载工作方式：NAT 方式、TUN 方式和 DR 方式。为了说明这三种方式的工作原理，我们首先需要了解一下基础的 IP/TCP 报文（注意，IP 报文和 TCP 报文是两种不同的报文格式），以及链路层对 IP 数据的封装方式。然后我们采用看图说话的方式，以图文结合的方式为您介绍这三种工作方式中对报文或重写或封装的过程。</p>

<p>为了说清楚我们将要讲解的基础知识，就要提到 OSI7 层网络模型。</p>

<p><img src="https://img-blog.csdn.net/20150716191755069" alt=""/></p>

<p>章文嵩博士及其团队是我的偶像，LVS 体系之所以高效是因为其直接对链路层报文、IP 报文、TCP 报文进行了修改或封装。所以要真正理解 LVS 的三种工作方式，就不能像网络上的抄袭贴一样转几句不知所以的文字就完了，必须对链路层报文、网络层报文、传输层报文有所了解。下面我们就进行概述。</p>

<p>为了保证这个系列的博文不发生偏差，我们只讲解其中需要用到的属性和含义，如果读者对网络的核心原理有兴趣，可以读读《TCP/IP 详解，卷 1：协议》这本书。</p>

<h2 id="toc_2"><a></a>2.1、链路层报文</h2>

<p>链路层的数据格式有一个共同特点，都包括目标 MAC 地址和源 MAC 地址。下面这个图主要说明了我们最常用的 Ethernet 帧（以太帧）的报文格式：</p>

<p><img src="https://img-blog.csdn.net/20150716193001867" alt=""/></p>

<ul>
<li><p>目标 MAC 地址 / 源 MAC 地址：00:00:00:00:00:00——FF:FF:FF:FF:FF:FF 这个范围是全球 MAC 地址的可用范围。一张物理网卡肯定有一个唯一的 MAC 地址。实际上网络层常用的 IP 协议，就是基于 MAC 地址的。一个子网范围内某个 IP 对应的 MAC 地址是通过 ARP 查询协议从 NAT 设备（可能是路由器、交换机或者网络代理设备）上查询得到的。</p></li>
<li><p>上层协议类型：链路层的报文是为了承载网络层的协议而存在的，所以链路层的数据格式中需要有一个属性说明这个链路层所承载的上层协议是什么类型。</p>

<ul>
<li>  IPv4: 0x0800</li>
<li>  ARP:0x0806</li>
<li>  PPPoE:0x8864</li>
<li>  IPV6: 0x86DD</li>
</ul></li>
<li><p>封装的上层数据：最多可以有 1500 个字节。</p></li>
</ul>

<p>请记住这个链路层的数据格式，因为讲到 LVS-DR 方式的时候，主要就是对链路层的数据格式进行修改，而不会对 IP 数据报文和 TCP 数据报文进行修改。</p>

<h2 id="toc_3"><a></a>2.2、网络层 IP 报文</h2>

<ul>
<li><p>TCP 协议和 IP 协议是两种不同的协议。对应的，也就是两种不同的描述格式。</p></li>
<li><p>IP 协议是网络层协议，顾名思义，就是用来描述整个网络构成情况的；TCP 协议是通讯层协议，是用来表示两个或多个网络上的点如何进行通信和当前通信状态的。</p></li>
<li><p>这两种协议有很多共同特点，例如这两种协议都分为 “头部” 和“数据部”；针对 IP 协议来说，TCP 协议的描述就存放在其“数据部”。</p></li>
</ul>

<p>我们首先来看看 IP 协议是怎么描述的：</p>

<p><img src="https://img-blog.csdn.net/20150712092543018" alt=""/></p>

<p>好吧，图我是直接从百度百科直接粘过来的，因为要我来画，结构也是这样 <sup>_<sup>。其中有几个重要的我们后面要使用的属性，要给大家说一下：</sup></sup></p>

<ul>
<li><p>header.version：IP 协议的版本号，你猜对了就是 IPV4 还是 IPV6。4 表示 IPV4 版本，6 表示 IPV6。你要问我什么是 IPV4，还是 IPV6：192.168.220.141，这就是 IPV4 的格式；FE80:0000:0000:0000:AAAA:0000:00C2:0002，这就是 IPV6 的格式。</p></li>
<li><p>header.Total Length：总长度，这个总长度是 IP 头和 IP 数据两个区域的总长度。主要还是用于生成头验证码和为了操作系统处理方便。</p></li>
<li><p>header.IP Flags：这个位置有 3 位 000，但实际只有后两位才有值 010，这个就是 “D” 位 = 1，这个时候表示由于要传输的整个数据不大，所以这个 IP 数据报的数据部分已经描述了整个数据描述，不需要进行 IP 数据报的分片；”D”为 = 0，表示要传输的整个数据比较大，所以 IP 数据报进行了拆分，这个时候就要用到最后一位了：最后一位 “M” 中“1”表示还有后续分片；0 表示这个数据报就是 IP 分片的最后一个数据片了。</p></li>
<li><p>header.Protocol：IP 协议是网络层协议，在网络层以上是传输层协议。TCP、UDP、ICMP 和 IGMP 是传输层协议。这个位置的 8 位说明 IP 协议数据部分携带的是哪种上层协议。</p></li>
<li><p>header.Source Address：这个当然就是 IP 数据报的来源地址咯。</p></li>
<li><p>header. Destination Address：这个当然就是 IP 数据报的目标地址咯。</p></li>
<li><p>header.checksum：首部校验值。这个值校验 IP 数据报首部的传输完整性（注意校验不包括 IP 数据报的数据部分）。这就意味着 NAT 设备重写这个数据报的来源或者目标 IP 后，校验值要重新进行计算。Source Address、Destination Address、Checksum 是各种 NAT 设备主要的改写属性。而且很多时候 NAT 设备只改写这三个值就可以实现 IP 数据报的转发（当然 TCP 报文中的端口也会被改写，以便在端口映射的情况下进行端口转换）。</p></li>
</ul>

<h2 id="toc_4"><a></a>2.3、传输层 TCP 报文</h2>

<p>上文已经说过，TCP 的报文信息是装在到 IP 报文的数据部分的，当成网络上进行传输的数据从 Srouce Address 传到 Destination Address 中。下面是 TCP 报文的信息：</p>

<p><img src="https://img-blog.csdn.net/20150712093802321" alt=""/></p>

<ul>
<li><p>头. 源端口号：TCP 信息来源的端口号。</p></li>
<li><p>头. 目的端口号：TCP 信息数据的目标端口。</p></li>
<li><p>头. 状态位（URG/ACK/PSH/RST/SYN/FIN）：如果您已经看到我之前写的《标准 Web 系统的架构分层》（<a href="http://blog.csdn.net/yinwenjie/article/details/46480485">http://blog.csdn.net/yinwenjie/article/details/46480485</a>）这边文章的第 3.2 小节，那么您对 ACK、SYN、FIN 这三个标记肯定不陌生，因为 TCP 的三次握手和连接中断就要用到这三个标记，需要注意 SYN SEQ 和 ACK SEQ 就是 TCP 数据报的确认号；另外解释一下 PSH 和 RST 两个状态标记。应用层的 TCP 数据报有一个缓存区，也就是说多个正确的 TCP 数据报会首先放到这个缓存区，达到一定条件后，再推送给上层的应用层协议，例如 http。PSH 为 1 的时候，表示不需要再等到后续的 TCP 数据报文了，直接将目前接收方缓存中的 tcp 数据报进行数据段组合后推送给上层协议，并且清空缓存区；RST 表示复位，您可以理解成放弃当前缓存区的所有未发送给上层协议的 TCP 数据报文，一般这种情况都是 TCP 报文传输出现了问题。</p></li>
<li><p>头. TCP 校验和：TCP 报文的校验和比起 IP 头校验要稍微复杂点。TCP 校验的输入包括三部分：TCP 伪首部、TCP 首部长度和 TCP 数据部长度。TCP 伪首部是一个虚拟概念，它包括承载 TCP 数据报文的 IP 报文的一部分，和 TCP 首部的一部分数据（源 IP、目标 IP、IP 报文中的 protocoly、以及 TCP 报文的报文头长度和 TCP 报文的数据长度）。</p></li>
</ul>

<p>从上面的描述可以看出，一旦 IP 报文中的源 IP 和目标 IP 发生改变了，TCP 报文校验信息就会改变。</p>

<h2 id="toc_5"><a></a>3、LVS 的三种工作方式</h2>

<h2 id="toc_6"><a></a>3.1、LVS-NAT 工作方式</h2>

<p>NAT 方式是一种由 LVS Master 服务节点收到数据报，然后转给下层的 Real Server 节点，当 Real Server 处理完成后回发给 LVS Master 节点然后又由 LVS Master 节点转发出去的工作方式。LVS 的管理程序 IPVSADMIN 负责绑定转发规则，并完成 IP 数据报文和 TCP 数据报文中属性的重写。请用几分钟时间仔细看看下图（为了简单，图里面只画了一个 Real Server。如果看不清楚，可点击右键 “查看原图”）：</p>

<p><img src="https://img-blog.csdn.net/20150716170216002" alt=""/></p>

<ul>
<li><p>1、在正式的机房环境中，一般有两种方式为一个机器分配外网地址：在核心交换机上直接绑定外网地址到主机网卡的，这样使用 ifconfig 命令看到的 IP 地址为外网地址；在核心交换机上使用映射规则，将一个外网地址映射到内网地址，这样使用 ifconfig 命令看到的 IP 地址为内网地址。上图中我们采用的是后一种映射规则。如果使用前一种外网 IP 的分配规则，也不会影响 LVS NAT 的工作方式，因为这个 IP 被限制在 LVS NAT 工作以外。只不过 eth1 的 IP 从 192.168.100.10 换成 100.64.92.199 而已。</p></li>
<li><p>2、我们用中文描述一下转换规则：<strong>凡是发送到 “192.168.100.10:80” 的数据报，目标地址全部改写为“192.168.220.121:8080”</strong>，所以来自于 100.64.92.199:80 的报文被改写了。被改写的属性包括：IP.header.destinationIP、IP.header.checksum、TCP.header.sourcePort、TCP.header.targetPort、TCP.header.checksum。注意 IP 报文的 Source IP 不会发生变化，还是 “互联网某个 IP”。</p></li>
<li><p>3、这个包最终被送到了 192.168.220.121 的 8080 端口进行处理，并由下层的 Real Server 生成了返回的数据报（至于这个 Real Server 是不是 “真正的 Real Server”，LVS 不会关心）。你要问它是怎么被发送过去的，请参考 ARP 查询协议。</p></li>
<li><p>4、注意：因为 LVS 服务器和 Real Server（可能有多个），组成了一个封闭的局域网。除了 LVS 节点以外，这个子网的任何节点都是无法访问外网的。所以要求 192.168.220.121 这个 Real Server 直接把数据报给 “互联网某个 IP” 这个外网地址，显然是不行的，因为在局域网中根本就找不到这个 IP。Real Server 只能将数据报返给网关，再由网关去寻找这个外网地址。整个服务器中只有 LVS 节点能够找到这个外网地址，<strong>这就是为什么在 LVS-NAT 工作模式下，所有的 Real Server 节点必须设置自己的 Gateway 为 LVS 节点的原因</strong>。</p></li>
<li><p>5、收到来源于 “192.168.220.121:8080” 的数据报文后，IPVS 又要进行数据报文的重写了。重写规则是：<strong>凡是来源于 “192.168.220.121:8080” 的数据报，源地址全部改写为“192.168.100.10:80”</strong>。于是数据报文的 Source IP、Source Port 被改写成 “192.168.100.10:80”。<strong>在外层的核心交换机（或者是机房以外的请求方）看来，LVS 接受了数据报，并进行了处理，返回了结果。它并不知道 LVS 节点的下层还有什么。</strong></p></li>
</ul>

<p>LVS-NAT 的优点在于：</p>

<ul>
<li><p>配置管理简单。LVS-NAT 的工作方式是 LVS 三种工作模式中最容易理解、最容易配置、最容易管理的工作模式。</p></li>
<li><p>节省外网 IP 资源，一般机房分配给使用者的 IP 数量是有限的，特别是您购买的机架的数量不多时。LVS-NAT 工作方式将您的系统架构封装在局域网中，只需要 LVS 有一个外网地址或外网地址映射就可以实现访问了。</p></li>
<li><p>系统架构相对封闭。在内网环境下我们对防火墙的设置要求不会很高，也相对容易进行物理服务器的运维。您可以设置来源于外网的请求需要进行防火墙过滤，而对内网请求开放访问。</p></li>
<li><p>另外改写后转给 Real Server 的数据报文，Real Server 并不会关心它的真实性，只要 TCP 校验和 IP 校验都能通过，Real Server 就可以进行处理。所以 LVS-NAT 工作模式下 Real Server 可以是任何操作系统，只要它支持 TCP/IP 协议即可。</p></li>
<li><p>当然作为 Linux 系统忠实拥护者，我并不建议使用 Window 服务器。但如果您的 Real Server 是. Net 系统，又有业务场景需要用到 LVS，那么 LVS-NAT 可能是一个不错的选择。</p></li>
</ul>

<p>LVS-NAT 的缺点是由于这种转发模式本身所造成的：</p>

<ul>
<li>  转发点就是瓶颈点。您可以想象 100 台 Real Server 将处理结果全部转到一个 LVS 进行发送是一个怎么样的场景。事实上，LVS-NAT 的极限负载是达不到 100 台 Real Server 的。</li>
</ul>

<h2 id="toc_7"><a></a>3.2、LVS-DR 工作方式</h2>

<p>LVS 的 DR 工作模式，是目前生产环境中最常用的一种工作模式，网上的资料也是最多的，有的文章对 DR 工作模式的讲解还是比较透彻的。这里我们通过图文的方式再向您介绍一下 DR 的工作模式（同样，如果看不清楚，请右键 “查看原图”）：</p>

<p><img src="https://img-blog.csdn.net/20150716210428765" alt=""/></p>

<p>上图反映了 DR 模式的整个工作过程，同样为了简单起见，这里的 Real Server 也只画了一个。如果是多个 Real Server 的话，LVS 会通过调度算法来决定发往哪台 Real Server。LVS-DR 工作模式的几个关键点在于：</p>

<ul>
<li><p>被 Real Server 处理后形成的响应报文，不再回发到 LVS 节点，而是直接路由给中心交换机然后发送出去。省去了 LVS-NAT 方式中的 LVS 回发过程。</p></li>
<li><p>LVS 节点只会改写链路层的报文封装，对网络层和传输层报文是不进行改写的。</p></li>
<li><p>有网帖说 DR 工作模式，不能跨子网，也就是说 LVS 节点和各个 Real Server 节点必须处于同一个网段中。这是为什么呢？事实又真的是这样吗？很多网络帖子没有回答这个问题，这篇文章马上回答一下（实际上章文嵩先生已经回答过这个问题）。</p></li>
<li><p>使用 DR 模式时，需要 Real Server 设置 LVS 上的 VIP 为自己的一个回环 IP，不然包会被丢弃。这又是为什么呢？很多网贴同样没有回答这个问题，好吧，我们马上回答一下。</p></li>
</ul>

<p>先来说一说上图的工作原理：</p>

<ul>
<li><p>1、同样的，我们为了演示整个生产环境中，从机房中心交换机收到一个数据报文后开始讲解。中心交换机同样采取的 IP 映射方式。但是与 LVS-NAT 方式不一样，Real Server 在机房的中心交换机上也需要绑定一个外网映射。这样保证 Real Server 回发的响应报文能够被发送到外网。</p></li>
<li><p>2、LVS 节点接收到请求报文后，会改写报文的数据链路层格式。将 Target Mac 改写成 Real Server 的 Mac，<strong>但是网络层和传输层报文不会改写</strong>，然后重新回发给交换机。这里就涉及一个问题，现在 <strong>target Mac 和 Destination IP 的对应关系的错误的，这个数据报文到了交换机后，由于这种错位的关系，是不能进行三层交换的，只能进行二层交换（一旦进行 IP 交换，数据报文的验证就会出错，被丢弃）。所以 LVS-DR 方式要求 Real Server 和 LVS 节点必须在同一个局域网内，或者这样说更确切：LVS 节点需要找到一个二层链路，将改写了 Mac 地址的报文发送给 Real Server，而不能进行三层交换的校验</strong>。这样来看，实际上 LVS 节点和 Real Server 界面不一定要在同一个子网，您用一个独立网卡独立组网，传送报文也是可行的。</p></li>
<li><p>3、通过二层交换，数据被发送到 Real Server 节点。那么 Real Server 节点怎么来判断这个包的正确性呢？首先当然是传输层 TCP/IP 报文校验没有问题，LVS-NAT 没有改写 TCP/IP，当然校验就没有问题（除非报文本身就存在问题）；然后是链路层的 MAC 地址能够被识别，这时就是回环 IP 的功劳了。对于 Real Server 节点来说，192.168.100.10 这个 VIP 就是自己的回环 IP，绑定的 MAC 也就是被 LVS 替换后的 target mac。<strong>那么 Real Server 会认为这个包是在本机运行的某一个应用程序通过回环 IP 发给自己的，所以这个包不能被丢弃，必须处理</strong>。</p></li>
<li><p>4、被处理后的生成的响应报文，被直接发送给网管。这个就没有太多的解释的了，只要保证 Real server 的默认路由设置成到核心交换机的 192.168.100.1 就 OK 了。另外，需要说明的是，<strong>由于 LVS-DR 模式并没有更改原有的 IP 报文和 TCP 报文，所以 LVS-DR 模式本身是不支持端口映射的</strong>，实际上在日常使用实践中，我们一般使用 Nginx 做端口映射，因为: 灵. 活.。</p></li>
</ul>

<p>LVS-DR 工作模式的优点在于：</p>

<ul>
<li><p>解决了 LVS-NAT 工作模式中的转发瓶颈问题，能够支撑规模更大的负载均衡场景。</p></li>
<li><p>比较耗费网外 IP 资源，机房的外网 IP 资源都是有限的，如果在正式生产环境中确实存在这个问题，可以采用 LVS-NAT 和 LVS-DR 混合使用的方式来缓解。</p></li>
</ul>

<p>LVS-DR 当然也有缺点：</p>

<ul>
<li><p>配置工作较 LVS-NAT 方式稍微麻烦一点，您至少需要了解 LVS-DR 模式的基本工作方式才能更好的指导自己进行 LVS-DR 模式的配置和运行过程中问题的解决。</p></li>
<li><p>由于 LVS-DR 模式的报文改写规则，导致 LVS 节点和 Real Server 节点必须在一个网段，因为二层交换是没法跨子网的。但是这个问题针对大多数系统架构方案来说，实际上并没有本质限制。</p></li>
</ul>

<h2 id="toc_8"><a></a>3.3、LVS-TUN 工作方式</h2>

<p>很多网络上的文章都为读者介绍 DR 和 TUN 的工作方式类似，要么就是直接讲解 DR 模式和 TUN 模式的安装配置方式，然后总结两种模式类似。那为什么有了 DR 模式后还需要 TUN 模式呢？为什么 ipvsadmin 针对两种模式的配置参数不一样呢？</p>

<p>实际上 LVS-DR 模式和 LVS-TUN 模式的工作原理完全不一样，工作场景完全不一样。DR 基于数据报文重写，TUN 模式基于 IP 隧道，后者是对数据报文的重新封装。下面我们就来讲解一下 LVS-TUN 模式的工作原理。</p>

<p>首先要介绍一个概念 IPIP 隧道。将一个完整的 IP 报文封装成另一个新的 IP 报文的数据部分，并通过路由器传送到指定的地点。在这个过程中路由器并不在意被封装的原始协议的内容。到达目的地点后，由目的地方依靠自己的计算能力和对 IPIP 隧道协议的支持，打开封装协议，取得原始协议。如下图：</p>

<p><img src="https://img-blog.csdn.net/20150717135916686" alt=""/></p>

<p>可以说隧道协议就是为了解决跨子网传输准备的，在生产环境中由于业务需要、技术需要或者安全需要，可能使用交换机进行 VLAN 隔离（即形成若干个虚拟的独立的局域网），我们可能需要 LVS 节点在局域网 A，而需要进行负载的多台 Mysql 读服务器可能在局域网 B 中。这个时候，我们就要配置 LVS 的隧道方式。LVS-TUN 模式如下图所示（注意，目标节点要能够解开隧道协议，好消息是 Linux 支持 IPIP 隧道协议）：</p>

<p><img src="https://img-blog.csdn.net/20150717160306570" alt=""/></p>

<p>上图中的线优点多，您只需要关注关心 “有箭头” 的虚线就可以了。</p>

<ul>
<li><p>1、一旦 LVS 节点发现来目标为 192.168.100.10VIP 的请求，就会使用 IPIP 隧道协议对这个请求报文进行封装。而不是像 LVS-DR 模式重写数据报文的 MAC 信息。如果配置了多个 Real Server，那么 LVS 会使用设置的调度算法确定一个 Real Server（这里为了简单，就只画了一个 Real Server 节点）。</p></li>
<li><p>2、重新封装后的 IPIP 隧道协议报文会重新被回发到路由器，路由器（或三层交换机）会根据设置的 LVAN 映射情况，找到目标服务器，并将这个 IPIP 隧道报文发送过去。</p></li>
<li><p>3、Real Server 收到这个 IPIP 隧道报文后，会将这个报文进行解包。这里注意一下，一般情况下 IPIP 隧道报文会进行分片，就如同 IP 报文分片一样，只是为了讲解方便，我们假定这个报文不需要分片。解压后得到的数据报文就是原来发送给 VIP 的请求报文。</p></li>
<li><p>4、Real Server 设置的回环 IP，让 Real Server 认为原始的请求报文是从自己本地的某个应用程序发出的，完成原始报文的校验后，它会对这个报文进行处理。剩下的过程就和 LVS-DR 相同了，这里就不再进行复述了。</p></li>
</ul>

<p>可以说 LVS-TUN 方式基本上具有 LVS-DR 的优点。在此基础上又支持跨子网间穿透。这样的敷在方案能够给我们架构师足够的系统设计场景。</p>

<h1 id="toc_9"><a></a>4、LVS 调度方式</h1>

<p>在本文第 3 节中，为了集中介绍 LVS 的三种工作模式，我们在三幅图中都为 LVS 画了一个 Real Server。但实际应用中，一般都是多个 Real Server。<strong>LVS 使用多种调度算法来决定 “当前的数据报文” 由哪个 Real Server 进行处理</strong>。在我的上篇文章《架构设计：负载均衡层设计方案（2）——Nginx 安装》（<a href="http://blog.csdn.net/yinwenjie/article/details/46620711">http://blog.csdn.net/yinwenjie/article/details/46620711</a>）中，已经花了较多的篇幅介绍了 Nginx 中的调度方式，并明确说明了这些调度方式是<strong>万变不离其宗的</strong>。</p>

<p>文章中包括了 Hash 算法，并且说明了任何属性都可以做 Hash，包括 IP、用户名等；还介绍了轮询和加权轮询，加权轮询也可以依据各种属性作为权值，例如节点的 CPU 使用情况、内存使用情况、或者管理员自己设置的一个固定权值。LVS 的调度也是这样的。</p>

<ul>
<li><p>利用一致性 Hash 算法完成调度</p>

<ul>
<li>  目标地址 Hash（DH）：调度算法根据请求的目标 IP 地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，将请求发送到该服务器，否则返回空。</li>
<li>  原地址 Hash（SH）：根据请求的源 IP 地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，将请求发送到该服务器，否则返回空。</li>
</ul></li>
<li><p>轮询调度</p>

<ul>
<li><p>最简轮询（RR）：调度算法将外部请求按顺序轮流分配到集群中的真实服务器上，它均等地对待每一台服务器，而不管服务器上实际的连接数和系统负载。</p></li>
<li><p>最少连接轮询（LC）：请注意 “最少连接轮询” 和“最少连接加权轮询”两种调度算法的区别。调度器通过 “最少连接” 调度算法动态地将网络请求调度到已建立的链接数最少的服务器上。注意请求肯定会被分配到这台目前连接数最少的 Real Server 上面，不会考虑几率问题</p></li>
</ul></li>
<li><p>加权轮询调度：</p>

<ul>
<li>  性能加权轮询（WRR）：调度算法根据真实服务器的不同处理能力来调度访问请求。这样可以保证处理能力强的服务器能处理更多的访问流量。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。</li>
<li>  最少连接数的加权轮询（WLC）：具有较高权值的服务器将承受较大比例的活动连接负载。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。注意，是按照一个比例，有较高的分配几率，而不是 LC 一样 “肯定分配”。</li>
</ul></li>
</ul>

<p>在 LVS 官方中文资料中，提到了更为完整的调度算法。可以进行参考（<a href="http://zh.linuxvirtualserver.org/node/2903">http://zh.linuxvirtualserver.org/node/2903</a>）。<strong>但一定记住各种调度算法肯定是逃不开哈希一致性、轮询、加权轮询这大思路的</strong>。</p>

<h1 id="toc_10"><a></a>5、后文介绍</h1>

<p>这篇文章我们的重点是说明 LVS 的工作原理，并详细介绍了 LVS 三种工作模式的工作过程、优缺点、应用场景。只要您理解了原理，那么下一篇介绍的 LVS 的安装和配置就是小菜一碟。我们在下一篇文章中，将介绍 LVS 三种工作模式的安装和配置方式，之后我们介绍 LVS + Keepalived 的安装和配置方式。有了这个知识基础，我们将回到 Nginx，介绍 LVS + Keepalived + Nginx 的安装和配置方式。</p>

<p><link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/markdown_views-ea0013b516.css"></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[架构设计：负载均衡层设计方案（3）——Nginx 进阶]]></title>
    <link href="http://panlw.github.io/15301797659390.html"/>
    <updated>2018-06-28T17:56:05+08:00</updated>
    <id>http://panlw.github.io/15301797659390.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://blog.csdn.net/yinwenjie/article/details/46742661">https://blog.csdn.net/yinwenjie/article/details/46742661</a></p>
</blockquote>

<p>上篇文章《架构设计：负载均衡层设计方案（2）——Nginx 安装》（<a href="http://blog.csdn.net/yinwenjie/article/details/46620711">http://blog.csdn.net/yinwenjie/article/details/46620711</a>），我们介绍了 Nginx 的核心设计思想、基本安装和使用。本来准备继续介绍 Nginx 的几个使用特性，但是奈何博文篇幅太长，只有将一篇文章拆成两篇。本文我们将承接上文，继续讲解 Nginx 的实用特性，包括 gzip 功能、rewirte 功能和一个第三方的节点监测模块。本文我们还将提到 Taobao 团队对 Nginx 的深度改造 Tengine。</p>

<h1 id="toc_0"><a></a>1、Nginx 继续进阶</h1>

<h2 id="toc_1"><a></a>1.1、gzip</h2>

<p>nginx 对返回给浏览器的 http response body 是可以进行压缩的。虽然需要消耗一点 cpu 和内存资源，但是想到 100KB 的数据量可以压缩到 60KB 甚至更小进行传输，是否有一定的吸引力？这里我的建议是，不要为了节约成本将业务服务和负载层服务放在一台物理服务器上，这样做既影响性能又增加了运维难度。http 返回数据进行压缩的功能在很多场景下都实用：</p>

<ul>
<li><p>如果浏览器使用的是 3G/4G 网络，那么流量对于用户来说就是 money。</p></li>
<li><p>压缩可节约服务器机房的对外带宽，为更多用户服务。按照目前的市场价良好的机房带宽资源的一般在 200RMB/Mbps，而服务器方案的压力往往也来自于机房带宽。</p></li>
<li><p>主要注意的是，不是 Nginx 开启了 gzip 功能，HTTP 响应的数据就一定会被压缩，除了满足 Nginx 设置的 “需要压缩的 http 格式” 以外，客户端（浏览器）也需要支持 gzip（不然它怎么解压呢），一个好消息是，目前大多数浏览器和 API 都支持 http 压缩。</p>

<p>我们首先来讲解 Nginx 中的 gzip 的设置参数，然后我们讲解当开启压缩功能后，HTTP 的交互过程和过程中关键的几个属性。我们首先来看看 Nginx 中开启 gzip 的属性（gzip 的设置放置在 HTTP 主配置区域）：</p></li>
</ul>

<pre><code class="language-conf">#开启 gzip 压缩服务，
gzip on;

#gzip 压缩是要申请临时内存空间的，假设前提是压缩后大小是小于等于压缩前的。例如，如果原始文件大小为 10K，那么它超过了 8K，所以分配的内存是 8 * 2 = 16K; 再例如，原始文件大小为 18K，很明显 16K 也是不够的，那么按照 8 * 2 * 2 = 32K 的大小申请内存。如果没有设置，默认值是申请跟原始数据相同大小的内存空间去存储 gzip 压缩结果。
gzip_buffers 2 8k;

#进行压缩的原始文件的最小大小值，也就是说如果原始文件小于 5K，那么就不会进行压缩了
gzip_min_length 5K;

#gzip 压缩基于的 http 协议版本，默认就是 HTTP 1.1
gzip_http_version 1.1;

# gzip 压缩级别 1-9，级别越高压缩率越大，压缩时间也就越长 CPU 越高
gzip_comp_level 5;

#需要进行 gzip 压缩的 Content-Type 的 Header 的类型。建议 js、text、css、xml、json 都要进行压缩；图片就没必要了，gif、jpge 文件已经压缩得很好了，就算再压，效果也不好，而且还耗费 cpu。
Igzip_types text/HTML text/plain application/x-javascript text/css application/xml;
</code></pre>

<pre><code>设置完成后，重启 nginx，即可生效。下面我们来看看浏览器和服务器进行 gzip 压缩的请求和处理返回过程（实际上在我的《标准 Web 系统的架构分层》文章中，已经有所提及）：
</code></pre>

<p><img src="https://img-blog.csdn.net/20150709162940954" alt=""/></p>

<ul>
<li><p>整个请求过程来看，开启 gzip 和不开启 gip 功能，其 http 的请求和返回过程是一致的，不同的是参数。这个可以看看我的另外一篇文章《标准 Web 系统的架构分层》<a href="http://blog.csdn.net/yinwenjie/article/details/46480485">http://blog.csdn.net/yinwenjie/article/details/46480485</a></p></li>
<li><p>当开启 HTTP 的 gzip 功能时，客户端发出 http 请求时，会通过 headers 中的 Accept-Encoding 属性告诉服务器 “我支持 gzip 解压，解压格式（算法）deflate,sdch 为：”。Accept-Encoding:gzip,deflate,sdch</p></li>
<li><p>注意，不是 request 说自己支持解压，Nginx 返回 response 数据的时候就一定会压缩。这还要看本次 Nginx 返回数据的格式是什么，如果返回数据的原始数据格式，和设置的 gzip_types 相符合，这时 Nginx 才会进行压缩。</p></li>
<li><p>Nginx 返回 response headers 是，如果数据被压缩了，就会在 Content-Encoding 属性中标示 gzip，表示接下来返回的 response content 是经过压缩的；并且在 Content-Type 属性中表示数据的原始格式。</p></li>
<li><p>最后返回经过压缩的 response content 给客户端，客户端再进行解压。这里注意一下，在客户端发送的 headers 里面，有一个 deflate,sdch。这是两种压缩算法，如果读者感兴趣，可以查查相关的资料（我建议查查，了解哈弗曼压缩算法对扩展自己的架构思路很有帮助）</p></li>
</ul>

<h2 id="toc_2"><a></a>1.2、rewrite</h2>

<p>本小结内容，假定读者了解正则表达式。如果您不清楚正则表达式，请首先 Google 或者百度，正则表达式不在我们讨论的范围内。</p>

<p>Nginx 的强大在于其对 URL 请求的重写（重定位）。Nginx 的 rewrite 功能依赖于 PCRE Lib，请一定在 Nginx 编译安装时，安装 Pcre lib。请参见我的上一篇文章《架构设计：负载均衡层设计方案（2）——Nginx 安装》<a href="http://blog.csdn.net/yinwenjie/article/details/46620711">http://blog.csdn.net/yinwenjie/article/details/46620711</a></p>

<p>我们先从讲解 rewrite 的关键语法，然后出示几个示例，由示例进行讲解。先来说一下 Nginx 中几个关键的语法：</p>

<p><strong>正在表达式匹配：</strong></p>

<ul>
<li>  ~ 区分大小写进行正则表达式匹配</li>
<li>  ~* 不区分大小写进行正则表达式匹配</li>
<li>  !~ 区分大小写进行正则表达式不匹配</li>
<li>  !~* 不区分大小写进行正则表达式不匹配</li>
</ul>

<p>举例说明:</p>

<pre><code>示例1：location ~* \.(jpg|gif|png|ioc|jpeg)$

location是Nginx中的关键字，代表当前的URL请求值。
以上表达式表示对URL进行不区分大小写的匹配，一旦URL以jpg或gif或png或ioc或jpeg结尾时，匹配成功。

示例2：$var1 ~ ^(\d+)$

var1是Nginx中使用set关键字定义的变量，以上语句代表var1和一个整数进行匹配。

</code></pre>

<p><strong>Nginx 中的全局变量：</strong><br/>
从上面的各个实例中，我们已经发现 Nginx 是支持变量的，Nginx 还内置了一些全局变量，这里列举一些比较重要的全局变量：</p>

<ul>
<li>  $content_length: 获取 request 中 header 部分的 “Content_Length” 值。</li>
<li>  $content_type: 获取 request 中 header 部分的 “Content_type” 值。</li>
<li>  $request_method: 请求方式，常用的有两种请求方式：POST、GET</li>
<li>  $remote_addr: 发送请求的客户端 ip</li>
<li>  $remote_port: 发送请求的客户端端口</li>
<li>  $request_uri: 含有参数的完整的初始 URI</li>
<li>  $server_addr: request 到达的 server 的 ip。</li>
<li>  $server_port: 请求到达的服务器的端口号。</li>
</ul>

<p><strong>rewrite 语法</strong></p>

<pre><code>rewrite regex replacement flag

#regex：表示当前匹配的正则表达式。只有$url大小写相关匹配regex正则表达式，这个$url才会被rewrite进行重定向。

#replacement：重定向目标规则。这个目标规则支持动态变量绑定，这个问题下文马上用示例来讲。

#flag：重定向规则。

</code></pre>

<p><strong>rewrite 中的 Flag 关键字</strong></p>

<ul>
<li>  redirect：通知客户端重定向到 rewrtie 后面的地址。</li>
<li>  permanent：通知客户端永久重定向到 rewrtie 后面的地址。</li>
<li>  last：将 rewrite 后的地址重新在 server 标签执行。</li>
<li>  break：将 rewrite 后地址重新在当前的 location 标签执行。</li>
</ul>

<p>实际上针对客户端来说，其效果是一样的，都是由客户端重新发起 http 请求，请求地址重新定位到 replacement 规则的 URL 地址；这里关键要讲解最常用的 last 和 break 两个关键字：</p>

<pre><code>所有的rewrite语句都是要在server中的location中书写的，如下：
server {
    。。。。。。
    。。。。。。
    location ... {
        if(...) {
            rewirte regex replacement flag;
        }
        rewirte regex replacement flag;
    }
}

那么，break关键字说明重写的replacement地址在当前location的区域马上执行。
last关键字说明重写的replacement地址在当前server所有的location中重新再做匹配。

</code></pre>

<p>下面我们结合 rewrite 关键字和 rewrite flag 关键字给出典型的示例进行讲解：</p>

<pre><code>示例1：
location ~* ^/(.+)/(.+)\.(jpg|gif|png|jpeg)$ {
    rewrite ^/orderinfo/(.+)\.(jpg|gif|png|jpeg)$   /img/$1.$2   break;
    root   /cephclient;
}

location在不进行大小写区分的情况下利用正则表达式对$url进行匹配。当匹配成功后进行rewrite重定位。
rewrite进行重写url的规则是：regex表达式第一个括号中的内容对应$1，regex表达式第二个括号中的内容对应$2，以此类推。
这样重定位的意义就很明确了：将任何目录下的文件名重定位到img目录下的对应文件名，
并且马上在这个location中（注意是Nginx，而不是客户端）执行这个重写后的URL定位。

示例2：
server {
    。。。。
    。。。。
    location ~* ^/orderinfo/(.+)\.(jpg|gif|png|jpeg)$ {
        rewrite ^/orderinfo/(.+)\.(.+)$  /img/$1.$2   last;
    }

    location / {
        root   /cephclient;
    }
}

在server中，有两个location位置，当url需要访问orderinfo目录下的某一个图片时，rewrite将重写这个url，
并且重新带入这个url到server执行，这样“location /”这个location就会执行了，并找到图片存储的目录。

</code></pre>

<h2 id="toc_3"><a></a>1.3、健康检查模块</h2>

<p>在本小节我们介绍一个用于 Nginx 对后端 UpStream 集群节点健康状态检查的第三方模块：nginx_upstream_check_module（<a href="https://github.com/yaoweibin/nginx_upstream_check_module">https://github.com/yaoweibin/nginx_upstream_check_module</a>）。这个模块有资料介绍是 TaoBao 团队开发的，但是我在 GitHua 上试图求证时并没有找到直接证据。</p>

<p>这里需要说明的是，目前有很多 Nginx 模块实现 Nginx 对后端集群节点的健康监测，不止 nginx_upstream_check_module。Nginx 官方有一个模块 healthcheck_nginx_upstreams 也可以实现对后端节点的健康监测（<a href="https://github.com/cep21/healthcheck_nginx_upstreams">https://github.com/cep21/healthcheck_nginx_upstreams</a> 有详细的安装和使用介绍）</p>

<p>我们回到对 nginx_upstream_check_module 的讲解，<strong>要使用这个第三方模块首先您需要进行下载，然后通过 patch 命令将补丁打入您原有的 Nginx 源码中，并且重新进行编译安装</strong>。下面我们来重点讲解一下这个模块的安装和使用。</p>

<p><strong>下载 nginx_upstream_check_module 模块：</strong></p>

<pre><code>wget https://codeload.github.com/yaoweibin/nginx_upstream_check_module/zip/master

您也可以直接到GitHua上进行下载，还一个在linux系统上使用git命令进行下载。

</code></pre>

<p><strong>解压安装，并补丁打入 Nginx 源码</strong></p>

<pre><code># unzip ./nginx_upstream_check_module-master.zip

注意是将补丁打入Nginx源码，不是Nginx的安装路径：

# cd ./nginx-1.6.2

# patch -p1 &lt; ../nginx_upstream_check_module-master/check_1.5.12+.patch

如果补丁安装成功，您将看到以下的提示信息：
patching file src/http/modules/ngx_http_upstream_ip_hash_module.c
patching file src/http/modules/ngx_http_upstream_least_conn_module.c
patching file src/http/ngx_http_upstream_round_robin.c
patching file src/http/ngx_http_upstream_round_robin.h

这里请注意：在nginx_upstream_check_module官网的安装说明中，有一个打补丁的注意事项：
If you use nginx-1.2.1 or nginx-1.3.0, the nginx upstream round robin
module changed greatly. You should use the patch named
&#39;check_1.2.1.patch&#39;.
If you use nginx-1.2.2+ or nginx-1.3.1+, It added the upstream
least_conn module. You should use the patch named &#39;check_1.2.2+.patch&#39;.
If you use nginx-1.2.6+ or nginx-1.3.9+, It adjusted the round robin
module. You should use the patch named &#39;check_1.2.6+.patch&#39;.
If you use nginx-1.5.12+, You should use the patch named
&#39;check_1.5.12+.patch&#39;.
If you use nginx-1.7.2+, You should use the patch named
&#39;check_1.7.2+.patch&#39;.

这里我们的Nginx的版本是1.6.2，那么就应该打入check_1.5.12+.patch这个补丁

</code></pre>

<p><strong>重新编译安装 Nginx：</strong></p>

<pre><code>注意重新编译Nginx，要使用add-module参数将这个第三方模块安装进去：

# ./configure --prefix=/usr/nginx-1.6.2/ --add-module=../nginx_upstream_check_module-master/

# make &amp;&amp; make install

</code></pre>

<p>通过以上的步骤，第三方的 nginx_upstream_check_module 模块就在 Nginx 中准备好了。接下来我们讲解一下如何使用这个模块。首先看一下 upstream 的配置信息：</p>

<pre><code>upstream cluster {
    # simple round-robin
    server 192.168.0.1:80;
    server 192.168.0.2:80;

    check interval=5000 rise=1 fall=3 timeout=4000;

    #check interval=3000 rise=2 fall=5 timeout=1000 type=ssl_hello;
    #check interval=3000 rise=2 fall=5 timeout=1000 type=http;
    #check_http_send &quot;HEAD / HTTP/1.0\r\n\r\n&quot;;
    #check_http_expect_alive http_2xx http_3xx;
}

</code></pre>

<p>上面的代码中，check 部分就是调用 nginx_upstream_check_module 模块的语法：</p>

<pre><code>check interval=milliseconds [fall=count] [rise=count]
[timeout=milliseconds] [default_down=true|false]
[type=tcp|http|ssl_hello|mysql|ajp|fastcgi]

</code></pre>

<p>interval：必要参数，检查请求的间隔时间。</p>

<p>fall：当检查失败次数超过了 fall，这个服务节点就变成 down 状态。</p>

<p>rise：当检查成功的次数超过了 rise，这个服务节点又会变成 up 状态。</p>

<p>timeout：请求超时时间，超过等待时间后，这次检查就算失败。</p>

<p>default_down：后端服务器的初始状态。默认情况下，检查功能在 Nginx 启动的时候将会把所有后端节点的状态置为 down，检查成功后，在置为 up。</p>

<p>type：这是检查通信的协议类型，默认为 http。以上类型是检查功能所支持的所有协议类型。</p>

<pre><code>check_http_send http_packet

http_packet的默认格式为：&quot;GET / HTTP/1.0\r\n\r\n&quot;

</code></pre>

<p>check_http_send 设置，这个设置描述了检查模块在每次检查时，向后端节点发送什么样的信息</p>

<pre><code>check_http_expect_alive [ http_2xx | http_3xx | http_4xx | http_5xx ]

</code></pre>

<p>这些状态代码表示服务器的 HTTP 响应上是 OK 的，后端节点是可用的。默认情况的设置是：http_2xx | http_3xx</p>

<p>当您根据您的配置要求完成检查模块的配置后，请首先使用 nginx -t 命令监测配置文件是否可用，然后在用 nginx -s reload 重启 nginx。</p>

<h2 id="toc_4"><a></a>1.4、不得不提的 tengine</h2>

<p>Tengine 是由淘宝网发起的 Web 服务器项目。它在 Nginx 的基础上，针对大访问量网站的需求，添加了很多高级功能和特性。Tengine 的性能和稳定性已经在大型的网站如淘宝网，天猫商城等得到了很好的检验。它的最终目标是打造一个高效、稳定、安全、易用的 Web 平台（<a href="http://tengine.taobao.org/">http://tengine.taobao.org/</a>）。</p>

<p>您应该懂了，我建议您根据业务的实际情况，适时在生产环境引入 Tengine。但在本博客发布时，Tengine 的 2.X 版本还不稳定，所以建议实用 1.5.2 的稳定版本。<strong>请记住 Tengine 就是经过升读改造后的 Nginx</strong>。</p>

<h1 id="toc_5"><a></a>2、后文介绍</h1>

<p>花了两篇文章的功夫，终于将我想给大家讲解的 nginx 的实用特性讲完了，但是 nginx 远远不止这些特性。后面有时间我们会再回到 Nginx，重点讲解针对 Nginx 的脚本开发，我们还会讲解 Nginx 和 Lua 的集成。但是为了不扰乱这个系列博文的时间安排，下篇文章我们将开始介绍 LVS 技术，争取用一篇文章的篇幅讲清楚 LVS 核心设计思想、单节点安装和使用。再下篇文章我们介绍 Keepalived 技术，以及 keepalived 和 LVS、Nginx 分别进行集成，敬请关注。</p>

<p><link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/markdown_views-ea0013b516.css"></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[架构设计：负载均衡层设计方案（2）——Nginx 安装]]></title>
    <link href="http://panlw.github.io/15301794059091.html"/>
    <updated>2018-06-28T17:50:05+08:00</updated>
    <id>http://panlw.github.io/15301794059091.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://blog.csdn.net/yinwenjie/article/details/46620711">https://blog.csdn.net/yinwenjie/article/details/46620711</a></p>
</blockquote>

<p>前一篇文章<a href="http://blog.csdn.net/yinwenjie/article/details/46605451">《架构设计：负载均衡层设计方案（1）——负载场景和解决方式》</a>中我们描述了要搭设负载均衡层的业务场景和负载均衡层搭建和扩展思路。从这篇文章开始的后几篇文章，我们将详细介绍 Nginx、LVS 和 Nginx+Keepalived、LVS+Keepalived 和 LVS+Nginx+Keepalived 的安装细节，以及它们的性能优化方式。</p>

<p>Nginx 和 LVS 都是可以独立工作的，Keepalived 作为检测机制，不但可以和 Nginx、LVS 集成也可以和其他软件集成形成高可用方案（例如可以和 MySQL 数据库集成、可以和 Jetty 服务器集成、还可以和自己写的程序集成）。所以首先我们先来详细讲述 Nginx 和 LVS 的核心工作原理、安装过程和优化方式，再分别讲解他们和 Keepalived 的集成方式。这样的方式应该可以使您更快的掌握其中的核心，并能最快的融会贯通。</p>

<h1 id="toc_0"><a></a>1、Nginx 重要算法介绍</h1>

<p>Nginx 是什么，请自行百度。我们先介绍几个关键的算法，如果您还不了解这些算法在 Nginx 中所起的作用，请不要着急，本文后半部分将说明它们的作用。</p>

<h2 id="toc_1"><a></a>1.1、一致性 Hash 算法</h2>

<p><img src="https://img-blog.csdn.net/20150703172857890" alt=""/></p>

<p>一致性 Hash 算法是现代系统架构中的最关键算法之一，在分布式计算系统、分布式存储系统、数据分析等众多领域中广泛应用。针对这个系列的博文，在负载均衡层、业务通信层、数据存储层都会有他的身影。</p>

<ul>
<li><p>hash 算法的关键在于它能够根据不同的属性数据，生成一串不相同的 hash 值，并且能够将这个 hash 值转换为</p>

<p><span class="MathJax" id="MathJax-Element-1-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><nobr aria-hidden="true">0—232−1</nobr><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mn>0</mn><mo>—</mo><msup><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mn>32</mn></mrow></msup><mo>−</mo><mn>1</mn></math><script type="math/tex; mode=display" id="MathJax-Element-1">0 — 2<sup>{32}-1</script>范围整数（即上图中的圆环）<mn>0</mn><mo>&amp;#x2014;</mo><msup><mn>2</mn>&lt;mrow</sup> class=&quot;MJX-TeXAtom-ORD&quot;&gt;<mn>32</mn></mrow></msup><mo>&amp;#x2212;</mo><mn>1</mn></math>&quot; role=&quot;presentation&quot;&gt;</span></p></li>
<li><p>一台服务器的某个或者某一些属性当然也可以进行 hash 计算（通常是这个服务器的 IP 地址和开放端口），并且根据计算分布在这个圆环上的某一个点。也就是图中圆环上的蓝色点。</p></li>
<li><p>一个处理请求当然也可以根据这个请求的某一个或者某一些属性进行 hash 计算（可以是这个请求的 IP、端口、cookie 值、URL 值或者请求时间等等），并且根据计算记过分布在这个圆环上的某一个点上。也就是上图圆环上的黄色点。</p></li>
<li><p>我们约定落在某一个蓝点 A 左侧和蓝点 B 右侧的黄色点所代表的请求，都有蓝点 A 所代表的服务器进行处理，这样就完成解决了 “谁来处理” 的问题。在蓝色点稳定存在的前提下，来自于同一个 Hash 约定的请求所落在的位置都是一样的，这就保证了服务处理映射的稳定性。</p></li>
<li><p>当某一个蓝色点由于某种原因下线，其所影响到的黄色点也是有限的。即下一次客户端的请求将由其他的蓝色点所代表的服务器进行处理。</p></li>
</ul>

<h2 id="toc_2"><a></a>1.2、轮询与加权轮询</h2>

<p><img src="https://img-blog.csdn.net/20150705093244323" alt=""/></p>

<ul>
<li><p>当有任务需要传递到下层节点进行处理时，任务来源点会按照一个固定的顺序，将任务依次分配到下层节点，如果下层可用的节点数量为 X，那么第 N 个任务的分配规则为：</p>

<p><span class="MathJax" id="MathJax-Element-2-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><nobr aria-hidden="true">目标节点 =(NmodX)+1</nobr> <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow class="MJX-TeXAtom-ORD"><mo>目</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo>标</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo>节</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo>点</mo></mrow> <mo>=</mo><mo stretchy="false">(</mo><mi>N</mi><mi>m</mi><mi>o</mi><mi>d</mi><mi>X</mi><mo stretchy="false">)</mo><mo>+</mo><mn>1</mn></math> <script type="math/tex; mode=display" id="MathJax-Element-2">目标节点 = (N mod X) + 1</script><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x76EE;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x6807;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x8282;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x70B9;</mo></mrow><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mi>m</mi><mi>o</mi><mi>d</mi><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mn>1</mn></math>&quot; role=&quot;presentation&quot;&gt;</span></p></li>
<li><p>轮询处理在很多架构思想中都有体现：DNS 解析多 IP 时、LVS 向下转发消息时、Nginx 向下转发消息时、Zookeeper 向计算节点分配任务时。了解基本的轮询过程有助于我们在进行软件架构设计时进行思想借鉴。</p></li>
<li><p>但是上面的轮询方式是有缺陷的，由于各种客观原因我们可能无法保证任务处理节点的处理能力都是一样的（CPU、IO、内存频率等不同）。所以 A 节点业务能同时处理 100 个任务，但是 B 节点可能同时只能处理 50 个任务。</p></li>
<li><p>在这种情况下我们需要依据下层节点某个或者多个属性设置权值。这个属性可能是网络带宽、CPU 繁忙程度或者就是各一个固定的权值。</p></li>
</ul>

<p>那么加权轮询的分配依据是什么呢？有很多分配依据，例如：概率算法（此算法中包括蒙特卡罗算法，拉斯维加斯算法和舍伍德算法，在网络上有很多介绍资料）、最大公约数法。这里我们对最大公约数算法进行介绍，因为该方法简单实用：</p>

<p><img src="https://img-blog.csdn.net/20150705113306874" alt=""/></p>

<ul>
<li><p>首先按照某种规则计算得到每个处理节点的权值，上文已经说到计算规则可能是这个服务节点的 CPU 利用率、网络占用情况或者在配置文件中的固定权重。</p></li>
<li><p>求这些权值的最大公约数，在上图中三个节点的权值分别是 100、80、60. 那么求得的最大公约数就是 20（如果您忘记了最大公约数的定义，请自行复习）。那么这三个节点的被除结果分别是 5、4、3，求和值为 12。</p></li>
<li><p>得到以上的计算结果，就可以开始进行请求分配了，公式同样为：</p>

<p><span class="MathJax" id="MathJax-Element-3-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><nobr aria-hidden="true">(NmodX)+1=Y</nobr><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mo stretchy="false">(</mo><mi>N</mi><mi>m</mi><mi>o</mi><mi>d</mi><mi>X</mi><mo stretchy="false">)</mo><mo>+</mo><mn>1</mn><mo>=</mo><mi>Y</mi></math><script type="math/tex; mode=display" id="MathJax-Element-3">(N mod X) + 1 = Y</script><br/>
其中 N 表示当前的第 N 次任务；X 表示整除后的求和结果；Y 为处理节点。<mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mi>m</mi><mi>o</mi><mi>d</mi><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mn>1</mn><mo>=</mo><mi>Y</mi></math>&quot; role=&quot;presentation&quot;&gt;</span></p></li>
</ul>

<blockquote>
<p>总结一下：加权轮询是轮询方案的补充，通过将处理节点的属性转换成权值可以有效的描述处理节点的处理能力，实现更科学的处理任务分配。加权轮询的关键在于加权算法，最大公约数算法简单实用，定位效率高。</p>
</blockquote>

<h1 id="toc_3"><a></a>2、Nginx 的安装</h1>

<h2 id="toc_4"><a></a>2.1、准备工作</h2>

<p>操作系统：centOS 6.5。</p>

<p>Nginx 的下载地址：<a href="http://nginx.org/en/download.html">http://nginx.org/en/download.html</a>。请下载 stable 的版本 1.8.0。后续 Nginx 肯定还会有升级，官网上面会持续更新 stable version。</p>

<p>最小必备组件：<br/>
yum -y install make zlib zlib-devel gcc gcc-c++ ssh libtool</p>

<h2 id="toc_5"><a></a>2.2、正式安装</h2>

<ul>
<li><p>下载 nginx1.8.0 版本<br/>
<img src="https://img-blog.csdn.net/20150705120030369" alt=""/></p></li>
<li><p>解压 nginx 的 tar 文件<br/>
<img src="https://img-blog.csdn.net/20150705120225553" alt=""/></p></li>
<li><p>进行源码编译<br/>
<img src="https://img-blog.csdn.net/20150705120707338" alt=""/></p></li>
</ul>

<p>我们看到这时编译检查报错，报错写得很清楚，为了支持 HTTP 重写模块，Nginx 需要 PCRElib 的支持。那我们到 <a href="http://ncu.dl.sourceforge.net/project/pcre/pcre/8.37/pcre-8.37.tar.gz">http://ncu.dl.sourceforge.net/project/pcre/pcre/8.37/pcre-8.37.tar.gz</a> 下载一个稳定的 pcre 版本编译安装即可（不一定是 8.37 版本）。</p>

<ul>
<li><p>再进行源码编译安装<br/>
./configure –prefix=/usr/nginx-1.8.0<br/>
make &amp;&amp; make install<br/>
<img src="https://img-blog.csdn.net/20150705130214392" alt=""/></p></li>
<li><p>整个验证、编译、安装过程不应该报任何错误。如果您使用 prefix 设置了安装目标目录，那么可能您还需要在 / etc/profix 文件中设置环境变量：<br/>
<img src="https://img-blog.csdn.net/20150705130747519" alt=""/></p></li>
</ul>

<h2 id="toc_6"><a></a>2.3、安装验证和启动</h2>

<p>下面介绍几个 nginx 常用的命令，如果您可以正常使用这些命令，那么说明 nginx 已经安装成功了。</p>

<p>nginx：直接在命令行键入 nginx，就可以启动 nginx。</p>

<p>nginx -t：检查配置文件是否正确。这个命令可以检查 nginx.conf 配置文件其格式、语法是否正确。如果配置文件存在错误，则会出现相应提示；如果 nginx.conf 文件正确，也会出现相应的成功提示。</p>

<p>nginx -s reload：重加载 / 重启 nginx——以新的 nginx.conf 配置文件中的定义。</p>

<p>nginx -s stop：停止 nginx。</p>

<h1 id="toc_7"><a></a>3、进阶</h1>

<p>Nginx 在安装完成后，不用更改任何配置信息就可以直接运行。但是很显然这不会满足我们生产环境的要求。所以我们要重点介绍 Nginx 的配置文件，以及其中重要的配置项的含义。</p>

<h2 id="toc_8"><a></a>3.1、重要配置项</h2>

<p>如果您是按照本文的描述方式安装的 Nginx，那么 Nginx 的主配置文件在：/usr/nginx-1.8.0/conf/nginx.conf 的位置，如果您在编译安装的时候并没有指定安装目录，那么 Nginx 的主配置文件在：/usr/local/nginx/conf/nginx.conf 的位置。当然您还可以在启动 Nginx 的时候使用 -c 的参数人为指定 Nginx 的配置文件位置（但是这种方式不建议）。</p>

<p>我们重新整理了 Nginx 的配置文件，将其分块，以便于讲解：</p>

<pre><code>#================================以下是全局配置项
#指定运行nginx的用户和用户组，默认情况下该选项关闭（关闭的情况就是nobody）
#user  nobody nobody;     
#运行nginx的进程数量，后文详细讲解
worker_processes  1;      
#nginx运行错误的日志存放位置。当然您还可以指定错误级别
#error_log  logs/error.log;    
#error_log  logs/error.log  notice;
#error_log  logs/error.log  info;
#指定主进程id文件的存放位置，虽然worker_processes != 1的情况下，会有很多进程，管理进程只有一个
#pid        logs/nginx.pid;    

events {
    #每一个进程可同时建立的连接数量，后问详细讲解
    worker_connections  1024;   
    #连接规则，可以采用[kqueue rtsig epoll select poll eventport ]，后文详细讲解
    use   epoll;    
}
#================================以上是全局配置项

http {
    #================================以下是Nginx后端服务配置项
    upstream backendserver1 {
        #nginx向后端服务器分配请求任务的方式，默认为轮询；如果指定了ip_hash，就是hash算法（上文介绍的算法内容）
        #ip_hash    
        #后端服务器 ip:port ，如果有多个服务节点，这里就配置多个
        server 192.168.220.131:8080; 
        server 192.168.220.132:8080;    
        #backup表示，这个是一个备份节点，只有当所有节点失效后，nginx才会往这个节点分配请求任务
        #server 192.168.220.133:8080 backup;        
        #weight，固定权重，还记得我们上文提到的加权轮询方式吧。
        #server 192.168.220.134:8080 weight=100;    
    }
    #================================以上是Nginx后端服务配置项

    #=================================================以下是 http 协议主配置
    #安装nginx后，在conf目录下除了nginx.conf主配置文件以外，有很多模板配置文件，这里就是导入这些模板文件
    include       mime.types;
    #HTTP核心模块指令，这里设定默认类型为二进制流，也就是当文件类型未定义时使用这种方式
    default_type  application/octet-stream;     
    #日志格式
    #log_format  main  &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39;   
                      &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39;
                      &#39;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#39;;

    #日志文件存放的位置
    #access_log  logs/access.log  main;         

    #sendfile 规则开启
    sendfile        on;
    #指定一个连接的等待时间（单位秒），如果超过等待时间，连接就会断掉。注意一定要设置，否则高并发情况下会产生性能问题。
    keepalive_timeout  65;                      

    #开启数据压缩，后文详细介绍
    gzip  on;                                   
    #=================================================以上是 http 协议主配置

    #=================================================以下是一个服务实例的配置
    server {
        #这个代理实例的监听端口
        listen       80;
        #server_name 取个唯一的实例名都要想半天？
        server_name  localhost; 
        #文字格式
        charset utf-8;  
        #access_log  logs/host.access.log  main;    

        #location将按照规则分流满足条件的URL。&quot;location /&quot;您可以理解为“默认分流位置”。
        location / {
            #root目录，这个html表示nginx主安装目录下的“html”目录。
            root   html;   
            #目录中的默认展示页面
            index  index.html index.htm;        
        }

        #location支持正则表达式，“~” 表示匹配正则表达式。
        location ~ ^/business/ {   
            #方向代理。后文详细讲解。
            proxy_pass http://backendserver1;   
        }

        #redirect server error pages to the static page /50x.html
        #error_page  404              /404.html;
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
    #=================================================以上是一个服务实例的配置
}

</code></pre>

<p>以上是一个完整的 nginx 配置信息，可以直接粘贴使用，但不建议您这样做。因为上文中很多关键的配置属性都还没有讲解。这样粘贴复制不利于您的工作和学习。</p>

<h3 id="toc_9"><a></a>3.1.1、use [kqueue | rtsig | epoll | select | poll]</h3>

<p>科技在发展，社会在进步，满足摩尔定义的 IT 行业更是这样。以前所有的连接都是阻断式的（blocking I/O）也就是说一个 TCP 连接线程在发出 Request 后，代码就不会再往下执行了，直到得到远端的 Response 为止；服务器端也一样的，在处理完某一个客户端的 Request 之前，其他客户端的请求都会等待。这种处理方式使客户端和服务器端的通讯性能大打折扣。</p>

<p>很明显多线程貌似能够解决这个问题：一个线程处理不了，我可以再开线程来处理啊。但是多线程是有局限性的：</p>

<ul>
<li><p>创建一个线程会消耗有限的资源。以 JAVA JVM 为例，创建一个新的线程 JVM 会单独开放 1MB 的栈内存空间（通过 - Xss 参数可设置），虽然栈内存不受 - Xmx 和 - Xms 两个参数影响，但是可以说明线程的创建是需要消耗额外资源的。</p></li>
<li><p>多线程工作时，计算机的 CPU 会耗费大量的资源让多线程在不同的状态下进行切换。在后续的文章中本书还会介绍依据这样的原理让计算机的 CPU 呈波形变化的编程方式。</p></li>
<li><p>在 Linux 操作系统下，单个用户能够创建的线程和进程总数、整个操作系统能够创建的线程总数都是有限的。通过 limit -a 命令，您可以查看相关的内核参数。</p></li>
<li><p>所以依靠线程来解决 bio 的问题是不靠谱的，只能起到缓解处理并行请求的作用。您可以想象一次并发 10 万个处理请求的问题，是不可能在计算机上同时创建 10 万个线程来解决的。</p></li>
</ul>

<p>基于上面的描述，NIO（no blocking I/O）技术就这样诞生了。依靠 event loop 机制（想看这个机制的详细分析，请持续关注我的博客 <sup>_<sup>），单个线程可以同时处理多个</sup></sup> request 请求，并在处理完产生 response 的时候，回调相关的远程事件。根据 NIO 实现机制的不同，技术名称也就不同了。我要说什么，您，应该懂了。</p>

<p>epoll、kqueue 等这些组件都是对多路复用网络 I/O 模型的实现，其中 epoll 是 poll 的升级版本，在 linux 环境下可以使用但限于 linux2.6 及以上版本。kqueue 用在 bsd 上使用。</p>

<h3 id="toc_10"><a></a>3.1.2、worker_processes 和 worker_connections</h3>

<p><strong>worker_processes</strong>：操作系统启动多少个工作进程运行 Nginx。注意是工作进程，不是有多少个 nginx 工程。在 Nginx 运行的时候，会启动两种进程，一种是主进程 master process；一种是工作进程 worker process。例如我在配置文件中将 worker_processes 设置为 4，启动 Nginx 后，使用进程查看命令观察名字叫做 nginx 的进程信息，我会看到如下结果：</p>

<p><img src="https://img-blog.csdn.net/20150707193821782" alt=""/></p>

<p>图中可以看到 1 个 nginx 主进程，master process；还有四个工作进程，worker process。主进程负责监控端口，协调工作进程的工作状态，分配工作任务，工作进程负责进行任务处理。一般这个参数要和操作系统的 CPU 内核数成倍数。</p>

<p><strong>worker_connections</strong>：这个属性是指单个工作进程可以允许同时建立外部连接的数量。无论这个连接是外部主动建立的，还是内部建立的。这里需要注意的是，一个工作进程建立一个连接后，进程将打开一个文件副本。所以这个数量还受<strong>操作系统设定的，进程最大可打开的文件数有关</strong>。网上 50% 的文章告诉了您这个事实，并要求您修改 worker_connections 属性的时候，一定要使用 ulimit -n 修改操作系统对进程最大文件数的限制，但是这样更改只能在当次用户的当次 shell 回话中起作用，并不是永久了。接着您继续 Google / 百度，发现 30% 的文章还告诉您，要想使 “进程最大可打开的文件数” 永久有效，还需要修改 / etc/security/limits.conf 这个主配置文件，但是您应该如何正确检查 “进程的最大可打开文件” 的方式，却没有说。</p>

<p>下面本文告诉您全面的、正确的设置方式:</p>

<ul>
<li><p><strong>更改操作系统级别的 “进程最大可打开文件数” 的设置</strong>：<br/>
首先您需要操作系统的 root 权限：</p>

<blockquote>
<p>叫您的操作系统管理员给您。</p>
</blockquote>

<p>修改 limits.conf 主配置文件</p>

<blockquote>
<p>vim /etc/security/limits.conf</p>
</blockquote>

<p>在主配置文件最后加入下面两句：</p>

<blockquote>
<ul>
<li>soft nofile 65535</li>
<li>hard nofile 65535</li>
</ul>
</blockquote>

<p>注意 “_” 是要加到文件里面的。这两句话的含义是 soft（应用软件）级别限制的最大可打开文件数的限制，hard 表示操作系统级别限制的最大可打开文件数的限制，“_” 表示所有用户都生效。保存这个文件（只有 root 用户能够有权限）。</p>

<p>保存这个文件后，配置是不会马上生效的，为了保证本次 shell 会话中的配置马上有效，我们需要通过 ulimit 命令更改本次的 shell 会话设置（当然您如果要重启系统，我也不能说什么）。</p>

<blockquote>
<p>ulimit -n 65535</p>
</blockquote>

<p>执行命令后，配置马上生效。您可以用 ulimit -a 查看目前会话中的所有核心配置：</p>

<blockquote>
<p>ulimit -a<br/>
core file size (blocks, -c) 0<br/>
data seg size (kbytes, -d) unlimited<br/>
scheduling priority (-e) 0<br/>
file size (blocks, -f) unlimited<br/>
pending signals (-i) 7746<br/>
max locked memory (kbytes, -l) 64<br/>
max memory size (kbytes, -m) unlimited<br/>
open files (-n) 65535<br/>
pipe size (512 bytes, -p) 8<br/>
POSIX message queues (bytes, -q) 819200<br/>
real-time priority (-r) 0<br/>
stack size (kbytes, -s) 10240<br/>
cpu time (seconds, -t) unlimited<br/>
max user processes (-u) 7746<br/>
virtual memory (kbytes, -v) unlimited<br/>
file locks (-x) unlimited</p>
</blockquote>

<p>请注意 open files 这一项。</p></li>
<li><p><strong>更改 Nginx 软件级别的 “进程最大可打开文件数” 的设置：</strong><br/>
刚才更改的只是操作系统级别的 “进程最大可打开文件” 的限制，作为 Nginx 来说，我们还要对这个软件进行更改。打开 nginx.conf 主陪文件。您需要配合 worker_rlimit_nofile 属性。如下：</p>

<blockquote>
<p>user root root;<br/>
worker_processes 4;<br/>
<strong>worker_rlimit_nofile 65535;</strong></p>

<h1 id="toc_11">error_log logs/error.log;</h1>

<h1 id="toc_12">error_log logs/error.log notice;</h1>

<h1 id="toc_13">error_log logs/error.log info;</h1>

<h1 id="toc_14">pid logs/nginx.pid;</h1>

<p>events {<br/>
use epoll;<br/>
<strong>worker_connections 65535;</strong><br/>
}</p>
</blockquote>

<p>这里只粘贴了部分代码，其他的配置代码和主题无关，也就不需要粘贴了。请注意代码行中加粗的两个配置项，请一定两个属性全部配置。配置完成后，请通过 nginx -s reload 命令重新启动 Nginx。</p></li>
<li><p><strong>验证 Nginx 的 “进程最大可打开文件数” 是否起作用：</strong></p>

<p>那么我们如何来验证配置是否起作用了呢？在 linux 系统中，所有的进程都会有一个临时的核心配置文件描述，存放路径在 / pro / 进程号 / limit。</p>

<p>首先我们来看一下，没有进行参数优化前的进程配置信息：</p>

<blockquote>
<p>ps -elf | grep nginx<br/>
1 S root 1594 1 0 80 0 - 6070 rt_sig 05:06 ? 00:00:00 nginx: master process /usr/nginx-1.8.0/sbin/nginx<br/>
5 S root 1596 1594 0 80 0 - 6176 ep_pol 05:06 ? 00:00:00 nginx: worker process<br/>
5 S root 1597 1594 0 80 0 - 6176 ep_pol 05:06 ? 00:00:00 nginx: worker process<br/>
5 S root 1598 1594 0 80 0 - 6176 ep_pol 05:06 ? 00:00:00 nginx: worker process<br/>
5 S root 1599 1594 0 80 0 - 6176 ep_pol 05:06 ? 00:00:00 nginx: worker process</p>
</blockquote>

<p>可以看到，nginx 工作进程的进程号是：1596 1597 1598 1599。我们选择一个进程，查看其核心配置信息：</p>

<blockquote>
<p>cat /proc/1598/limits</p>
</blockquote>

<p><img src="https://img-blog.csdn.net/20150707204548230" alt=""/></p>

<p>请注意其中的 Max open files ，分别是 1024 和 4096。那么更改配置信息，并重启 Nginx 后，配置信息就是下图所示了：</p>

<p><img src="https://img-blog.csdn.net/20150707204756789" alt=""/></p></li>
</ul>

<h3 id="toc_15"><a></a>3.1.3、max client 的计算方式：</h3>

<p>这个小结我们主要来说明两个在网上经常说的公式：</p>

<ul>
<li><p>max_client = worker_processes * worker_connections</p></li>
<li><p>max_client = worker_processes * worker_connections / 4</p></li>
</ul>

<p>这两个公式分别说明，在 Nginx 充当服务器（例如 nginx 上面装载 PHP）的时候，Nginx 可同时承载的连接数量是最大工作线程 * 每个线程允许的连接数量；当 Nginx 充当反向代理服务的时候，其可同时承载的连接数量是最大工作线程 * 每个线程允许的连接数量 / 4。</p>

<p>第一个问题很好理解，关键是第二个问题：为什么会除以 4。网上的帖子给出的答案是。浏览器 -&gt;Nginx、Nginx-&gt; 后端服务器、后端服务器 -&gt;Nginx、Nginx-&gt; 浏览器，所以需要除以四，我想说 TCP 协议是双向全双工协议，为什么需要这样建立连接呢？所以这个说法肯定是错误的。</p>

<p>在 nginx 官方文档上有这样一句话：</p>

<blockquote>
<p>Since a browser opens 2 connections by default to a server and nginx uses the fds (file descriptors) from the same pool to connect to the upstream backend。</p>
</blockquote>

<p>翻译成中文的描述就是，浏览器会建立两条连接到 Nginx（注意两条连接都是浏览器建立的），Nginx 也会建立对应的两条连接到后端服务器。这样就是四条连接了。</p>

<h3 id="toc_16"><a></a>3.1.4、gzip</h3>

<p>后文补讲，放在这里算是一个扣子 <sup>_^</sup></p>

<h2 id="toc_17"><a></a>3.2、健康检查模块</h2>

<p>后文补讲，放在这里算是一个扣子 <sup>_^</sup></p>

<h2 id="toc_18"><a></a>3.3、图片处理模块</h2>

<p>后文补讲，放在这里算是一个扣子 <sup>_^</sup></p>

<h2 id="toc_19"><a></a>3.4、Nginx 的 Rewrite 功能</h2>

<p>后文补讲，放在这里算是一个扣子 <sup>_^</sup></p>

<h1 id="toc_20"><a></a>4、后文介绍</h1>

<p>我原本计划一篇文章就把 Nginx 的主要特性都进行介绍，奈何 Nginx 的强大功能确实太多了。为了保证您对知识的全面消化，这边文章就写到这里了。LVS 的讲解再往后拖一周左右吧，下篇文章我们继续讲解 Nginx 的强大功能，包括 gzip 功能、强大的 rewrite 功能，以及两个扩展模块：健康检查模块和图片处理模块，至于 Nginx 集成 PHP 作为服务器的特性，为了保证这个系列文章的中心线络就不再讲了，那又是一套完整的知识体系。敬请期待我的下一篇博客，谢谢。另外，目前一周一篇文章的频率我觉得是比较合适的，后面的文章我争取保持这个速度。</p>

<p><link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/markdown_views-ea0013b516.css"></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[架构设计：负载均衡层设计方案（1）——负载场景和解决方式]]></title>
    <link href="http://panlw.github.io/15301793061134.html"/>
    <updated>2018-06-28T17:48:26+08:00</updated>
    <id>http://panlw.github.io/15301793061134.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://blog.csdn.net/yinwenjie/article/details/46605451">https://blog.csdn.net/yinwenjie/article/details/46605451</a></p>
</blockquote>

<p>在上一篇<a href="http://blog.csdn.net/yinwenjie/article/details/46480485%20%E6%A0%87%E5%87%86Web%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%9E%B6%E6%9E%84%E5%88%86%E5%B1%82">《标准 Web 系统的架构分层》</a>文章中，我们概述了 WEB 系统架构中的分层架设体系，介绍了包括负载均衡层、业务层、业务通信层、数据存储层的作用和存在意义。从本片文章开始，我们将首先详细讲解负载均衡层的架构原理和选型场景。</p>

<h1 id="toc_0"><a></a>1、不同的负载场景</h1>

<p>我们知道负载均衡层的作用是 “将来源于外部的处理压力通过某种规律 / 手段分摊到内部各个处理节点上”，那么不同的业务场景需要的负载均衡方式又是不一样的，架构师还要考虑架构方案的成本、可扩展性、运维难易度等问题。下面我们先介绍几种典型的不同业务场景，大家也可以先想一下如果是您，会怎么架设这些场景的负载均衡层。</p>

<p>需要注意的是，这个系统的文章，我们都将使用这几个典型的业务场景来讲解系统架构的设计递归设计。在后续几篇介绍负载层架构的文章中，我们也将通过这几个典型的业务场景讲解负载层的架构方案。</p>

<h2 id="toc_1"><a></a>1.1、负载场景一</h2>

<p>这是一个国家级物流园区的货运订单和物流管理系统。在物流园区内的货运代理商、合作司机（货运车辆）、园区管理员和客服人员都要使用这套系统。每日 RUV 在 1 万人次，日 PV 在 10 万左右。甲方总经理使用这套系统的原有是抱着 “试一下移动互联网对物流产品是否能起到提高效率的作用”。可以看出整个系统基本上没有访问压力，甲方对您设计的系统只有一个要求：能够保证系统以后的功能和性能扩展性。</p>

<p><img src="https://img-blog.csdn.net/20150627081654476" alt=""/></p>

<h2 id="toc_2"><a></a>1.2、负载场景二</h2>

<p>效果不错！在第一版系统架设后的 6 个月，货场丢货的情况大大减少，并且由于货车在途情况的监控，按时到达率也显著提升，货车司机也反映由于整个货场货车信息都是共享的，货车的待货时间也明显缩短。在这期间物流园中越来越多的货运代理商、货车司机都开始使用这套系统了，整个系统的访问量成线性增长。</p>

<p>物流园的总经理对整个系统的作用感到满意，决定扩大系统的使用范围，并增加新的功能。经过讨论甲方最终决定把整套系统开放给货主：或者可以在系统上查看货运代理商的线路报价、线上通知代理商上门取货、监控目前自己货品的运输状态、了解第三方签收情况。初步估计系统的日 RUV 将达到 10 万，日 PV 将突破 50 万。</p>

<p><img src="https://img-blog.csdn.net/20150627091936459" alt=""/></p>

<h2 id="toc_3"><a></a>1.3、负载场景三</h2>

<p>一年后，赞不绝口的大宗货品运输服务质量终于传到了政府领导的耳朵里。省里分管运输的领导亲自领队到物流园区参观考察，最终决定由省政府牵头，各地方政府参与，将这套管理办法在整个省级范围进行推广使用。全省 10 家大型物流园和 50 家二级物流园中的上万货运代理商、散落省内的零散代理商、10 万个人 / 企业货主、40 万优等资质车源共同接入系统。</p>

<p>新的功能上，增加了费用结算和运费保障功能，从货主预付款开始到第三方确认收货的整个环节都进行费用管理。为了保证线上收货环节的顺利，新版本中还增加了代理商之间的合作收货功能。新系统的日 RUV 将超过 50 万，日 PV 将突破 250 万。</p>

<p><img src="https://img-blog.csdn.net/20150627103725131" alt=""/></p>

<h2 id="toc_4"><a></a>1.4、负载场景四</h2>

<p>服务效应、经济效应、口碑效应不断发酵，经过近两年多的发展，目前这套系统已经是省内知名的物流配送平台，专门服务大宗货运物流。联合政府向全国推广服务的时机终于到来。预计全国 1000 多个物流园区，50 万左右物流代理商，500 万货运车辆、数不清的个人和企业货主都将使用该系统。预估的 RUV 和 PV 是多少呢？无法预估，如果按照全国 32 省来进行一个简单的乘法，是可以得到一个大概的值（50 万 * 32 = 1500 万 +；500 万 * 32 = 1.5 亿 +，已经超过了 JD.com 的平峰流量），但是各省的物流业规模是不一样的，从业者数量也不一样，所以这样的预估并不科学。而且再这样的系统规模下我们应该更过的考虑系统的峰值冗余。</p>

<p>业务功能的情况：为了保证注册货车的有效性，您所在的公司被政府允许访问政府的车辆信息库，在车辆注册的过程中进行车辆信息有效性的验证（第三方系统接口调用，我们并不知道第三方系统是否能够接收一个较高水平的并发量，所以这个问题留给我我们的架构师，我们将在业务层讲解时进行详细的描述）。</p>

<h2 id="toc_5"><a></a>1.5、沉思片刻</h2>

<p>看到这里，我们已经将几个递进的业务场景进行了详细的说明（甚至在后文中我们讨论业务层、业务通信层、数据存储层时所涉及的业务场景也不会有什么大的变化了）。看客们看到这里，可以稍作休息，先想想如果是您，您会如何搭建负载层，甚至整个系统的顶层架构。</p>

<blockquote>
<p>由于整个系统的性能除了和硬件有关外，业务层的拆分规则，代码质量，缓存技术的使用方式，数据库的优化水平都可能对其产生影响。所以：</p>

<p><strong>我们在讨论负载层的几篇文章中，我们要假设系统架构中各层的设计都没有对系统性能产生瓶颈</strong></p>
</blockquote>

<p>如果您已经思考好了，那么可以继续看以下的内容。</p>

<h1 id="toc_6"><a></a>2、负载方案构想</h1>

<h2 id="toc_7"><a></a>2.1、解决方案一：独立的 Nginx/Haproxy 方案</h2>

<p>很显然，第一个业务场景下，系统并没有多大的压力就是一套简单业务系统，日访问量也完全没有 “有访问压力” 这样的说法。但是客户有一个要求值得我们关注：要保证系统以后的功能和性能扩展性。为了保证功能和性能扩展性，在系统建立之初就要有一个很好的业务拆分规划，例如我们首先会把用户信息权限子系统和订单系统进行拆分，独立的车辆信息和定位系统可能也需要拆分出来。</p>

<p>这也是我们在系统建立时就要引入负载均衡层的一个重要原因。也是负载均衡层的重要作用之一。如下图所示：</p>

<p><img src="https://img-blog.csdn.net/20150630091204501" alt=""/></p>

<p>可以看出，这时负载均衡层只有一个作用，就是按照设定的访问规则，将访问不同系统的请求转发给对应的系统，并且在出现错误访问的情况下转发到错误提示页面。</p>

<h2 id="toc_8"><a></a>2.2、解决方案二：Nginx/Haproxy + Keepalived 方案</h2>

<p>此后，系统的访问压力进一步加大，系统的稳定性越来越受到我们的关注。所以在单节点处理还能满足业务要求的情况下，我们为负载层（还有各层）引入热备方案，以保证一个节点在崩溃的情况下，另一个节点能够自动接替其工作，为工程师解决问题赢得时间。如下图所示：</p>

<p><img src="https://img-blog.csdn.net/20150630091331246" alt=""/></p>

<h2 id="toc_9"><a></a>2.3、解决方案三：LVS（DR）+ Keepalived+ Nginx 方案</h2>

<p>在第三版本架构方案中，为了保证负载层足够稳定的状态下，适应更大的访问吞吐量还要应付可能的访问洪峰，我们加入了 LVS 技术。LVS 负责第一层负载，然后再将访问请求转发到后端的若干台 Nginx 上。LVS 的 DR 工作模式，只是将请求转到后端，后端的 Nginx 服务器必须有一个外网 IP，在收到请求并处理完成后，Nginx 将直接发送结果到请求方，不会再经 LVS 回发（具体的 LVS 工作原理介绍将在后文中详细介绍）。</p>

<p><img src="https://img-blog.csdn.net/20150630091527570" alt=""/></p>

<p>这里要注意的是：</p>

<ul>
<li><p>有了上层的 LVS 的支撑 Nginx 就不再需要使用 Keepalived 作为热备方案。因为首先 Nginx 不再是单个节点进行负载处理，而是一个集群多台 Nginx 节点；另外 LVS 对于下后端的服务器自带基于端口的健康检查功能；</p></li>
<li><p>LVS 是单节点处理的，虽然 LVS 是非常稳定的，但是为了保证 LVS 更稳定的工作，我们还是需要使用 Keepalived 为 LVS 做一个热备节点，以防不时之需。</p></li>
</ul>

<h2 id="toc_10"><a></a>2.4、解决方案四：DNS 轮询 + LVS（DR）+ Keepalived + Nginx 方案</h2>

<p><img src="https://img-blog.csdn.net/20150627232428537" alt=""/></p>

<p>场景四中，为了满足平均上亿的日 PV 访问，在对业务进行外网暴露的基础上，我们在互联网的最前端做了一个 DNS 轮询。然后将（对用户信息系统）访问压力首先分摊到两个对称 LVS 组上，再由每个组向下继续分拆访问压力。</p>

<p>注意上图的负载层方案的不同：</p>

<ul>
<li><p>首先我们不在像前面的方案中，使用目录名分割业务系统了，而是直接将业务系统的访问使用不同的二级域名进行拆分。这样的变化有利于每个业务系统都拥有自己独立的负载均衡层。</p></li>
<li><p>请注意上图中的细节，这个负载均衡层是专门为 “用户信息子系统” 提供负载均衡支撑的，而可能还存在的 “订单子系统”、“车辆信息子系统” 都会有他们独立的负载均衡层。</p></li>
<li><p>在 LVS 下方的 Nginx 服务可以实现无限制的扩展，同样的就像场景三种所给出的解决方案一样，Nginx 本身不在需要 Keepalived 保持热备，而是全部交由上层的 LVS 进行健康情况检查。而即使有一两台 Nginx 服务器出现故障，对整个负载集群来说问题也不大。</p></li>
</ul>

<p>方案扩展到了这一步，LVS 层就没有必要再进行扩展新的节点了。为什么呢？根据您的业务选择的合适的 LVS 工作模式，两个 LVS 节点的性能足以支撑地球上的所有核心 WEB 站点。如果您对 LVS 的性能有疑惑，请自行谷歌百度。这里我们提供了一份参考资料：《LVS 性能，转发数据的理论极限》<a href="http://www.zhihu.com/question/21237968">http://www.zhihu.com/question/21237968</a></p>

<h1 id="toc_11"><a></a>3、为什么没有独立的 LVS 方案</h1>

<p>在下一篇文章《架构设计：负载均衡层设计方案（2）——LVS、keepalived、Nginx 安装和核心原理解析》中我们将提到这个问题。实际上通过本篇文章的架构演进分析，一些读者都可以看出端倪。如果用一句话说明其中的原因，那就是 LVS 为了保证其性能对配置性有所牺牲，单独使用的话往往无法满足业务层对负载层灵活分配请求的要求。</p>

<h1 id="toc_12"><a></a>4、说明</h1>

<h2 id="toc_13"><a></a>4.1、术语说明</h2>

<ul>
<li><p>TPS: 衡量业务层处理性能的重要指标（每秒钟 request / 事务的处理数量）。业务服务处理一个完整的业务过程，并向上层返回处理结果的过程就是一个 request / 事务。那么在一秒钟内整个业务系统能够完成多少个这样的过程，其衡量单位就是 TPS。TPS 不但和系统架构有很大的关系（特别是业务层和业务通信层的架构祥泰），和物理环境、代码质量的关系也非常密切。</p></li>
<li><p>PV: 网页浏览数是评价网站流量最常用的指标之一，简称为 PV。Page Views 中的 Page 一般是指普通的 html 网页，也包含 php、jsp 等动态产生的 html 内容。注意是完整的显示一个 Page 成为一个 PV。但是一个 PV，一般需要多次 HTTP 请求，以便获取多个静态资源，这是需要注意的。</p></li>
<li><p>UV: Unique Visitor 一个独立 IP，在一个单位时间内（例如一日 / 一小时）对系统的一个 PV 请求，成为一个 UV（重复的 PV 不在进行计数）。</p></li>
<li><p>RUV: Repeat User Visitor 一个独立用户，在一个单位时间内（例如一日 / 一小时）对系统的一个 PV 请求，并且重复的访问要进行计数。</p></li>
</ul>

<h2 id="toc_14"><a></a>4.2、后文介绍</h2>

<p>下篇文章《架构设计：负载均衡层设计方案（2）——LVS、keepalived、Nginx 安装和核心原理解析》中，我们将提到 LVS 的工作原理和 Nginx 中核心负载策略的工作原理（Hash、轮询、权重），以及独立的 LVS、Nginx、keepalived 的安装方式（虽然安装方式在网络上一个搜索就能搜出来一大把，但是很对都是抄袭，且没有经过详细的正误检查，所以我觉得还是需要重新进行一下说明）</p>

<p><link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/markdown_views-ea0013b516.css"></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[hbase 的架构及设计]]></title>
    <link href="http://panlw.github.io/15299316548658.html"/>
    <updated>2018-06-25T21:00:54+08:00</updated>
    <id>http://panlw.github.io/15299316548658.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://blog.csdn.net/liweisnake/article/details/78086262">https://blog.csdn.net/liweisnake/article/details/78086262</a></p>
</blockquote>

<p>hbase 是强一致性的海量数据库，无论是读写性能，或是数据容量，还是一致性方面，hbase 都有非常优秀的表现。</p>

<p>本文从架构方面探讨 hbase 的主要设计，从而在需要 hbase 的场合能够更好的设计和判断。</p>

<p>首先，先来看看 <strong>hbase 的整体架构</strong>。除了 DFS 组件，hbase 的基本组件图实际上就是 Zookeeper，HMaster，RegionServer。</p>

<p><img src="https://img-blog.csdn.net/20171007104837576?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWlzbmFrZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""/></p>

<p>其中，<strong>RegionServer</strong> 作为数据的实际存取服务器，主要负责数据的最终存取，一般情况都是多台；</p>

<p>RegionServer 根据不同的 row key 划分为许多 region，每个 region 按顺序存放从 startKey 到 endKey 的数据。每个 RegionServer 有下面这些组件：</p>

<p><img src="https://img-blog.csdn.net/20171015092617159?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWlzbmFrZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""/><img src="https://img-blog.csdn.net/20171007212610301?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWlzbmFrZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""/></p>

<p><strong>一个 WAL</strong>: write ahead log. 听名知其意，该文件是落库前先写的日志文件，它最主要的作用是恢复数据用，类似于 mysql 的 binlog。保存在 HDFS 中</p>

<p><strong>一个 BlockCache</strong>: regionServer 的读缓存。保存使用最频繁的数据，使用 LRU 算法换出不需要的数据。</p>

<p><strong>多个 Region</strong>: 每个 region 包含多个 store，每个 CF 拥有一个 store</p>

<p><strong>store</strong>: 每个 store 包含多个 storeFile 和一个 memstore</p>

<p><strong>Memstore</strong>: region 的写缓存。保存还未写入 HFile 的数据，写入数据前会先做排序，每个 region 每个 CF 都会拥有一个 Memstore，这就是为什么 CF 不能建太多的原因。</p>

<p><strong>storeFile</strong>: 真正存储 keyvalue 数据的文件，其保存的文件是排序过的。一个 storeFile 对应一个 HFile。保存在 HDFS 中</p>

<p>HFile 分为数据块，索引块，bloom 过滤器以及 trailer。</p>

<p><img src="https://img-blog.csdn.net/20171015100418759?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWlzbmFrZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""/></p>

<p><strong>Trailer</strong> 主要记录了 HFile 的基本信息，各个部分的偏移和地址信息。</p>

<p><strong>Data block</strong> 主要存储用户的 key-value 数据</p>

<p><strong>Bloom filter</strong> 主要用来快速定位文件是否不在数据块。</p>

<p><img src="https://img-blog.csdn.net/20171015103532031?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWlzbmFrZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""/></p>

<p><img src="https://img-blog.csdn.net/20171015103549504?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWlzbmFrZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""/></p>

<p>比较容易混淆的是 <strong>zookeeper 和 hmaster</strong>。</p>

<p><strong>Zookeeper</strong> 负责保持多台 Hmaster 中只有一台是活跃的；存储 Hbase 的 schema，table，CF 等元信息；存储所有的 region 入口；监控 regionServer 的状态，并将该信息通知 hmaster。可以看出来，zookeeper 几乎是负责整个集群的关键信息存取以及关键状态监控。<strong>如果 zookeeper 挂了，那么整个 hbase 集群几乎就是不可用的状态</strong>。</p>

<p><img src="https://img-blog.csdn.net/20171007233209495?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWlzbmFrZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""/></p>

<p><strong>Hmaster</strong> 则是负责对 table 元数据的管理；对 HRegion 的负载均衡，调整 HRegion 的布局，比如分裂和合并；包括恢复数据的迁移等。Hmaster 相当于对 RegionServer 的后台管理，对于一些定制的管理行为，zookeeper 不可能帮你完成，于是乎才有了 hmaster。<strong>如果 hmaster 挂了，除了不能对 table 进行管理配置，不能扩展 region，并不会影响整体服务的可用性</strong>。</p>

<p>接下来我们来关注一些关键流程。</p>

<p><strong>客户端首次读写的流程</strong>：</p>

<p>1. 客户端首先从 zookeeper 中得到 META table 的位置，根据 META table 的存储位置得到具体的 RegionServer 是哪台</p>

<p>2. 询问具体的 RegionServer</p>

<p><strong>写流程</strong>：</p>

<p>1. 首先写入 WAL 日志，以防 crash。</p>

<p>2. 紧接着写入 Memstore，即写缓存。由于是内存写入，速度较快。</p>

<p>3. 立马返回客户端表示写入完毕。</p>

<p>4. 当 Memstore 满时，从 Memstore 刷新到 HFile，磁盘的顺序写速度非常快，并记录下最后一次最高的 sequence 号。这样系统能知道哪些记录已经持久化，哪些没有。</p>

<p><strong>读流程</strong>：</p>

<p>1. 首先到读缓存 BlockCache 中查找可能被缓存的数据</p>

<p>2. 如果未找到，到写缓存查找已提交但是未落 HFile 的数据</p>

<p>3. 如果还未找到， 到 HFile 中继续查找数据</p>

<p><img src="https://img-blog.csdn.net/20171008110233034?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWlzbmFrZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""/></p>

<p><strong>数据紧凑</strong>：</p>

<p>数据从 memStore 刷新到 HFile 时，为了保持简单，都是每个 memStore 放一个 HFile，这会带来大量小 HFile 文件，使得查询时效率相对较低，于是，采用数据紧凑的方式将多个小文件压缩为几个大文件。其中，minor compaction 是自动将相关的小文件做一些适当的紧凑，但不彻底；而 major compaction 则是放在午夜跑的定时任务，将文件做最大化的紧凑。</p>

<p><img src="https://img-blog.csdn.net/20171008114505743?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWlzbmFrZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""/></p>

<p><img src="https://img-blog.csdn.net/20171008114703447?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWlzbmFrZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""/></p>

<p><strong>数据恢复流程</strong>：</p>

<p>当 RegionServer 挂了，zookeeper 很快就能检测到，于是将其下的 region 状态设置为不可用。Hmaster 随即开始恢复的流程。</p>

<p>1. HFile 本身有 2 个备份，而且有专门的 HDFS 来管理其下的文件。因此对 HFile 来说并不需要恢复。</p>

<p>2. Hmaster 重置 region 到新的 regionServer</p>

<p>3. 之前在 MemStore 中丢失的数据，通过 WAL 分裂先将 WAL 按照 region 切分。切分的原因是 WAL 并不区分 region，而是所有 region 的 log 都写入同一个 WAL。</p>

<p>4. 根据 WAL 回放并恢复数据。回放的过程实际上先进 MemStore，再 flush 到 HFile</p>

<p><img src="https://img-blog.csdn.net/20171008154723461?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWlzbmFrZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""/></p>

<p><img src="https://img-blog.csdn.net/20171008154745990?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWlzbmFrZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""/></p>

<h2 id="toc_0">参考资料</h2>

<ul>
<li><a href="https://yq.aliyun.com/articles/54410">我们为什么选择 hbase</a></li>
<li><a href="https://mapr.com/blog/in-depth-look-hbase-architecture/">hbase 架构</a></li>
<li><a href="http://hbasefly.com/2016/10/29/hbase-regionserver-recovering/">HBase 原理－RegionServer 宕机数据恢复</a></li>
<li><a href="http://hbasefly.com/2016/03/25/hbase-hfile/">HBase – 存储文件 HFile 结构解析</a></li>
<li><a href="http://blog.cloudera.com/blog/2012/06/hbase-io-hfile-input-output/">Apache HBase I/O – HFile</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java中的十个"单行代码编程"(One Liner)]]></title>
    <link href="http://panlw.github.io/15298011985624.html"/>
    <updated>2018-06-24T08:46:38+08:00</updated>
    <id>http://panlw.github.io/15298011985624.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>译文: <a href="http://www.rowkey.me/blog/2017/09/09/java-oneliners/">http://www.rowkey.me/blog/2017/09/09/java-oneliners/</a><br/>
原文: <a href="https://github.com/aruld/java-oneliners/wiki">https://github.com/aruld/java-oneliners/wiki</a></p>
</blockquote>

<p>本文列举了十个使用一行代码即可独立完成 (不依赖其他代码) 的业务逻辑，主要依赖的是 Java8 中的 Lambda 和 Stream 等新特性以及 try-with-resources、JAXB 等。</p>

<ol>
<li><p>对列表 / 数组中的每个元素都乘以 2</p>

<pre><code class="language-java"> // Range是半开区间
 int[] ia = range(1, 10).map(i -&gt; i * 2).toArray();
 List&lt;Integer&gt; result = range(1, 10).map(i -&gt; i * 2).boxed().collect(toList());
</code></pre></li>
<li><p>计算集合 / 数组中的数字之和</p>

<pre><code class="language-java"> range(1, 1000).sum();
 range(1, 1000).reduce(0, Integer::sum);
 Stream.iterate(0, i -&gt; i + 1).limit(1000).reduce(0, Integer::sum);
 IntStream.iterate(0, i -&gt; i + 1).limit(1000).reduce(0, Integer::sum);
</code></pre></li>
<li><p>验证字符串是否包含集合中的某一字符串</p>

<pre><code class="language-java">final List&lt;String&gt; keywords = Arrays.asList(&quot;brown&quot;, &quot;fox&quot;, &quot;dog&quot;, &quot;pangram&quot;);
final String tweet = &quot;The quick brown fox jumps over a lazy dog. #pangram http://www.rinkworks.com/words/pangrams.shtml&quot;;

keywords.stream().anyMatch(tweet::contains);
keywords.stream().reduce(false, (b, keyword) -&gt; b || tweet.contains(keyword), (l, r) -&gt; l || r);
</code></pre></li>
<li><p>读取文件内容</p>

<blockquote>
<p>原作者认为 try with resources 也是一种单行代码编程。</p>
</blockquote>

<pre><code class="language-java"> try (BufferedReader reader = new BufferedReader(new FileReader(&quot;data.txt&quot;))) {
   String fileText = reader.lines().reduce(&quot;&quot;, String::concat);
 }

 try (BufferedReader reader = new BufferedReader(new FileReader(&quot;data.txt&quot;))) {
   List&lt;String&gt; fileLines = reader.lines().collect(toCollection(LinkedList&lt;String&gt;::new));
 }

 try (Stream&lt;String&gt; lines = Files.lines(new File(&quot;data.txt&quot;).toPath(), Charset.defaultCharset())) {
   List&lt;String&gt; fileLines = lines.collect(toCollection(LinkedList&lt;String&gt;::new));
 }
</code></pre></li>
<li><p>输出歌曲《Happy Birthday to You!》 - 根据集合中不同的元素输出不同的字符串</p>

<pre><code class="language-java"> range(1, 5).boxed().map(i -&gt; { out.print(&quot;Happy Birthday &quot;); if (i == 3) return &quot;dear NAME&quot;; else return &quot;to You&quot;; }).forEach(out::println);
</code></pre></li>
<li><p>过滤并分组集合中的数字</p>

<pre><code class="language-java"> Map&lt;String, List&lt;Integer&gt;&gt; result = Stream.of(49, 58, 76, 82, 88, 90).collect(groupingBy(forPredicate(i -&gt; i &gt; 60, &quot;passed&quot;, &quot;failed&quot;)));
</code></pre></li>
<li><p>获取并解析 xml 协议的 Web Service</p>

<pre><code class="language-java">FeedType feed = JAXB.unmarshal(new URL(&quot;http://search.twitter.com/search.atom?&amp;q=java8&quot;), FeedType.class);
JAXB.marshal(feed, System.out);
</code></pre></li>
<li><p>获得集合中最小 / 最大的数字</p>

<pre><code class="language-java"> int min = Stream.of(14, 35, -7, 46, 98).reduce(Integer::min).get();
 min = Stream.of(14, 35, -7, 46, 98).min(Integer::compare).get();
 min = Stream.of(14, 35, -7, 46, 98).mapToInt(Integer::new).min();

 int max = Stream.of(14, 35, -7, 46, 98).reduce(Integer::max).get();
 max = Stream.of(14, 35, -7, 46, 98).max(Integer::compare).get();
 max = Stream.of(14, 35, -7, 46, 98).mapToInt(Integer::new).max();
</code></pre></li>
<li><p>并行处理</p>

<pre><code class="language-java"> long result = dataList.parallelStream().mapToInt(line -&gt; processItem(line)).sum();
</code></pre></li>
<li><p>集合上的各种查询 (LINQ in Java)</p>

<pre><code class="language-java">List&lt;Album&gt; albums = Arrays.asList(unapologetic, tailgates, red);

//筛选出至少有一个track评级4分以上的专辑，并按照名称排序后打印出来。
albums.stream()
  .filter(a -&gt; a.tracks.stream().anyMatch(t -&gt; (t.rating &gt;= 4)))
  .sorted(comparing(album -&gt; album.name))
  .forEach(album -&gt; System.out.println(album.name));

//合并所有专辑的track
List&lt;Track&gt; allTracks = albums.stream()
  .flatMap(album -&gt; album.tracks.stream())
  .collect(toList());

//根据track的评分对所有track分组
Map&lt;Integer, List&lt;Track&gt;&gt; tracksByRating = allTracks.stream()
  .collect(groupingBy(Track::getRating));
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Open Sourcing Zuul 2]]></title>
    <link href="http://panlw.github.io/15295434052821.html"/>
    <updated>2018-06-21T09:10:05+08:00</updated>
    <id>http://panlw.github.io/15295434052821.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://medium.com/netflix-techblog/open-sourcing-zuul-2-82ea476cb2b3">https://medium.com/netflix-techblog/open-sourcing-zuul-2-82ea476cb2b3</a></p>
</blockquote>

<p>We are excited to announce the open sourcing of <a href="https://github.com/netflix/zuul/">Zuul 2</a>, Netflix’s cloud gateway. We use Zuul 2 at Netflix as the front door for all requests coming into Netflix’s cloud infrastructure. Zuul 2 significantly improves the architecture and features that allow our gateway to handle, route, and protect Netflix’s cloud systems, and helps provide our 125 million members the best experience possible. The Cloud Gateway team at Netflix runs and operates more than 80 clusters of Zuul 2, sending traffic to about 100 (and growing) backend service clusters which amounts to more than 1 million requests per second. Nearly all of this traffic is from customer devices and browsers that enable the discovery and playback experience you are likely familiar with.</p>

<p>This post will overview Zuul 2, provide details on some of the interesting features we are releasing today, and discuss some of the other projects that we’re building with Zuul 2.</p>

<h3 id="toc_0">How Zuul 2 Works</h3>

<p>For context, here’s a high-level diagram of Zuul 2’s architecture:</p>

<p><img src="https://cdn-images-1.medium.com/max/1600/0*ycjEWsSKCaPemEg3." alt=""/></p>

<p>The Netty handlers on the front and back of the filters are mainly responsible for handling the network protocol, web server, connection management and proxying work. With those inner workings abstracted away, the filters do all of the heavy lifting. The inbound filters run before proxying the request and can be used for authentication, routing, or decorating the request. The endpoint filters can either be used to return a static response or proxy the request to the backend service (or origin as we call it). The outbound filters run after a response has been returned and can be used for things like gzipping, metrics, or adding/removing custom headers.</p>

<p>Zuul’s functionality depends almost entirely on the logic that you add in each filter. That means you can deploy it in multiple contexts and have it solve different problems based on the configurations and filters it is running.</p>

<p>We use Zuul at the entrypoint of all external traffic into Netflix’s cloud services and we’ve started using it for routing internal traffic, as well. We deploy the same core but with a substantially reduced amount of functionality (i.e. fewer filters). This allows us to leverage load balancing, self service routing, and resiliency features for internal traffic.</p>

<h3 id="toc_1">Open Source</h3>

<p>The Zuul code that’s running today is the most stable and resilient version of Zuul yet. The various phases of evolving and refactoring the codebase have paid dividends and we couldn’t be happier to share it with you.</p>

<p>Today we are releasing many <a href="https://github.com/Netflix/zuul/wiki/Core-Features">core features</a>. Here are the ones we’re most excited about:</p>

<h4 id="toc_2">Server Protocols</h4>

<ul>
<li>  <strong>HTTP/2</strong> — full server support for inbound HTTP/2 connections</li>
<li>  <strong>Mutual TLS</strong> — allow for running Zuul in more secure scenarios</li>
</ul>

<h4 id="toc_3">Resiliency Features</h4>

<ul>
<li>  <strong>Adaptive Retries</strong> — the core retry logic that we use at Netflix to increase our resiliency and availability</li>
<li>  <strong>Origin Concurrency Protection </strong>— configurable concurrency limits to protect your origins from getting overloaded and protect other origins behind Zuul from each other</li>
</ul>

<h4 id="toc_4">Operational Features</h4>

<ul>
<li>  <strong>Request Passport </strong>— track all the lifecycle events for each request, which is invaluable for debugging async requests</li>
<li>  <strong>Status Categories</strong> — an enumeration of possible success and failure states for requests that are more granular than HTTP status codes</li>
<li>  <strong>Request Attempts</strong> — track proxy attempts and status of each, particularly useful for debugging retries and routing</li>
</ul>

<p>We are also working on some features that will be <a href="https://github.com/Netflix/zuul/wiki/Coming-Soon">coming soon</a>, including:</p>

<ul>
<li>  <strong>Websocket/SSE </strong>— support for side-channel push notifications</li>
<li>  <strong>Throttling and rate-limiting</strong> — protection from malicious client connections and requests, helping defend against volumetric attacks</li>
<li>  <strong>Brownout filters</strong> — for disabling certain CPU-intensive features when Zuul is overloaded</li>
<li>  <strong>Configurable routing</strong> — file-based routing configuration, instead of having to create routing filters in Zuul</li>
</ul>

<p>We would love to hear from you and see all the new and interesting applications of Zuul. For instructions on getting started, please visit our <a href="https://github.com/Netflix/zuul/wiki/Getting-Started-2.0">wiki page</a>.</p>

<h3 id="toc_5">Leveraging Zuul 2 at Netflix</h3>

<p>Internally, there are several major features that we’ve been working on but have not open sourced yet. Each one deserves its own blog post, but let’s go over them briefly.</p>

<h3 id="toc_6">Self Service Routing</h3>

<p>The most widely-used feature by our partners is self service routing. We provide an application and API for users to create routing rules based on any criteria in the request URL, path, query params, or headers. We then publish these routing rules to all the Zuul instances.</p>

<p>The main use case is for routing traffic to a specific test or staging cluster. However, there are many use cases for real production traffic. For example:</p>

<ul>
<li>  Services needing to shard their traffic create routing rules that map certain paths or prefixes to separate origins</li>
<li>  Developers onboard new services by creating a route that maps a new hostname to their new origin</li>
<li>  Developers run load tests by routing a percentage of existing traffic to a small cluster and ensuring applications will degrade gracefully under load</li>
<li>  Teams refactoring applications migrate to a new origin slowly by creating rules mapping traffic gradually, one path at a time</li>
<li>  Teams test changes (canary testing) by sending a small percentage of traffic to an instrumented cluster running the new build</li>
<li>  If teams need to test changes requiring multiple consecutive requests on their new build, they run sticky canary tests that route the same users to their new build for brief periods of time</li>
<li>  Security teams create rules that reject “bad” requests based on path or header rules across all Zuul clusters</li>
</ul>

<p>As you can see we use self service routing extensively and are increasing the customizability and scope of routes to allow for even more use cases.</p>

<h3 id="toc_7">Load Balancing for Resiliency</h3>

<p>Another major feature we’ve worked on is making load balancing to origins more intelligent. We are able to route around failures, slowness, GC issues, and various other things that crop up often when running large amounts of nodes. The goal of this work is to increase resiliency, availability, and quality of service for all Netflix services.</p>

<p>We have several cases that we handle:</p>

<h4 id="toc_8">Cold Instances</h4>

<p>When new origin instances start up, we send them a reduced amount of traffic for some time, until they’re warmed up. This was an issue we observed for applications with large codebases and huge metaspace usage. It takes a significant amount of time for these apps to JIT their code and be ready to handle a large amount of traffic.</p>

<p>We also generally bias the traffic to older instances and if we happen to hit a cold instance that throttles, we can always retry on a warm one. This gives us an order of magnitude improvement in availability.</p>

<h4 id="toc_9">High Error Rates</h4>

<p>Errors happen all the time and for varying reasons, whether it’s because of a bug in the code, a bad instance, or an invalid configuration property being set. Fortunately, as a proxy, we can detect errors reliably — either we get a 5xx error or there are connectivity problems to the service.</p>

<p>We track error rates for each origin and if the error rate is high enough, it implies the entire service is in trouble. We throttle retries from devices and disable internal retries to allow the service to recover. Moreover, we also track successive failures per instance and blacklist the bad ones for a period of time.</p>

<h4 id="toc_10">Overloaded Instances</h4>

<p>With the above approaches we send less traffic to servers in a cluster that are throttling or refusing connections, and lessened the impact by retrying those failed requests on other servers.</p>

<p>We’re now rolling out an additional approach where we aim to avoid overloading servers in the first place. This is achieved by allowing origins to signal to Zuul their current utilization, which Zuul then uses as a factor in its load-balancing choices — leading to reduced error rates, retries, and latency.</p>

<p>The origins add a header to all responses stating their utilization as a percentage, along with a target utilization they would like to have across the cluster. Calculating the percentage is completely up to each application and engineers can use whatever metric suits them best. This allows for a general solution as opposed to us trying to come up with a one-size-fits-all approach.</p>

<p>With this functionality in place, we assign a score (combination of instance utilization and other factors like the ones above) to each instance and do a choice-of-two load balancing selection.</p>

<h3 id="toc_11">Anomaly Detection and Contextual Alerting</h3>

<p>As we grew from just a handful of origins to a new world where anyone can quickly spin up a container cluster and put it behind Zuul, we found there was a need to automatically detect and pinpoint origin failures.</p>

<p>With the help of <a href="https://medium.com/netflix-techblog/stream-processing-with-mantis-78af913f51a6">Mantis real time event streaming</a>, we built an anomaly detector that aggregates error rates per service and notifies us in real time when services are in trouble. It takes all of the anomalies in a given time window and creates a timeline of all the origins in trouble. We then create a contextual alert email with the timeline of events and services affected. This allows an operator to quickly correlate these events and orient themselves to debug a specific app or feature, and ultimately find the root cause.</p>

<p>In fact, it was so useful that we expanded it to send notifications to the origin teams themselves. We’ve also added more internal applications, other than Zuul, and can build a much more extensive timeline of events. This has been a huge help during production incidents and helps operators quickly detect and fix problems before they cascade into massive outages.</p>

<p>We hope to open source as many of the above features as we can. Keep watching the tech blog for more depth on them in the future. If you want to help us solve these kinds of problem, please check out our <a href="https://jobs.netflix.com/teams/engineering?team=Product%20Engineering&amp;organization=Engineering">jobs site</a>.</p>

<p>— Arthur Gonigberg (@agonigberg), Mikey Cohen (@moldfarm ), Michael Smith (@kerumai ), Gaya Varadarajan ( @gaya3varadhu ), Sudheer Vinukonda ( @apachesudheerv ), Susheel Aroskar (@susheelaroskar )</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深入理解 RPC 之协议篇]]></title>
    <link href="http://panlw.github.io/15293292942972.html"/>
    <updated>2018-06-18T21:41:34+08:00</updated>
    <id>http://panlw.github.io/15293292942972.html</id>
    <content type="html"><![CDATA[
<ul>
<li>  Protocol 在 RPC 中的层次关系</li>
<li>  Dubbo 中的协议</li>
<li>  Motan 中的协议</li>
<li>  总结</li>
</ul>

<p>协议（Protocol）是个很广的概念，RPC 被称为远程过程调用协议，HTTP 和 TCP 也是大家熟悉的协议，也有人经常拿 RPC 和 RESTFUL 做对比，后者也可以被理解为一种协议… 我个人偏向于把 “协议” 理解为不同厂家不同用户之间的“约定”，而在 RPC 中，协议的含义也有多层。</p>

<h2 id="toc_0">Protocol 在 RPC 中的层次关系</h2>

<p>翻看 dubbo 和 motan 两个国内知名度数一数二的 RPC 框架（或者叫服务治理框架可能更合适）的文档，他们都有专门的一章介绍自身对多种协议的支持。RPC 框架是一个分层结构，从我的这个《深入理解 RPC》系列就可以看出，是按照分层来介绍 RPC 的原理的，前面已经介绍过了传输层，序列化层，动态代理层，他们各自负责 RPC 调用生命周期中的一环，而协议层则是凌驾于它们所有层之上的一层。简单描述下各个层之间的关系：</p>

<blockquote>
<p>protocol 层主要用于配置 refer（发现服务） 和 exporter（暴露服务） 的实现方式，transport 层定义了传输的方式，codec 层诠释了具体传输过程中报文解析的方式，serialize 层负责将对象转换成字节，以用于传输，proxy 层负责将这些细节屏蔽。</p>

<p>它们的包含关系如下：protocol &gt; transport &gt; codec &gt; serialize</p>
</blockquote>

<p>motan 的 Protocol 接口可以佐证这一点：</p>

<pre><code class="language-java">public interface Protocol {
    &lt;T&gt; Exporter&lt;T&gt; export(Provider&lt;T&gt; provider, URL url);
    &lt;T&gt; Referer&lt;T&gt; refer(Class&lt;T&gt; clz, URL url, URL serviceUrl);
    void destroy();
}
</code></pre>

<p>我们都知道 RPC 框架支持多种协议，由于协议处于框架层次的较高位置，任何一种协议的替换，都可能会导致服务发现和服务注册的方式，传输的方式，以及序列化的方式，而不同的协议也给不同的业务场景带来了更多的选择，下面就来看看一些常用协议。</p>

<h2 id="toc_1">Dubbo 中的协议</h2>

<h3 id="toc_2">dubbo://</h3>

<p>Dubbo 缺省协议采用单一长连接和 NIO 异步通讯，适合于小数据量高并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况。</p>

<p>反之，Dubbo 缺省协议不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。</p>

<p>适用场景：常规远程服务方法调用</p>

<h3 id="toc_3">rmi://</h3>

<p>RMI 协议采用 JDK 标准的 <code>java.rmi.*</code> 实现，采用阻塞式短连接和 JDK 标准序列化方式。</p>

<p>适用场景：常规远程服务方法调用，与原生 RMI 服务互操作</p>

<h3 id="toc_4">hessian://</h3>

<p>Hessian 协议用于集成 Hessian 的服务，Hessian 底层采用 Http 通讯，采用 Servlet 暴露服务，Dubbo 缺省内嵌 Jetty 作为服务器实现。</p>

<p>Dubbo 的 Hessian 协议可以和原生 Hessian 服务互操作，即：</p>

<ul>
<li><p>提供者用 Dubbo 的 Hessian 协议暴露服务，消费者直接用标准 Hessian 接口调用</p></li>
<li><p>或者提供方用标准 Hessian 暴露服务，消费方用 Dubbo 的 Hessian 协议调用。</p></li>
</ul>

<p>Hessian 在之前介绍过，当时仅仅是用它来作为序列化工具，但其本身其实就是一个协议，可以用来做远程通信。</p>

<p>适用场景：页面传输，文件传输，或与原生 hessian 服务互操作</p>

<h3 id="toc_5">http://</h3>

<p>基于 HTTP 表单的远程调用协议，采用 Spring 的 HttpInvoker 实现</p>

<p>适用场景：需同时给应用程序和浏览器 JS 使用的服务。</p>

<h3 id="toc_6">webserivice://</h3>

<p>基于 WebService 的远程调用协议，基于 Apache CXF 的 <code>frontend-simple</code> 和 <code>transports-http</code> 实现。</p>

<p>可以和原生 WebService 服务互操作，即：</p>

<ul>
<li><p>提供者用 Dubbo 的 WebService 协议暴露服务，消费者直接用标准 WebService 接口调用，</p></li>
<li><p>或者提供方用标准 WebService 暴露服务，消费方用 Dubbo 的 WebService 协议调用</p></li>
</ul>

<p>适用场景：系统集成，跨语言调用</p>

<h3 id="toc_7">thrift://</h3>

<p>当前 dubbo 支持的 thrift 协议是对 thrift 原生协议的扩展，在原生协议的基础上添加了一些额外的头信息，比如 service name，magic number 等。</p>

<h3 id="toc_8">memcached://</h3>

<p>基于 memcached 实现的 RPC 协议</p>

<h3 id="toc_9">redis://</h3>

<p>基于 Redis 实现的 RPC 协议。</p>

<blockquote>
<p>dubbo 支持的众多协议详见 <a href="http://dubbo.io/books/dubbo-user-book/references/protocol/dubbo.html">http://dubbo.io/books/dubbo-user-book/references/protocol/dubbo.html</a></p>
</blockquote>

<p>dubbo 的一个分支 dangdangdotcom/dubbox 扩展了 REST 协议</p>

<h3 id="toc_10">rest://</h3>

<p>JAX-RS 是标准的 Java REST API，得到了业界的广泛支持和应用，其著名的开源实现就有很多，包括 Oracle 的 Jersey，RedHat 的 RestEasy，Apache 的 CXF 和 Wink，以及 restlet 等等。另外，所有支持 JavaEE 6.0 以上规范的商用 JavaEE 应用服务器都对 JAX-RS 提供了支持。因此，JAX-RS 是一种已经非常成熟的解决方案，并且采用它没有任何所谓 vendor lock-in 的问题。</p>

<p>JAX-RS 在网上的资料非常丰富，例如下面的入门教程：</p>

<ul>
<li><p>Oracle 官方的 tutorial：<a href="http://docs.oracle.com/javaee/7/tutorial/doc/jaxrs.htm">http://docs.oracle.com/javaee/7/tutorial/doc/jaxrs.htm</a></p></li>
<li><p>IBM developerWorks 中国站文章：<a href="http://www.ibm.com/developerworks/cn/java/j-lo-jaxrs/">http://www.ibm.com/developerworks/cn/java/j-lo-jaxrs/</a></p></li>
</ul>

<p>更多的资料请自行 google 或者百度一下。就学习 JAX-RS 来说，一般主要掌握其各种 annotation 的用法即可。</p>

<blockquote>
<p>注意：dubbo 是基于 JAX-RS 2.0 版本的，有时候需要注意一下资料或 REST 实现所涉及的版本。</p>
</blockquote>

<p>适用场景：跨语言调用</p>

<p>千米网也给 dubbo 贡献了一个扩展协议：<a href="https://github.com/dubbo/dubbo-rpc-jsonrpc">https://github.com/dubbo/dubbo-rpc-jsonrpc</a></p>

<h3 id="toc_11">jsonrpc://</h3>

<h4 id="toc_12">Why HTTP</h4>

<p>在互联网快速迭代的大潮下，越来越多的公司选择 nodejs、django、rails 这样的快速脚本框架来开发 web 端应用 而后端的服务用 Java 又是最合适的，这就产生了大量的跨语言的调用需求。<br/>
而 http、json 是天然合适作为跨语言的标准，各种语言都有成熟的类库<br/>
虽然 Dubbo 的异步长连接协议效率很高，但是在脚本语言中，这点效率的损失并不重要。</p>

<h4 id="toc_13">Why Not RESTful</h4>

<p>Dubbox 在 RESTful 接口上已经做出了尝试，但是 REST 架构和 dubbo 原有的 RPC 架构是有区别的，<br/>
区别在于 REST 架构需要有资源 (Resources) 的定义， 需要用到 HTTP 协议的基本操作 GET、POST、PUT、DELETE 对资源进行操作。<br/>
Dubbox 需要重新定义接口的属性，这对原有的 Dubbo 接口迁移是一个较大的负担。<br/>
相比之下，RESTful 更合适互联网系统之间的调用，而 RPC 更合适一个系统内的调用，<br/>
所以我们使用了和 Dubbo 理念较为一致的 JsonRPC</p>

<p>JSON-RPC 2.0 规范 和 JAX-RS 一样，也是一个规范，JAVA 对其的支持可参考 jsonrpc4j</p>

<p>适用场景：跨语言调用</p>

<h2 id="toc_14">Motan 中的协议</h2>

<h3 id="toc_15">motan://</h3>

<p>motan 协议之于 motan，地位等同于 dubbo 协议之于 dubbo，两者都是各自默认的且都是自定义的协议。内部使用 netty 进行通信（旧版本使用 netty3 ，最新版本支持 netty4），默认使用 hessian 作为序列化器。</p>

<p>适用场景：常规远程服务方法调用</p>

<h3 id="toc_16">injvm://</h3>

<p>顾名思义，如果 Provider 和 Consumer 位于同一个 jvm，motan 提供了 injvm 协议。这个协议是 jvm 内部调用，不经过本地网络，一般在服务化拆分时，作为过渡方案使用，可以通过开关机制在本地和远程调用之间进行切换，等过渡完成后再去除本地实现的引用。</p>

<h3 id="toc_17">grpc:// 和 yar://</h3>

<p>这两个协议的诞生缘起于一定的历史遗留问题，moton 是新浪微博开源的，而其内部有很多 PHP 应用，为解决跨语言问题，这两个协议进而出现了。</p>

<p>适用场景：较为局限的跨语言调用</p>

<h3 id="toc_18">restful://</h3>

<p>motan 在 0.3.1 (2017-07-11) 版本发布了 restful 协议的支持（和 dubbo 的 rest 协议本质一样），dubbo 默认使用 jetty 作为 http server，而 motan 使用则是 netty 。主要实现的是 java 对 restful 指定的规范，即 javax.ws.rs 包下的类。</p>

<p>适用场景：跨语言调用</p>

<h3 id="toc_19">motan2://</h3>

<p>motan 1.0.0 (2017-10-31) 版本发布了 motan2 协议，用于对跨语言的支持，不同于 restful，jsonrpc 这样的通用协议，motan2 把请求的一些元数据作为单独的部分传输，更适合不同语言解析。</p>

<p>适用场景：跨语言调用</p>

<blockquote>
<p>Motan is a cross-language remote procedure call(RPC) framework for rapid development of high performance distributed services.</p>

<p>Motan-go is golang implementation.</p>

<p>Motan-PHP is PHP client can interactive with Motan server directly or through Motan-go agent.</p>

<p>Motan-openresty is a Lua(Luajit) implementation based on Openresty</p>
</blockquote>

<p>从 motan 的 changeLog 以及 github 首页的介绍来看，其致力于打造成一个跨语言的服务治理框架，这倒是比较亦可赛艇的事。</p>

<h3 id="toc_20">面向未来的协议</h3>

<p>motan 已经支持 motan2://，计划支持 mcq://，kafka:// … 支持更多的协议，以应对复杂的业务场景。对这个感兴趣的朋友，可以参见这篇文章：<a href="http://mp.weixin.qq.com/s?__biz=MzU0MTMyMDg1NQ==&amp;mid=2247483710&amp;idx=1&amp;sn=5daea9c54b9835b660dac5b0ce02f462&amp;scene=21#wechat_redirect">http://mp.weixin.qq.com/s/XZVCHZZzCX8wwgNKZtsmcA</a></p>

<h2 id="toc_21">总结</h2>

<p>如果仅仅是将 dubbo，motan 作为一个 RPC 框架使用，那大多人会选择其默认的协议（dubbo 协议，motan 协议），而如果是有历史遗留原因，如需要对接异构系统，就需要替换成其他协议了。大多数互联网公司选择自研 RPC 框架，或者改造自己的协议，都是为了适配自身业务的特殊性，协议层的选择非常重要。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[精讲 Redis 内存模型]]></title>
    <link href="http://panlw.github.io/15293283879436.html"/>
    <updated>2018-06-18T21:26:27+08:00</updated>
    <id>http://panlw.github.io/15293283879436.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://my.oschina.net/u/3779583/blog/1829617?origin=wechat">https://my.oschina.net/u/3779583/blog/1829617?origin=wechat</a></p>
</blockquote>

<h2 id="toc_0">前言</h2>

<p>Redis 是目前最火爆的内存数据库之一，通过在内存中读写数据，大大提高了读写速度，可以说 Redis 是实现网站高并发不可或缺的一部分。</p>

<p>我们使用 Redis 时，会接触 Redis 的 5 种对象类型（字符串、哈希、列表、集合、有序集合），丰富的类型是 Redis 相对于 Memcached 等的一大优势。在了解 Redis 的 5 种对象类型的用法和特点的基础上，进一步了解 Redis 的内存模型，对 Redis 的使用有很大帮助，例如：</p>

<p>1、估算 Redis 内存使用量。目前为止，内存的使用成本仍然相对较高，使用内存不能无所顾忌；根据需求合理的评估 Redis 的内存使用量，选择合适的机器配置，可以在满足需求的情况下节约成本。</p>

<p>2、优化内存占用。了解 Redis 内存模型可以选择更合适的数据类型和编码，更好的利用 Redis 内存。</p>

<p>3、分析解决问题。当 Redis 出现阻塞、内存占用等问题时，尽快发现导致问题的原因，便于分析解决问题。</p>

<p>这篇文章主要介绍 Redis 的内存模型（以 3.0 为例），包括 Redis 占用内存的情况及如何查询、不同的对象类型在内存中的编码方式、内存分配器 (jemalloc)、简单动态字符串 (SDS)、RedisObject 等；然后在此基础上介绍几个 Redis 内存模型的应用。</p>

<p>在后面的文章中，会陆续介绍关于 Redis 高可用的内容，包括主从复制、哨兵、集群等等，欢迎关注。</p>

<h2 id="toc_1">一、Redis 内存统计</h2>

<p>工欲善其事必先利其器，在说明 Redis 内存之前首先说明如何统计 Redis 使用内存的情况。</p>

<p>在客户端通过 redis-cli 连接服务器后（后面如无特殊说明，客户端一律使用 redis-cli），通过 info 命令可以查看内存使用情况：</p>

<pre><code>info memory
</code></pre>

<p><img src="https://images2018.cnblogs.com/blog/1174710/201803/1174710-20180327000947752-2103814952.png" alt=""/></p>

<p>其中，info 命令可以显示 redis 服务器的许多信息，包括服务器基本信息、CPU、内存、持久化、客户端连接信息等等；memory 是参数，表示只显示内存相关的信息。</p>

<p>返回结果中比较重要的几个说明如下：</p>

<p>（1）<strong>used_memory</strong><strong>：</strong>Redis 分配器分配的内存总量（单位是字节），包括使用的虚拟内存（即 swap）；Redis 分配器后面会介绍。used_memory_human 只是显示更友好。</p>

<p>（2）<strong>used_memory_rss</strong><strong>：</strong>Redis 进程占据操作系统的内存（单位是字节），与 top 及 ps 命令看到的值是一致的；除了分配器分配的内存之外，used_memory_rss 还包括进程运行本身需要的内存、内存碎片等，但是不包括虚拟内存。</p>

<p>因此，used_memory 和 used_memory_rss，前者是从 Redis 角度得到的量，后者是从操作系统角度得到的量。二者之所以有所不同，一方面是因为内存碎片和 Redis 进程运行需要占用内存，使得前者可能比后者小，另一方面虚拟内存的存在，使得前者可能比后者大。</p>

<p>由于在实际应用中，Redis 的数据量会比较大，此时进程运行占用的内存与 Redis 数据量和内存碎片相比，都会小得多；因此 used_memory_rss 和 used_memory 的比例，便成了衡量 Redis 内存碎片率的参数；这个参数就是 mem_fragmentation_ratio。</p>

<p>（3）<strong>mem_fragmentation_ratio</strong><strong>：</strong>内存碎片比率，该值是 used_memory_rss / used_memory 的比值。</p>

<p>mem_fragmentation_ratio 一般大于 1，且该值越大，内存碎片比例越大。mem_fragmentation_ratio&lt;1，说明 Redis 使用了虚拟内存，由于虚拟内存的媒介是磁盘，比内存速度要慢很多，当这种情况出现时，应该及时排查，如果内存不足应该及时处理，如增加 Redis 节点、增加 Redis 服务器的内存、优化应用等。</p>

<p>一般来说，mem_fragmentation_ratio 在 1.03 左右是比较健康的状态（对于 jemalloc 来说）；上面截图中的 mem_fragmentation_ratio 值很大，是因为还没有向 Redis 中存入数据，Redis 进程本身运行的内存使得 used_memory_rss 比 used_memory 大得多。</p>

<p>（4）<strong>mem_allocator</strong><strong>：</strong>Redis 使用的内存分配器，在编译时指定；可以是 libc 、jemalloc 或者 tcmalloc，默认是 jemalloc；截图中使用的便是默认的 jemalloc。</p>

<h1 id="toc_2">二、Redis 内存划分</h1>

<p>Redis 作为内存数据库，在内存中存储的内容主要是数据（键值对）；通过前面的叙述可以知道，除了数据以外，Redis 的其他部分也会占用内存。</p>

<p>Redis 的内存占用主要可以划分为以下几个部分：</p>

<h2 id="toc_3">1、数据</h2>

<p>作为数据库，数据是最主要的部分；这部分占用的内存会统计在 used_memory 中。</p>

<p>Redis 使用键值对存储数据，其中的值（对象）包括 5 种类型，即字符串、哈希、列表、集合、有序集合。这 5 种类型是 Redis 对外提供的，实际上，在 Redis 内部，每种类型可能有 2 种或更多的内部编码实现；此外，Redis 在存储对象时，并不是直接将数据扔进内存，而是会对对象进行各种包装：如 redisObject、SDS 等；这篇文章后面将重点介绍 Redis 中数据存储的细节。</p>

<h2 id="toc_4">2、进程本身运行需要的内存</h2>

<p>Redis 主进程本身运行肯定需要占用内存，如代码、常量池等等；这部分内存大约几兆，在大多数生产环境中与 Redis 数据占用的内存相比可以忽略。这部分内存不是由 jemalloc 分配，因此不会统计在 used_memory 中。</p>

<p>补充说明：除了主进程外，Redis 创建的子进程运行也会占用内存，如 Redis 执行 AOF、RDB 重写时创建的子进程。当然，这部分内存不属于 Redis 进程，也不会统计在 used_memory 和 used_memory_rss 中。</p>

<h2 id="toc_5">3、缓冲内存</h2>

<p>缓冲内存包括客户端缓冲区、复制积压缓冲区、AOF 缓冲区等；其中，客户端缓冲存储客户端连接的输入输出缓冲；复制积压缓冲用于部分复制功能；AOF 缓冲区用于在进行 AOF 重写时，保存最近的写入命令。在了解相应功能之前，不需要知道这些缓冲的细节；这部分内存由 jemalloc 分配，因此会统计在 used_memory 中。</p>

<h2 id="toc_6">4、内存碎片</h2>

<p>内存碎片是 Redis 在分配、回收物理内存过程中产生的。例如，如果对数据的更改频繁，而且数据之间的大小相差很大，可能导致 redis 释放的空间在物理内存中并没有释放，但 redis 又无法有效利用，这就形成了内存碎片。内存碎片不会统计在 used_memory 中。</p>

<p>内存碎片的产生与对数据进行的操作、数据的特点等都有关；此外，与使用的内存分配器也有关系：如果内存分配器设计合理，可以尽可能的减少内存碎片的产生。后面将要说到的 jemalloc 便在控制内存碎片方面做的很好。</p>

<p>如果 Redis 服务器中的内存碎片已经很大，可以通过安全重启的方式减小内存碎片：因为重启之后，Redis 重新从备份文件中读取数据，在内存中进行重排，为每个数据重新选择合适的内存单元，减小内存碎片。</p>

<h1 id="toc_7">三、Redis 数据存储的细节</h1>

<h2 id="toc_8">1、概述</h2>

<p>关于 Redis 数据存储的细节，涉及到内存分配器（如 jemalloc）、简单动态字符串（SDS）、5 种对象类型及内部编码、redisObject。在讲述具体内容之前，先说明一下这几个概念之间的关系。</p>

<p>下图是执行 set hello world 时，所涉及到的数据模型。</p>

<p> <img src="https://images2018.cnblogs.com/blog/1174710/201803/1174710-20180327001055927-1896197804.png" alt=""/></p>

<p>图片来源：<a href="https://searchdatabase.techtarget.com.cn/7-20218/">https://searchdatabase.techtarget.com.cn/7-20218/</a></p>

<p>（1）dictEntry：Redis 是 Key-Value 数据库，因此对每个键值对都会有一个 dictEntry，里面存储了指向 Key 和 Value 的指针；next 指向下一个 dictEntry，与本 Key-Value 无关。</p>

<p>（2）Key：图中右上角可见，Key（”hello”）并不是直接以字符串存储，而是存储在 SDS 结构中。</p>

<p>（3）redisObject：Value(“world”) 既不是直接以字符串存储，也不是像 Key 一样直接存储在 SDS 中，而是存储在 redisObject 中。实际上，不论 Value 是 5 种类型的哪一种，都是通过 redisObject 来存储的；而 redisObject 中的 type 字段指明了 Value 对象的类型，ptr 字段则指向对象所在的地址。不过可以看出，字符串对象虽然经过了 redisObject 的包装，但仍然需要通过 SDS 存储。</p>

<p>实际上，redisObject 除了 type 和 ptr 字段以外，还有其他字段图中没有给出，如用于指定对象内部编码的字段；后面会详细介绍。</p>

<p>（4）jemalloc：无论是 DictEntry 对象，还是 redisObject、SDS 对象，都需要内存分配器（如 jemalloc）分配内存进行存储。以 DictEntry 对象为例，有 3 个指针组成，在 64 位机器下占 24 个字节，jemalloc 会为它分配 32 字节大小的内存单元。</p>

<p>下面来分别介绍 jemalloc、redisObject、SDS、对象类型及内部编码。</p>

<h2 id="toc_9">2、jemalloc</h2>

<p>Redis 在编译时便会指定内存分配器；内存分配器可以是 libc 、jemalloc 或者 tcmalloc，默认是 jemalloc。</p>

<p>jemalloc 作为 Redis 的默认内存分配器，在减小内存碎片方面做的相对比较好。jemalloc 在 64 位系统中，将内存空间划分为小、大、巨大三个范围；每个范围内又划分了许多小的内存块单位；当 Redis 存储数据时，会选择大小最合适的内存块进行存储。</p>

<p>jemalloc 划分的内存单元如下图所示：</p>

<p> <img src="https://images2018.cnblogs.com/blog/1174710/201803/1174710-20180327001126509-2023165562.png" alt=""/></p>

<p>图片来源：<a href="http://blog.csdn.net/zhengpeitao/article/details/76573053">http://blog.csdn.net/zhengpeitao/article/details/76573053</a></p>

<p>例如，如果需要存储大小为 130 字节的对象，jemalloc 会将其放入 160 字节的内存单元中。</p>

<h2 id="toc_10">3、redisObject</h2>

<p>前面说到，Redis 对象有 5 种类型；无论是哪种类型，Redis 都不会直接存储，而是通过 redisObject 对象进行存储。</p>

<p>redisObject 对象非常重要，Redis 对象的类型、内部编码、内存回收、共享对象等功能，都需要 redisObject 支持，下面将通过 redisObject 的结构来说明它是如何起作用的。</p>

<p>redisObject 的定义如下（不同版本的 Redis 可能稍稍有所不同）：</p>

<pre><code class="language-c">typedef struct redisObject {
　　unsigned type:4;
　　unsigned encoding:4;
　　unsigned lru:REDIS_LRU_BITS; /* lru time (relative to server.lruclock) */
　　int refcount;
　　void *ptr;
} robj;
</code></pre>

<p>redisObject 的每个字段的含义和作用如下：</p>

<h3 id="toc_11">（1）type</h3>

<p>type 字段表示对象的类型，占 4 个比特；目前包括 REDIS_STRING(字符串)、REDIS_LIST (列表)、REDIS_HASH(哈希)、REDIS_SET(集合)、REDIS_ZSET(有序集合)。</p>

<p>当我们执行 type 命令时，便是通过读取 RedisObject 的 type 字段获得对象的类型；如下图所示：</p>

<p><img src="https://images2018.cnblogs.com/blog/1174710/201803/1174710-20180327001214189-1733420705.png" alt=""/></p>

<h3 id="toc_12">（2）encoding</h3>

<p>encoding 表示对象的内部编码，占 4 个比特。</p>

<p>对于 Redis 支持的每种类型，都有至少两种内部编码，例如对于字符串，有 int、embstr、raw 三种编码。通过 encoding 属性，Redis 可以根据不同的使用场景来为对象设置不同的编码，大大提高了 Redis 的灵活性和效率。以列表对象为例，有压缩列表和双端链表两种编码方式；如果列表中的元素较少，Redis 倾向于使用压缩列表进行存储，因为压缩列表占用内存更少，而且比双端链表可以更快载入；当列表对象元素较多时，压缩列表就会转化为更适合存储大量元素的双端链表。</p>

<p>通过 object encoding 命令，可以查看对象采用的编码方式，如下图所示：</p>

<p><img src="https://images2018.cnblogs.com/blog/1174710/201803/1174710-20180327001228807-998910409.png" alt=""/></p>

<p>5 种对象类型对应的编码方式以及使用条件，将在后面介绍。</p>

<h3 id="toc_13">（3）lru</h3>

<p>lru 记录的是对象最后一次被命令程序访问的时间，占据的比特数不同的版本有所不同（如 4.0 版本占 24 比特，2.6 版本占 22 比特）。</p>

<p>通过对比 lru 时间与当前时间，可以计算某个对象的空转时间；object idletime 命令可以显示该空转时间（单位是秒）。object idletime 命令的一个特殊之处在于它不改变对象的 lru 值。</p>

<p><img src="https://images2018.cnblogs.com/blog/1174710/201803/1174710-20180327001239788-1325383307.png" alt=""/></p>

<p>lru 值除了通过 object idletime 命令打印之外，还与 Redis 的内存回收有关系：如果 Redis 打开了 maxmemory 选项，且内存回收算法选择的是 volatile-lru 或 allkeys—lru，那么当 Redis 内存占用超过 maxmemory 指定的值时，Redis 会优先选择空转时间最长的对象进行释放。</p>

<h3 id="toc_14">（4）refcount</h3>

<p><strong>refcount</strong> <strong>与共享对象</strong></p>

<p>refcount 记录的是该对象被引用的次数，类型为整型。refcount 的作用，主要在于对象的引用计数和内存回收。当创建新对象时，refcount 初始化为 1；当有新程序使用该对象时，refcount 加 1；当对象不再被一个新程序使用时，refcount 减 1；当 refcount 变为 0 时，对象占用的内存会被释放。</p>

<p>Redis 中被多次使用的对象 (refcount&gt;1)，称为共享对象。Redis 为了节省内存，当有一些对象重复出现时，新的程序不会创建新的对象，而是仍然使用原来的对象。这个被重复使用的对象，就是共享对象。目前共享对象仅支持整数值的字符串对象。</p>

<p><strong>共享对象的具体实现</strong></p>

<p>Redis 的共享对象目前只支持整数值的字符串对象。之所以如此，实际上是对内存和 CPU（时间）的平衡：共享对象虽然会降低内存消耗，但是判断两个对象是否相等却需要消耗额外的时间。对于整数值，判断操作复杂度为 O(1)；对于普通字符串，判断复杂度为 O(n)；而对于哈希、列表、集合和有序集合，判断的复杂度为 O(n<sup>2)。进群</sup> 619881427 可以免费获取文中知识点的视频资料。</p>

<p>虽然共享对象只能是整数值的字符串对象，但是 5 种类型都可能使用共享对象（如哈希、列表等的元素可以使用）。</p>

<p>就目前的实现来说，Redis 服务器在初始化时，会创建 10000 个字符串对象，值分别是 0~9999 的整数值；当 Redis 需要使用值为 0~9999 的字符串对象时，可以直接使用这些共享对象。10000 这个数字可以通过调整参数 REDIS_SHARED_INTEGERS（4.0 中是 OBJ_SHARED_INTEGERS）的值进行改变。</p>

<p>共享对象的引用次数可以通过 object refcount 命令查看，如下图所示。命令执行的结果页佐证了只有 0~9999 之间的整数会作为共享对象。</p>

<p><img src="https://images2018.cnblogs.com/blog/1174710/201803/1174710-20180327001256958-1309209644.png" alt=""/></p>

<h3 id="toc_15">（5）ptr</h3>

<p>ptr 指针指向具体的数据，如前面的例子中，set hello world，ptr 指向包含字符串 world 的 SDS。</p>

<h3 id="toc_16">（6）总结</h3>

<p>综上所述，redisObject 的结构与对象类型、编码、内存回收、共享对象都有关系；一个 redisObject 对象的大小为 16 字节：</p>

<p>4bit+4bit+24bit+4Byte+8Byte=16Byte。</p>

<h2 id="toc_17">4、SDS</h2>

<p>Redis 没有直接使用 C 字符串 (即以空字符’\0’结尾的字符数组) 作为默认的字符串表示，而是使用了 SDS。SDS 是简单动态字符串 (Simple Dynamic String) 的缩写。</p>

<h3 id="toc_18">（1）SDS 结构</h3>

<p>sds 的结构如下：</p>

<pre><code class="language-c">struct sdshdr {
    int len;
    int free;
    char buf[];
};
</code></pre>

<p>其中，buf 表示字节数组，用来存储字符串；len 表示 buf 已使用的长度，free 表示 buf 未使用的长度。下面是两个例子。</p>

<p><img src="https://images2018.cnblogs.com/blog/1174710/201803/1174710-20180327001321434-1043595793.png" alt=""/></p>

<p><img src="https://images2018.cnblogs.com/blog/1174710/201803/1174710-20180327001325561-890602831.png" alt=""/></p>

<p>图片来源：《Redis 设计与实现》</p>

<p>通过 SDS 的结构可以看出，buf 数组的长度 = free+len+1（其中 1 表示字符串结尾的空字符）；所以，一个 SDS 结构占据的空间为：free 所占长度 + len 所占长度 + buf 数组的长度 = 4+4+free+len+1=free+len+9。</p>

<h3 id="toc_19">（2）SDS 与 C 字符串的比较</h3>

<p>SDS 在 C 字符串的基础上加入了 free 和 len 字段，带来了很多好处：</p>

<ul>
<li>  获取字符串长度：SDS 是 O(1)，C 字符串是 O(n)</li>
<li>  缓冲区溢出：使用 C 字符串的 API 时，如果字符串长度增加（如 strcat 操作）而忘记重新分配内存，很容易造成缓冲区的溢出；而 SDS 由于记录了长度，相应的 API 在可能造成缓冲区溢出时会自动重新分配内存，杜绝了缓冲区溢出。</li>
<li>  修改字符串时内存的重分配：对于 C 字符串，如果要修改字符串，必须要重新分配内存（先释放再申请），因为如果没有重新分配，字符串长度增大时会造成内存缓冲区溢出，字符串长度减小时会造成内存泄露。而对于 SDS，由于可以记录 len 和 free，因此解除了字符串长度和空间数组长度之间的关联，可以在此基础上进行优化：空间预分配策略（即分配内存时比实际需要的多）使得字符串长度增大时重新分配内存的概率大大减小；惰性空间释放策略使得字符串长度减小时重新分配内存的概率大大减小。</li>
<li>  存取二进制数据：SDS 可以，C 字符串不可以。因为 C 字符串以空字符作为字符串结束的标识，而对于一些二进制文件（如图片等），内容可能包括空字符串，因此 C 字符串无法正确存取；而 SDS 以字符串长度 len 来作为字符串结束标识，因此没有这个问题。</li>
</ul>

<p>此外，由于 SDS 中的 buf 仍然使用了 C 字符串（即以’\0’结尾），因此 SDS 可以使用 C 字符串库中的部分函数；但是需要注意的是，只有当 SDS 用来存储文本数据时才可以这样使用，在存储二进制数据时则不行（’\0’不一定是结尾）。</p>

<h3 id="toc_20">（3）SDS 与 C 字符串的应用</h3>

<p>Redis 在存储对象时，一律使用 SDS 代替 C 字符串。例如 set hello world 命令，hello 和 world 都是以 SDS 的形式存储的。而 sadd myset member1 member2 member3 命令，不论是键（”myset”），还是集合中的元素（”member1”、 ”member2” 和”member3”），都是以 SDS 的形式存储。除了存储对象，SDS 还用于存储各种缓冲区。</p>

<p>只有在字符串不会改变的情况下，如打印日志时，才会使用 C 字符串。</p>

<h1 id="toc_21">四、Redis 的对象类型与内部编码</h1>

<p>前面已经说过，Redis 支持 5 种对象类型，而每种结构都有至少两种编码；这样做的好处在于：一方面接口与实现分离，当需要增加或改变内部编码时，用户使用不受影响，另一方面可以根据不同的应用场景切换内部编码，提高效率。</p>

<p>Redis 各种对象类型支持的内部编码如下图所示 (图中版本是 Redis3.0，Redis 后面版本中又增加了内部编码，略过不提；本章所介绍的内部编码都是基于 3.0 的)：</p>

<p><img src="https://images2018.cnblogs.com/blog/1174710/201803/1174710-20180327001358239-1304238510.png" alt=""/></p>

<p>图片来源：《Redis 设计与实现》</p>

<p>关于 Redis 内部编码的转换，都符合以下规律：<strong>编码转换在</strong> <strong>Redis</strong> <strong>写入数据时完成，且转换过程不可逆，只能从小内存编码向大内存编码转换。</strong></p>

<h2 id="toc_22">1、字符串</h2>

<h3 id="toc_23">（1）概况</h3>

<p>字符串是最基础的类型，因为所有的键都是字符串类型，且字符串之外的其他几种复杂类型的元素也是字符串。</p>

<p>字符串长度不能超过 512MB。</p>

<h3 id="toc_24">（2）内部编码</h3>

<p>字符串类型的内部编码有 3 种，它们的应用场景如下：</p>

<ul>
<li>  int：8 个字节的长整型。字符串值是整型时，这个值使用 long 整型表示。</li>
<li>  embstr：&lt;=39 字节的字符串。embstr 与 raw 都使用 redisObject 和 sds 保存数据，区别在于，embstr 的使用只分配一次内存空间（因此 redisObject 和 sds 是连续的），而 raw 需要分配两次内存空间（分别为 redisObject 和 sds 分配空间）。因此与 raw 相比，embstr 的好处在于创建时少分配一次空间，删除时少释放一次空间，以及对象的所有数据连在一起，寻找方便。而 embstr 的坏处也很明显，如果字符串的长度增加需要重新分配内存时，整个 redisObject 和 sds 都需要重新分配空间，因此 redis 中的 embstr 实现为只读。</li>
<li>  raw：大于 39 个字节的字符串</li>
</ul>

<p>示例如下图所示：</p>

<p><img src="https://images2018.cnblogs.com/blog/1174710/201803/1174710-20180327001417703-15851809.png" alt=""/></p>

<p>embstr 和 raw 进行区分的长度，是 39；是因为 redisObject 的长度是 16 字节，sds 的长度是 9 + 字符串长度；因此当字符串长度是 39 时，embstr 的长度正好是 16+9+39=64，jemalloc 正好可以分配 64 字节的内存单元。</p>

<h3 id="toc_25">（3）编码转换</h3>

<p>当 int 数据不再是整数，或大小超过了 long 的范围时，自动转化为 raw。</p>

<p>而对于 embstr，由于其实现是只读的，因此在对 embstr 对象进行修改时，都会先转化为 raw 再进行修改，因此，只要是修改 embstr 对象，修改后的对象一定是 raw 的，无论是否达到了 39 个字节。示例如下图所示：</p>

<p><img src="https://images2018.cnblogs.com/blog/1174710/201803/1174710-20180327001426651-1225081171.png" alt=""/></p>

<h2 id="toc_26">2、列表</h2>

<h3 id="toc_27">（1）概况</h3>

<p>列表（list）用来存储多个有序的字符串，每个字符串称为元素；一个列表可以存储 2<sup>32-1</sup> 个元素。Redis 中的列表支持两端插入和弹出，并可以获得指定位置（或范围）的元素，可以充当数组、队列、栈等。</p>

<h3 id="toc_28">（2）内部编码</h3>

<p>列表的内部编码可以是压缩列表（ziplist）或双端链表（linkedlist）。</p>

<p>双端链表：由一个 list 结构和多个 listNode 结构组成；典型结构如下图所示：</p>

<p><img src="https://images2018.cnblogs.com/blog/1174710/201803/1174710-20180327001435577-242733744.png" alt=""/></p>

<p>图片来源：《Redis 设计与实现》</p>

<p>通过图中可以看出，双端链表同时保存了表头指针和表尾指针，并且每个节点都有指向前和指向后的指针；链表中保存了列表的长度；dup、free 和 match 为节点值设置类型特定函数，所以链表可以用于保存各种不同类型的值。而链表中每个节点指向的是 type 为字符串的 redisObject。</p>

<p>压缩列表：压缩列表是 Redis 为了节约内存而开发的，是由一系列特殊编码的<strong>连续内存块</strong> (而不是像双端链表一样每个节点是指针) 组成的顺序型数据结构；具体结构相对比较复杂，略。与双端链表相比，压缩列表可以节省内存空间，但是进行修改或增删操作时，复杂度较高；因此当节点数量较少时，可以使用压缩列表；但是节点数量多时，还是使用双端链表划算。</p>

<p>压缩列表不仅用于实现列表，也用于实现哈希、有序列表；使用非常广泛。</p>

<h3 id="toc_29">（3）编码转换</h3>

<p>只有同时满足下面两个条件时，才会使用压缩列表：列表中元素数量小于 512 个；列表中所有字符串对象都不足 64 字节。如果有一个条件不满足，则使用双端列表；且编码只可能由压缩列表转化为双端链表，反方向则不可能。</p>

<p>下图展示了列表编码转换的特点：</p>

<p><img src="https://images2018.cnblogs.com/blog/1174710/201803/1174710-20180327001457636-673470263.png" alt=""/></p>

<p>其中，单个字符串不能超过 64 字节，是为了便于统一分配每个节点的长度；这里的 64 字节是指字符串的长度，不包括 SDS 结构，因为压缩列表使用连续、定长内存块存储字符串，不需要 SDS 结构指明长度。后面提到压缩列表，也会强调长度不超过 64 字节，原理与这里类似。</p>

<h2 id="toc_30">3、哈希</h2>

<h3 id="toc_31">（1）概况</h3>

<p>哈希（作为一种数据结构），不仅是 redis 对外提供的 5 种对象类型的一种（与字符串、列表、集合、有序结合并列），也是 Redis 作为 Key-Value 数据库所使用的数据结构。为了说明的方便，在本文后面当使用 “内层的哈希” 时，代表的是 redis 对外提供的 5 种对象类型的一种；使用 “外层的哈希” 代指 Redis 作为 Key-Value 数据库所使用的数据结构。</p>

<h3 id="toc_32">（2）内部编码</h3>

<p>内层的哈希使用的内部编码可以是压缩列表（ziplist）和哈希表（hashtable）两种；Redis 的外层的哈希则只使用了 hashtable。</p>

<p>压缩列表前面已介绍。与哈希表相比，压缩列表用于元素个数少、元素长度小的场景；其优势在于集中存储，节省空间；同时，虽然对于元素的操作复杂度也由 O(n) 变为了 O(1)，但由于哈希中元素数量较少，因此操作的时间并没有明显劣势。进群 619881427 可以免费获取文中知识点的视频资料。</p>

<p>hashtable：一个 hashtable 由 1 个 dict 结构、2 个 dictht 结构、1 个 dictEntry 指针数组（称为 bucket）和多个 dictEntry 结构组成。</p>

<p>正常情况下（即 hashtable 没有进行 rehash 时）各部分关系如下图所示：</p>

<p><img src="https://images2018.cnblogs.com/blog/1174710/201803/1174710-20180327001627028-325473621.png" alt=""/> </p>

<p>图片改编自：《Redis 设计与实现》</p>

<p>下面从底层向上依次介绍各个部分：</p>

<p><strong>dictEntry</strong></p>

<p>dictEntry 结构用于保存键值对，结构定义如下：</p>

<pre><code class="language-c">typedef struct dictEntry{
    void *key;
    union{
        void *val;
        uint64_tu64;
        int64_ts64;
    }v;
    struct dictEntry *next;
}dictEntry;
</code></pre>

<p>其中，各个属性的功能如下：</p>

<ul>
<li>  key：键值对中的键；</li>
<li>  val：键值对中的值，使用 union(即共用体) 实现，存储的内容既可能是一个指向值的指针，也可能是 64 位整型，或无符号 64 位整型；</li>
<li>  next：指向下一个 dictEntry，用于解决哈希冲突问题</li>
</ul>

<p>在 64 位系统中，一个 dictEntry 对象占 24 字节（key/val/next 各占 8 字节）。</p>

<p><strong>bucket</strong></p>

<p>bucket 是一个数组，数组的每个元素都是指向 dictEntry 结构的指针。redis 中 bucket 数组的大小计算规则如下：大于 dictEntry 的、最小的 2<sup>n；例如，如果有</sup> 1000 个 dictEntry，那么 bucket 大小为 1024；如果有 1500 个 dictEntry，则 bucket 大小为 2048。</p>

<p><strong>dictht</strong></p>

<p>dictht 结构如下：</p>

<pre><code class="language-c">typedef struct dictht{
    dictEntry **table;
    unsigned long size;
    unsigned long sizemask;
    unsigned long used;
}dictht;
</code></pre>

<p>其中，各个属性的功能说明如下：</p>

<ul>
<li>  table 属性是一个指针，指向 bucket；</li>
<li>  size 属性记录了哈希表的大小，即 bucket 的大小；</li>
<li>  used 记录了已使用的 dictEntry 的数量；</li>
<li>  sizemask 属性的值总是为 size-1，这个属性和哈希值一起决定一个键在 table 中存储的位置。</li>
</ul>

<p><strong>dict</strong></p>

<p>一般来说，通过使用 dictht 和 dictEntry 结构，便可以实现普通哈希表的功能；但是 Redis 的实现中，在 dictht 结构的上层，还有一个 dict 结构。下面说明 dict 结构的定义及作用。</p>

<p>dict 结构如下：</p>

<pre><code class="language-c">typedef struct dict{
    dictType *type;
    void *privdata;
    dictht ht[2];
    int trehashidx;
} dict;
</code></pre>

<p>其中，type 属性和 privdata 属性是为了适应不同类型的键值对，用于创建多态字典。</p>

<p>ht 属性和 trehashidx 属性则用于 rehash，即当哈希表需要扩展或收缩时使用。ht 是一个包含两个项的数组，每项都指向一个 dictht 结构，这也是 Redis 的哈希会有 1 个 dict、2 个 dictht 结构的原因。通常情况下，所有的数据都是存在放 dict 的 ht[0] 中，ht[1] 只在 rehash 的时候使用。dict 进行 rehash 操作的时候，将 ht[0] 中的所有数据 rehash 到 ht[1] 中。然后将 ht[1] 赋值给 ht[0]，并清空 ht[1]。</p>

<p>因此，Redis 中的哈希之所以在 dictht 和 dictEntry 结构之外还有一个 dict 结构，一方面是为了适应不同类型的键值对，另一方面是为了 rehash。</p>

<h3 id="toc_33">（3）编码转换</h3>

<p>如前所述，Redis 中内层的哈希既可能使用哈希表，也可能使用压缩列表。</p>

<p>只有同时满足下面两个条件时，才会使用压缩列表：哈希中元素数量小于 512 个；哈希中所有键值对的键和值字符串长度都小于 64 字节。如果有一个条件不满足，则使用哈希表；且编码只可能由压缩列表转化为哈希表，反方向则不可能。</p>

<p>下图展示了 Redis 内层的哈希编码转换的特点：</p>

<p><img src="https://images2018.cnblogs.com/blog/1174710/201803/1174710-20180327001855681-1566128865.png" alt=""/></p>

<h2 id="toc_34">4、集合</h2>

<h3 id="toc_35">（1）概况</h3>

<p>集合（set）与列表类似，都是用来保存多个字符串，但集合与列表有两点不同：集合中的元素是无序的，因此不能通过索引来操作元素；集合中的元素不能有重复。</p>

<p>一个集合中最多可以存储 2<sup>32-1</sup> 个元素；除了支持常规的增删改查，Redis 还支持多个集合取交集、并集、差集。</p>

<h3 id="toc_36">（2）内部编码</h3>

<p>集合的内部编码可以是整数集合（intset）或哈希表（hashtable）。</p>

<p>哈希表前面已经讲过，这里略过不提；需要注意的是，集合在使用哈希表时，值全部被置为 null。</p>

<p>整数集合的结构定义如下：</p>

<pre><code class="language-c">typedef struct intset{
    uint32_t encoding;
    uint32_t length;
    int8_t contents[];
} intset;
</code></pre>

<p>其中，encoding 代表 contents 中存储内容的类型，虽然 contents（存储集合中的元素）是 int8_t 类型，但实际上其存储的值是 int16_t、int32_t 或 int64_t，具体的类型便是由 encoding 决定的；length 表示元素个数。</p>

<p>整数集合适用于集合所有元素都是整数且集合元素数量较小的时候，与哈希表相比，整数集合的优势在于集中存储，节省空间；同时，虽然对于元素的操作复杂度也由 O(n) 变为了 O(1)，但由于集合数量较少，因此操作的时间并没有明显劣势。</p>

<h3 id="toc_37">（3）编码转换</h3>

<p>只有同时满足下面两个条件时，集合才会使用整数集合：集合中元素数量小于 512 个；集合中所有元素都是整数值。如果有一个条件不满足，则使用哈希表；且编码只可能由整数集合转化为哈希表，反方向则不可能。</p>

<p>下图展示了集合编码转换的特点：</p>

<p><img src="https://images2018.cnblogs.com/blog/1174710/201803/1174710-20180327001926146-2105183556.png" alt=""/></p>

<h2 id="toc_38">5、有序集合</h2>

<h3 id="toc_39">（1）概况</h3>

<p>有序集合与集合一样，元素都不能重复；但与集合不同的是，有序集合中的元素是有顺序的。与列表使用索引下标作为排序依据不同，有序集合为每个元素设置一个分数（score）作为排序依据。</p>

<h3 id="toc_40">（2）内部编码</h3>

<p>有序集合的内部编码可以是压缩列表（ziplist）或跳跃表（skiplist）。ziplist 在列表和哈希中都有使用，前面已经讲过，这里略过不提。</p>

<p>跳跃表是一种有序数据结构，通过在每个节点中维持多个指向其他节点的指针，从而达到快速访问节点的目的。除了跳跃表，实现有序数据结构的另一种典型实现是平衡树；大多数情况下，跳跃表的效率可以和平衡树媲美，且跳跃表实现比平衡树简单很多，因此 redis 中选用跳跃表代替平衡树。跳跃表支持平均 O(logN)、最坏 O(N) 的复杂点进行节点查找，并支持顺序操作。Redis 的跳跃表实现由 zskiplist 和 zskiplistNode 两个结构组成：前者用于保存跳跃表信息（如头结点、尾节点、长度等），后者用于表示跳跃表节点。具体结构相对比较复杂，略。</p>

<h3 id="toc_41">（3）编码转换</h3>

<p>只有同时满足下面两个条件时，才会使用压缩列表：有序集合中元素数量小于 128 个；有序集合中所有成员长度都不足 64 字节。如果有一个条件不满足，则使用跳跃表；且编码只可能由压缩列表转化为跳跃表，反方向则不可能。</p>

<p>下图展示了有序集合编码转换的特点：</p>

<p><img src="https://images2018.cnblogs.com/blog/1174710/201803/1174710-20180327001936290-955216194.png" alt=""/></p>

<h1 id="toc_42">五、应用举例</h1>

<p>了解 Redis 的内存模型之后，下面通过几个例子说明其应用。</p>

<h2 id="toc_43">1、估算 Redis 内存使用量</h2>

<p>要估算 redis 中的数据占据的内存大小，需要对 redis 的内存模型有比较全面的了解，包括前面介绍的 hashtable、sds、redisobject、各种对象类型的编码方式等。</p>

<p>下面以最简单的字符串类型来进行说明。</p>

<p>假设有 90000 个键值对，每个 key 的长度是 7 个字节，每个 value 的长度也是 7 个字节（且 key 和 value 都不是整数）；下面来估算这 90000 个键值对所占用的空间。在估算占据空间之前，首先可以判定字符串类型使用的编码方式：embstr。</p>

<p>90000 个键值对占据的内存空间主要可以分为两部分：一部分是 90000 个 dictEntry 占据的空间；一部分是键值对所需要的 bucket 空间。</p>

<p>每个 dictEntry 占据的空间包括：</p>

<p>1)       一个 dictEntry，24 字节，jemalloc 会分配 32 字节的内存块</p>

<p>2)       一个 key，7 字节，所以 SDS(key) 需要 7+9=16 个字节，jemalloc 会分配 16 字节的内存块</p>

<p>3)       一个 redisObject，16 字节，jemalloc 会分配 16 字节的内存块</p>

<p>4)       一个 value，7 字节，所以 SDS(value) 需要 7+9=16 个字节，jemalloc 会分配 16 字节的内存块</p>

<p>5)       综上，一个 dictEntry 需要 32+16+16+16=80 个字节。</p>

<p>bucket 空间：bucket 数组的大小为大于 90000 的最小的 2<sup>n，是</sup> 131072；每个 bucket 元素为 8 字节（因为 64 位系统中指针大小为 8 字节）。</p>

<p>因此，可以估算出这 90000 个键值对占据的内存大小为：90000<em>80 + 131072</em>8 = 8248576。</p>

<p>进群 619881427 可以免费获取文中知识点的视频资料。</p>

<p>下面写个程序在 redis 中验证一下：</p>

<pre><code class="language-java">public class RedisTest {

　　public static Jedis jedis = new Jedis(&quot;localhost&quot;, 6379);
　　
　　public static void main(String[] args) throws Exception{
　　　　Long m1 = Long.valueOf(getMemory());
　　　　insertData();
　　　　Long m2 = Long.valueOf(getMemory());
　　　　System.out.println(m2 - m1);
　　}

　　public static void insertData(){
　　　　for(int i = 10000; i &lt; 100000; i++){
　　　　　　jedis.set(&quot;aa&quot; + i, &quot;aa&quot; + i); //key和value长度都是7字节，且不是整数
　　　　}
　　}
　　
　　public static String getMemory(){
　　　　String memoryAllLine = jedis.info(&quot;memory&quot;);
　　　　String usedMemoryLine = memoryAllLine.split(&quot;\r\n&quot;)[1];
　　　　String memory = usedMemoryLine.substring(usedMemoryLine.indexOf(&#39;:&#39;) + 1);
　　　　return memory;
　　}

}
</code></pre>

<p>运行结果：8247552</p>

<p>理论值与结果值误差在万分之 1.2，对于计算需要多少内存来说，这个精度已经足够了。之所以会存在误差，是因为在我们插入 90000 条数据之前 redis 已分配了一定的 bucket 空间，而这些 bucket 空间尚未使用。</p>

<p>作为对比将 key 和 value 的长度由 7 字节增加到 8 字节，则对应的 SDS 变为 17 个字节，jemalloc 会分配 32 个字节，因此每个 dictEntry 占用的字节数也由 80 字节变为 112 字节。此时估算这 90000 个键值对占据内存大小为：90000<em>112 + 131072</em>8 = 11128576。</p>

<p>在 redis 中验证代码如下（只修改插入数据的代码）：</p>

<pre><code class="language-java">public static void insertData(){
　　for(int i = 10000; i &lt; 100000; i++){
　　　　jedis.set(&quot;aaa&quot; + i, &quot;aaa&quot; + i); //key和value长度都是8字节，且不是整数
　　}
}
</code></pre>

<p>运行结果：11128576；估算准确。</p>

<p>对于字符串类型之外的其他类型，对内存占用的估算方法是类似的，需要结合具体类型的编码方式来确定。</p>

<h2 id="toc_44">2、优化内存占用</h2>

<p>了解 redis 的内存模型，对优化 redis 内存占用有很大帮助。下面介绍几种优化场景。</p>

<p>（1）利用 jemalloc 特性进行优化</p>

<p>上一小节所讲述的 90000 个键值便是一个例子。由于 jemalloc 分配内存时数值是不连续的，因此 key/value 字符串变化一个字节，可能会引起占用内存很大的变动；在设计时可以利用这一点。</p>

<p>例如，如果 key 的长度如果是 8 个字节，则 SDS 为 17 字节，jemalloc 分配 32 字节；此时将 key 长度缩减为 7 个字节，则 SDS 为 16 字节，jemalloc 分配 16 字节；则每个 key 所占用的空间都可以缩小一半。</p>

<p>（2）使用整型 / 长整型</p>

<p>如果是整型 / 长整型，Redis 会使用 int 类型（8 字节）存储来代替字符串，可以节省更多空间。因此在可以使用长整型 / 整型代替字符串的场景下，尽量使用长整型 / 整型。</p>

<p>（3）共享对象</p>

<p>利用共享对象，可以减少对象的创建（同时减少了 redisObject 的创建），节省内存空间。目前 redis 中的共享对象只包括 10000 个整数（0-9999）；可以通过调整 REDIS_SHARED_INTEGERS 参数提高共享对象的个数；例如将 REDIS_SHARED_INTEGERS 调整到 20000，则 0-19999 之间的对象都可以共享。</p>

<p>考虑这样一种场景：论坛网站在 redis 中存储了每个帖子的浏览数，而这些浏览数绝大多数分布在 0-20000 之间，这时候通过适当增大 REDIS_SHARED_INTEGERS 参数，便可以利用共享对象节省内存空间。</p>

<p>（4）避免过度设计</p>

<p>然而需要注意的是，不论是哪种优化场景，都要考虑内存空间与设计复杂度的权衡；而设计复杂度会影响到代码的复杂度、可维护性。</p>

<p>如果数据量较小，那么为了节省内存而使得代码的开发、维护变得更加困难并不划算；还是以前面讲到的 90000 个键值对为例，实际上节省的内存空间只有几 MB。但是如果数据量有几千万甚至上亿，考虑内存的优化就比较必要了。</p>

<h2 id="toc_45">3、关注内存碎片率</h2>

<p>内存碎片率是一个重要的参数，对 redis 内存的优化有重要意义。</p>

<p>如果内存碎片率过高（jemalloc 在 1.03 左右比较正常），说明内存碎片多，内存浪费严重；这时便可以考虑重启 redis 服务，在内存中对数据进行重排，减少内存碎片。</p>

<p>如果内存碎片率小于 1，说明 redis 内存不足，部分数据使用了虚拟内存（即 swap）；由于虚拟内存的存取速度比物理内存差很多（2-3 个数量级），此时 redis 的访问速度可能会变得很慢。因此必须设法增大物理内存（可以增加服务器节点数量，或提高单机内存），或减少 redis 中的数据。</p>

<p>要减少 redis 中的数据，除了选用合适的数据类型、利用共享对象等，还有一点是要设置合理的数据回收策略（maxmemory-policy），当内存达到一定量后，根据不同的优先级对内存进行回收。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java 并发编程：阻塞队列]]></title>
    <link href="http://panlw.github.io/15293277939094.html"/>
    <updated>2018-06-18T21:16:33+08:00</updated>
    <id>http://panlw.github.io/15293277939094.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>作者：海子<br/>
原文：<a href="http://www.cnblogs.com/dolphin0520/p/3932906.html">http://www.cnblogs.com/dolphin0520/p/3932906.html</a></p>
</blockquote>

<p>使用非阻塞队列的时候有一个很大问题就是：它不会对当前线程产生阻塞，那么在面对类似消费者 - 生产者的模型时，就必须额外地实现同步策略以及线程间唤醒策略，这个实现起来就非常麻烦。但是有了阻塞队列就不一样了，它会对当前线程产生阻塞，比如一个线程从一个空的阻塞队列中取元素，此时线程会被阻塞直到阻塞队列中有了元素。当队列中有元素后，被阻塞的线程会自动被唤醒（不需要我们编写代码去唤醒）。这样提供了极大的方便性。</p>

<p>本文先讲述一下 java.util.concurrent 包下提供主要的几种阻塞队列，然后分析了阻塞队列和非阻塞队列的中的各个方法，接着分析了阻塞队列的实现原理，最后给出了一个实际例子和几个使用场景。</p>

<p>　　一. 几种主要的阻塞队列</p>

<p>　　二. 阻塞队列中的方法 VS 非阻塞队列中的方法</p>

<p>　　三. 阻塞队列的实现原理</p>

<p>　　四. 示例和使用场景</p>

<p>若有不正之处请多多谅解，并欢迎批评指正。</p>

<h2 id="toc_0">一. 几种主要的阻塞队列</h2>

<p>自从 Java 1.5 之后，在 java.util.concurrent 包下提供了若干个阻塞队列，主要有以下几个：</p>

<ul>
<li><p>ArrayBlockingQueue：基于数组实现的一个阻塞队列，在创建 ArrayBlockingQueue 对象时必须制定容量大小。并且可以指定公平性与非公平性，默认情况下为非公平的，即不保证等待时间最长的队列最优先能够访问队列。</p></li>
<li><p>LinkedBlockingQueue：基于链表实现的一个阻塞队列，在创建 LinkedBlockingQueue 对象时如果不指定容量大小，则默认大小为 Integer.MAX_VALUE。</p></li>
<li><p>PriorityBlockingQueue：以上 2 种队列都是先进先出队列，而 PriorityBlockingQueue 却不是，它会按照元素的优先级对元素进行排序，按照优先级顺序出队，每次出队的元素都是优先级最高的元素。注意，此阻塞队列为无界阻塞队列，即容量没有上限（通过源码就可以知道，它没有容器满的信号标志），前面 2 种都是有界队列。</p></li>
<li><p>DelayQueue：基于 PriorityQueue，一种延时阻塞队列，DelayQueue 中的元素只有当其指定的延迟时间到了，才能够从队列中获取到该元素。DelayQueue 也是一个无界队列，因此往队列中插入数据的操作（生产者）永远不会被阻塞，而只有获取数据的操作（消费者）才会被阻塞。</p></li>
</ul>

<h2 id="toc_1">二. 阻塞队列中的方法 VS 非阻塞队列中的方法</h2>

<p><strong>1. 非阻塞队列中的几个主要方法：</strong></p>

<ul>
<li><p><strong>add(E e)：</strong>将元素 e 插入到队列末尾，如果插入成功，则返回 true；如果插入失败（即队列已满），则会抛出异常；</p></li>
<li><p><strong>remove()：</strong>移除队首元素，若移除成功，则返回 true；如果移除失败（队列为空），则会抛出异常；</p></li>
<li><p><strong>offer(E e)：</strong>将元素 e 插入到队列末尾，如果插入成功，则返回 true；如果插入失败（即队列已满），则返回 false；</p></li>
<li><p><strong>poll()：</strong>移除并获取队首元素，若成功，则返回队首元素；否则返回 null；</p></li>
<li><p><strong>peek()：</strong>获取队首元素，若成功，则返回队首元素；否则返回 null</p></li>
</ul>

<p>对于非阻塞队列，一般情况下建议使用 offer、poll 和 peek 三个方法，不建议使用 add 和 remove 方法。因为使用 offer、poll 和 peek 三个方法可以通过返回值判断操作成功与否，而使用 add 和 remove 方法却不能达到这样的效果。注意，非阻塞队列中的方法都没有进行同步措施。</p>

<p><strong>2. 阻塞队列中的几个主要方法：</strong></p>

<p>阻塞队列包括了非阻塞队列中的大部分方法，上面列举的 5 个方法在阻塞队列中都存在，但是要注意这 5 个方法在阻塞队列中都进行了同步措施。</p>

<p>除此之外，阻塞队列提供了另外 4 个非常有用的方法：</p>

<p>　　put(E e)</p>

<p>　　take()</p>

<p>　　offer(E e,long timeout, TimeUnit unit)</p>

<p>　　poll(long timeout, TimeUnit unit)</p>

<p>这四个方法的理解：</p>

<p>　　put 方法用来向队尾存入元素，如果队列满，则等待；</p>

<p>　　take 方法用来从队首取元素，如果队列为空，则等待；</p>

<p>　　offer 方法用来向队尾存入元素，如果队列满，则等待一定的时间，当时间期限达到时，如果还没有插入成功，则返回 false；否则返回 true；</p>

<p>　　poll 方法用来从队首取元素，如果队列空，则等待一定的时间，当时间期限达到时，如果取到，则返回 null；否则返回取得的元素；</p>

<h2 id="toc_2">三. 阻塞队列的实现原理</h2>

<p>前面谈到了非阻塞队列和阻塞队列中常用的方法，下面来探讨阻塞队列的实现原理，本文以 ArrayBlockingQueue 为例，其他阻塞队列实现原理可能和 ArrayBlockingQueue 有一些差别，但是大体思路应该类似，有兴趣的朋友可自行查看其他阻塞队列的实现源码。</p>

<p>首先看一下 ArrayBlockingQueue 类中的几个成员变量：</p>

<pre><code class="language-java">public class ArrayBlockingQueue&lt;E&gt; extends AbstractQueue&lt;E&gt;
implements BlockingQueue&lt;E&gt;, java.io.Serializable {

private static final long serialVersionUID = -817911632652898426L;

/** The queued items  */
private final E[] items;
/** items index for next take, poll or remove */
private int takeIndex;
/** items index for next put, offer, or add. */
private int putIndex;
/** Number of items in the queue */
private int count;

/*
* Concurrency control uses the classic two-condition algorithm
* found in any textbook.
*/

/** Main lock guarding all access */
private final ReentrantLock lock;
/** Condition for waiting takes */
private final Condition notEmpty;
/** Condition for waiting puts */
private final Condition notFull;
}
</code></pre>

<p>可以看出，ArrayBlockingQueue 中用来存储元素的实际上是一个数组，takeIndex 和 putIndex 分别表示队首元素和队尾元素的下标，count 表示队列中元素的个数。</p>

<p>lock 是一个可重入锁，notEmpty 和 notFull 是等待条件。</p>

<p>下面看一下 ArrayBlockingQueue 的构造器，构造器有三个重载版本：</p>

<pre><code class="language-java">public ArrayBlockingQueue(int capacity) {
}
public ArrayBlockingQueue(int capacity, boolean fair) {

}
public ArrayBlockingQueue(int capacity, boolean fair,
                  Collection&lt;? extends E&gt; c) {
}
</code></pre>

<p>第一个构造器只有一个参数用来指定容量，第二个构造器可以指定容量和公平性，第三个构造器可以指定容量、公平性以及用另外一个集合进行初始化。</p>

<p>然后看它的两个关键方法的实现：put() 和 take()：</p>

<pre><code class="language-java">public void put(E e) throws InterruptedException {
   if (e == null) throw new NullPointerException();
   final E[] items = this.items;
   final ReentrantLock lock = this.lock;
   lock.lockInterruptibly();
   try {
       try {
           while (count == items.length)
               notFull.await();
       } catch (InterruptedException ie) {
           notFull.signal(); // propagate to non-interrupted thread
           throw ie;
       }
       insert(e);
   } finally {
       lock.unlock();
   }
}
</code></pre>

<p>从 put 方法的实现可以看出，它先获取了锁，并且获取的是可中断锁，然后判断当前元素个数是否等于数组的长度，如果相等，则调用 notFull.await() 进行等待，如果捕获到中断异常，则唤醒线程并抛出异常。</p>

<p>当被其他线程唤醒时，通过 insert(e) 方法插入元素，最后解锁。</p>

<p>我们看一下 insert 方法的实现：</p>

<pre><code class="language-java">private void insert(E x) {
   items[putIndex] = x;
   putIndex = inc(putIndex);
   ++count;
   notEmpty.signal();
}
</code></pre>

<p>它是一个 private 方法，插入成功后，通过 notEmpty 唤醒正在等待取元素的线程。</p>

<p>下面是 take() 方法的实现：</p>

<pre><code class="language-java">public E take() throws InterruptedException {
   final ReentrantLock lock = this.lock;
   lock.lockInterruptibly();
   try {
       try {
           while (count == 0)
               notEmpty.await();
       } catch (InterruptedException ie) {
           notEmpty.signal(); 
           // propagate to non-interrupted thread
           throw ie;
       }
       E x = extract();
       return x;
   } finally {
       lock.unlock();
   }
}
</code></pre>

<p>跟 put 方法实现很类似，只不过 put 方法等待的是 notFull 信号，而 take 方法等待的是 notEmpty 信号。</p>

<p>在 take 方法中，如果可以取元素，则通过 extract 方法取得元素，下面是 extract 方法的实现：</p>

<pre><code class="language-java">private E extract() {
   final E[] items = this.items;
   E x = items[takeIndex];
   items[takeIndex] = null;
   takeIndex = inc(takeIndex);
   --count;
   notFull.signal();
   return x;
}
</code></pre>

<p>跟 insert 方法也很类似。</p>

<p>其实从这里大家应该明白了阻塞队列的实现原理，事实它和我们用 Object.wait()、Object.notify() 和非阻塞队列实现生产者 - 消费者的思路类似，只不过它把这些工作一起集成到了阻塞队列中实现。</p>

<h2 id="toc_3">四. 示例和使用场景</h2>

<p>下面先使用 Object.wait() 和 Object.notify()、非阻塞队列实现生产者 - 消费者模式：</p>

<pre><code class="language-java">public class Test {
   private int queueSize = 10;
   private PriorityQueue&lt;Integer&gt; queue = new PriorityQueue&lt;Integer&gt;(queueSize);

   public static void main(String[] args)  {
       Test test = new Test();
       Producer producer = test.new Producer();
       Consumer consumer = test.new Consumer();

       producer.start();
       consumer.start();
   }

   class Consumer extends Thread{

       @Override
       public void run() {
           consume();
       }

       private void consume() {
           while(true){
               synchronized (queue) {
                   while(queue.size() == 0){
                       try {
                           System.out.println(&quot;队列空，等待数据&quot;);
                           queue.wait();
                       } catch (InterruptedException e) {
                           e.printStackTrace();
                           queue.notify();
                       }
                   }
                   queue.poll();          //每次移走队首元素
                   queue.notify();
                   System.out.println(&quot;从队列取走一个元素，队列剩余&quot;+
                   queue.size()+&quot;个元素&quot;);
               }
           }
       }
   }

   class Producer extends Thread{

       @Override
       public void run() {
           produce();
       }

       private void produce() {
           while(true){
               synchronized (queue) {
                   while(queue.size() == queueSize){
                       try {
                           System.out.println(&quot;队列满，等待有空余空间&quot;);
                           queue.wait();
                       } catch (InterruptedException e) {
                           e.printStackTrace();
                           queue.notify();
                       }
                   }
                   queue.offer(1);        //每次插入一个元素
                   queue.notify();
                   System.out.println(&quot;向队列取中插入一个元素，队列剩余空间：&quot;+
                   (queueSize-queue.size()));
               }
           }
       }
   }
}
</code></pre>

<p>这个是经典的生产者 - 消费者模式，通过阻塞队列和 Object.wait() 和 Object.notify() 实现，wait() 和 notify() 主要用来实现线程间通信。</p>

<p>具体的线程间通信方式（wait 和 notify 的使用）在后续问章中会讲述到。</p>

<p>下面是使用阻塞队列实现的生产者 - 消费者模式：</p>

<pre><code class="language-java">public class Test {
   private int queueSize = 10;
   private ArrayBlockingQueue&lt;Integer&gt; queue = 
   new ArrayBlockingQueue&lt;Integer&gt;(queueSize);

   public static void main(String[] args)  {
       Test test = new Test();
       Producer producer = test.new Producer();
       Consumer consumer = test.new Consumer();

       producer.start();
       consumer.start();
   }

   class Consumer extends Thread{

       @Override
       public void run() {
           consume();
       }

       private void consume() {
           while(true){
               try {
                   queue.take();
                   System.out.println(&quot;从队列取走一个元素，队列剩余&quot;+
                   queue.size()+&quot;个元素&quot;);
               } catch (InterruptedException e) {
                   e.printStackTrace();
               }
           }
       }
   }

   class Producer extends Thread{

       @Override
       public void run() {
           produce();
       }

       private void produce() {
           while(true){
               try {
                   queue.put(1);
                   System.out.println(&quot;向队列取中插入一个元素，队列剩余空间：&quot;+
                   (queueSize-queue.size()));
               } catch (InterruptedException e) {
                   e.printStackTrace();
               }
           }
       }
   }
}
</code></pre>

<p>有没有发现，使用阻塞队列代码要简单得多，不需要再单独考虑同步和线程间通信的问题。</p>

<p>在并发编程中，一般推荐使用阻塞队列，这样实现可以尽量地避免程序出现意外的错误。</p>

<p>阻塞队列使用最经典的场景就是 socket 客户端数据的读取和解析，读取数据的线程不断将数据放入队列，然后解析线程不断从队列取数据解析。还有其他类似的场景，只要符合生产者 - 消费者模型的都可以使用阻塞队列。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java如何实现任务超时处理]]></title>
    <link href="http://panlw.github.io/15293276617956.html"/>
    <updated>2018-06-18T21:14:21+08:00</updated>
    <id>http://panlw.github.io/15293276617956.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://mp.weixin.qq.com/s/2yEMqv6v26nnDR6s0kLZnw">https://mp.weixin.qq.com/s/2yEMqv6v26nnDR6s0kLZnw</a></p>
</blockquote>

<p>任务超时处理是比较常见的需求，比如在进行一些比较耗时的操作（如网络请求）或者在占用一些比较宝贵的资源（如数据库连接）时，我们通常需要给这些操作设置一个超时时间，当执行时长超过设置的阈值的时候，就终止操作并回收资源。Java 中对超时任务的处理有两种方式：一种是基于异步任务结果的超时获取，一种则是使用延时任务来终止超时操作。下文将详细说明。</p>

<p>一、基于异步任务结果的超时获取</p>

<p>基于异步任务结果的获取通常是跟线程池一起使用的，我们向线程池提交任务时会返回一个 Future 对象，在调用 Future 的 get 方法时，可以设置一个超时时间，如果超过设置的时间任务还没结束，就抛出异常。接下来看代码：</p>

<pre><code class="language-java">public class FutureDemo {

    static ExecutorService executorService= Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors()*2);

    public static void main(String[] args) {
        Future&lt;String&gt; future = executorService.submit(new Callable&lt;String&gt;() {
            @Override
            public String call() {
                try {
                    TimeUnit.SECONDS.sleep(10);
                } catch (InterruptedException e) {
                    System.out.println(&quot;任务被中断。&quot;);
                }
                return  &quot;OK&quot;;
            }
        });
        try {
            String result = future.get(2, TimeUnit.SECONDS);
        } catch (InterruptedException |ExecutionException | TimeoutException e) {
            future.cancel(true);
            System.out.println(&quot;任务超时。&quot;);
        }finally {
            System.out.println(&quot;清理资源。&quot;);
        }
    }
}
</code></pre>

<p>运行代码，输出如下：</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/XA3sPCPib1l5IMwDuGO307gROb7pTu3FKDlcv4ZjuXvM2BuhpB15GeE19ga65YynWsicJl2BWr0zWljJzFJPH4zA/640?wx_fmt=png" alt=""/></p>

<p>二、使用延时任务来终止超时操作</p>

<p>还有一种实现任务超时处理的思路是在提交任务之前先设置一个定时器，这个定时器会在设置的时间间隔之后去取消任务。当然如果任务在规定的时间内完成了，要记得取消定时器。首先来看一下我们的工作线程：</p>

<pre><code class="language-java">public class RunningTask  {

    private volatile boolean isStop;

    public void stop(){
        this.isStop=true;
    }

    public void doing() {
        int i=1;
        while (!isStop){
            try {
                TimeUnit.SECONDS.sleep(1);
            } catch (InterruptedException e) {

            }
        }
        System.out.println(&quot;任务被中断。&quot;);
    }
}
</code></pre>

<p>这个工作线程每隔一秒钟会去检查下 isStop 变量，因此我们可以通过 isStop 变量来取消任务。至于取消任务的逻辑我们放在了定时器里面，代码如下：</p>

<pre><code class="language-java">public class CancelTask implements Runnable {

    private RunningTask runningTask;

    public CancelTask(RunningTask runningTask) {
        this.runningTask = runningTask;
    }

    @Override
    public void run() {
        runningTask.stop();
    }
}
</code></pre>

<p>可以看到，该定时器的作用就是在一定的时间之后去中断工作线程的运行。接下来测试一下：</p>

<pre><code class="language-java">public class ScheduleDemo {

    static ScheduledExecutorService executorService= Executors.newScheduledThreadPool(Runtime.getRuntime().availableProcessors()*2);

    public static void main(String[] args) {
        RunningTask runningTask=new RunningTask();
        ScheduledFuture&lt;?&gt; scheduledFuture = executorService.schedule(new CancelTask(runningTask), 3, TimeUnit.SECONDS);
        runningTask.doing();
        if(!scheduledFuture.isDone()){
            scheduledFuture.cancel(true);
        }
    }
}
</code></pre>

<p>运行结果如下：</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/XA3sPCPib1l5IMwDuGO307gROb7pTu3FKFIFzub5qZic3f9xxSRFI1KeXXG7VbGTJujrN4EbIriaqaeor9dfA3d5Q/640?wx_fmt=png" alt=""/></p>

<p>可以看到，任务在超时之后也可以被取消。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[对高并发流量控制的一点思考]]></title>
    <link href="http://panlw.github.io/15293273764686.html"/>
    <updated>2018-06-18T21:09:36+08:00</updated>
    <id>http://panlw.github.io/15293273764686.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="http://blog.51cto.com/zhangfengzhe">zfz_linux_boy</a> 2018-01-30<br/>
<a href="http://blog.51cto.com/zhangfengzhe/2066683">http://blog.51cto.com/zhangfengzhe/2066683</a></p>
</blockquote>

<h2 id="toc_0">前言</h2>

<p>在实际项目中，曾经遭遇过线上 5W+QPS 的峰值，也在压测状态下经历过 10W+QPS 的大流量请求，本篇博客的话题主要就是自己对高并发流量控制的一点思考。</p>

<h2 id="toc_1">应对大流量的一些思路</h2>

<blockquote>
<p>首先，我们来说一下什么是大流量？</p>

<p>大流量，我们很可能会冒出：TPS（每秒事务量），QPS（每秒请求量），1W+，5W+，10W+，100W+...。其实并没有一个绝对的数字，如果这个量造成了系统的压力，影响了系统的性能，那么这个量就可以称之为大流量了。</p>

<p>其次，应对大流量的一些常见手段是什么？</p>

<p>缓存：说白了，就是让数据尽早进入缓存，离程序近一点，不要大量频繁的访问 DB。</p>

<p>降级：如果不是核心链路，那么就把这个服务降级掉。打个比喻，现在的 APP 都讲究千人千面，拿到数据后，做个性化排序展示，如果在大流量下，这个排序就可以降级掉！</p>

<p>限流：大家都知道，北京地铁早高峰，地铁站都会做一件事情，就是限流了！想法很直接，就是想在一定时间内把请求限制在一定范围内，保证系统不被冲垮，同时尽可能提升系统的吞吐量。</p>

<p>注意到，有些时候，缓存和降级是解决不了问题的，比如，电商的双十一，用户的购买，下单等行为，是涉及到大量写操作，而且是核心链路，无法降级的，这个时候，限流就比较重要了。</p>

<p>那么接下来，我们重点说一下，限流。</p>
</blockquote>

<h2 id="toc_2">限流的常用方式</h2>

<blockquote>
<p>限流的常用处理手段有：计数器、滑动窗口、漏桶、令牌。</p>
</blockquote>

<p><strong>计数器</strong> </p>

<blockquote>
<p>计数器是一种比较简单的限流算法，用途比较广泛，在接口层面，很多地方使用这种方式限流。在一段时间内，进行计数，与阀值进行比较，到了时间临界点，将计数器清 0。</p>
</blockquote>

<p><img src="http://i2.51cto.com/images/blog/201801/30/26433210356cfa9d26407d07d0819b14.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" alt=""/></p>

<p><img src="http://i2.51cto.com/images/blog/201801/30/36bd0e076d23e7bcd10ac0b1ac498bb7.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" alt=""/></p>

<blockquote>
<p>这里需要注意的是，存在一个时间临界点的问题。举个栗子，在 12:01:00 到 12:01:58 这段时间内没有用户请求，然后在 12:01:59 这一瞬时发出 100 个请求，OK，然后在 12:02:00 这一瞬时又发出了 100 个请求。这里你应该能感受到，在这个临界点可能会承受恶意用户的大量请求，甚至超出系统预期的承受。</p>
</blockquote>

<p><strong>滑动窗口</strong> </p>

<blockquote>
<p>由于计数器存在临界点缺陷，后来出现了滑动窗口算法来解决。</p>

<p><img src="http://i2.51cto.com/images/blog/201801/30/292f2358a31662de2846a122e25bb005.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" alt=""/></p>

<p>滑动窗口的意思是说把固定时间片，进行划分，并且随着时间的流逝，进行移动，这样就巧妙的避开了计数器的临界点问题。也就是说这些固定数量的可以移动的格子，将会进行计数判断阀值，因此格子的数量影响着滑动窗口算法的精度。</p>
</blockquote>

<p><strong>漏桶</strong> </p>

<blockquote>
<p>虽然滑动窗口有效避免了时间临界点的问题，但是依然有时间片的概念，而漏桶算法在这方面比滑动窗口而言，更加先进。</p>

<p>有一个固定的桶，进水的速率是不确定的，但是出水的速率是恒定的，当水满的时候是会溢出的。</p>
</blockquote>

<p><img src="http://i2.51cto.com/images/blog/201801/30/0bdc37469876669d8ff3813da367705d.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" alt=""/></p>

<p><img src="http://i2.51cto.com/images/blog/201801/30/244ab47205fd764fd74e34f4f0da1cb8.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" alt=""/></p>

<p><strong>令牌桶</strong> </p>

<blockquote>
<p>注意到，漏桶的出水速度是恒定的，那么意味着如果瞬时大流量的话，将有大部分请求被丢弃掉（也就是所谓的溢出）。为了解决这个问题，令牌桶进行了算法改进。</p>

<p><img src="http://i2.51cto.com/images/blog/201801/30/35a6b4a0842d6ac0e5f5b14b7c64189d.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" alt=""/></p>

<p>生成令牌的速度是恒定的，而请求去拿令牌是没有速度限制的。这意味，面对瞬时大流量，该算法可以在短时间内请求拿到大量令牌，而且拿令牌的过程并不是消耗很大的事情。（有一点生产令牌，消费令牌的意味）</p>

<p>不论是对于令牌桶拿不到令牌被拒绝，还是漏桶的水满了溢出，都是为了保证大部分流量的正常使用，而牺牲掉了少部分流量，这是合理的，如果因为极少部分流量需要保证的话，那么就可能导致系统达到极限而挂掉，得不偿失。</p>
</blockquote>

<p><img src="http://i2.51cto.com/images/blog/201801/30/6b63bf4412c0d412095131091144e357.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" alt=""/></p>

<h2 id="toc_3">限流神器：Guava RateLimiter</h2>

<blockquote>
<p>Guava 不仅仅在集合、缓存、异步回调等方面功能强大，而且还给我们封装好了限流的 API！</p>

<p>Guava RateLimiter 基于令牌桶算法，我们只需要告诉 RateLimiter 系统限制的 QPS 是多少，那么 RateLimiter 将以这个速度往桶里面放入令牌，然后请求的时候，通过 tryAcquire() 方法向 RateLimiter 获取许可（令牌）。</p>
</blockquote>

<p><img src="http://i2.51cto.com/images/blog/201801/30/57203303c36a7c5fb5a0974ba98116d7.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" alt=""/></p>

<h2 id="toc_4">分布式场景下的限流</h2>

<p><strong>上面所说的限流的一些方式，都是针对单机而言的，其实大部分的场景，单机的限流已经足够了。分布式下限流的手段常常需要多种技术相结合，比如 Nginx+Lua，Redis+Lua 等去做。本文主要讨论的是单机的限流，这里就不在详细介绍分布式场景下的限流了。</strong> <br/>
<strong>一句话，让系统的流量，先到队列中排队、限流，不要让流量直接打到系统上。</strong></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[从节省Redis内存空间说开去]]></title>
    <link href="http://panlw.github.io/15293215099975.html"/>
    <updated>2018-06-18T19:31:49+08:00</updated>
    <id>http://panlw.github.io/15293215099975.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://mp.weixin.qq.com/s/tsZxYwoFtWwzOH1E28y6zg">https://mp.weixin.qq.com/s/tsZxYwoFtWwzOH1E28y6zg</a><br/>
<a href="https://blog.csdn.net/clevercode/article/details/46691645">https://blog.csdn.net/clevercode/article/details/46691645</a></p>
</blockquote>

<p><strong>前言</strong></p>

<p>上周部门会议上讨论的一个议题是如何节省 Redis 内存空间，其中有个小伙伴提到可以从压缩字符串入手，我觉得这是一个可以尝试的思路。因为有时候我们存在 Redis 中的值比较大，如果能对这些大字符串进行压缩，那么节省的内存空间还是很可观的。接下来将介绍几种常见的数据压缩算法，供大家参考。</p>

<h1 id="toc_0">1 RLE</h1>

<p>RLE 又叫 Run Length Encoding ，是一个针对无损压缩的非常简单的算法。它用重复字节和重复的次数来简单描述来代替重复的字节。尽管简单并且对于通常的压缩非常低效，但它有的时候却非常有用（例如， JPEG 就使用它）。</p>

<h2 id="toc_1">1.1 原理</h2>

<p>图 2.1 显示了一个如何使用 RLE 算法来对一个数据流编码的例子，其中出现六次的符号‘ 93 ’已经用 3 个字节来代替：一个标记字节（‘ 0 ’在本例中）重复的次数（‘ 6 ’）和符号本身（‘ 93 ’）。</p>

<p>RLE 解码器遇到符号‘ 0 ’ 的时候，它表明后面的两个字节决定了需要输出哪个符号以及输出多少次。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/XA3sPCPib1l6jia5WKgazEEVDPjibKHOz8rslINaKDFMvfr4bOibjNz1LLxJSDU1oI34BCTibPjokcy8BgFbKjwXHNg/640?wx_fmt=jpeg" alt=""/></p>

<p>1.2 实现</p>

<p>RLE 可以使用很多不同的方法。基本压缩库中详细实现的方式是非常有效的一个。一个特殊的标记字节用来指示重复节的开始，而不是对于重复非重复节都 coding run 。</p>

<p>因此非重复节可以有任意长度而不被控制字节打断，除非指定的标记字节出现在非重复节（顶多以两个字节来编码）的稀有情况下。为了最优化效率，标记字节应该是输入流中最少出现的符号（或许就不存在）。</p>

<p>重复 runs 能够在 32768 字节的时候运转。少于 129 字节的要求 3 个字节编码（标记 + 次数 + 符号），而大雨 128 字节要求四个字节（标记 + 次数的高 4 位 |0x80+ 次数的低 4 位）。这是通常所有采用的压缩的做法，并且也是相比较三个字节固定编码（允许使用 3 个字节来编码 256 个字节）而言非常少见的有损压缩率的方法。</p>

<p>在这种模式下，最坏的压缩结果是：</p>

<p>输出大小 =257/256* 输入大小 +1</p>

<h1 id="toc_2">2 哈夫曼</h1>

<p>哈夫曼编码是无损压缩当中最好的方法。它使用预先二进制描述来替换每个符号，长度由特殊符号出现的频率决定。常见的符号需要很少的位来表示，而不常见的符号需要很多位来表示。</p>

<p>哈夫曼算法在改变任何符号二进制编码引起少量密集表现方面是最佳的。然而，它并不处理符号的顺序和重复或序号的序列。</p>

<h2 id="toc_3">2.1 原理</h2>

<p>我不打算探究哈夫曼编码的所有实际的细节，但基本的原理是为每个符号找到新的二进制表示，从而通常符号使用很少的位，不常见的符号使用较多的位。</p>

<p>简短的说，这个问题的解决方案是为了查找每个符号的通用程度，我们建立一个未压缩数据的柱状图；通过递归拆分这个柱状图为两部分来创建一个二叉树，每个递归的一半应该和另一半具有同样的权（权是 ∑ <sup style="box-sizing: border-box;outline: 0px;word-break: break-all;">N </sup><sub style="box-sizing: border-box;outline: 0px;word-break: break-all;">K </sub>=<sub style="box-sizing: border-box;outline: 0px;word-break: break-all;">1</sub> 符号数 <sub style="box-sizing: border-box;outline: 0px;word-break: break-all;">k </sub>, N 是分之中符号的数量，符号数 <sub style="box-sizing: border-box;outline: 0px;word-break: break-all;">k </sub>是符号 k 出现的次数 ）</p>

<p>这棵树有两个目的：</p>

<p>1．   编码器使用这棵树来找到每个符号最优的表示方法<br/>
2．   解码器使用这棵树唯一的标识在压缩流中每个编码的开始和结束，其通过在读压缩数据位的时候自顶向底的遍历树，选择基于数据流中的每个独立位的分支，一旦一个到达叶子节点，解码器知道一个完整的编码已经读出来了。</p>

<p>我们来看一个例子会让我们更清楚。图 2.2 显示了一个 10 个字节的未压缩的数据。</p>

<p>根据符号频率，哈夫曼编码器生成哈夫曼树（图 2.4 ）和相应的编码表示（图 2.3 ）。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/XA3sPCPib1l6jia5WKgazEEVDPjibKHOz8rN4YBTn5M9deDfQ84oS1YicO18tc4XJVmgZcpCSGQP0nEnn1FDgf6O4w/640?wx_fmt=jpeg" alt=""/></p>

<p>你可以看到，常见的符号接近根，因此只要少数位来表示。于是最终的压缩数据流如图 2.5 所示。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/XA3sPCPib1l6jia5WKgazEEVDPjibKHOz8rN5XbjThAK0X8TjTjvx1sATgTetFwyn6jL9sA8HFBX35RVOTHooYt1A/640?wx_fmt=jpeg" alt=""/></p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/XA3sPCPib1l6jia5WKgazEEVDPjibKHOz8rpfNt5vdooNB0tu2XO2wrByeZzTdHxlxeJWKsjyAB0EFUv0ibvC7W9Hg/640?wx_fmt=jpeg" alt=""/></p>

<p>压缩后的数据流是 24 位（三个字节），原来是 80 位（ 10 个字节）。当然，我应该存储哈夫曼树，这样解码器就能够解码出对应的压缩流了，这就使得该例子中的真正数据流比输入的流数据量大。这是相对较短的数据上的副作用。对于大数据量来说，上面的哈夫曼树就不占太多比例了。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/XA3sPCPib1l6jia5WKgazEEVDPjibKHOz8rwpdS3euibfb2eNiaAWD0PpCeF3qppC2KQbaZz2Zv7fVdts3becE6LYJw/640?wx_fmt=jpeg" alt=""/></p>

<p>解码的时候，从上到下遍历树，为压缩的流选择从左 / 右分支，每次碰到一个叶子节点的时候，就可以将对应的字节写到解压输出流中，然后再从根开始遍历。</p>

<h2 id="toc_4">2.2 实现</h2>

<p>哈夫曼编码器可以在基本压缩库中找到，其是非常直接的实现。</p>

<p>这个实现的基本缺陷是：</p>

<p>1．   慢位流实现<br/>
2．   相当慢的解码（比编码慢）<br/>
3．   最大的树深度是 32 （编码器在任何超过 32 位大小的时候退出）。如果我不是搞错的话，这是不可能的，除非输出的数据大于 2 <sup style="box-sizing: border-box;outline: 0px;word-break: break-all;">32</sup> 字节。</p>

<p>另一方面，这个实现有几个优点：</p>

<p>1．   哈夫曼树以一个紧密的形式每个符号要求 12 位（对于 8 位的符号）的方式存储，这意味着最大的头为 384 。<br/>
2．   编码相当容易理解</p>

<p>哈夫曼编码在数据有噪音的情况（不是有规律的，例如 RLE ）下非常好，这中情况下大多数基于字典方式的编码器都有问题。</p>

<h1 id="toc_5">3 Rice</h1>

<p>对于由大 word （例如： 16 或 32 位）组成的数据和较低的数据值， Rice 编码能够获得较好的压缩比。音频和高动态变化的图像都是这种类型的数据，它们被预处理过（例如 delta 相邻的采样）。</p>

<p>尽管哈夫曼编码处理这种数据是最优的，却由于几个原因而不适合处理这种数据（例如： 32 位大小要求 16GB 的柱状图缓冲区来进行哈夫曼树编码）。因此一个比较动态的方式更适合由大 word 组成的数据。</p>

<h2 id="toc_6">3.1 原理</h2>

<p>Rice 编码背后的基本思想是尽可能的用较少的位来存储多个字（正像使用哈夫曼编码一样）。实际上，有人可能想到 Rice 是静态的哈夫曼编码（例如，编码不是由实际数据内容的统计信息决定，而是由小的值比高的值常见的假定决定）。</p>

<p>编码非常简单：将值 X 用 X 个‘ 1 ’位之后跟一个 0 位来表示。</p>

<h2 id="toc_7">3.2 实现</h2>

<p>在基本压缩库针对 Rice 做了许多优化：</p>

<p>1．   每个字最没有意义的位被存储为 k 和最有意义的 N-k 位用 Rice 编码。 K 作为先前流中少许采样的位平均数。这是通常最好使用 Rice 编码的方法，隐藏噪音且对于动态变化的范围并不导致非常长的 Rice 编码。<br/>
2．   如果 rice 编码比固定的开端长， T ，一个可选的编码：输出 T 个‘ 1 ’位，紧跟（ log2(X-T) ）个‘ 1 ’和一个‘ 0 ’位，接着是 X-T （最没有意义的 (log2(X-T))-1 位）。这对于大值来说都是比较高效的代码并且阻止可笑的长 Rice 编码（最坏的情况，对于一个 32 位 word 单个 Rice 编码可能变成 2 <sup style="box-sizing: border-box;outline: 0px;word-break: break-all;">32 </sup>位或 512MB ）。</p>

<p>如果开端是 4 ，下面是结果编码表：</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/XA3sPCPib1l6jia5WKgazEEVDPjibKHOz8rOOibqB37z6QKMg3grXJh2GSTCkVFrLo5oIy3J9oxc8OORZ3nib9yCd9w/640?wx_fmt=png" alt=""/></p>

<p>就像你看到的一样，在这个实现中使用 threshold 方法仅仅两个编码导致一个最坏的情况；剩下的编码产生比标准 Rice 编码还要短的编码。</p>

<h1 id="toc_8">4 Lempel-Ziv (LZ77)</h1>

<p>Lempel-Ziv 压缩模式有许多不同的变量。基本压缩库有清晰的 LZ77 算法的实现（ Lempel-Ziv ， 1977 ），执行的很好，源代码也非常容易理解。</p>

<p>LZ 编码器能用来通用目标的压缩，特别对于文本执行的很好。它也在 RLE 和哈夫曼编码器（ RLE ， LZ ，哈夫曼）中使用来大多数情况下获得更多的压缩。</p>

<h2 id="toc_9">4.1 原理</h2>

<p>在 LZ 压缩算法的背后是使用 RLE 算法用先前出现的相同字节序列的引用来替代。</p>

<p>简单的讲， LZ 算法被认为是字符串匹配的算法。例如：在一段文本中某字符串经常出现，并且可以通过前面文本中出现的字符串指针来表示。当然这个想法的前提是指针应该比字符串本身要短。</p>

<p>例如，在上一段短语 “字符串” 经常出现，可以将除第一个字符串之外的所有用第一个字符串引用来表示从而节省一些空间。</p>

<p>一个字符串引用通过下面的方式来表示：</p>

<p>1．   唯一的标记<br/>
2．   偏移数量<br/>
3．   字符串长度</p>

<p>由编码的模式决定引用是一个固定的或变动的长度。后面的情况经常是首选，因为它允许编码器用引用的大小来交换字符串的大小（例如，如果字符串相当长，增加引用的长度可能是值得的）。</p>

<h2 id="toc_10">4.2 实现</h2>

<p>使用 LZ77 的一个问题是由于算法需要字符串匹配，对于每个输入流的单个字节，每个流中此字节前面的哪个字节都必须被作为字符串的开始从而尽可能的进行字符串匹配，这意味着算法非常慢。</p>

<p>另一个问题是为了最优化压缩而调整字符串引用的表示形式并不容易。例如，必须决定是否所有的引用和非压缩字节应该在压缩流中的字节边界发生。</p>

<p>基本压缩库使用一个清晰的实现来保证所有的符号和引用是字节对齐的，因此牺牲了压缩比率，并且字符串匹配程序并不是最优化的（没有缓存、历史缓冲区或提高速度的小技巧），这意味着程序非常慢。</p>

<p>另一方面，解压缩程序非常简单。</p>

<p>一个提高 LZ77 速度的试验已经进行了，这个试验中使用数组索引来加速字符串匹配的过程。然而，它还是比通常的压缩程序慢。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[一个支付服务的最终一致性实践案例（含伪代码）]]></title>
    <link href="http://panlw.github.io/15293209479597.html"/>
    <updated>2018-06-18T19:22:27+08:00</updated>
    <id>http://panlw.github.io/15293209479597.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://mp.weixin.qq.com/s/3liPJulRZu4f72pmcabCSA">https://mp.weixin.qq.com/s/3liPJulRZu4f72pmcabCSA</a></p>
</blockquote>

<p>“功夫贷” 是一款线上贷款 APP，主要是给信用卡优质用户提供纯线上的信用贷款，以期限长、额度高、利息低为主要优势（类似的业务模式主要有宜人贷）。</p>

<p>和任何一种分期贷款一样，符合资质的用户，在功夫贷成功贷款之后，需要在约定还款日还款。目前还款主要有以下这几种方式：</p>

<ul>
<li>  用户在 APP 上主动还款；</li>
<li>  系统定时通过后台任务扣款；</li>
<li>  催收人员通过内部作业系统，手动发起扣款；</li>
</ul>

<p>真正的扣款操作（从银行卡扣款）主要是通过第三方支付来完成，比如京东支付、通联等。不同的第三方支付，支持的银行列表和限额不同，费用和稳定性也不尽相同，我们会选择出个最优通道、以及多层级备用通道，为此研发了支付路由系统，同时这些服务商的业务限制 / 出错概率还不低，所以我们又要考虑业务上的一致性，这也是本文要介绍的主题。</p>

<p>扣款业务是比较复杂的，包括如下几个主要步骤：</p>

<ol>
<li> 对业务表 (扣款任务表 / 还款计划表等) 的数据库操作</li>
<li> 调用第三方支付</li>
<li> 清算入账</li>
</ol>

<p>这多个子功能需要保证同时成功或者同时失败，其中既有外部第三方调用，又有内部微服务的调用，所以这是个比较典型的分布式事务的场景。由于外部的第三方支付服务有时不稳定、且部分交易可能很长时间才能确认成功。</p>

<p>因此 <strong>我们没考虑两阶段提交的分布式事务，而是选择了最终一致性</strong>，而为了保证在状态不一致这个时间窗口的准确性 (比如不能在该窗口对用户重复扣款)，我们也额外多做了很多的考虑。</p>

<h2 id="toc_0">主流程分析</h2>

<p>扣款服务的主流程如下图所示（在这里仅举 “第三方支付渠道是同步返回扣款结果” 作为例子，在实际情况中，各家第三方支付渠道的接口并不一致，有同步返回的、也有异步 + 轮询方式的，这两种形式，在我们这的处理逻辑上没有明显区别）。</p>

<p><img src="media/15293209479597/15293210151187.jpg" alt=""/></p>

<p>为了避免对业务流程造成干扰，上图中把同样处于主执行路径上的、起着日志记录作用的 &quot;log-x&quot; 这些步骤，在各自所处的位置以虚线表示，记得它们是主流程的一部分。这些 “log-x” 步骤在实现上，是建立一张日志表，以持久化、结构化的方式来记录，并不是 logback 之类的文件日志，因为这些日志在异常时的恢复，起着重要作用。</p>

<p>从上图可以看出，由 1、2、3、4、6 这五个步骤，形成一个整体，我们需要保证的是，这 5 个步骤同时成功、或者同时失败。其中包含几类操作：</p>

<ul>
<li>  本地 DB 的 SQL 执行，包括步骤 1、4；</li>
<li>  远程 HTTP/RPC 调用，包括步骤 2；</li>
<li>  发送 MQ 消息，包括步骤 3；</li>
<li>  异步系统执行，包括步骤 6；</li>
</ul>

<p>其中步骤 6 是另外一个服务（账务服务），是在支付服务之外的，所以用虚线框来表示，但在逻辑上是整体不可分的一部分，需要共同成功 / 失败。下面我们来看，在这些步骤中，会有哪些失败场景和各自特点：</p>

<ul>
<li>  本地 DB 的 SQL 执行：SQL 错误、与 DB 网络中断或者 DB 不可用的时候，会失败，但这种失败可补偿，且概率很低；</li>
<li>  远程调用：在本例中是 “同步调用第三方支付渠道扣款”，因为这是网络调用，最复杂的一种，可能会超时、也可能会连接中断或其他错误原因中断，这里的失败是有无法补偿的可能的，尤其是业务类错误——用户余额不足、用户银行卡状态不对等，都可能导致业务终止而无法继续下去；</li>
<li>  发送 MQ 消息：和本地 DB 的 SQL 执行类似，是可补偿的失败，从可用性的角度来看，比 SQL 执行的失败概率略高一些，在我们实际场景中，就有发送失败的情况（我们使用的是 RocketMQ，曾经出现过几次 broker 刷盘缓慢导致流控的发送失败）；</li>
<li>  异步系统执行：我们这里是触发账务系统入账，是 RPC 类（我们用的 Dubbo）操作，有一定的失败可能性（账务系统压力过大、内存溢出、磁盘占满等都可能导致其不能或部分服务器不能提供服务），但又因为它在业务上是肯定能成功的记账操作，所以即使失败，也是可以补偿的；</li>
</ul>

<p>综合上面这些分析，考虑到步骤 2“同步调用第三方支付渠道扣款” 是唯一一种无法补偿的业务，且处于流程链最靠前的地方，所以整个业务流，我们是向着可补偿的方式，即保证最终都会成功的最终一致性的方向去做。如果步骤 2 靠后，则由于它的不可补偿性，我们就必须在前面步骤的步骤考虑回滚——或 DB 事务回滚、或二阶段回滚、或提供撤销功能，以达到最终都会失败的最终一致性。</p>

<h2 id="toc_1">详细设计</h2>

<p><strong>难题一：出现预期内的异常时，如何保证最终一致性？</strong></p>

<p>我们先分析，如果主流程上的各个环节，出现了预期内的异常，我们大概要怎么处理，以保证最终一致性。预期内的异常，是指程序提前考虑到的——主要是 try/catch 中 catch 到 Exception 部分的逻辑。</p>

<p>步骤 1：更新 DB 的还款记录状态为 “扣款中”：其是流程第一步，如果它失败，流程结束，不需补偿；</p>

<p>步骤 2：同步调用第三方支付渠道来扣款：例子中的这家服务商的扣款接口，提供的是只有两种结果状态的契约：“扣款成功”或 “扣款失败”。如果在扣款中的话，则调用程序就在同步阻塞着。无论是由于调用超时、或调用中连接中断、或系统 Crash，导致失败，我们无法判定是否扣款是否成功，因此需要辅助以主动查询——轮询调用此家第三方支付服务商的查询接口，以确定扣款状态，达到“成功” 或“失败”的终态为止，如下图所示。</p>

<p><img src="media/15293209479597/15293210573189.jpg" alt=""/></p>

<p>步骤 3：发送 MQ 通知下游账务系统入账：如果失败的话，和上一步类似，需要日志表 + 定时任务补偿。</p>

<p>步骤 4/5：更新 DB 的还款记录状态为 “扣款成功” 或“扣款失败”：如果更新 DB 操作出现了失败，则需要定时任务，重试补偿，这需要借助日志表来恢复，后台定时任务去扫描该日志表，以从之前失败的步骤，继续执行下去，类似于“断点续传”，这里我们暂不详述；</p>

<p>步骤 5：发送 MQ 通知下游账务系统入账：如果发送失败的话，和上一步类似，需要日志表 + 定时任务补偿；</p>

<p>步骤 6：账务系统入账：由于通常的 MQ（我们用的是 RocketMQ）本身有 at-least-once 的重试机制，这就保证了消息必须被正确消费（只要账务系统程序不会主动 ignore 掉）才会被 ack，所以这个地方的最终成功，就由消息中间件来保证了；如果使用的 MQ 组件没有这种重试机制，则需要在账务系统端建立日志表，来补偿（如果 MQ 有丢失消息的风险，那仍然可能不一致）。</p>

<p><strong>难题二：出现预期外的异常，如何保证最终一致性？</strong></p>

<p>顾名思义，预期外的异常就是非程序提前感知到的，比如进程被强制 KILL、机器 CRASH，在这种情况下，程序执行到一半，突然结束了，这时怎么保证最终一致性?</p>

<p>在这种情况下，只能是靠日志表了，主流程或任何依赖内存记录的恢复程序都无效了。</p>

<p>定时任务的目的是补偿未能正常结束的扣款任务。一般来说，如果扣款任务未能正常结束，可能会有如下几种原因：</p>

<ol>
<li><p>系统意外退出（进程被 KILL、宕机等）；</p></li>
<li><p>系统重启——如当前某笔扣款记录在轮询第三方支付服务的扣款状态，此时重启也造成了流程中断；</p></li>
<li><p>执行过程中出错，如数据库异常、调用超时、MQ 不可用等；</p></li>
</ol>

<p>为了达到补偿目标，需要设计若干张日志表来辅助。我们设计了 2 张，如图：<br/>
<img src="media/15293209479597/15293210830707.jpg" alt=""/></p>

<p>其一，“扣款途中日志表”是用于标识扣款任务是否仍然在途中。在扣款开始之前，往该表插入记录，扣款完成后 (成功或失败) 更新状态。该表主要目的是：可以方便地找出来，哪些扣款任务是没有正常结束的。为什么没直接用业务表 “还款记录表” 来查询在途扣款呢? 主要是从便捷性和性能上考虑——业务表的数据是不能删除的，而该日志表可以定期将已完成的扣款任务清除掉，以控制该表其数据量，保证查询效率；</p>

<p>其二，“扣款执行日志表” 是用于记录扣款任务的执行过程。该表的记录不更新，只插入。如果某个扣款任务需要恢复补偿，则从该表中找到上次执行的 “断点”，继续向后执行。上图中举了 3 组数据作为例子：黄色背景是一笔完成的、扣款成功的日志；浅绿色背景是一笔完成的、扣款失败的日志；浅橙色背景是一笔进行中（正在执行调用第三方扣款）的日志。</p>

<p>下面是定时补偿任务的主流程：</p>

<p><img src="media/15293209479597/15293210937430.jpg" alt=""/></p>

<ol>
<li> 在实践中，一个正常的扣款任务在 1 分钟内都应该结束了，时间主要花费在调用第三方扣款服务上，绝大部分 30 秒内结束，少量的会拖的时间比较长，甚至跨日；</li>
<li> 定时任务 3 分钟执行一次，每次扫描 3 分钟前开始的、且当前未结束的任务。3 分钟以内的任务不处理的原因是：它们可能仍然在自己的正常处理过程中，此时还不需要定时任务来接管；</li>
</ol>

<h3 id="toc_2">伪代码</h3>

<p>为了便于读者理解，这里以伪代码的形式把整个扣款过程写出来，且分几个迭代版本不断增强。</p>

<h4 id="toc_3">版本一</h4>

<p><img src="media/15293209479597/15293211306294.jpg" alt=""/></p>

<ol>
<li> 在执行之前，注意要把数据库事务设为自动提交，即不可把整个过程纳入到一个事务里——不仅是性能问题，更重要的是，如果过程中失败了，日志数据也被回滚掉了，无法恢复；</li>
<li> 面对预期内的异常和预期外的异常，如详细设计里所述，或抛出异常结束、或 return 结束，后期由定时任务补偿。在主流程中不做各种各样繁杂的异常处理，既避免繁琐，也避免出错；</li>
<li> 上面只是伪代码，在实践中应该打印出详细的 Exception 信息、以及 log 文件日志，以便于定位和查找问题；</li>
</ol>

<p>版本一有 2 个问题</p>

<ol>
<li> 如果失败了，都要等定时任务补偿，那样响应有些慢，毕竟定时任务几分钟才执行一次；</li>
<li> 定时任务补偿时，要判断之前执行到哪，如果补偿的起始阶段不同、代码逻辑也不一样，这也比较麻烦；</li>
</ol>

<p>基于此，有了版本 II，这里取 “调用第三方支付渠道扣款” 的片段来说明。</p>

<p><section class="" style="box-sizing: border-box;font-size: 16px;text-align: left;margin-top: 30px;margin-left: 8px;color: rgb(60, 112, 198);">版本二</section><br/>
<img src="media/15293209479597/15293213207297.jpg" alt=""/></p>

<ol>
<li> 红色部分增加了日志状态的判断。如果是补偿性的，如该步骤以前已经成功了，则跳过这段调用第三方的逻辑；</li>
<li> 蓝色部分增加了先查询的操作，不论是否已经调用过扣款；</li>
<li> 褐色部分增加了后台线程池轮询，而不是单单等定时任务去触发；这地方实践中稍微控制下线程池数量、且最好有多路复用的模式，防止很多线程都挂在那轮询；</li>
<li> 绿色部分，其实是出现异常的话，上面这些步骤可以再来一遍；</li>
</ol>

<p>不难看出，该版本主要是增加各个逻辑段的幂等性，既使其能安全执行、又使代码逻辑简洁。</p>

<p>版本二还可以更为严谨一点——拿下面这个代码段红框里的来说，如果在两段 SQL 之间失败了，有造成不一致的可能（概率很小）。<br/>
<img src="media/15293209479597/15293213305162.jpg" alt=""/></p>

<h3 id="toc_4">版本三</h3>

<p><img src="media/15293209479597/15293213453102.jpg" alt=""/></p>

<p>通过事务保证逻辑段能同时成功或同时失败。虽然概率很小，但如果线上发生了，很难找到原因。</p>

<p>上面这些伪码是本人用 markdown 纯粹手敲的，并不是生产代码，没有经过严格测试，所以如果有些地方写的笔误或逻辑有漏洞，请读者谅解。</p>

<p>通过上面分析，我们看到有多个地方可能会对同一笔还款记录扣款，包括：</p>

<ol>
<li> 正常执行扣款；</li>
<li> 提交到后台线程池的重试 / 轮询；</li>
<li> 定时任务补偿；</li>
<li> 人工执行扣款</li>
</ol>

<p>所以针对单笔还款记录的扣款操作，我们需要使用锁定，实践中我们采用的是 redission 来做的分布式锁，这比较简单，这里不多叙述，不忽略这一点就好。</p>

<h2 id="toc_5">兜底方案</h2>

<p>上面我们分析了很多，对主流程中的分支都做了很多的考虑，但仍然有这两个风险：</p>

<ol>
<li> 有些异常分支没有考虑到；</li>
<li> 随着业务的发展，新加进来的逻辑，或者新人进来，很可能有些新的分支点没有被充分考虑；</li>
</ol>

<p>所以从严谨的角度，我们需要个兜底方案——主动检查 + 对账，以主动识别任何异常现象。从实践上看，由于业务的复杂性以及持续变化，可能很难完全梳理清楚所有的异常点，因此 “主动检查 + 对账” 可能更为重要。</p>

<h3 id="toc_6">主动检查</h3>

<p>我们创建了个 Thread，定时查询还款计划表中，处于”扣款中 “的异常数据，进行检查，如果有问题，自动修正或者通知出来人工干预。比如某条还款记录，从“还款中” 的状态到现在，已经过去了 1 个小时了，这种情况就会被判定为可疑现象，需要人工介入。</p>

<h3 id="toc_7">对账</h3>

<p>仍然有一些情况，是系统所覆盖不到的，需要双方对账 (我们和第三方支付对账、第三方支付和银行对账)。主要有以下这些场景：</p>

<ul>
<li>  跨日——双方把订单归到不同日期。比如 23:59 的订单，我们归到今天，第三方支付那边可能归到第二天；</li>
<li>  第三方支付开始告诉我们是成功的，我们已经结束操作了，后来对账时，第三方支付说支付失败了（可能它的信息是来自于银行）；</li>
<li>  我这边还款 1 笔，第三方支付那边搞成了 2 笔（可能是它们自己的原因，也可能是银行的原因）；</li>
</ul>

<p>对账主要是根据 “订单号”、“状态”、“日期”，主要是看状态和日期，是否对的。金额之类的，一般是不核对的，因为它不会出错。</p>

<p>兜底方案虽然好，但往往需要人工介入，成本高、反馈慢，如果能够系统自动就识别并修正，保证系统一致，那么在用户体验和成本角度考虑，都是很合适的。所以兜底方案和系统一致性是相互补充、各自取长补短的事情。</p>

<h2 id="toc_8">总结</h2>

<p>上面以我们的支付服务，作为一个最终一致性的例子。虽然场景不是很复杂，但写的比较细致，需要考虑的点也还是不少，希望能帮助到读者，将来在处理类似问题时，能够有比较清晰的思路。</p>

<h2 id="toc_9">作者介绍</h2>

<p>作者张轲目前任职于杭州大树网络技术有限公司，担任首席架构师，负责系统整体业务架构以及基础架构，熟悉微服务、分布式设计、中间件领域，对运维、测试、敏捷开发等相关领域也有所涉猎，个人微信公众号：zhangke83。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MySQL Innodb 如何找出阻塞事务源头 SQL]]></title>
    <link href="http://panlw.github.io/15290802805331.html"/>
    <updated>2018-06-16T00:31:20+08:00</updated>
    <id>http://panlw.github.io/15290802805331.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>来源：潇湘隐者<br/>
<a href="http://www.cnblogs.com/kerrycode/p/8948335.html">www.cnblogs.com/kerrycode/p/8948335.html</a></p>
</blockquote>

<p><strong>在MySQL数据库中出现了阻塞问题，如何快速查找定位问题根源？</strong>在实验开始前，我们先梳理一下有什么工具或命令查看MySQL的阻塞，另外，我们也要一一对比其优劣，因为有些命令可能在实际环境下可能并不适用。</p>

<ol>
<li>show engine innodb status</li>
<li>Innotop工具 </li>
<li>INNODB_TRX 等系统表</li>
</ol>

<p>下面我们理论联系实际，通过实验来测试总结一下这个问题。首先构造测试环境，数据库测试环境为（ 5.7.21 MySQL Community Server 和5.6.20-enterprise-commercial，这两个测试环境我都测试验证过）</p>

<pre><code>&gt; mysql&gt; use MyDB;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A
 
Database changed
mysql&gt; create table test_blocking(id int primary key, name varchar(12));
Query OK, 0 rows affected (0.05 sec)
 
mysql&gt; insert into test_blocking
 -&gt; select 1, &#39;kerry&#39; from dual;
Query OK, 1 row affected (0.00 sec)
Records: 1 Duplicates: 0 Warnings: 0
 
mysql&gt; insert into test_blocking
 -&gt; select 2, &#39;jimmy&#39; from dual;
Query OK, 1 row affected (0.00 sec)
Records: 1 Duplicates: 0 Warnings: 0
 
mysql&gt; insert into test_blocking
 -&gt; select 3, &#39;kkk&#39; from dual;
Query OK, 1 row affected (0.00 sec)
Records: 1 Duplicates: 0 Warnings: 0
</code></pre>

<p>准备好测试环境数据后，那么我们接下来开始实验，为了实验效果，我们先将参数innodb_lock_wait_timeout设置为100。</p>

<pre><code>&gt; mysql&gt; show variables like &#39;innodb_lock_wait_timeout&#39;;
+--------------------------+-------+
| Variable_name | Value |
+--------------------------+-------+
| innodb_lock_wait_timeout | 50 |
+--------------------------+-------+
1 row in set (0.00 sec)
 
mysql&gt; set global innodb_lock_wait_timeout=100 ;
Query OK, 0 rows affected (0.00 sec)
 
 
mysql&gt; select connection_id() from dual;
+-----------------+
| connection_id() |
+-----------------+
| 8 |
+-----------------+
1 row in set (0.00 sec)
 
mysql&gt; set session autocommit=0;
Query OK, 0 rows affected (0.00 sec)
 
mysql&gt; select * from test_blocking where id=1 for update;
+----+-------+
| id | name |
+----+-------+
| 1 | kerry |
+----+-------+
1 row in set (0.00 sec)
</code></pre>

<p>然后在第二个连接会话中执行更新脚本，构造被阻塞的案例</p>

<pre><code>&gt; mysql&gt; select connection_id() from dual;
+-----------------+
| connection_id() |
+-----------------+
| 9 |
+-----------------+
1 row in set (0.00 sec)
 
mysql&gt; update test_blocking set name=&#39;kk&#39; where id=1;
</code></pre>

<p>在第三个连接会话执行下面命令，查看TRANSACTIONS相关信息： </p>

<pre><code>&gt; mysql&gt; show engine innodb statusG;
</code></pre>

<p><img src="media/15290802805331/15290814361997.jpg" alt=""/></p>

<p>使用show engine innodb status命令后，可以查看其输出的TRANSACTIONS部分信息，如上截图所示，找到类似TRX HAS BEEN WATING …部分的信息，</p>

<p>通过那部分信息，我们可以看到update test_blocking set name=’kk’ where id=1这个SQL语句被阻塞了14秒，一直在等待获取X Lock。</p>

<pre><code>&gt; TRANSACTIONS
 
------------
 
Trx id counter 148281 #下一个事务ID
 
Purge done for trx&#39;s n:o &lt; 148273 undo n:o &lt; 0 state: running but idle
 
History list length 552
 
LIST OF TRANSACTIONS FOR EACH SESSION:
 
---TRANSACTION 0, not started
 
MySQL thread id 15, OS thread handle 0x4cc64940, query id 261 localhost root cleaning up
 
---TRANSACTION 0, not started
 
MySQL thread id 14, OS thread handle 0x4cbe2940, query id 278 localhost root init
 
show engine innodb status
 
---TRANSACTION 148280, ACTIVE 24 sec
 
2 lock struct(s), heap size 360, 1 row lock(s)
 
MySQL thread id 8, OS thread handle 0x4cba1940, query id 276 localhost root cleaning up
 
---TRANSACTION 148279, ACTIVE 313 sec starting index read
 
mysql tables in use 1, locked 1
 
LOCK WAIT 2 lock struct(s), heap size 360, 1 row lock(s)
 
MySQL thread id 9, OS thread handle 0x4cc23940, query id 277 localhost root updating #线程ID为9， 操作系统线程句柄为0x4cc23940， 查询ID为277，账号为root的UPDATE操作
 
update test_blocking set name=&#39;kk&#39; where id=1 #具体SQL语句
 
------- TRX HAS BEEN WAITING 14 SEC FOR THIS LOCK TO BE GRANTED: #TRX等待授予锁已经有14秒了
 
RECORD LOCKS space id 337 page no 3 n bits 72 index `PRIMARY` of table `MyDB`.`test_blocking` trx id 148279 lock_mode X locks rec but not gap waiting
</code></pre>

<h1 id="toc_0">在space id=337（test_blocking表的表空间），page no=3的页上，表test_blocking上的主键索引在等待X锁</h1>

<pre><code>Record lock, heap no 2 PHYSICAL RECORD: n_fields 4; compact format; info bits 0
 
 0: len 4; hex 80000001; asc ;; #第一个字段是主键，制度按长为4，值为1
 
 1: len 6; hex 000000024322; asc C&quot;;; #该字段为6个字节的事务id，这个id表示最近一次被更新的事务id（对应十进制为148258）
 
 2: len 7; hex 9a000001f20110; asc ;; #该字段为7个字节的回滚指针，用于mvcc
 
 3: len 5; hex 6b65727279; asc kerry;; #该字段表示的是此记录的第二个字段，长度为5，值为kerry（如果表有多个字段，那么此处后面还有记录）
</code></pre>

<p><img src="media/15290802805331/15290814738831.jpg" alt=""/></p>

<pre><code>&gt; mysql&gt; select * from information_schema.INNODB_SYS_DATAFILES where space=337;
+-------+--------------------------+
| SPACE | PATH |
+-------+--------------------------+
| 337 | ./MyDB/test_blocking.ibd |
+-------+--------------------------+
1 row in set (0.00 sec)
 
mysql&gt;
</code></pre>

<p><img src="media/15290802805331/15290814799530.jpg" alt=""/></p>

<p>但是这种方式也有一些弊端，例如生产环境很复杂，尤其是有大量事务的情况下。诸多信息根本无法清晰判断知道谁阻塞了谁；其次一点也不直观； 另外，这个也无法定位blocker 的SQL语句。这种方式只能作为辅助分析用途，通过查看取锁的详细信息，帮助进一步诊断问题。 </p>

<h2 id="toc_1">*<strong><em>2: Innotop工具</em></strong>*</h2>

<p>如下所示，Innotop工具很多情况下也不能定位到阻塞的语句（Blocking Query）， 也仅仅能获取一些锁相关信息<br/>
<img src="media/15290802805331/15290814903714.jpg" alt=""/><br/>
<img src="media/15290802805331/15290814996755.jpg" alt=""/><br/>
<img src="media/15290802805331/15290815034760.jpg" alt=""/></p>

<h2 id="toc_2">*<strong><em>3：通过查询information_schema数据库下与事务相关的几个系统表</em></strong>*</h2>

<p>还是构造之前的测试案例，在第一个会话中使用SELECT FOR UPDATE锁定其中一行记录</p>

<pre><code>&gt; mysql&gt; use MyDB;
Database changed
mysql&gt; set session autocommit=0;
Query OK, 0 rows affected (0.00 sec)
mysql&gt; select connection_id() from dual;
+-----------------+
| connection_id() |
+-----------------+
| 17 |
+-----------------+
1 row in set (0.00 sec)
 
mysql&gt; select * from test_blocking where id=1 for update;
+----+-------+
| id | name |
+----+-------+
| 1 | kerry |
+----+-------+
1 row in set (0.00 sec)
 
mysql&gt;
</code></pre>

<p>然后在第二个连接会话中执行更新脚本，构造被阻塞的案例</p>

<pre><code>&gt; mysql&gt; use MyDB;
Database changed
mysql&gt; select connection_id() from dual;
+-----------------+
| connection_id() |
+-----------------+
| 19 |
+-----------------+
1 row in set (0.00 sec)
 
mysql&gt; update test_blocking set name=&#39;kk&#39; where id=1;
</code></pre>

<p>此时阻我们在第三个连接会话查找谁被阻塞了</p>

<pre><code>&gt; SELECT b.trx_mysql_thread_id AS &#39;blocked_thread_id&#39;
 ,b.trx_query AS &#39;blocked_sql_text&#39;
 ,c.trx_mysql_thread_id AS &#39;blocker_thread_id&#39;
 ,c.trx_query AS &#39;blocker_sql_text&#39;
 ,( Unix_timestamp() - Unix_timestamp(c.trx_started) ) 
 AS &#39;blocked_time&#39;
FROM information_schema.innodb_lock_waits a 
 INNER JOIN information_schema.innodb_trx b 
 ON a.requesting_trx_id = b.trx_id 
 INNER JOIN information_schema.innodb_trx c 
 ON a.blocking_trx_id = c.trx_id 
WHERE ( Unix_timestamp() - Unix_timestamp(c.trx_started) ) &gt; 4; 
 
SELECT a.sql_text, 
 c.id, 
 d.trx_started 
FROM performance_schema.events_statements_current a 
 join performance_schema.threads b 
 ON a.thread_id = b.thread_id 
 join information_schema.processlist c 
 ON b.processlist_id = c.id 
 join information_schema.innodb_trx d 
 ON c.id = d.trx_mysql_thread_id 
where c.id=17
ORDER BY d.trx_startedG;
</code></pre>

<p>如下截图所示，第一个SQL语句能够查到线程19 被线程 17阻塞了， 被阻塞的SQL语句为“update test_blocking set name=’kk’ where id=1;”, 能够查到被阻塞了多长时间，但是无法查到源头SQL语句。此时就需要第二个SQL语句登场，找到源头语句。</p>

<p><img src="media/15290802805331/15290815196795.jpg" alt=""/></p>

<p>但是不要太天真的认为第二个SQL语句能够获取所有场景下的阻塞源头SQL语句，实际业务场景，会话可能在执行一个存储过程或复杂的业务，有可能它执行完阻塞源头SQL后，继续在执行其它SQL语句，此时，你抓取的是这个连接会话最后执行的SQL语句，如下所示，我简单构造了一个例子。就能构造这样的一个场景。这个我曾经写过一篇博客“_为什么数据库有时候不能定位阻塞（Blocker）源头的SQL语句_”，分析SQL Server和ORACLE 定位查找阻塞源头SQL语句，现在看来真是大道同源，殊途同归。</p>

<pre><code>&gt; mysql&gt; select * from test_blocking where id=1 for update;
+----+-------+
| id | name |
+----+-------+
| 1 | kerry |
+----+-------+
1 row in set (0.00 sec)
 
mysql&gt; delete from student where stu_id=1001;
Query OK, 1 row affected (0.00 sec)
 
mysql&gt;
</code></pre>

<p><img src="media/15290802805331/15290815269311.jpg" alt=""/></p>

<p>总结： 最简单、方便的还是上面两个SQL查询定位blocker的SQL语句，但是需要注意：有时候它也查不到真正阻塞的源头SQL语句。所以还需结合应用程序代码与上下文环境进行整体分析、判断！</p>

]]></content>
  </entry>
  
</feed>
