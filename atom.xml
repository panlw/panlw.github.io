<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Junkman]]></title>
  <link href="http://panlw.github.io/atom.xml" rel="self"/>
  <link href="http://panlw.github.io/"/>
  <updated>2018-12-27T18:43:52+08:00</updated>
  <id>http://panlw.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[Reliable Domain Events]]></title>
    <link href="http://panlw.github.io/15459058392039.html"/>
    <updated>2018-12-27T18:17:19+08:00</updated>
    <id>http://panlw.github.io/15459058392039.html</id>
    <content type="html"><![CDATA[
<pre><code>Written on April 23, 2016
</code></pre>

<blockquote>
<p>原文地址 <a href="http://pillopl.github.io/reliable-domain-events/">http://pillopl.github.io/reliable-domain-events/</a></p>
</blockquote>

<p>We already know that domain events are a great tool to integrate different parts of your system (so called bounded contexts) and decouple them. Instead of directly calling outside component, we can notify our infrastructure, which in turn informs interested parties about what has just occurred. This another form of <strong>Inversion of Control</strong> helps us to achieve our goal. Consider following piece of code:</p>

<pre><code>    class User {

        //...

        @Transactional
        public void register(LocalDateTime at, RegisteredBy by) {
            markRegistered();
            markSuspiciousIfAtNight(at);
            DomainEventsPublisher.publish(new UserRegistered(this.id(), at));
        }

    }

    public class UserRegistered implements DomainEvent {

        private final UserRegistered userId;
        private final LocalDateTime at;

        //getters, constructor

    }

    class DomainEventsPublisher {

        //...

        private final Register handlersRegistry;

        public static void publish(DomainEvent event) {
            handlersRegistry
                .getHandlers(event.getClass())
                .forEach(handler -&gt; handler.handle(event));
        }

    }

    class SendCommunicationOnUserRegistered implements DomainEventHandler&lt;UserRegistered&gt; {

    @Override
    public void handle(UserRegistered event) {
        Communication c = sendRegistrationEmail(event.getUserId());
        storeCommunication(c);
    }

</code></pre>

<p>There is a big problem with this simple <strong>Observer</strong> pattern in above example. Namely, everything happens in the same database transaction, which has started at the time of calling <u>register</u> method. This has several implications. First of all, we did not decouple user registration from e-mail sending and we just have mentioned this is what domain events are for. If our mail server is down, registration fails. Why would our user care about an email? He correctly filled registration form and does not even know there is an email coming.</p>

<p>From the <strong>use case</strong> point of view, sending an email should not imply successful invocation. Secondly, even though it looks like the code deals only with users (it registers and marks as suspicious when needed), it modifies another aggregate - communications (communication would be probably modeled as a Generic Subdomain, but to simplify things consider it as another bounded context). The rule of thumb says that we should not modify several aggregates in one transaction, because these concepts should not be so directly related. Sending an email (or any other action done as a consequence of registering user) may take a lot of time, do I/O calls, etc.</p>

<p>But things get worse. Consider situation when everything was fine with our mail server, we want to save user to database and something fails and transaction rollbacks. Now we have a big problem, because confused user got error page as a response, but seconds later successful email with registration greetings.</p>

<p>This clearly shows that those concepts should be unrelated. One may come with an idea to fire all handlers asynchronously (so outside current transaction) but that solves only one issue: we could register users when we cannot send emails. We need something better. Fortunately, we can fire our emails just after transaction commit. Thus, we are sure everything was fine during registration and without doubts we can send the welcoming message. Spring gives us possibility to do that in a few lines of code with <a href="http://docs.spring.io/spring/docs/3.0.6.RELEASE_to_3.1.0.BUILD-SNAPSHOT/3.0.6.RELEASE/org/springframework/transaction/support/TransactionSynchronizationManager.html">TransactionSynchronizationManager</a>:</p>

<pre><code>    class DomainEventsPublisher {

        //...

        public static void publish(DomainEvent event) {
            handlersRegistry
                .getHandlers(event.getClass())
                .forEach(handler -&gt; handleAsynchronously(event, handlers));
        }

        private static void handleAsynchronously(DomainEvent event, DomainEventHandler handler) {
            if (TransactionSynchronizationManager.isActualTransactionActive()) {
                processAfterCommit(event, handler);
            } else {
                processNow(event, handler);
            }
        }

        private static void processNow(final DomainEvent event, DomainEventHandler handler) {
            handler.handle(event);
        }

        private static void processAfterCommit(final DomainEvent event, DomainEventHandler handler) {
            TransactionSynchronizationManager.registerSynchronization(new TransactionSynchronizationAdapter() {
                @Override
                public void afterCommit() {
                    handler.handle(event);
                }
            });
        }
   }

</code></pre>

<p></p></p>

<p>This example could be simplified with Spring 4.2 and <a href="http://docs.spring.io/spring/docs/current/spring-framework-reference/htmlsingle/#transaction-event">@TransactionalEventListener</a>. Thanks to this code, we are fine with cause and effect relationship. Taking advantage of java memory model nomenclature: we know that successful registration <strong>happens before</strong> sending an email. One may argue that there is a slight window of time when we are not consistent. As stated at the beginning, we don&#39;t need to be consistent when working with several bounded context. The important is that they will be <strong>eventually consistent</strong>. Anyway, we still did not solve the problem with unresponsive mail server. When it fails, the information is lost. We can implement something which will retry this operation with a reasonable back-off, but when our application crashes, we still don&#39;t have any mean to recover to former state. We need to deal with that issue.</p>

<p>It gets clearer that we need to store somewhere the <strong>intent</strong> of sending the email. The intent was of course <u>UserRegistered</u> event. By exposing this intent to for example JMS infrastructure, we can implement the retry mechanism. We can go further and store any event coming from a given bounded context. Repository of all domain events is in fact called <strong>Event Store</strong>. Below example contains code that serializes events to JSON and publishes them to JMS. We could publish them anywhere else, for example to a file on a local disk or to an Akka actor.</p>

<pre><code>class ExternalEventStore {

    //...

    private final ProducerTemplate producerTemplate;
    private final ObjectMapper objectMapper = new ObjectMapper();

    void publish(DomainEvent event) throws JsonProcessingException {
        final String body = objectMapper.writeValueAsString(event);
        producerTemplate.sendBody(&quot;queue.url&quot;, InOnly, body);
    }
}

</code></pre>

<p>We could invoke <u>ExternalEventStore</u> from our <u>handleNow</u> method from previous example and make some other component consume those messages and send emails. But that would be just delegating the problem from mail server to another part of infrastructure - our queue. Putting a message to a queue might fail and the message would be lost. Moreover, our application can fail somewhere in between commit and invoking this asynchronous transaction listener. We also cannot move this code back to the transaction, because that would make us stuck at the beginning of our problem and queue failures would result in our users not being able to register.</p>

<p>We have to develop a consistent solution in which an <strong>occurrence of an event reflects that it really happened in our system</strong>. Also <strong>when it really happens, it should be followed by an event</strong>. In other words, those two statements should be in bi-conditional logical connective.</p>

<p>The problem is that our JMS component is not backed by the same data source as our domain model. One solution is to use global transaction and two-phase commits. The problem is that it might decrease performance significantly. Plus, not every part of our infrastructure must support that mechanism. More clever idea is to share the same data source for our messaging infrastructure and domain model (if those two parts support this kind of data soure). The side effect here is that we need to share the same schema, which might not be the nicest solution.</p>

<p>In my opinion the best option is to translate our events to our domain model storage and save them in the same transaction. Later on, another pool of threads should process them and publish somewhere else. Now our event store would look as follows:</p>

<pre><code>
public class InternalEventStore {

    //...

    private final ObjectMapper eventSerializer;
    private final SessionFactory sessionFactory;

    void store(DomainEvent event) throws JsonProcessingException  {
        session().save(serialized(event));
    }

    private PersistentEvent serialized(DomainEvent event) throws JsonProcessingException   {
        return new PersistentEvent(eventSerializer.writeValueAsString(event));
    }

    List&lt;PersistentEvent&gt; listPending(int limit) {
       //...
    }
}

public class PersistentEvent {

    public enum Status {
        PENDING, SENT
    }

    private Long id;

    private String uuid = UUID.randomUUID().toString();

    private LocalDateTime occuredAt = LocalDateTime.now();

    private LocalDateTime sentAt;

    private String body;

    private Status status = PENDING;

    public void sent(LocalDateTime at) {
        this.sentAt = at;
        this.status = SENT;
    }

    //...

}

</code></pre>

<p>and it would be invoked from <u>DomainEventProcessor</u> in the same transaction (so we came back to synchronous observator pattern):</p>

<pre><code>
 class DomainEventsPublisher {

    //...

    private final Register handlersRegistry;
    private final EventStore eventStore;

    public void publish(DomainEvent event) {
        handlersRegistry
            .getHandlers(event.getClass())
            .forEach(handler -&gt; handler.handle(event));
        eventStore.store(event);
    }
}

</code></pre>

<p>Note that we left the register with handlers working under the same transaction. It is fine, because we may need to listen to this event somewhere else in the same bounded context. That way we won&#39;t modify another aggregates.</p>

<p>But we still want our events to appear in our JMS infrastructure. We can run a periodic job which scans list of our events and sends them to a queue or a topic: </p></p>

<pre><code>class PublishPendingEventsScheduler {

    private final ExternalEventStore eventStore;
    private final ExternalEventStore publisher;

    @Scheduled(initialDelay = 3000, fixedDelayString = &quot;${events.publisher.freq:3000}&quot;)
    public void sendEvents() {
        eventStore.listPending(100).forEach(this::publish);
    }

    private void publish(PersistentEvent event) {
        publisher.publish(event); // publisher needs to have publish(PersistentEvent event) method
        event.sent(now());
    }
}

</code></pre>

<p>We mark every event as sent, so that it won&#39;t be picked up in further invocation. If our message infrastructure fails, we try again soon. We might argue that this code suffers from the same problem as the whole example. Our database might fails at the time we want to save this event as sent. That is a fair concern, because we already have sent it to JMS. That means it can arrive at consumer side several times. It is important that consumer handles those events in an idempotent way - by for example storing PersistentEvent&#39;s uuid and doing de-duplication. Resending at producer side and idempotency at consumer side gives us <strong>at most once delivery.</strong></p>

<p>In this post I tried to describe reliable domain events mechanism by implementing simple event store. Actually, events stores have much more benefits: we can examine every historical result of commands invoked in our system, run some forecasting algorithms, do event sourcing (reconstructing an aggregate by composing events it has produced) in different bounded contexts.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Transaction Synchronization and Spring Application Events: Understanding @TransactionalEventListener]]></title>
    <link href="http://panlw.github.io/15459049161482.html"/>
    <updated>2018-12-27T18:01:56+08:00</updated>
    <id>http://panlw.github.io/15459049161482.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://dzone.com/articles/transaction-synchronization-and-spring-application">https://dzone.com/articles/transaction-synchronization-and-spring-application</a></p>
</blockquote>

<p>The aim of this article is to explain how  <code>@TransactionalEventListener</code>  works, how it differs from a simple  <code>@EventListener</code>, and finally - what are the threats that we should take into account before using it. Giving a real-life example, I will mainly focus on transaction synchronization issues, not paying too much attention neither to consistency nor application event reliability. A complete SpringBoot project with described examples can be found <a href="https://github.com/bslota/transactional-event-listener">here</a>.</p>

<h2 id="toc_0">Example overview</h2>

<p>Imagine we have a microservice which manages customers&#39;basic information and triggers activation token generation after a customer is created. From the business perspective, token generation is not an integral part of user creation and should be a separate process (this is a very important assumption, which I will refer to later). To keep things simple, let&#39;s assume that a customer looks like this:</p>

<pre><code class="language-java">@Entity
public class Customer {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    private String name;
    private String email;
    private String token;
    public Customer() {}
    public Customer(String name, String email) {
        this.name = name;
        this.email = email;
    }
    public void activatedWith(String token) {
        this.token = token;
    }
    public boolean hasToken() {
        return !StringUtils.isEmpty(token);
    }
    ...
    //getters
    //equals and hashcode
}
</code></pre>

<p>We have a simple <u>spring-data-jpa</u> repository:</p>

<pre><code class="language-java">public interface CustomerRepository extends JpaRepository&lt;Customer, Long&gt; {}
</code></pre>

<p>And below you can see an essence of the business problem - creating a new customer (persisting it in DB) and returning it.</p>

<pre><code class="language-java">@Service
public class CustomerService {
    private final CustomerRepository customerRepository;
    private final ApplicationEventPublisher applicationEventPublisher;
    public CustomerService(CustomerRepository customerRepository, ApplicationEventPublisher applicationEventPublisher) {
        this.customerRepository = customerRepository;
        this.applicationEventPublisher = applicationEventPublisher;
    }
    @Transactional
    public Customer createCustomer(String name, String email) {
        final Customer newCustomer = customerRepository.save(new Customer(name, email));
        final CustomerCreatedEvent event = new CustomerCreatedEvent(newCustomer);
        applicationEventPublisher.publishEvent(event);
        return newCustomer;
    }
}
</code></pre>

<p>As you can see,  <code>CustomerService</code>  depends on two beans:</p>

<ol>
<li> <code>CustomerRepository</code>  - interface for the purpose of saving customer</li>
<li> <code>ApplicationEventPublisher</code>  - Spring&#39;s super-interface for  <code>ApplicationContext</code> , which declares the way of event publishing inside Spring application</li>
</ol>

<p><u>Please note the constructor injection. If you are not familiar with this technique or not aware of its benefits relative to field injection, please read <a href="http://pillopl.github.io/constructor-injection/">this article</a>.</u></p>

<p>Remember to give -1 during the code review if there is no test included! But wait, take it easy, mine is here:</p>

<pre><code class="language-java">@SpringBootTest
@RunWith(SpringRunner.class)
public class CustomerServiceTest {
  @Autowired
  private CustomerService customerService;
  @Autowired
  private CustomerRepository customerRepository;
  @Test
  public void shouldPersistCustomer() throws Exception {
    //when
    final Customer returnedCustomer = customerService.createCustomer(&quot;Matt&quot;, &quot;matt@gmail.com&quot;);
    //then
    final Customer persistedCustomer = customerRepository.findOne(returnedCustomer.getId());
    assertEquals(&quot;matt@gmail.com&quot;, returnedCustomer.getEmail());
    assertEquals(&quot;Matt&quot;, returnedCustomer.getName());
    assertEquals(returnedCustomer, persistedCustomer);
}
</code></pre>

<p>The test does one simple thing - it checks whether  <code>createCustomer</code>  method creates a proper customer. One could say that in these kinds of tests I shouldn&#39;t pay attention to implementation details (persisting entity through the repository, etc.) and rather put it in some unit test, and I would agree, but let&#39;s just leave it to keep the example clearer.</p>

<p>You may ask now, where is the token generation. Well, due to the business case that we are discussing,  <code>createCustomer</code>  method does not seem to be a good place to put any logic apart from simple creation of user (method name should always reflect its responsibility). In that kind of cases it might be a good idea to use the observer pattern to inform interested parties that a particular event took place. Following this considerations, you can see that we are calling  <code>publishEvent</code>  method on  <code>applicationEventPublisher</code> . We are propagating an event of the following type:</p>

<pre><code class="language-java">public class CustomerCreatedEvent {
  private final Customer customer;
  public CustomerCreatedEvent(Customer customer) {
    this.customer = customer;
  }
  public Customer getCustomer() {
    return customer;
  }
  ...
  //equals and hashCode
}
</code></pre>

<p>Note that it is just a POJO. Since Spring 4.2, we are no longer obliged to extend  <code>ApplicationEvent</code>  and can publish any object we like instead. Spring wraps it in  <code>PayloadApplicationEvent</code>  itself.</p>

<p>We do also have an event listener component, like this:</p>

<pre><code class="language-java">@Component
public class CustomerCreatedEventListener {
    private static final Logger LOGGER = LoggerFactory.getLogger(CustomerCreatedEventListener.class);
    private final TokenGenerator tokenGenerator;
    public CustomerCreatedEventListener(TokenGenerator tokenGenerator) {
        this.tokenGenerator = tokenGenerator;
    }
    @EventListener
    public void processCustomerCreatedEvent(CustomerCreatedEvent event) {
        LOGGER.info(&quot;Event received: &quot; + event);
        tokenGenerator.generateToken(event.getCustomer());
    }
}
</code></pre>

<p>Before we discuss this listener, let&#39;s briefly look at the  TokenGenerator  interface:</p>

<pre><code class="language-java">public interface TokenGenerator {
    void generateToken(Customer customer);
}
</code></pre>

<p>and its implementation:</p>

<pre><code class="language-java">@Service
public class DefaultTokenGenerator implements TokenGenerator {
    private final CustomerRepository customerRepository;
    public DefaultTokenGenerator(CustomerRepository customerRepository) {
        this.customerRepository = customerRepository;
    }
    @Override
    public void generateToken(Customer customer) {
        final String token = String.valueOf(customer.hashCode());
        customer.activatedWith(token);
        customerRepository.save(customer);
    }
}
</code></pre>

<p>We are simply generating a token, setting it as a customer&#39;s property and updating the entity in the database. Good, let&#39;s update our test class now, so that it checks not only the customer creation, but also token generation.</p>

<pre><code class="language-java">@SpringBootTest
@RunWith(SpringRunner.class)
public class CustomerServiceTest {
    @Autowired
    private CustomerService customerService;
    @Autowired
    private CustomerRepository customerRepository;
    @Test
    public void shouldPersistCustomerWithToken() throws Exception {
        //when
        final Customer returnedCustomer = customerService.createCustomer(&quot;Matt&quot;, &quot;matt@gmail.com&quot;);
        //then
        final Customer persistedCustomer = customerRepository.findOne(returnedCustomer.getId());
        assertEquals(&quot;matt@gmail.com&quot;, returnedCustomer.getEmail());
        assertEquals(&quot;Matt&quot;, returnedCustomer.getName());
        assertTrue(returnedCustomer.hasToken());
        assertEquals(returnedCustomer, persistedCustomer);
    }
}
</code></pre>

<h2 id="toc_1">@EventListener</h2>

<p>As you can see, we have moved token generation logic into a separate component which is good (note the assumption at the beginning of the previous chapter), but do we have a real separation of concerns? Nope.  <code>@EventListener</code>  registers the  <code>processCustomerCreatedEvent</code>  as the listener of  <code>CustomerCreatedEvent</code> , but it is called synchronously within the bounds of the same transaction as  <code>CustomerService</code> . It means, that if something goes wrong with token generation - customer won&#39;t be created. Is this the way it should really work? Surely not. Before we generate and set token, we would rather have a customer already created and saved in database (committed). Now this is the time to introduce  <code>@TransactionalEventListener</code>annotation.</p>

<h2 id="toc_2">@TransactionalEventListener - transaction synchronization</h2>

<p><code>@TransactionalEventListener</code>  is an  <code>@EventListener</code>  enhanced with the ability to collaborate with the surrounding transaction&#39;s phase. We call this a transaction synchronization - in other words, it is a way of registering callback methods to be invoked when the transaction is being completed. Synchronization is possible within the following transaction phases (phase attribute):</p>

<ul>
<li>  AFTER_COMMIT (default setting) - specialization of AFTER_COMPLETION, used when transaction has successfully committed</li>
<li>  AFTER_ROLLBACK - specialization of AFTER_COMPLETION, used when transaction has rolled back</li>
<li>  AFTER_COMPLETION - used when transaction has completed (regardless the success)</li>
<li>  BEFORE_COMMIT - used before transaction commit</li>
</ul>

<p>When there is no transaction running then the method annotated with  <code>@TransactionalEventListener</code>  won&#39;t be executed unless there is a parameter fallbackExecution set to true.</p>

<p>Good, looks like this is something that we are looking for! Let&#39;s change  <code>@EventListener</code>  annotation with  <code>@TransactionalEventListener</code>  in  <code>CustomerCreatedEventListener</code> , then:</p>

<pre><code class="language-java">@TransactionalEventListener
public void processCustomerCreatedEvent(CustomerCreatedEvent event) {
    LOGGER.info(&quot;Event received: &quot; + event);
    tokenGenerator.generateToken(event.getCustomer());
}
</code></pre>

<p>We need to check now if everything works as we expect - let&#39;s run our test:</p>

<pre><code class="language-log">java.lang.AssertionError: 
Expected :Customer{id=1, name=&#39;Matt&#39;, email=&#39;matt@gmail.com&#39;, token=&#39;1575323438&#39;}
Actual   :Customer{id=1, name=&#39;Matt&#39;, email=&#39;matt@gmail.com&#39;, token=&#39;null&#39;}
</code></pre>

<p>Why is that? What have we missed? I tell you what, we spent too little time on analyzing how the transaction synchronization works. Now the crucial thing is that we have synchronized token generation with the transaction after it has been committed - so we shouldn&#39;t even expect that anything will be committed again! Javadoc for  <code>afterCommit</code>  method of  <code>TransactionSynchronization</code>  interface says it clearly:</p>

<blockquote>
<p><u>The transaction will have been committed already, but the transactional resources might still be active and accessible. As a consequence, any data access code triggered at this point will still &quot;participate&quot; in the original transaction, allowing to perform some cleanup (with no commit following anymore!), unless it explicitly declares that it needs to run in a separate transaction. Hence: Use {@code PROPAGATION</u>REQUIRES_NEW} for any transactional operation that is called from here._</p>
</blockquote>

<p>As we have already stated, we need to have a strong separation of concerns between a service call and an event listener logic. This means that we can use the advice given by Spring authors. Let&#39;s try annotating a method inside  <code>DefaultTokenGenerator</code> :</p>

<pre><code class="language-java">@Transactional(propagation = Propagation.REQUIRES_NEW)
public void generateToken(Customer customer) {
    final String token = String.valueOf(customer.hashCode());
    customer.activatedWith(token);
    customerRepository.save(customer);
}
</code></pre>

<p>If we run our test now, it passes!</p>

<p><u>Caveat: We are discussing interacting with AFTER</u>COMMIT phase only, but all these considerations apply to all AFTER_COMPLETION phases. In the case of BEFORE_COMMIT phase, none of the above problems should worry you, although you need to make a conscious decision whether your listener&#39;s code should <u>really</u> be executed in the same transaction._</p>

<h2 id="toc_3">Asynchronous execution</h2>

<p>What if a token generation is a long lasting process? If it is not an essential part of creating a customer, then we could make one step forward and make our  <code>@TransactionalEventListener</code>-annotated method asynchronous one (via annotating it with  <code>@Async</code>). Asynchronous call means that we will execute listeners  <code>processCustomerCreatedEvent</code> in a separate thread. Bearing in mind that a single transaction in Spring framework is by default thread-bounded, we won&#39;t need autonomous transaction in  <code>DefaultTokenGenerator</code> anymore.</p>

<p>Good, let&#39;s write a test for this case:</p>

<pre><code class="language-java">@SpringBootTest
@RunWith(SpringRunner.class)
@ActiveProfiles(&quot;async&quot;)
public class CustomerServiceAsyncTest {
  @Autowired
  private CustomerService customerService;
  @Autowired
  private CustomerRepository customerRepository;
  @Test
  public void shouldPersistCustomerWithToken() throws Exception {
      //when
      final Customer returnedCustomer = customerService.createCustomer(&quot;Matt&quot;, &quot;matt@gmail.com&quot;);
      //then
      assertEquals(&quot;matt@gmail.com&quot;, returnedCustomer.getEmail());
      assertEquals(&quot;Matt&quot;, returnedCustomer.getName());
      //and
      await().atMost(5, SECONDS)
          .until(() -&gt; customerTokenIsPersisted(returnedCustomer.getId()));
  }
  private boolean customerTokenIsPersisted(Long id) {
      final Customer persistedCustomer = customerRepository.findOne(id);
      return persistedCustomer.hasToken();
  }
}
</code></pre>

<p>The only thing that differentiates this test from the previous one is that we are using the _Awaitility_ library (a great and powerful tool) in order to await for the async task to complete. We also wrote a simple  <code>customerTokenIsPersisted</code> helper method to check if the token was properly set. And surely test passes brilliantly!</p>

<p><u>Caveat: I don&#39;t recommend performing any async tasks in BEFORE</u>COMMIT phase as you won&#39;t have any guarantee that they will complete before producer&#39;s transaction is committed._</p>

<h2 id="toc_4">Conclusions</h2>

<p><code>@TransactionalEventListener</code>  is a great alternative to  <code>@EventListener</code>  in situations where you need to synchronize with one of the transaction phases. You can declare listeners as synchronous or asynchronous. You need to keep in mind that with the synchronous call you are by default working within the same transaction as the event producer. In case you synchronize to the  AFTER_COMPLETION phase (or one of its specializations), you won&#39;t be able to persist anything in the database as there won&#39;t be a commit procedure executed anymore. If you need to commit some changes anyway, you can declare an autonomous transaction on the event listener code. BEFORE_COMMIT phase is much simpler because commit will be performed after calling event listeners. With asynchronous calls, you don&#39;t have to worry about declaring autonomous transactions, as Spring&#39;s transactions are by default thread-bounded (you will get a new transaction anyway). It is a good idea if you have some long running task to be performed. I suggest using asynchronous tasks only when synchronizing to AFTER_COMPLETION phase or one of its specializations. As long as you don&#39;t need to have your event listener&#39;s method transactional - described problems shouldn&#39;t bother you at all.</p>

<h2 id="toc_5">Further considerations</h2>

<p>In a real-life scenario, numerous other requirements might occur. For example, you might need to both persist the customer and send an invitation email like it is depicted below:</p>

<pre><code class="language-java">@Component
public class CustomerCreatedEventListener {
    private final MailingFacade mailingFacade;
    private final CustomerRepository customerRepository;
    public CustomerCreatedEventListener(MailingFacade mailingFacade, CustomerRepository customerRepository) {
        this.mailingFacade = mailingFacade;
        this.customerRepository = customerRepository;
    }
    @EventListener
    public void processCustomerCreatedEvent(CustomerCreatedEvent event) {
        final Customer customer = event.getCustomer();
        // sending invitation email
        mailingFacade.sendInvitation(customer.getEmail());
    }
}
</code></pre>

<p>Imagine a situation when an email is sent successfully, but right after it, our database goes down and it is impossible to commit the transaction (persist customer) - thus, we lose consistency! If such a situation is acceptable within your business use case, then it is completely fine to leave it as it is, but if not then you have a much more complex problem to solve. I intentionally haven&#39;t covered consistency and event reliability issues here. If you want to have a broader picture of how you could possibly deal with such situations, I recommend reading <a href="http://pillopl.github.io/reliable-domain-events/">this article</a>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Reliable Delivery Pub/Sub Message Queues with Redis]]></title>
    <link href="http://panlw.github.io/15459021437244.html"/>
    <updated>2018-12-27T17:15:43+08:00</updated>
    <id>http://panlw.github.io/15459021437244.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://davidmarquis.wordpress.com/2013/01/03/reliable-delivery-message-queues-with-redis/">https://davidmarquis.wordpress.com/2013/01/03/reliable-delivery-message-queues-with-redis/</a></p>
</blockquote>

<p><img src="media/15459021437244/15459022953899.png" alt=""/></p>

<p><u><strong>UPDATE</strong></u>_<strong>:</strong>_ I have open-sourced a Java implementation of the below principles called “<a href="https://github.com/davidmarquis/redisq">RedisQ</a>“. Enjoy!</p>

<p>Redis is a high performance key-value datastore that differs from other key-value solutions in the way it handles values. Instead of just storing values as simple strings, it recognizes multiple specific data types such as Lists, Sets, Hashes (maps), Strings or Numbers. Each data type has its own set of features to manipulate the data it contains in an atomic manner, making it an ideal tool for highly distributed system where concurrency is a potential issue.</p>

<p>Combining those features in creative ways allows for novel ways of doing “traditional” things differently. One particular combination has recently allowed my team and I to implement a moderately (read: good enough) reliable message delivery mechanism for multiple consumers consuming messages at their own pace.</p>

<p>The solution has advantages and some caveats, but if your problem allows you to live with the possible drawbacks, it’s a nice lightweight solution to a problem that is usually answered using some more traditional (and more complex) tools, like *MQs.</p>

<p>In a recent project, we ended up choosing Redis mostly because:</p>

<ul>
<li>  It was already part of our architecture and we had a simple inter-component messaging use case but didn’t want to introduce a new component in our architecture just for this.</li>
<li>  Expected volume was low, which meant that our data set could fit in memory. <u>Note: although Redis requires everything you store in it to fit in memory, it supports persistence to disk.</u></li>
<li>  Redis allowed for all of the implementation characteristics we were looking for, namely:

<ul>
<li>  <strong>Concurrency:</strong> Because all operations in Redis are atomic, supporting concurrency without too much of a hassle is straightforward.</li>
<li>  <strong>Persistence:</strong> Configured properly, we can ensure persistence of our queues to disk using one of the supported Redis <a href="http://redis.io/topics/persistence" title="Redis persistence">persistence strategies</a>.</li>
<li>  <strong>Lightweight:</strong> Using Redis from any language/platform is extremely simple and provisioning it / maintaining it on a production server is dead easy.</li>
</ul></li>
</ul>

<p>In this post, I will go over the strategy we used with regards to Redis data structures and operations for handling the message publishing and consuming.</p>

<p>The high-level strategy consists of the following:</p>

<ul>
<li>  When each consumer starts up and gets ready to consume messages, it registers by adding itself to a Set representing all consumers registered on a queue.</li>
<li>  When a producers publishes a message on a queue, it:

<ul>
<li>  Saves the content of the message in a Redis key</li>
<li>  Iterates over the set of consumers registered on the queue, and pushes the message ID in a List for each of the registered consumers</li>
</ul></li>
<li>  Each consumer continuously looks out for a new entry in its consumer-specific list and when one comes in, removes the entry, handles the message and passes on to the next message.</li>
</ul>

<p><strong>Why not use Redis Pub/Sub?</strong></p>

<p>I already see you coming and asking why not using the <a href="http://redis.io/topics/pubsub">Pub/Sub semantics</a> supported out-of-the-box by Redis? The reason was two fold:</p>

<ol>
<li> What Redis offers with Pub/Sub is a listener model, where each subscriber receives each messages when it is listening, but won’t receive them when not connected.</li>
<li> In a clustered environment where you have multiple instances of your consumer component running at the same time, each instance would receive each message produced on the channel. We wanted to make sure any given message got consumed once per logical consumer, even when multiple instances of this component are running.</li>
</ol>

<p>Hence the name of this post “Reliable Delivery”, because we wanted to make sure every logical consumer eventually receives all messages produced on a queue once and only once, even when not connected – due to, for example, a deployment, a restart or a application failure/crash.</p>

<p><strong>Detailed look at the strategy</strong></p>

<p>Here’s a closer look at the different scenarios using a fictive example of an ordering system with multiple consumers interested in messages when new orders are created:</p>

<p><strong>Registering a consumer</strong><br/>
<img src="media/15459021437244/15459023170786.jpg" alt=""/></p>

<p>A “consumer” represents a logical entity of your architecture. You assign each concumer an identifier which it will use to register itself as a consumer on the queue.</p>

<p>Registering a consumer is only a matter of adding a Set entry to a key that is crafted with the name of the queue in it.</p>

<p>The semantics of a Set are helpful here: each consumer can just “add” an entry to the Set upon start up in a single operation, without the need to worry about any existing value.</p>

<p><strong>Publishing a message</strong><br/>
<img src="media/15459021437244/15459023263357.jpg" alt=""/></p>

<p>On the Producer side, a few things need to happen when we’re publishing a message to a specific queue:</p>

<ol>
<li> The Producer increments a counter to get the next message ID using the INC command on key “orders.nextid”</li>
<li> It then stores the message in a key containing the new message ID (“orders.messages.8” in our case). The actual format you store messages can be anything. We used a hash with some metadata information about each message, along with the actual payload. The payload can be serialized in JSON, XML or any format makes sense for your usage.</li>
<li> Then for each consumer registered in key “orders.consumers”, it pushes the message ID using the <a href="http://redis.io/commands/rpush">RPUSH</a> command on lists for each consumers.</li>
</ol>

<p>To prevent duplication of message content in Redis, we store the content once and then only add references to the messages in consumer-specific lists. When a consumer consumes messages (more on that later), it will remove the ID from its list (its queue), then read the actual message content in a separate operation.</p>

<p>But what happens when all consumers have read the message? If we stopped here, each message would end up being stored in Redis forever. An efficient solution to this problem is to use Redis’ ability to expire (clean up) keys after some time using the <a href="http://redis.io/commands/expire">EXPIRE</a> command. Using a reasonable amount of time for the expiration makes up for a cheap cleanup process.</p>

<p>A slight variation, at a cost of message content duplication, would be to store the actual message content in each consumer-specific list. For simpler use cases where messages are small enough, this could be a compelling tradeoff.</p>

<p><strong>Consuming messages</strong><br/>
<img src="media/15459021437244/15459023359516.jpg" alt=""/></p>

<p>Each consumer has a specific identifier and uses this identifier to “listen” on Lists stored in specially crafted Redis keys. Redis has this nice feature of “blocking pop”, which allows a client to remove the first or last element of a list, or wait until an element gets added.</p>

<p>Leveraging this feature, each consumer creates a thread that will continuously loop and do the following:</p>

<ol>
<li> Use <a href="http://redis.io/commands/blpop">BLPOP</a> (blocking left pop) with a moderately small timeout to continuously “remove an element from the list or wait a bit”.</li>
<li> When an element gets removed by the pop operation, read the message content and process it.</li>
<li> When an element does not get removed (no message available), just wait and start over again.</li>
</ol>

<p>You can have multiple threads or processes consuming messages with the same “consumer identifier” and the solution still works. This allows for both stability and scalability:</p>

<ul>
<li>  You can spawn multiple consumers consuming messages as the same logical entity, and ensure that if one goes down, the consumption of messages does not stop.</li>
<li>  You can also spawn more consumers when needed for added horsepower.</li>
</ul>

<p><strong>Caveats</strong></p>

<ul>
<li>  The solution as described above does not support retryability of messages in case of a failure to process on the consumer side. I could imagine a way to do it using Redis, but one has to wonder if Redis is still the right tool if such a characteristic is required by your use case.</li>
<li>  The solution also does not guarantee that messages will be consumed in the order they were produced. If you have a single consumer instance you’re covered, but as soon as you have multiple consumer instances you cannot guarantee the ordering of messages. Maintaining a lock in a specific key for each consumer would enable this, at the cost of scalability (only 1 message can be consumed at any time throughout your consumer instances).</li>
<li>  If your producers and consumers are using different languages, you must implement this strategy for each platform/language. Fortunately, there are Redis client for pretty much any popular platforms.</li>
</ul>

<p><strong>Wrap up</strong></p>

<p>There are many ways Redis can be leveraged using the simple data structures and atomic operations it provides. Using a particular combination of those, we’ve been able to implement a simple system that allowed reliable message delivery to multiple consumers in a clustered environment without too much of a hassle.</p>

<p>Because Redis was already part of our architecture, it proved to be a natural choice. The efforts required to build up this solution outweighed the efforts required to provision and maintain an additional component to manage our queues.</p>

<p>It might not be the most appropriate choice for your case. Be thoughtful in your architectural choices!</p>

<p><u><strong>UPDATE</strong></u>_<strong>:</strong>_ I have open-sourced a Java implementation of the above principles called “<a href="https://github.com/davidmarquis/redisq">RedisQ</a>“. Enjoy!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spring の Cache Abstraction で、複数の CacheManager を合わせる CompositeCacheManager を使う]]></title>
    <link href="http://panlw.github.io/15456691642791.html"/>
    <updated>2018-12-25T00:32:44+08:00</updated>
    <id>http://panlw.github.io/15456691642791.html</id>
    <content type="html"><![CDATA[
<pre><code>2017-02-04
</code></pre>

<blockquote>
<p>原文地址 <a href="https://kazuhira-r.hatenablog.com/entry/20170204/1486194922">https://kazuhira-r.hatenablog.com/entry/20170204/1486194922</a></p>
</blockquote>

<p>Spring の Cache Abstraction では、バックエンドのキャッシュに対する Cache Provider があり、それぞれ CacheManager の<br/>
実装を提供していますが、複数の CacheManager を組み合わせる CompositeCacheManager というものがあります。</p>

<p><a href="https://docs.spring.io/spring/docs/4.3.6.RELEASE/spring-framework-reference/html/cache.html#cache-annotations">Cache / Dealing with caches without a backing store</a></p>

<p>※CompositeCacheManager の説明というより、適切なキャッシュが存在しなかった時のフォールバックケースとして書いてありますが…</p>

<p><a href="http://docs.spring.io/spring-framework/docs/4.3.6.RELEASE/javadoc-api/org/springframework/cache/support/CompositeCacheManager.html">CompositeCacheManager (Spring Framework 4.3.6.RELEASE API)</a></p>

<p>こちらを使って、今回は Caffeine と Redis の 2 つのキャッシュを同時に使ってみたいと思います。ひとつ前のエントリで、<br/>
キャッシュの<a href="http://d.hatena.ne.jp/keyword/%A5%A2%A5%CE%A5%C6%A1%BC%A5%B7%A5%E7%A5%F3">アノテーション</a>の cacheNames に複数のキャッシュを指定しましたが、今回はキャッシュごとにバックエンドの<br/>
キャッシュを分けてみます。</p>

<p><a href="http://d.hatena.ne.jp/Kazuhira/20170204/1486188414">Spring の Cache Abstraction で、 アノテーションに複数のキャッシュを指定した場合の動きを確認する - CLOVER</a></p>

<p>サンプルコードは、この時使ったものと近いものを使用しています。</p>

<p>では、試してみましょう。</p>

<h4 id="toc_0">準備</h4>

<p>まずは <a href="http://d.hatena.ne.jp/keyword/Maven">Maven</a> 依存関係から。</p>

<pre data-lang="xml" data-unlink="">    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <java.version>1.8</java.version>
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
        <spring.boot.version>1.5.1.RELEASE</spring.boot.version>
    </properties>

    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-dependencies</artifactId>
                <version>${spring.boot.version}</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>
        </dependencies>
    </dependencyManagement>

    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-cache</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.ben-manes.caffeine</groupId>
            <artifactId>caffeine</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-redis</artifactId>
        </dependency>

        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>
</pre>

<p>Spring の Cache の依存関係と、Caffeine、Spring Data Redis を追加。</p>

<p>Redis は、3.2.7 を<a href="http://d.hatena.ne.jp/keyword/%A5%EA%A5%E2%A1%BC%A5%C8%A5%DB%A5%B9%A5%C8">リモートホスト</a>で起動させています。</p>

<h4 id="toc_1">サンプルコードの準備</h4>

<p>では、確認用のサンプルアプリケーションを作成します。</p>

<p>キャッシュに格納する用のクラス。</p>

<pre data-lang="java" data-unlink="">src/main/java/org/littlewings/spring/cache/Message.java 
package org.littlewings.spring.cache;

import java.io.Serializable;
import java.time.LocalDateTime;

public class Message implements Serializable {
    private static final long serialVersionUID = 1L;

    private String value;
    private LocalDateTime now;

    public Message() {
    }

    public Message(String value) {
        this.value = value;
        now = LocalDateTime.now();
    }

    public String getValue() {
        return value;
    }

    public LocalDateTime getNow() {
        return now;
    }
}
</pre>

<p>キャッシュ関連の<a href="http://d.hatena.ne.jp/keyword/%A5%A2%A5%CE%A5%C6%A1%BC%A5%B7%A5%E7%A5%F3">アノテーション</a>を付与したクラス。@Cacheable、@CachePut を使い、それぞれ「caffeineCache」と<br/>
「redisCache」を指定しています。@Cacheable を付与したメソッドは、キャッシュが存在しない場合は 3 秒間スリープします。<br/>
src/main/<a href="http://d.hatena.ne.jp/keyword/java">java</a>/org/littlewings/spring/cache/MessageService.<a href="http://d.hatena.ne.jp/keyword/java">java</a></p>

<pre data-lang="java" data-unlink="">package org.littlewings.spring.cache;

import java.util.concurrent.TimeUnit;

import org.springframework.cache.annotation.CachePut;
import org.springframework.cache.annotation.Cacheable;
import org.springframework.stereotype.Service;

@Service
public class MessageService {
    @Cacheable(cacheNames = {"caffeineCache", "redisCache"}, key = "#value")
    public Message build(String value) {
        try {
            TimeUnit.SECONDS.sleep(3L);
        } catch (InterruptedException e) {
            // ignore
        }

        return new Message(value);
    }

    @CachePut(cacheNames = {"caffeineCache", "redisCache"}, key = "#message.value")
    public Message update(Message message) {
        return message;
    }
}
</pre>

<p>CacheManager の設定。@EnableCaching <a href="http://d.hatena.ne.jp/keyword/%A5%A2%A5%CE%A5%C6%A1%BC%A5%B7%A5%E7%A5%F3">アノテーション</a>を設定してキャッシュ機能を有効化するとともに、Caffeine と Redis<br/>
それぞれの CacheManager を作成し、CompositeCacheManager で組み合わせています。<br/>
src/main/<a href="http://d.hatena.ne.jp/keyword/java">java</a>/org/littlewings/spring/cache/CacheConfig.<a href="http://d.hatena.ne.jp/keyword/java">java</a></p>

<pre data-lang="java" data-unlink="">package org.littlewings.spring.cache;

import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;

import org.springframework.cache.CacheManager;
import org.springframework.cache.annotation.EnableCaching;
import org.springframework.cache.caffeine.CaffeineCacheManager;
import org.springframework.cache.support.CompositeCacheManager;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.data.redis.cache.RedisCacheManager;
import org.springframework.data.redis.core.RedisTemplate;

@Configuration
@EnableCaching
public class CacheConfig {
    CacheManager caffeineCacheManager() {
        CaffeineCacheManager cacheManager = new CaffeineCacheManager();

        cacheManager.setCacheSpecification("expireAfterWrite=3s");
        cacheManager.setCacheNames(Arrays.asList("caffeineCache"));

        return cacheManager;
    }

    CacheManager redisCacheManager(RedisTemplate<Object, Object> redisTemplate) {
        RedisCacheManager cacheManager = new RedisCacheManager(redisTemplate);

        cacheManager.setUsePrefix(true);

        Map<String, Long> expires = new HashMap<>();
        expires.put("redisCache", 6L);

        cacheManager.setExpires(expires);

        cacheManager.afterPropertiesSet();
        return cacheManager;
    }

    @Bean
    public CacheManager compositeCacheManager(RedisTemplate<Object, Object> redisTemplate) {
        CompositeCacheManager cacheManager = new CompositeCacheManager(caffeineCacheManager(), redisCacheManager(redisTemplate));
        cacheManager.setFallbackToNoOpCache(false);  // キャッシュの定義が見つからない場合は、getCacheがnullを返すようにする
        cacheManager.afterPropertiesSet();
        return cacheManager;
    }
}
</pre>

<p>キャッシュの有効期限は、Caffeine は 3 秒、Redis は 6 秒にしています。</p>

<p>CompositeCacheManager を作成する時に、CompositeCacheManager#setFallbackToNoOpCache を true に設定すると<br/>
CompositeCacheManager が内部的に保持する CacheManager に NoOpCacheManager というものが追加されます。<br/>
NoOpCacheManager が追加されると、CacheManager#getCache でキャッシュ定義が見つからない時に<br/>
なにもしないキャッシュが返されるようになり、宣言的キャッシュを使ってキャッシュが見つからない時にエラーに<br/>
ならなくなります。</p>

<p>デフォルトは false（キャッシュが見つからないと null を返す）です。</p>

<p>@SpringBootApplication を付与したクラスを作成。テスト用です。<br/>
src/main/<a href="http://d.hatena.ne.jp/keyword/java">java</a>/org/littlewings/spring/cache/App.<a href="http://d.hatena.ne.jp/keyword/java">java</a></p>

<pre data-lang="java" data-unlink="">package org.littlewings.spring.cache;

import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class App {
}
</pre>

<p>最後に、Redis への接続設定。<br/>
src/main/resources/application.properties</p>

<pre data-lang="" data-unlink="">spring.redis.host=172.17.0.2
spring.redis.password=redispass</pre>

<p>ここまでで、準備は完了です。</p>

<h4 id="toc_2">動作確認</h4>

<p>それでは、テストコードを書いて動作確認をしてみます。テストコードの雛形は、こちら。<br/>
src/test/<a href="http://d.hatena.ne.jp/keyword/java">java</a>/org/littlewings/spring/cache/CompositeCacheTest.<a href="http://d.hatena.ne.jp/keyword/java">java</a></p>

<pre data-lang="java" data-unlink="">package org.littlewings.spring.cache;

import java.util.concurrent.TimeUnit;

import org.junit.Test;
import org.junit.runner.RunWith;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.cache.CacheManager;
import org.springframework.test.context.junit4.SpringRunner;
import org.springframework.util.StopWatch;

import static org.assertj.core.api.Assertions.assertThat;

@RunWith(SpringRunner.class)
@SpringBootTest
public class CompositeCacheTest {
    @Autowired
    CacheManager cacheManager;

    @Autowired
    MessageService messageService;

    // ここに、テストを書く！
}
</pre>

<p>まずは @Cacheable での確認から。テストコードは、こちら。</p>

<pre data-lang="java" data-unlink="">    @Test
    public void cacheable() throws InterruptedException {
        StopWatch stopWatch = new StopWatch();

        // 1回目
        stopWatch.start();
        Message m1 = messageService.build("Hello World");
        stopWatch.stop();

        // 低速
        assertThat((long)stopWatch.getLastTaskInfo().getTimeSeconds())
                .isGreaterThanOrEqualTo(3L);

        // キャッシュに登録
        assertThat(cacheManager.getCache("caffeineCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("caffeineCache").get("Hello World").get()).getNow())
                .isEqualTo(m1.getNow());
        assertThat(cacheManager.getCache("redisCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("redisCache").get("Hello World").get()).getNow())
                .isEqualTo(m1.getNow());

        // 2回目
        stopWatch.start();
        Message m2 = messageService.build("Hello World");
        stopWatch.stop();

        // 高速
        assertThat((long)stopWatch.getLastTaskInfo().getTimeSeconds())
                .isEqualTo(0L);

        // キャッシュにはまだ存在
        assertThat(cacheManager.getCache("caffeineCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("caffeineCache").get("Hello World").get()).getNow())
                .isEqualTo(m2.getNow())
                .isEqualTo(m1.getNow());
        assertThat(cacheManager.getCache("redisCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("redisCache").get("Hello World").get()).getNow())
                .isEqualTo(m2.getNow())
                .isEqualTo(m1.getNow());

        // 3秒スリープ
        TimeUnit.SECONDS.sleep(3L);

        // caffeineCacheのみ有効期限切れ
        assertThat(cacheManager.getCache("caffeineCache").get("Hello World"))
                .isNull();
        assertThat(cacheManager.getCache("redisCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("redisCache").get("Hello World").get()).getNow())
                .isEqualTo(m2.getNow())
                .isEqualTo(m1.getNow());

        // 3回目
        stopWatch.start();
        Message m3 = messageService.build("Hello World");
        stopWatch.stop();

        // 高速
        assertThat((long)stopWatch.getLastTaskInfo().getTimeSeconds())
                .isEqualTo(0L);

        // caffeineCacheのみ有効期限切れ
        assertThat(cacheManager.getCache("caffeineCache").get("Hello World"))
                .isNull();
        assertThat(cacheManager.getCache("redisCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("redisCache").get("Hello World").get()).getNow())
                .isEqualTo(m3.getNow())
                .isEqualTo(m1.getNow());

        // 3秒スリープ
        TimeUnit.SECONDS.sleep(3L);

        // どちらのキャッシュも有効期限切れ
        assertThat(cacheManager.getCache("caffeineCache").get("Hello World"))
                .isNull();
        assertThat(cacheManager.getCache("redisCache").get("Hello World"))
                .isNull();

        // 再度アクセス
        messageService.build("Hello World");

        // キャッシュに再登録
        assertThat(cacheManager.getCache("caffeineCache").get("Hello World"))
                .isNotNull();
        assertThat(cacheManager.getCache("redisCache").get("Hello World"))
                .isNotNull();

        // 後始末
        cacheManager.getCache("caffeineCache").evict("Hello World");
        cacheManager.getCache("redisCache").evict("Hello World");
    }
</pre>

<p>@Cacheable なメソッドにアクセスすると、キャッシュにエントリが保存されます。この時、Caffeine と Redis の両方に保存されます。</p>

<pre data-lang="java" data-unlink="">        // 1回目
        stopWatch.start();
        Message m1 = messageService.build("Hello World");
        stopWatch.stop();

        // 低速
        assertThat((long)stopWatch.getLastTaskInfo().getTimeSeconds())
                .isGreaterThanOrEqualTo(3L);

        // キャッシュに登録
        assertThat(cacheManager.getCache("caffeineCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("caffeineCache").get("Hello World").get()).getNow())
                .isEqualTo(m1.getNow());
        assertThat(cacheManager.getCache("redisCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("redisCache").get("Hello World").get()).getNow())
                .isEqualTo(m1.getNow());
</pre>

<p>2 回目のアクセスでは、高速になります。</p>

<pre data-lang="java" data-unlink="">        // 2回目
        stopWatch.start();
        Message m2 = messageService.build("Hello World");
        stopWatch.stop();

        // 高速
        assertThat((long)stopWatch.getLastTaskInfo().getTimeSeconds())
                .isEqualTo(0L);

        // キャッシュにはまだ存在
        assertThat(cacheManager.getCache("caffeineCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("caffeineCache").get("Hello World").get()).getNow())
                .isEqualTo(m2.getNow())
                .isEqualTo(m1.getNow());
        assertThat(cacheManager.getCache("redisCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("redisCache").get("Hello World").get()).getNow())
                .isEqualTo(m2.getNow())
                .isEqualTo(m1.getNow());
</pre>

<p>ここで、3 秒間スリープすると有効期限の短い Caffeine のキャッシュのみ有効期限切れします。</p>

<pre data-lang="java" data-unlink="">        // 3秒スリープ
        TimeUnit.SECONDS.sleep(3L);

        // caffeineCacheのみ有効期限切れ
        assertThat(cacheManager.getCache("caffeineCache").get("Hello World"))
                .isNull();
        assertThat(cacheManager.getCache("redisCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("redisCache").get("Hello World").get()).getNow())
                .isEqualTo(m2.getNow())
                .isEqualTo(m1.getNow());
</pre>

<p>3 回目のアクセス。Caffeine からはキャッシュエントリが消えましたが、Redis には残ったままなので高速です。</p>

<pre data-lang="java" data-unlink="">        // 3回目
        stopWatch.start();
        Message m3 = messageService.build("Hello World");
        stopWatch.stop();

        // 高速
        assertThat((long)stopWatch.getLastTaskInfo().getTimeSeconds())
                .isEqualTo(0L);

        // caffeineCacheのみ有効期限切れ
        assertThat(cacheManager.getCache("caffeineCache").get("Hello World"))
                .isNull();
        assertThat(cacheManager.getCache("redisCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("redisCache").get("Hello World").get()).getNow())
                .isEqualTo(m3.getNow())
                .isEqualTo(m1.getNow());
</pre>

<p>さらに 3 秒間スリープさせると、Redis からもエントリがなくなります。</p>

<pre data-lang="java" data-unlink="">        // 3秒スリープ
        TimeUnit.SECONDS.sleep(3L);

        // どちらのキャッシュも有効期限切れ
        assertThat(cacheManager.getCache("caffeineCache").get("Hello World"))
                .isNull();
        assertThat(cacheManager.getCache("redisCache").get("Hello World"))
                .isNull();
</pre>

<p>もちろん、この状態で再度 @Cachable なメソッドにアクセスすると両方のキャッシュにエントリが入ります。</p>

<pre data-lang="java" data-unlink="">        // 再度アクセス
        messageService.build("Hello World");

        // キャッシュに再登録
        assertThat(cacheManager.getCache("caffeineCache").get("Hello World"))
                .isNotNull();
        assertThat(cacheManager.getCache("redisCache").get("Hello World"))
                .isNotNull();
</pre>

<p>両方のそれぞれのキャッシュを使えてそうですね。</p>

<p>あと、@CachePut も確認しておきましょう。テストコードは、こちら。</p>

<pre data-lang="java" data-unlink="">    @Test
    public void cachePut() throws InterruptedException {
        StopWatch stopWatch = new StopWatch();

        // 1回目
        stopWatch.start();
        Message m1 = messageService.build("Hello World");
        stopWatch.stop();

        // 低速
        assertThat((long)stopWatch.getLastTaskInfo().getTimeSeconds())
                .isGreaterThanOrEqualTo(3L);

        // キャッシュに登録されている
        assertThat(cacheManager.getCache("caffeineCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("caffeineCache").get("Hello World").get()).getNow())
                .isEqualTo(m1.getNow());
        assertThat(cacheManager.getCache("redisCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("redisCache").get("Hello World").get()).getNow())
                .isEqualTo(m1.getNow());

        // 3秒スリープ
        TimeUnit.SECONDS.sleep(3L);

        // caffeineCacheのみ有効期限切れ
        assertThat(cacheManager.getCache("caffeineCache").get("Hello World"))
                .isNull();
        assertThat(cacheManager.getCache("redisCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("redisCache").get("Hello World").get()).getNow())
                .isEqualTo(m1.getNow());

        // キャッシュを更新
        Message newMessage = new Message("Hello World");
        messageService.update(newMessage);

        // キャッシュが更新されたことを確認
        assertThat(cacheManager.getCache("caffeineCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("caffeineCache").get("Hello World").get()).getNow())
                .isEqualTo(newMessage.getNow())
                .isNotEqualTo(m1.getNow());
        assertThat(cacheManager.getCache("redisCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("redisCache").get("Hello World").get()).getNow())
                .isEqualTo(newMessage.getNow())
                .isNotEqualTo(m1.getNow());
    }
</pre>

<p>とりあえず、キャッシュにエントリを入れます。</p>

<pre data-lang="java" data-unlink="">        // 1回目
        stopWatch.start();
        Message m1 = messageService.build("Hello World");
        stopWatch.stop();

        // 低速
        assertThat((long)stopWatch.getLastTaskInfo().getTimeSeconds())
                .isGreaterThanOrEqualTo(3L);

        // キャッシュに登録されている
        assertThat(cacheManager.getCache("caffeineCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("caffeineCache").get("Hello World").get()).getNow())
                .isEqualTo(m1.getNow());
        assertThat(cacheManager.getCache("redisCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("redisCache").get("Hello World").get()).getNow())
                .isEqualTo(m1.getNow());
</pre>

<p>3 秒間スリープさせ、Caffeine のみ有効期限が切れたところで</p>

<pre data-lang="java" data-unlink="">        // 3秒スリープ
        TimeUnit.SECONDS.sleep(3L);

        // caffeineCacheのみ有効期限切れ
        assertThat(cacheManager.getCache("caffeineCache").get("Hello World"))
                .isNull();
        assertThat(cacheManager.getCache("redisCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("redisCache").get("Hello World").get()).getNow())
                .isEqualTo(m1.getNow());
</pre>

<p>@CachePut なメソッドを呼び出しキャッシュを更新して、両方のキャッシュに反映されたことを確認。</p>

<pre data-lang="java" data-unlink="">        // キャッシュを更新
        Message newMessage = new Message("Hello World");
        messageService.update(newMessage);

        // キャッシュが更新されたことを確認
        assertThat(cacheManager.getCache("caffeineCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("caffeineCache").get("Hello World").get()).getNow())
                .isEqualTo(newMessage.getNow())
                .isNotEqualTo(m1.getNow());
        assertThat(cacheManager.getCache("redisCache").get("Hello World"))
                .isNotNull();
        assertThat(((Message)cacheManager.getCache("redisCache").get("Hello World").get()).getNow())
                .isEqualTo(newMessage.getNow())
                .isNotEqualTo(m1.getNow());
</pre>

<p>こちらも OK そうです。</p>

<h4 id="toc_3">まとめ</h4>

<p>CompositeCacheManager を使って、複数の CacheManager を組み合わせて使ってみました。</p>

<p>CompositeCacheManager のトリックですが、単に CacheManager#getCache した時に内部で持っている CacheManager から順次 Cache を<br/>
持っているか確認していき、最初に見つかったものを返すという単純な実装です。<br/>
<a href="https://github.com/spring-projects/spring-framework/blob/v4.3.6.RELEASE/spring-context/src/main/java/org/springframework/cache/support/CompositeCacheManager.java#L101">https://github.com/spring-projects/spring-framework/blob/v4.3.6.RELEASE/spring-context/src/main/java/org/springframework/cache/support/CompositeCacheManager.java#L101</a></p>

<pre data-lang="java" data-unlink="">   @Override
    public Cache getCache(String name) {
        for (CacheManager cacheManager : this.cacheManagers) {
            Cache cache = cacheManager.getCache(name);
            if (cache != null) {
                return cache;
            }
        }
        return null;
    }
</pre>

<p>とはいえ、こういうのがあることを知っていると便利さアップですね、と。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spring boot metrics monitoring using TICK stack]]></title>
    <link href="http://panlw.github.io/15456638361570.html"/>
    <updated>2018-12-24T23:03:56+08:00</updated>
    <id>http://panlw.github.io/15456638361570.html</id>
    <content type="html"><![CDATA[
<pre><code>Mohammed Aboullaite | 08 Apr 2018 | 6 min (1537 words)
</code></pre>

<blockquote>
<p>原文地址 <a href="https://aboullaite.me/spring-boot-metrics-monitoring-using-tick-stack/">https://aboullaite.me/spring-boot-metrics-monitoring-using-tick-stack/</a></p>
</blockquote>

<p>Worth nothing to mention that monitoring the performance and availability of applications is very important. This is one of the many cool features bundled with Spring boot, which since its first days comes with production ready features (i.e Actuator) to help us keeping an eye on apps performance. With the release of spring boot 2, <a href="https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready.html">Actuator</a> got a huge boost, offering dev/ops more application metrics facade that supports numerous monitoring systems.</p>

<p>In this post, we&#39;ll see how we can easily monitor Spring boot applications using the <a href="https://www.influxdata.com/time-series-platform/">TICK stack</a>.</p>

<p><img src="media/15456638361570/15456639113571.jpg" alt=""/></p>

<h3 id="toc_0">The TICK stack ?!</h3>

<p>The TICK stack is a collection of products from the developers of the time-series database InfluxDB. It is made up of the following components:</p>

<ul>
<li>  <strong>Telegraf</strong>: collects time-series data from a variety of sources.</li>
<li>  <strong>InfluxDB</strong>: stores time-series data.</li>
<li>  <strong>Chronograf</strong>: visualizes and graphs the time-series data.</li>
<li>  <strong>Kapacitor</strong>: provides alerting and detects anomalies in time-series data.</li>
</ul>

<p>We can use each of these components separately, but if we use them together, we&#39;ll have a scalable, integrated open-source system for processing time-series data.</p>

<h3 id="toc_1">Configuring the TICK stack and spring boot</h3>

<p>As I mentioned above, Spring Boot 2 actuator comes bundled with metrics facade easily integrated with many monitoring platforms including the Tick stack.</p>

<h4 id="toc_2">Configuring Spring Boot</h4>

<p>First of all we need, obviously, to add spring boot actuator dependency to our application:</p>

<pre><code>&lt;dependency&gt;  
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;
            &lt;version&gt;${actuator.version}&lt;/version&gt;
        &lt;/dependency&gt;

</code></pre>

<p>If you are using spring boot 2, Actuator autoconfigure micrometer magically. if not and you&#39;re using a previous version, please check <a href="https://micrometer.io/docs/ref/spring/1.5">these configuration steps</a>.</p>

<p>On the other hand, <a href="https://www.influxdata.com/time-series-platform/telegraf/">Telegraf</a> is an agent written in Go and accepts StatsD protocol metrics over UDP, then periodically forwards the metrics to InfluxDB. <a href="https://micrometer.io/">Micrometer</a> supports three flavors of StatsD: the original <code>Etsy</code> format plus the <code>Datadog</code> and <code>Telegraf</code> extensions of StatsD that add dimensional support. Obviously we&#39;re going to use this registry to publish metrics to telegraf. For that we need to add these lines to our <code>application.properties</code> file:</p>

<pre><code>management.metrics.export.statsd.enabled=true  
management.metrics.export.statsd.flavor=telegraf  
management.metrics.export.statsd.port=8125  

</code></pre>

<h4 id="toc_3">Configuring the TICK stack</h4>

<p>The <code>docker-compose</code> file below configures 4 services:</p>

<ul>
<li>  <code>telegraf</code> service to collect data sent by micrometer. This service is configured using the <code>./etc/telegraf.conf</code> file. this service expose the port <code>8125</code> used by Actuator to send metrics</li>
<li>  <code>influxdb</code> service that stores data sent from telegraf. we mount a volume to <code>./data/influxdb</code> to save data on host disk. It expose the port <code>8086</code> to access influxdb API.</li>
<li>  <code>chronograf</code> service to visualize influxdb data, create dashboards and show alerts triggered from kapacitor. this service expose port <code>8888</code> to access this web application.</li>
<li>  <code>kapacitor</code> service, analyse influxdb data and triggers alerts.</li>
</ul>

<pre><code>version: &#39;3&#39;

services:  
  # Define a Telegraf service
  telegraf:
    image: telegraf:1.5.3
    volumes:
      - ./etc/telegraf.conf:/etc/telegraf/telegraf.conf:ro
    links:
      - influxdb
    ports:
      - &quot;8125:8125/udp&quot;
  # Define an InfluxDB service
  influxdb:
    image: influxdb:1.5.1
    volumes:
      - ./data/influxdb:/var/lib/influxdb
    ports:
      - &quot;8086:8086&quot;
  # Define a Chronograf service
  chronograf:
    image: chronograf:1.4.3.1
    environment:
      INFLUXDB_URL: http://influxdb:8086
      KAPACITOR_URL: http://kapacitor:9092
    ports:
      - &quot;8888:8888&quot;
    links:
      - influxdb
      - kapacitor
  # Define a Kapacitor service
  kapacitor:
    image: kapacitor:1.4.1
    environment:
      KAPACITOR_HOSTNAME: kapacitor
      KAPACITOR_INFLUXDB_0_URLS_0: http://influxdb:8086
    links:
      - influxdb
    ports:
      - &quot;9092:9092&quot;

</code></pre>

<p>The <code>telegraf.conf</code> file I&#39;ve used is very minimalist. it configures the telegraf agent properties as well as StatsD as input plugin and influxdb as output plugin:</p>

<pre><code># Telegraf Configuration
# Global tags can be specified here in key=&quot;value&quot; format.
[global_tags]
# Configuration for telegraf agent
[agent]
## Default data collection interval for all inputs
interval = &quot;10s&quot;  
## Rounds collection interval to &#39;interval&#39;
round_interval = true  
## This controls the size of writes that Telegraf sends to output plugins.
metric_batch_size = 1000  
## This buffer only fills when writes fail to output plugin(s).
metric_buffer_limit = 10000  
## Collection jitter is used to jitter the collection by a random amount.
collection_jitter = &quot;0s&quot;  
flush_interval = &quot;10s&quot;  
flush_jitter = &quot;0s&quot;  
precision = &quot;&quot;

## Logging configuration:
debug = false  
## Run telegraf in quiet mode (error log messages only).
quiet = false  
## Specify the log file name. The empty string means to log to stderr.
logfile = &quot;&quot;  
## Override default hostname, if empty use os.Hostname()
hostname = &quot;&quot;  
## If set to true, do no set the &quot;host&quot; tag in the telegraf agent.
omit_hostname = false

###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################

# Configuration for influxdb server to send metrics to
[[outputs.influxdb]]
## The full HTTP or UDP URL for your InfluxDB instance.
urls = [&quot;http://influxdb:8086&quot;] # required  
## The target database for metrics (telegraf will create it if not exists).
database = &quot;telegraf&quot; # required

## Name of existing retention policy to write to.  Empty string writes to
## the default retention policy.
retention_policy = &quot;&quot;  
## Write consistency (clusters only), can be: &quot;any&quot;, &quot;one&quot;, &quot;quorum&quot;, &quot;all&quot;
write_consistency = &quot;any&quot;

## Write timeout (for the InfluxDB client), formatted as a string.
timeout = &quot;5s&quot;

###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################

# # Statsd UDP/TCP Server
 [[inputs.statsd]]
#   ## Protocol, must be &quot;tcp&quot;, &quot;udp&quot;, &quot;udp4&quot; or &quot;udp6&quot; (default=udp)
   protocol = &quot;udp&quot;
#
#   ## MaxTCPConnection - applicable when protocol is set to tcp (default=250)
   max_tcp_connections = 250
#
#   ## Address and port to host UDP listener on
   service_address = &quot;:8125&quot;
#
#   ## The following configuration options control when telegraf clears it&#39;s cache
#   ## of previous values. If set to false, then telegraf will only clear it&#39;s
#   ## cache when the daemon is restarted.
#   ## Reset gauges every interval (default=true)
   delete_gauges = true
#   ## Reset counters every interval (default=true)
   delete_counters = true
#   ## Reset sets every interval (default=true)
   delete_sets = true
#   ## Reset timings &amp; histograms every interval (default=true)
   delete_timings = true
#
#   ## Percentiles to calculate for timing &amp; histogram stats
   percentiles = [90]
#
#   ## separator to use between elements of a statsd metric
   metric_separator = &quot;_&quot;
#
#   ## Parses tags in the datadog statsd format
#   ## http://docs.datadoghq.com/guides/dogstatsd/
   parse_data_dog_tags = false
#   ## Number of UDP messages allowed to queue up, once filled,
#   ## the statsd server will start dropping packets
   allowed_pending_messages = 10000
#
#   ## Number of timing/histogram values to track per-measurement in the
#   ## calculation of percentiles. Raising this limit increases the accuracy
#   ## of percentiles but also increases the memory usage and cpu time.
   percentile_limit = 1000

</code></pre>

<h4 id="toc_4">Show me the graphs</h4>

<p>If everything is good, you should see metrics stored in <code>influxdb</code> from the <code>chronograf</code> application. The UX is so fluent and lean. After 5 min only I was able to build this very simple dashboard:<br/>
<img src="media/15456638361570/15456639407892.jpg" alt=""/><br/>
Yaaay visibility!</p>

<h3 id="toc_5">Configuring alerts</h3>

<p>One of the great features of the TICK stack is the alerting and detecting anomalies in the data. It support many alert endpoints(TCP, HipChat, Slack, SMTP, Talk, log, Telegram ...). The configuration of <code>Kapacitor</code> from <code>chronograf</code> is very easy and straightforward. I&#39;ve set up a <code>Memory usage</code> alert in a matter of seconds. (Well, it&#39;s a very basic alert! configuring real world, complexe alerts will need more time obviously ;))<br/>
<img src="media/15456638361570/15456639514924.jpg" alt=""/></p>

<p>The <a href="https://docs.influxdata.com/chronograf/v1.4/guides/create-a-kapacitor-alert/">official documentation</a> explains the required step to create your first alert!</p>

<h3 id="toc_6">Securing Chronograf</h3>

<p>The community edition of the TICK stack does not ship with security by default. To secure <code>Chronograf</code> we can put it behind a web server and configure Basic authentication. The second option, which I prefer, would be to use <a href="https://www.keycloak.org/">keycloak</a> proxy as I explained previously in <a href="https://github.com/aboullaite/kibana-keycloak">this post</a>!</p>

<hr/>

<h5 id="toc_7">Ressources</h5>

<ul>
<li>  <a href="https://micrometer.io/docs">https://micrometer.io/docs</a></li>
<li>  <a href="https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#production-ready">https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#production-ready</a></li>
<li>  <a href="https://www.influxdata.com/time-series-platform/">https://www.influxdata.com/time-series-platform/</a></li>
<li>  <a href="https://www.digitalocean.com/community/tutorials/how-to-monitor-system-metrics-with-the-tick-stack-on-centos-7">https://www.digitalocean.com/community/tutorials/how-to-monitor-system-metrics-with-the-tick-stack-on-centos-7</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Design Of A Modern Cache]]></title>
    <link href="http://panlw.github.io/15453866242265.html"/>
    <updated>2018-12-21T18:03:44+08:00</updated>
    <id>http://panlw.github.io/15453866242265.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="http://highscalability.com/blog/2016/1/25/design-of-a-modern-cache.html">http://highscalability.com/blog/2016/1/25/design-of-a-modern-cache.html</a></p>
</blockquote>

<pre><code>MONDAY, JANUARY 25, 2016 AT 8:56AM
</code></pre>

<p><img src="https://c2.staticflickr.com/2/1584/23979355273_e1ea8b2397_o.png" alt=""/></p>

<p><u>This is a guest post by <a href="https://github.com/ben-manes">Benjamin Manes</a>, who did engineery things for Google and is now doing engineery things for a new load documentation startup, <a href="https://loaddocs.co/">LoadDocs</a>.</u></p>

<p>Caching is a common approach for improving performance, yet most implementations use strictly classical techniques. In this article we will explore the modern methods used by <a href="https://github.com/ben-manes/caffeine">Caffeine</a>, an open-source Java caching library, that <strong>yield high hit rates and excellent concurrency</strong>. These ideas can be translated to your favorite language and hopefully some readers will be inspired to do just that.</p>

<h3 id="toc_0">Eviction Policy</h3>

<p>A cache’s <strong>eviction policy tries to predict which entries are most likely to be used again</strong> in the near future, thereby maximizing the hit ratio. The Least Recently Used (LRU) policy is perhaps the most popular due to its simplicity, good runtime performance, and a decent hit rate in common workloads. Its ability to predict the future is limited to the history of the entries residing in the cache, preferring to give the last access the highest priority by guessing that it is the most likely to be reused again soon.</p>

<p>Modern caches extend the usage history to include the recent past and give preference to entries based on recency and frequency. One approach for retaining history is to use a popularity sketch (a compact, probabilistic data structure) to identify the “heavy hitters” in a large stream of events. Take for example <a href="http://dimacs.rutgers.edu/%7Egraham/pubs/papers/cmsoft.pdf">CountMin Sketch</a>, which <strong>uses a matrix of counters and multiple hash functions</strong>. The addition of an entry increments a counter in each row and the frequency is estimated by taking the minimum value observed. This approach lets us tradeoff between space, efficiency, and the error rate due to collisions by adjusting the matrix’s width and depth.</p>

<p><img src="https://docs.google.com/drawings/d/sPS-0VWjzfHp6DtvEQKU0Hg/image?w=470&amp;h=177&amp;rev=1&amp;ac=1" alt=""/></p>

<p><a href="http://arxiv.org/pdf/1512.00727.pdf">Window TinyLFU</a> (W-TinyLFU) <strong>uses the sketch as a filter, admitting a new entry</strong> if it has a higher frequency than the entry that would have to be evicted to make room for it. Instead of filtering immediately, an admission window gives an entry a chance to build up its popularity. This avoids consecutive misses, especially in cases like sparse bursts where an entry may not be deemed suitable for long-term retention. To keep the history fresh an aging process is performed periodically or incrementally to halve all of the counters.</p>

<p><img src="https://docs.google.com/drawings/d/sHXTg0FelH7DQ0ctP3zAnYg/image?w=511&amp;h=123&amp;rev=94&amp;ac=1" alt=""/></p>

<p>W-TinyLFU uses the Segmented LRU (SLRU) policy for long term retention. An entry starts in the probationary segment and on a subsequent access it is promoted to the protected segment (capped at 80% capacity). When the protected segment is full it evicts into the probationary segment, which may trigger a probationary entry to be discarded. This ensures that entries with a small reuse interval (the hottest) are retained and those that are less often reused (the coldest) become eligible for eviction.</p>

<p><img src="https://lh5.googleusercontent.com/FMyO_6DWOaAU-ulD38My9G-KpmYdAe8IQupR0n9MUEDVlDJ9xjvnJtccy2lms-rzQ3bPkbB773lPsCS67WgvOrLn-LpVBkVNCjfavzi4QD3BSNwWp0gGQEI_21rfhKytOLp-N2A" alt=""/></p>

<p><img src="https://lh4.googleusercontent.com/KzGH8Y4W-2qZ4bGpqmHQVb6rXb6DbYiBcrm4UwKfdDyIoiwUXAAnLSPHW4r9WEqhIVEat7pelOPiB5w_VbWI-vC_trza1WU_DVsYW2quV-1RIb9uUnY32PSPF8GklwJySYruC94" alt=""/></p>

<p>As the database and search traces show, there is a lot of opportunity to improve upon LRU by taking into account recency and frequency. More advanced policies such as <a href="https://www.usenix.org/event/fast03/tech/full_papers/megiddo/megiddo.pdf">ARC</a>, <a href="http://web.cse.ohio-state.edu/hpcs/WWW/HTML/publications/papers/TR-02-6.pdf">LIRS</a>, and <a href="http://arxiv.org/pdf/1512.00727.pdf">W-TinyLFU</a> narrow the gap to provide a near optimal hit rate. For additional workloads see the research papers and try our <a href="https://github.com/ben-manes/caffeine/wiki/Simulator">simulator</a> if you have your own traces to experiment with.</p>

<h3 id="toc_1">Expiration Policy</h3>

<p>Expiration is often implemented as variable per entry and expired entries are evicted lazily due to a capacity constraint. This pollutes the cache with dead items, so sometimes a scavenger thread is used to periodically sweep the cache and reclaim free space. This strategy tends to work better than ordering entries by their expiration time on a O(lg n) priority queue due to hiding the cost from the user instead of incurring a penalty on every read or write operation.</p>

<p>Caffeine takes a different approach by <strong>observing that most often a fixed duration is preferred</strong>. This constraint allows for organizing entries on O(1) time ordered queues. A time to live duration is a write order queue and a time to idle duration is an access order queue. The cache can reuse the eviction policy’s queues and the concurrency mechanism described below, so that expired entries are discarded during the cache’s maintenance phase.</p>

<h3 id="toc_2">Concurrency</h3>

<p>Concurrent access to a cache is viewed as a difficult problem because in <strong>most policies every access is a write to some shared state</strong>. The traditional solution is to guard the cache with a single lock. This might then be improved through lock striping by splitting the cache into many smaller independent regions. Unfortunately that tends to have a limited benefit due to hot entries causing some locks to be more contented than others. When contention becomes a bottleneck the next classic step has been to <strong>update only per entry metadata</strong> and use either a <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.110.8469&amp;rep=rep1&amp;type=pdf">random sampling</a> or a <a href="https://en.wikipedia.org/wiki/Page_replacement_algorithm#Second-chance">FIFO-based</a> eviction policy. Those techniques can have great read performance, poor write performance, and difficulty in choosing a good victim.</p>

<p>An <a href="http://web.cse.ohio-state.edu/hpcs/WWW/HTML/publications/papers/TR-09-1.pdf">alternative</a> is to <strong>borrow an idea from database theory where writes are scaled by using a commit log</strong>. Instead of mutating the data structures immediately, the <strong>updates are written to a log and replayed in asynchronous batches</strong>. This same idea can be applied to a cache by performing the hash table operation, recording the operation to a buffer, and scheduling the replay activity against the policy when deemed necessary. The policy is still guarded by a lock, or a try lock to be more precise, but shifts contention onto appending to the log buffers instead.</p>

<p>In Caffeine <strong>separate buffers are used for cache reads and writes</strong>. An access is recorded into a <strong>striped ring buffer</strong> where the stripe is chosen by a thread specific hash and the number of stripes grows when contention is detected. When a ring buffer is full an asynchronous drain is scheduled and subsequent additions to that buffer are discarded until space becomes available. When the access is not recorded due to a full buffer the cached value is still returned to the caller. The loss of policy information does not have a meaningful impact because W-TinyLFU is able to identify the hot entries that we wish to retain. By using a thread-specific hash instead of the key’s hash the cache avoids popular entries from causing contention by more evenly spreading out the load.</p>

<p><img src="https://docs.google.com/drawings/d/sxSo8y3_uR8QZ_NbIqnCwzA/image?w=313&amp;h=121&amp;rev=371&amp;ac=1" alt=""/></p>

<p>In the case of a write a more traditional concurrent queue is used and every change schedules an immediate drain. While data loss is unacceptable, there are still ways to optimize the write buffer. Both types of buffers are written to by multiple threads but only consumed by a single one at a given time. This multiple producer / single consumer behavior allows for simpler, more efficient algorithms to be employed. </p>

<p>The buffers and fine grained writes introduce a race condition where operations for an entry may be recorded out of order. An insertion, read, update, and removal can be replayed in any order and if improperly handled the policy could retain dangling references. The solution to this is a state machine defining the lifecycle of an entry.</p>

<p> <img src="https://docs.google.com/drawings/d/scG-jsP3Cfr4YfwBQn9_bpQ/image?w=529&amp;h=133&amp;rev=173&amp;ac=1" alt=""/></p>

<p>In <a href="https://github.com/ben-manes/caffeine/wiki/Benchmarks#read-100-1">benchmarks</a> the cost of the buffers is relatively cheap and scales with the underlying hash table. <strong>Reads scale linearly with the number of CPUs</strong> at about 33% of the hash table’s throughput. Writes have a 10% penalty, but only because contention when updating the hash table is the dominant cost.</p>

<p><img src="https://lh5.googleusercontent.com/Zrb2daVCRylwYZpSKykg1HVxCwsJI6N4sB7KQZ6wTfVdhdZNf75PNi38kO9BKMGn0PW5hHgaEDg2JfFtqbAPU3r4kQZ1toHJ0k7b7gTFII0pXbAJw0pbJ1oPjmqL6ZVEKBHSzLY" alt=""/></p>

<h3 id="toc_3">Conclusion</h3>

<p>There are many pragmatic topics that have not been covered. This could include tricks to minimize the memory overhead, testing techniques to retain quality as complexity grows, and ways to analyze performance to determine whether a optimization is worthwhile. These are areas that practitioners must keep an eye on, because once neglected it can be difficult to restore confidence in one’s own ability to manage the ensuing complexity.</p>

<p>The design and implementation of <a href="https://github.com/ben-manes/caffeine">Caffeine</a> is the result of numerous insights and the hard work of many contributors. Its evolution over the years wouldn’t have been possible without the help from the following people: Charles Fry, Adam Zell, Gil Einziger, Roy Friedman, Kevin Bourrillion, Bob Lee, Doug Lea, Josh Bloch, Bob Lane, Nitsan Wakart, Thomas Müeller, Dominic Tootell, Louis Wasserman, and Vladimir Blagojevic. Thanks to Nitsan Wakart, Adam Zell, Roy Friedman, and Will Chu for their feedback on drafts of this article.</p>

<h2 id="toc_4">Related Articles </h2>

<ul>
<li>  <a href="https://news.ycombinator.com/item?id=10968521">On HackerNews</a></li>
<li>  <a href="https://www.reddit.com/r/programming/comments/42s3ku/design_of_a_modern_cache/">On Reddit</a> </li>
<li>  <a href="https://github.com/dgryski/go-tinylfu">TinyLFU</a> - cache admission policy</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深入理解「分布式事务」]]></title>
    <link href="http://panlw.github.io/15449526758162.html"/>
    <updated>2018-12-16T17:31:15+08:00</updated>
    <id>http://panlw.github.io/15449526758162.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://zhuanlan.zhihu.com/p/51684618">https://zhuanlan.zhihu.com/p/51684618</a></p>

<p>关键词 go/python、架构存储、分布式基础</p>

<p>如果一个事务调用了不同服务器上的操作，那么它就成为了一个分布式事务。</p>
</blockquote>

<p>考虑下面一种场景：当你发了工资之后，把你的当月工资 ¥1024 从支付宝转到了余额宝。</p>

<p>如果在支付宝账户扣除 ¥1024 之后，余额宝系统挂掉了，余额宝的账户并没有增加 ¥1024，这时候就出现了数据不一致的情况。</p>

<p>在很多系统中都能找到上述情况的影子：</p>

<ul>
<li>  在下单的时候，需要在订单表中插入一条数据，然后把库存减去一</li>
<li>  在搜索的时候如果点击了广告，需要先记录该点击事件，然后通知商家系统扣除广告费</li>
</ul>

<blockquote>
<p>在一个分布式事务结束的时候，事务的原子特性要求所有参与该事务的服务器必须全部提交或全部放弃该事务。为了实现这一点，其中一个服务器承担了协调者 (coordinater) 的角色，由它来保证所有的服务器获得相同的结果。</p>
</blockquote>

<p>协调者 (coordinater) 的工作方式取决于它选用的协议，“两阶段提交”是分布式事务最常用的协议。</p>

<h2 id="toc_0">1、two-phase commit protocol</h2>

<p><img src="media/15449526758162/15449527856306.jpg" alt=""/></p>

<p>两阶段提交协议 (two-phase commit protocol) 的设计出发点是允许任何一个参与者自行放弃它自己的那部分事务。由于事务原子性的要求，如果部分事务被放弃，那么整个分布式事务也必须被放弃。</p>

<p>在该协议的第一个阶段，每个参与者投票表决该事务是放弃还是提交，一旦参与者要求提交事务，那么就不允许放弃该事务。因此，在一个参与者要求提交事务之前，它必须保证最终能够执行分布式事务中自己的那部分，即使该参与者出现故障而被中途替换掉。</p>

<p>一个事务的参与者如果最终能提交事务，那么可以说参与者处于事务的准备好 (<strong>prepared</strong>) 状态。为了保证能够提交，每个参与者必须将事务中所有发生改变的对象以及自身的状态 (<strong>prepared</strong>) 保存到持久性存储中。</p>

<p>在该协议的第二个阶段，事务的每个参与者执行最终统一的决定。如果任何一个参与者投票放弃事务，那么最终的决定是放弃事务。如果所有的参与者都投票提交事务，那么最终的决定是提交事务。</p>

<p>问题在于，要保证每个参与者都投票，并且达成一个共同的决定。在无故障时，该协议相当简单。但是，协议必须在出现各种故障 (例如服务器崩溃，消息丢失或服务暂时无法通信）时能够正常工作。</p>

<h2 id="toc_1">2、两阶段提交的实现</h2>

<p>为了实现两阶段提交协议，分布式事务中的协调者和参与者通常按照下面的接口进行通信：</p>

<ul>
<li>  <strong>canCommit(trans) ?</strong></li>
</ul>

<p>协调者询问参与者是否可以提交事务，参与者回复自己的投票结果。</p>

<ul>
<li>  <strong>doCommit(trans)</strong></li>
</ul>

<p>协调者告诉参与者提交它的那部分事务。</p>

<ul>
<li>  <strong>doAbort(trans)</strong></li>
</ul>

<p>协调者告诉参与者放弃它的那部分事务。</p>

<ul>
<li>  <strong>haveCommitted(trans, participant)</strong></li>
</ul>

<p>参与者用该操作向协调者确认它提交了事务。</p>

<ul>
<li>  <strong>getDecision(trans) ?</strong></li>
</ul>

<p>当参与者在投 Yes 票后一段时间内未收到应答时，参与者用该操作向协调者询问事务的投票表决结果。该操作用于从服务器崩溃或从消息延迟中恢复。</p>

<pre><code>阶段一（投票阶段）：
1）协调者向分布式事务的所有参与者发送canCommit？请求
2）当参与者收到canCommit请求后，它向协调者回复自己的投票（Yes/No）。
   在投Yes票之前，它在持久性存储中保存所有对象，准备提交。如果投No票，参与者立即放弃。
阶段二（提交阶段）：
1）协调者收集所有的投票（包括它自己的投票）。
   a）如果不存在故障并且所有的投票都是Yes时，那么协调者将决定提交事务并向所有参与者发送doCommit
      请求
   b）否则，协调者决定放弃该事务，并向所有投Yes票的参与者发送doAbort请求
2）投Yes票的等待者等待协调者发送的doCommit或者doAbort请求。一旦参与者接收到任何一种请求消息，
   它将根据该请求放弃或者提交事务。如果请求是提交事务，那么他还要向协调者发送一个haveCommitted
   来确认事务已经提交

</code></pre>

<h2 id="toc_2">3、分布式事务的故障模型</h2>

<p>在分布式事务中执行的过程中，可能出现磁盘故障，进程崩溃以及消息的丢失，超时等。</p>

<p>两阶段提交是一种达成共识的协议，在该系统中，如果进程崩溃，那么是不可能达成共识的。但是，两阶段提交却是在这些条件下达成了共识，这是由于进程的崩溃被屏蔽，崩溃的进程被一个新的进程取代，新进程的状态根据持久性存储中保存的信息和其他进程拥有的信息来设定。</p>

<p><strong>3.1、故障模型</strong></p>

<p>Lampson 提出过一个分布式事务的故障模型，包括了硬盘故障、服务器故障以及通信故障。该故障模型声称：可以保证算法在出现故障时正确工作，但是对于不可预见的灾难性故障则不能正确处理。尽管会出现错误，但是可以在发生不正确行为之前发现并处理这些错误。Lampson 的故障模型包括以下故障：</p>

<ul>
<li>  对持久性存储的写操作可能发生故障（或因为写操作无效或因为写入错误的值）。例如，将数据写到错误的磁盘块被认为是灾难性故障。文件存储可能损坏。在持久性存储中读数据时可根据校验和来判断数据块是否损坏。</li>
<li>  服务器可能偶尔崩溃。当一个崩溃的服务器由一个新进程取代后，它的可变内存被重置，崩溃之前的数据均丢失。此后新进程执行一个可恢复过程，根据持久存储中的信息以及从其他进程获得的信息设置对象的值，包括两阶段提交协议有关对象的值。当一个处理器出现故障时，服务器也会崩溃，这样它就不会发送错误的信息或将错误的值写入持久存储，即它不会产生随机故障。服务器崩溃可能出现在任何时候，特别是在恢复时也可能出现。</li>
<li>  消息传递可能有任意长的延迟。消息可能丢失、重复或者损坏。接收方（通过校验和）能够检测到受损消息。未发现的受损消息和伪造的消息可能会导致灾难性故障。</li>
</ul>

<p>利用这个关于持久性存储、处理器和通信的故障模型能够设计出一个可靠系统，该系统的组件可对付任何单一故障，并提供一个简单的故障模型。特别是，<strong>可靠存储 (stable storage)</strong> 可以在出现一个 write 操作故障或者进程崩溃的情况下提供原子写操作。它是通过将每一个数据块复制到两个磁盘上实现的。此时一个 write 操作用于两个磁盘块，在一个磁盘出现故障的前提下，另一个好的磁盘也可以提供正确数据。<strong>可靠处理器 (stable processor)</strong> 使用可靠存储，用于在崩溃之后恢复对象。可通过可靠的远程过程调用机制来屏蔽通信错误。</p>

<p><strong>3.2、两阶段提交协议的超时</strong></p>

<p>在两阶段协议的不同阶段，协调者或参与者都会遇到这种场景：不能处理它的那部分协议，直到接收到下一个请求或应答为止。</p>

<p>首先考虑这样的情形：某个投票者投 Yes 票并等待协调者发回最终决定，即告诉它是提交事务还是放弃事务。这样参与者的结果是<strong>不确定 (uncertain)</strong> 的，它在协调者处得到投票结果之前不能进行进一步处理。参与者不能单方面决定下一步做什么，同时该事务使用的对象也不能释放以用于其他事物。参与者向协调者发出 getDecision 请求来获取事务的结果，直到收到应答时，才能进入两阶段协议的第二阶段。</p>

<p>同理，如果协调者发生故障，那么参与者将不能获得协定，直到协调者被替代为止，这可能导致不确定状态的参与者长时间的延迟。</p>

<p>不依赖协调者获取最终决定的方法是通过参与者协作来获得决定。这种策略的优点是可以在协调者出故障时使用。（在本篇文章中我们不讨论这种方式）</p>

<h2 id="toc_3">4、两阶段提交的故障处理</h2>

<ul>
<li><p>当参与者发生故障的时候：<br/>
<img src="media/15449526758162/15449528046242.jpg" alt=""/></p></li>
<li><p>当协调者发生故障的时候：<br/>
<img src="media/15449526758162/15449528150927.jpg" alt=""/></p></li>
</ul>

<h2 id="toc_4">5、两阶段提交的性能</h2>

<p>假设一切运转正常，即协调者参与者不出现故障，通信也正常时，有 N 个参与者的两阶段提交协议需要 N 个 <strong>canCommit</strong> 消息和应答，然后再有 N 个 <strong>doCommit</strong> 消息。这样消息开销和 3N 成正比，时间开销是 3 次消息往返。由于协议在没有 <strong>haveCommitted</strong> 消息时仍可以正常运作（它们的作用只是通知服务器删除过时的协调者消息），因此在估计协议开销上，不将 <strong>haveCommitted</strong> 消息计算在内。</p>

<p>在最坏的情况下，两阶段提交协议在执行过程中可能出现任意多次服务器和通信故障。尽管协议不能指定协议完成的时间限制，但它能正确处理连续故障（服务崩溃或者消息丢失），并保证最终完成。</p>

<h2 id="toc_5">6、使用消息队列来避免分布式事务</h2>

<p><strong>5.1、消息队列</strong></p>

<p>由于分布式事务存在严重的性能问题，在设计高并发服务的时候，往往通过其他途径来解决数据一致性问题。</p>

<blockquote>
<p>举例来讲，你在北京很有名的姚记炒肝点了炒肝并付了钱后，他们并不会直接把你点的炒肝给你，而是给你一张小票，然后让你拿着小票到出货区排队去取。为什么他们要将付钱和取货两个动作分开呢？原因很多，其中一个很重要的原因是为了使他们接待能力增强（并发量更高）。</p>
</blockquote>

<p>还是回到我们的问题，只要这张小票在，你最终是能拿到炒肝的。同理转账服务也是如此，当支付宝账户扣除 1 万后，我们只要生成一个凭证（消息）即可，这个凭证（消息）上写着 “让余额宝账户增加 1 万”，只要这个凭证（消息）能可靠保存，我们最终是可以拿着这个凭证（消息）让余额宝账户增加 1 万的，即我们能依靠这个凭证（消息）完成最终一致性。</p>

<p>这样我们上述的转账就变成了如下过程：</p>

<ul>
<li>  支付宝在扣款事务提交之前，向消息队列发送消息。此时的消息队列只记录消息，而并没有将消息发往余额宝。</li>
<li>  当支付宝扣款事务提交成功，向消息队列发送确认。在得到确认的指令后，消息队列向该消息发往余额宝。</li>
<li>  当支付宝扣款事务提交失败，向消息队列发送取消。在得到取消的指令后，消息队列取消该消息，该消息将不会被发送。</li>
<li>  对于那么未确认的消息，需要消息队列去支付宝系统查询这个消息的状态，并进行更新。（因为支付宝可能在扣款事务提交成功后挂掉，此时消息的状态未被更新为：“<strong>确认发送 “。</strong>从而导致消息不能被发送。</li>
</ul>

<p><strong>5.2、重复投递</strong></p>

<p>还有一个严重的问题是<strong>消息重复投递</strong>，以我们支付宝转账到余额宝为例，如果相同的消息被重复投递两次，那么我们余额宝账户将会增加 2 万而不是 1 万了。<br/>
<img src="media/15449526758162/15449528386270.jpg" alt=""/></p>

<p>我们将在后续的文章讨论消息队列的两个问题：</p>

<ul>
<li>  Exactly-once delivery</li>
<li>  Guaranteed order of messages</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Uber 开源 Fusion.js：一个基于插件架构的通用 Web 框架]]></title>
    <link href="http://panlw.github.io/15444606302745.html"/>
    <updated>2018-12-11T00:50:30+08:00</updated>
    <id>http://panlw.github.io/15444606302745.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://www.infoq.cn/article/2018%2F08%2Fuber-Web-open-fusion-js">https://www.infoq.cn/article/2018%2F08%2Fuber-Web-open-fusion-js</a></p>
</blockquote>

<p>可能很多人都不知道，Uber 其实开发了很多基于 Web 的应用程序，可能有数百个，而且这个数字还在不断增加中，它们中的大部分被用在公司内部，用于管理各种业务。</p>

<p>我们知道，Web 技术变化得很快，而最佳技术实践也在不断发展。为数百名 Web 工程师提供高质量的框架和功能，同时又要保持 Web 平台的动态特性，一直以来都是一个巨大的挑战。</p>

<p>为了应对这一挑战，Uber 的 Web 平台团队开发了 Fusion.js，一个开源的 Web 框架，用于简化 Web 开发，并构建出高性能的轻量级 Web 应用程序。</p>

<h2 id="toc_0">动机</h2>

<p>随着 Web 最佳实践的发展，Uber 需要改造已有的单体 Web 框架，解决长达数年的技术债务所带来的挑战。我们还希望让工程师们能够继续使用他们喜欢的技术（例如 React 和 Redux），同时保持与 Uber 健康监控基础设施的兼容性。</p>

<p>具体来说，我们希望核心框架能够解决以下痛点：</p>

<ul>
<li><p>服务器端渲染、代码拆分和模块热加载所需的复杂配置和工具样板代码</p></li>
<li><p>在涉及服务器端渲染的 React 应用程序时，缺乏用于实现和共享特性的良好抽象</p></li>
<li><p>不同位置的代码紧密耦合而导致的脆弱性</p></li>
<li><p>测试难度攀升</p></li>
<li><p>单体框架​​缺乏灵活性</p></li>
</ul>

<p>虽然现有的解决方案解决了其中的一些挑战，但我们发现，基于框架添加新库通常需要修改多个不相关的文件。例如，要让可进行服务器端渲染的应用程序支持 Redux, 通常需要在服务器相关的文件中添加代码，并在客户端添加类似的代码，还要向 HTML 模板中添加 hydration 代码，使用 React Provider 组件等。要集成 i18n 库或添加浏览器性能指标库也是一样。</p>

<p>很多特定于应用程序的代码可能会依赖用于管理副作用的库（例如用于日志记录或数据持久化的库），工程师很难在没有服务层抽象的帮助下以可测试的方式集成这些库。</p>

<p>我们既希望能够为与 Uber 现有库集成提供简单且经过实战考验的解决方案，也希望能够避免使用单体框架，从而保持捆绑包的小体积。</p>

<p>我们倾向于选择模块化方法的另一个原因是，我们必须明确指定依赖关系，这样可以更容易避免技术债务，如 God Object（<a href="https://en.wikipedia.org/wiki/God_object">https://en.wikipedia.org/wiki/God_object</a>）、临时内部接口和紧密耦合。</p>

<p>Fusion.js 是我们努力的结晶。</p>

<h2 id="toc_1">谁应该使用 Fusion.js？</h2>

<p>简单地说，Fusion.js 是一个 MIT 许可的 JavaScript 框架，支持 React 和 Redux 等流行库，并提供了很多现代特性，如模块热加载、数据感知服务器端渲染和捆绑拆分支持。</p>

<p>除了预配置的样板，Fusion.js 还提供了灵活的基于插件的架构。因此它非常适合用于现代单页应用程序以及依赖复杂服务层来满足各种质量要求的现代 Web 应用程序。</p>

<p>有关 Fusion.js 的更多信息，请查看项目文档。</p>

<h2 id="toc_2">基于插件的架构</h2>

<p>Fusion.js 应用程序是通用的，也就是说它有一个单入口文件，并且可以在服务器和浏览器上重用代码。在通用的应用程序中，React 组件还可以获取数据并在服务器上渲染 HTML，从而可以利用浏览器的原生 HTML 解析器和避免 JavaScript DOM API 的开销来减少页面加载时间。</p>

<p>单入口架构使 Fusion.js 插件本身也具有通用性，插件开发人员可以将代码片段与代码所属的库放在一起，而不是与代码运行的环境放在一起。<br/>
<img src="media/15444606302745/15444607091155.jpg" alt=""/><br/>
<strong>Fusion.js 插件基于逻辑分组封装逻辑，而不是基于需要添加代码的位置</strong></p>

<p>插件可以通过中间件访问 HTTP 请求生命周期，也可以访问 React 树，以便添加 Provider 组件。它们还可以初始化浏览器代码。</p>

<p>最后，由于这些特性，我们可以通过单行代码将库安装到应用程序中，无论库需要多少个不同的集成点。由于插件易于添加和删除，因此在重构时也很容易推断它们之间的耦合度、对包大小的影响以及其他代码质量属性。它们也可以初始化浏览器代码。</p>

<h2 id="toc_3">类型依赖注入</h2>

<p>插件利用了依赖注入，这意味着它们可以将定义良好的 API 作为服务暴露给其他插件，并且在测试期间可以轻松地模拟插件的依赖项。当依赖关系负责与数据存储基础设施打交道或与可观察性（例如日志记录、分析和指标）相关时，这一点尤为重要。</p>

<p>复制代码</p>

<pre><code>/*这个例子实现了一个从 session 读取数据的端点。Session 是通过依赖注入的方式提供的。SessionToken 是一个标签 (用于确保类型安全)。*/ // src/plugins/user.jsimport {createPlugin} from &#39;fusion-core&#39;;import {SessionToken} from &#39;fusion-tokens&#39;; export default __NODE__ &amp;&amp; createPlugin({  deps: {Session: SessionToken},  middleware({Session}) {    return async (ctx, next) =&gt; {      if (ctx.path === &#39;/api/user&#39;) {        ctx.body = JSON.parse(await Session.from(ctx).get(&#39;user&#39;));      }      return next();    }  }});
</code></pre>

<p>还可以借助 Flow.js 来确保依赖之间的静态类型安全，如下所示：<br/>
<img src="media/15444606302745/15444607377246.jpg" alt=""/><br/>
<strong>直接在代码编辑器中显示错误有助于在代码运行之前捕获错误</strong></p>

<h2 id="toc_4">中间件管理</h2>

<p>几年前就存在这样的一个挑战，流行的 HTTP 服务器库 Express 有一个 API 让复杂的响应转换变得难以封装和测试。在 Uber 以前的架构中，应用程序开发人员经常需要采用临时包含 Express 请求 / 响应对象的特定补丁。自然而然地，因为子系统对时间要求的高度耦合，测试变得极其困难。</p>

<p>在开始设计 Fusion.js 时，我们就一直关注这个问题。经过大量调研，我们决定使用 Koa（<a href="https://koajs.com/">https://koajs.com</a>），它提供了基于上下文的 API，对单元测试非常友好，并为请求生命周期管理提供了一个基于下游和上游概念的轻量级抽象。</p>

<p>事实证明，采用 Koa 是一个正确的设计决策。</p>

<p>Koa 中间件为 React Provider 组件提供逻辑集成点，下游 / 上游抽象与 React 服务器渲染上下文的生命周期完美匹配。网络副作用与应用程序逻辑分离，从而提高了可测性。</p>

<p>Fusion.js 的依赖注入和图解析机制解决了困扰我们已久的 God Object 和操作顺序问题。<br/>
<img src="media/15444606302745/15444607505348.jpg" alt=""/><br/>
<strong>Fusion.js 核心将网络副作用与应用程序状态隔离开来，并利用 Koa 和 DI 来实现子系统之间的松散耦合</strong></p>

<h2 id="toc_5">可测性</h2>

<p>在过去的几年里，JavaScript 生态系统中出现了大量高质量的测试工具，并提高了对测试技术的认识。</p>

<p>除了支持 Jest、Enzyme 和 Puppeteer 等现代测试工具外，Fusion.js 还为开发人员提供了测试插件的工具。fusion-test-utils 包允可用于模拟服务器本身，从而可以在插件和各种桩的组合上快速运行集成测试。</p>

<h2 id="toc_6">前行之路</h2>

<p>在 Uber 内部，已有 60 多个项目代码库在使用 Fusion.js。我们预计这个数字会继续增加，因为新的 Web 项目也在不断创建，同时旧项目被自动迁移到 Fusion.js。因此，框架级别的改进应该能够显著改善这些项目的软件质量基准。</p>

<p>我们的路线图包括添加更多的性能优化和面向测试的工具，以及更好的 Flow 支持。</p>

<p>项目地址：<a href="https://github.com/fusionjs">https://github.com/fusionjs</a></p>

<p><strong>查看英文原文：</strong><a href="https://eng.uber.com/fusionjs/">https://eng.uber.com/fusionjs/</a>  </p>

<p>感谢<a href="http://www.infoq.com/cn/profile/%E8%A6%83%E4%BA%91">覃云</a>对本文的审校。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Spring Boot Actuator、Jolokia和Grafana实现准实时监控]]></title>
    <link href="http://panlw.github.io/15431584440254.html"/>
    <updated>2018-11-25T23:07:24+08:00</updated>
    <id>http://panlw.github.io/15431584440254.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="http://blog.didispace.com/spring-boot-jolokia-grafana-monitor/">http://blog.didispace.com/spring-boot-jolokia-grafana-monitor/</a></p>

<p>Spring’s JMX support: <a href="https://docs.spring.io/spring/docs/current/spring-framework-reference/integration.html#jmx">https://docs.spring.io/spring/docs/current/spring-framework-reference/integration.html#jmx</a></p>

<p>由于最近在做监控方面的工作，因此也读了不少相关的经验分享。其中有这样一篇文章总结了一些基于 Spring Boot 的监控方案，因此翻译了一下，希望可以对大家有所帮助。</p>

<p>原文：<a href="https://medium.com/@brunosimioni/near-real-time-monitoring-charts-with-spring-boot-actuator-jolokia-and-grafana-1ce267c50bcc#.il5xmlnv7">Near real-time monitoring charts with Spring Boot Actuator, Jolokia and Grafana</a></p>
</blockquote>

<p>Spring Boot Actuator 通过<code>/metrics</code>端点，以开箱即用的方式为应用程序的性能指标与响应统计提供了一个非常友好的监控方式。</p>

<p>由于在集群化的弹性环境中，应用程序的节点可以增长、扩展，并由非常大量的应用实例所组成。对于孤立节点的监控可能即费力又没有什么实际效果。所以，使用基于时间序列的数据聚合工具将获得更好的效果。</p>

<p>本文的目标在于找出一种仅需要通过工具和配置的方式就能实现的解决方案，来对 Spring Boot Metrics 实现基于时间序列的监控。</p>

<p>像 NewRelic, AppDynamics 或 DataDog 这些 APM 系统都能很好地完成这样的任务，它们通过使用 JVM 和字节码工具来生成自己的指标、分析工具和相关事务。也可以通过使用<code>@Timed</code>注释方法来实现。但是，这些方法将忽略所有 Spring Boot Actuator 库所提供的可用资源。另外，使用这些方法还有一个与保留数据相关的问题，它们对于短时间窗口内的监控是相对模糊的。</p>

<p><a href="http://blog.didispace.com/assets/1-W_O7bNYmgarjHNasow-PsQ.png" title="NewRelic在1分钟时间窗口内被发现和检测的事务"><img src="http://blog.didispace.com/assets/1-W_O7bNYmgarjHNasow-PsQ.png" alt=""/></a>NewRelic 在 1 分钟时间窗口内被发现和检测的事务</p>

<p><code>spring-boot-admin</code> 可以作为另外一个备选方案，因为它可以连接到 Spring Boot 的实例、并且可以聚合节点等。但是， <code>/metrics</code> 端点并不是根据时间轴来进行监控的，同时在不同节点上的相同应用模块（水平扩展）也没有得到聚合。这意味着您将面对这两种情况：没有时间序列的监控数据、只有对孤立节点的监控数据快照。</p>

<p><a href="http://blog.didispace.com/assets/1-KK_x2uD66NIIIfmyFIraEg.png" title="Spring Boot Admin with metrics from Actuator: a snapshot of metrics data of a given application node"><img src="http://blog.didispace.com/assets/1-KK_x2uD66NIIIfmyFIraEg.png" alt=""/></a>Spring Boot Admin with metrics from Actuator: a snapshot of metrics data of a given application node</p>

<p><a href="http://blog.didispace.com/assets/1-MztwgrZsF2wtXL_OMZCfdQ.png" title="Spring Boot Admin with JMX and MBeans read data of a give application node"><img src="http://blog.didispace.com/assets/1-MztwgrZsF2wtXL_OMZCfdQ.png" alt=""/></a>Spring Boot Admin with JMX and MBeans read data of a give application node</p>

<p><code>jconsole</code>和<code>visualvm</code>可能是另外一种选择，它们通过 RMI 直接连接到 JMX 节点。Actuator 存储来自 JMX 的 MBean 内的 Metrics 数据。另外，通过使用 <a href="https://jolokia.org/">Jolokia</a>，MBeans 以 RESTful HTTP 端点的方式暴露，<code>/jolokia</code>。所以，相同的信息可以通过两个端点来获取：JMX MBean Metrics 和 Rest HTTP Jolokia 端点。然而，这种方式存在同样的问题，它们直接连接到集群环境中的单个节点，另外还伴随着痛苦的老式 RMI 协议。</p>

<p><a href="http://blog.didispace.com/assets/1-NOkLUyGMydiFYYotF2rKQ.png" title="JConsole old-school JMX Metrics of a given application node"><img src="http://blog.didispace.com/assets/1-NOkLUyGMydiFYYotF2rKQ.png" alt=""/></a>JConsole old-school JMX Metrics of a given application node</p>

<p><a href="http://blog.didispace.com/assets/1-mH5Wey1GDlCmDRdZPlHYtg.png" title="VisualVM JMX Metrics of a give application node"><img src="http://blog.didispace.com/assets/1-mH5Wey1GDlCmDRdZPlHYtg.png" alt=""/></a>VisualVM JMX Metrics of a give application node</p>

<p>继续前进，我尝试了一些可能可以解决这些问题的现代化运维工具：</p>

<ul>
<li>  <a href="https://prometheus.io/"><strong>Prometheus</strong></a>: 由 SoundCloud 编写，它存储一系列的监控数据并赋予漂亮的图标展现。Prometheus Gauges 和 Actuator Metrics 并不完全兼容，所以人们写了 <a href="https://github.com/prometheus/client_java/pull/114">一个数据转换器</a>。你也可以配置 Prometheus 来收集 JMX 数据。</li>
<li>  <a href="https://sensuapp.org/"><strong>Sensu</strong></a>: 作为 Nagios 和 Zabbix 的现代化替代品，它有一个插件可以直接连接到 Spring Boot，<a href="https://github.com/sensu-plugins/sensu-plugins-springboot">但是这个仓库最近已经不太更新了</a>，所以我决定放弃它。</li>
<li>  <a href="https://github.com/etsy/statsd"><strong>StatsD</strong></a>: Spring Boot 有一篇文章是<a href="http://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#production-ready-metric-writers-export-to-statsd">关于自定义导出数据给 StatsD</a>。然而，你除了要为 Spring Boot 应用安装 StatsD 实例之外，还不得不实现一些存根来让它工作起来。</li>
<li>  <a href="http://graphiteapp.org/"><strong>Graphite</strong></a><strong>:</strong> You got to be <a href="https://graphite.readthedocs.io/en/latest/install.html">a hero to install and get Graphite</a> running. If you get there, <a href="https://matt.aimonetti.net/posts/2013/06/26/practical-guide-to-graphite-monitoring/">you can configure it along StatsD to get metrics working in a chart</a>.</li>
<li>  <a href="http://opentsdb.net/"><strong>OpenTSDB</strong></a><strong>:</strong> Spring Boot 有一篇文章<a href="http://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#production-ready-metric-writers-export-to-open-tsdb">关于连接数据到 OpenTSBD</a>. 然而，这种方式与 StatsD 类似，你必须实现和维护自定义的代码来让它工作起来。另外，OpenTSDB 没有开箱即用的图形可视化工具。</li>
<li>  <a href="https://github.com/jmxtrans/jmxtrans"><strong>JMXTrans</strong></a>: 可以用来提取数据并发送到其他的监控工具，它也需要具体的实现。</li>
<li>  <a href="http://ganglia.info/"><strong>Ganglia</strong></a>: 也是基于 JVM 上的工具，记录所有 Actuator 资源。与之前所说的 APM 有相同问题。</li>
</ul>

<p>经过一番研究，我发现了一个更好的解决方案：通过 InfluxDB 和 Telegraf 实现，零编码，只需要通过一些正确的配置。</p>

<ul>
<li>  <a href="https://jolokia.org/"><strong>Jolokia</strong></a>: Spring Boot <a href="http://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#production-ready-jolokia">认可使用 Jolokia 来通过 HTTP 导出 export JMX 数据</a>。你只需要在工程类路径中增加一些依赖项，一切都是开箱即用的。不需要任何额外的实现。</li>
<li>  <a href="https://www.influxdata.com/time-series-platform/telegraf/"><strong>Telegraf</strong></a><strong>:</strong> Telegraf 支持通过整合 Jolokia 来集成 JMX 数据的收集。<a href="https://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia">它有一个预制的输入插件</a>，它是开箱即用的。不需要任何额外的实现。只需要做一些配置即可。</li>
<li>  <a href="https://www.influxdata.com/time-series-platform/influxdb/"><strong>InfluxDB</strong></a><strong>:</strong> InfluxDB 通过 <a href="https://github.com/influxdata/telegraf/tree/master/plugins/outputs/influxdb">输出插件</a>从 Telegraf 接收指标数据，它是开箱即用的，不需要任何额外的实现。</li>
<li>  <a href="http://grafana.org/"><strong>Grafana</strong></a>: Grafana 通过<a href="http://docs.grafana.org/datasources/influxdb/">连接 InfluxDB 作为数据源</a>来渲染图标。它是开箱即用的，不需要额外的实现。</li>
</ul>

<p>简而言之，配置所有这些东西都非常的简单。</p>

<p><a href="http://blog.didispace.com/assets/1-r252t1MBNRc3thU3DMRTcQ.png" title="Spring Boot Actuator Raw Metrics"><img src="http://blog.didispace.com/assets/1-r252t1MBNRc3thU3DMRTcQ.png" alt=""/></a>Spring Boot Actuator Raw Metrics</p>

<p><a href="http://blog.didispace.com/assets/1-XdrZlKh1Q2G0yd6xNkSrgQ.png" title="Metrics sent by Telegraf to InfluxDB, collected by Jolokia and JMX over HTTP"><img src="http://blog.didispace.com/assets/1-XdrZlKh1Q2G0yd6xNkSrgQ.png" alt=""/></a>Metrics sent by Telegraf to InfluxDB, collected by Jolokia and JMX over HTTP</p>

<p><a href="http://blog.didispace.com/assets/1-K8NTfF4Z2ms1YM_4cCjyvQ.png" title="Grafana InfluxDB data source configuration"><img src="http://blog.didispace.com/assets/1-K8NTfF4Z2ms1YM_4cCjyvQ.png" alt=""/></a>Grafana InfluxDB data source configuration</p>

<p><a href="http://blog.didispace.com/assets/1-ZoQcStClNQf7iz_cQNZHxA.png" title="Grafana Metric chart query and configuration: gauges of an API"><img src="http://blog.didispace.com/assets/1-ZoQcStClNQf7iz_cQNZHxA.png" alt=""/></a>Grafana Metric chart query and configuration: gauges of an API</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spring Boot Actuator]]></title>
    <link href="http://panlw.github.io/15431573797418.html"/>
    <updated>2018-11-25T22:49:39+08:00</updated>
    <id>http://panlw.github.io/15431573797418.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://www.baeldung.com/spring-boot-actuators">https://www.baeldung.com/spring-boot-actuators</a></p>
</blockquote>

<h2 id="toc_0"><strong>1. Overview</strong></h2>

<p>In this article, we’re going to introduce Spring Boot Actuator. <strong>We’ll cover the basics first, then discuss in detail what’s available in Spring Boot 1.x vs 2.x.</strong></p>

<p>We’ll learn how to use, configure and extend this monitoring tool in Spring Boot 1.x. Then, we’ll discuss how to do the same using Boot 2.x and WebFlux taking advantage of the reactive programming model.</p>

<p>Spring Boot Actuator is available since April 2014, together with the first Spring Boot release.</p>

<p>With the <a href="/new-spring-boot-2">release of Spring Boot 2</a>, Actuator has been redesigned, and new exciting endpoints were added.</p>

<p>This guide is split into 3 main sections:</p>

<ul>
<li>  <a href="#understanding-actuator">What is an Actuator?</a></li>
<li>  <a href="#boot-1x-actuator">Spring Boot 1.x Actuator</a></li>
<li>  <a href="#boot-2x-actuator">Spring Boot 2.x Actuator</a></li>
</ul>

<p><section class="further-reading-posts"></p>

<h1 id="toc_1">Further reading:</h1>

<h2 id="toc_2"><a href="https://www.baeldung.com/spring-boot-application-configuration">Configure a Spring Boot Web Application</a></h2>

<p>Some of the more useful configs for a Spring Boot application.</p>

<p><a href="https://www.baeldung.com/spring-boot-application-configuration">Read more</a> →</p>

<h2 id="toc_3"><a href="https://www.baeldung.com/spring-boot-custom-starter">Creating a Custom Starter with Spring Boot</a></h2>

<p>A quick and practical guide to creating custom Spring Boot starters.</p>

<p><a href="https://www.baeldung.com/spring-boot-custom-starter">Read more</a> →</p>

<h2 id="toc_4"><a href="https://www.baeldung.com/spring-boot-testing">Testing in Spring Boot</a></h2>

<p>Learn about how the Spring Boot supports testing, to write unit tests efficiently.</p>

<p><a href="https://www.baeldung.com/spring-boot-testing">Read more</a> →</p>

<p></section></p>

<h2 id="toc_5"><strong>2. What is an Actuator?</strong></h2>

<p>In essence, Actuator brings production-ready features to our application.</p>

<p><strong>Monitoring our app, gathering metrics, understanding traffic or the state of our database becomes trivial with this dependency.</strong></p>

<p>The main benefit of this library is that we can get production grade tools without having to actually implement these features ourselves.</p>

<p>Actuator is mainly used to <strong>expose operational information about the running application</strong> – health, metrics, info, dump, env, etc. It uses HTTP endpoints or JMX beans to enable us to interact with it.</p>

<p>Once this dependency is on the classpath several endpoints are available for us out of the box. As with most Spring modules, we can easily configure or extend it in many ways.</p>

<h3 id="toc_6"><strong>2.1. Getting started</strong></h3>

<p>To enable Spring Boot Actuator we’ll just need to add the <u>spring-boot-actuator</u> dependency to our package manager. In Maven:</p>

<pre><code class="language-xml">&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>

<p>Note that this remains valid regardless of the Boot version, as versions are specified in Spring Boot Bill of Materials (BOM).</p>

<h2 id="toc_7"><strong>3. Spring Boot 1.x Actuator</strong></h2>

<p>In 1.x Actuator follows a R/W model, that means we can either read from it or write to it. E.g. we can retrieve metrics or the health of our application. Alternatively, we could gracefully terminate our app or change our logging configuration.</p>

<p>In order to get it working, Actuator requires Spring MVC to expose its endpoints through HTTP. No other technology is supported.</p>

<h3 id="toc_8"><strong>3.1. Endpoints</strong></h3>

<p><strong>In 1.x, Actuator brings its own security model. It takes advantage of Spring Security constructs, but needs to be configured independently from the rest of the application.</strong></p>

<p>Also, most endpoints are sensitive – meaning they’re not fully public, or in other words, most information will be omitted – while a handful is not e.g. <u>/info</u>.</p>

<p>Here are some of the most common endpoints Boot provides out of the box:</p>

<ul>
<li>  <u>/health</u> – Shows application health information (a simple <u>‘status’</u> when accessed over an unauthenticated connection or full message details when authenticated); it’s not sensitive by default</li>
<li>  <u>/info –</u> Displays arbitrary application info; not sensitive by default</li>
<li>  <u>/metrics –</u> Shows ‘metrics’ information for the current application; it’s also sensitive by default</li>
<li>  <u>/trace –</u> Displays trace information (by default the last few HTTP requests)</li>
</ul>

<p>We can find the full list of existing endpoints over <a href="http://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#production-ready-endpoints">on the official docs</a>.</p>

<h3 id="toc_9"><strong>3.2. Configuring Existing Endpoints</strong></h3>

<p>Each endpoint can be customized with properties using the following format: <u>endpoints.[endpoint name].[property to customize]</u></p>

<p>Three properties are available:</p>

<ul>
<li>  <u>id –</u> by which this endpoint will be accessed over HTTP</li>
<li>  <u>enabled</u> – if true then it can be accessed otherwise not</li>
<li>  <u>sensitive</u> – if true then need the authorization to show crucial information over HTTP</li>
</ul>

<p>For example, add the following properties will customize the /_beans_ endpoint_:_ </p>

<pre><code class="language-properties">endpoints.beans.id=springbeans
endpoints.beans.sensitive=false
endpoints.beans.enabled=true
</code></pre>

<h3 id="toc_10"><strong>3.3. <u>/health</u> Endpoint</strong></h3>

<p><strong>The <u>/health</u> endpoint is used to check the health or state of the running application.</strong> It’s usually exercised by monitoring software to alert us if the running instance goes down or gets unhealthy for other reasons. E.g. Connectivity issues with our DB, lack of disk space…</p>

<p>By default only health information is shown to unauthorized access over HTTP:</p>

<pre><code class="language-json">{
    &quot;status&quot; : &quot;UP&quot;
}
</code></pre>

<p>This health information is collected from all the beans implementing the <u>HealthIndicator</u> interface configured in our application context.</p>

<p>Some information returned by <u>HealthIndicator</u> is sensitive in nature – but we can configure <u>endpoints.health.sensitive=false</u> to expose more detailed information like disk space, messaging broker connectivity, custom checks etc.</p>

<p>We could also <strong>implement our own custom health indicator</strong> – which can collect any type of custom health data specific to the application and automatically expose it through the <u>/health</u> endpoint:</p>

<pre><code class="language-java">@Component
public class HealthCheck implements HealthIndicator {
  
    @Override
    public Health health() {
        int errorCode = check(); // perform some specific health check
        if (errorCode != 0) {
            return Health.down()
              .withDetail(&quot;Error Code&quot;, errorCode).build();
        }
        return Health.up().build();
    }
     
    public int check() {
        // Our logic to check health
        return 0;
    }
}
</code></pre>

<p>Here’s how the output would look like:</p>

<pre><code class="language-json">{
    &quot;status&quot; : &quot;DOWN&quot;,
    &quot;myHealthCheck&quot; : {
        &quot;status&quot; : &quot;DOWN&quot;,
        &quot;Error Code&quot; : 1
     },
     &quot;diskSpace&quot; : {
         &quot;status&quot; : &quot;UP&quot;,
         &quot;free&quot; : 209047318528,
         &quot;threshold&quot; : 10485760
     }
}
</code></pre>

<h3 id="toc_11"><strong>3.4. <u>/info</u> Endpoint</strong></h3>

<p>We can also customize the data shown by the <u>/info</u> endpoint – for example:</p>

<pre><code class="language-properties">info.app.name=Spring Sample Application
info.app.description=This is my first spring boot application
info.app.version=1.0.0
</code></pre>

<p>And the sample output:</p>

<pre><code class="language-json">{
    &quot;app&quot; : {
        &quot;version&quot; : &quot;1.0.0&quot;,
        &quot;description&quot; : &quot;This is my first spring boot application&quot;,
        &quot;name&quot; : &quot;Spring Sample Application&quot;
    }
}
</code></pre>

<h3 id="toc_12"><strong>3.5. <u>/metrics</u> Endpoint</strong></h3>

<p><strong>The metrics endpoint publishes information about OS, JVM as well as application level metrics</strong>. Once enabled, we get information such as memory, heap, processors, threads, classes loaded, classes unloaded, thread pools along with some HTTP metrics as well.</p>

<p>Here’s what the output of this endpoint looks like out of the box:</p>

<pre><code class="language-json">{
    &quot;mem&quot; : 193024,
    &quot;mem.free&quot; : 87693,
    &quot;processors&quot; : 4,
    &quot;instance.uptime&quot; : 305027,
    &quot;uptime&quot; : 307077,
    &quot;systemload.average&quot; : 0.11,
    &quot;heap.committed&quot; : 193024,
    &quot;heap.init&quot; : 124928,
    &quot;heap.used&quot; : 105330,
    &quot;heap&quot; : 1764352,
    &quot;threads.peak&quot; : 22,
    &quot;threads.daemon&quot; : 19,
    &quot;threads&quot; : 22,
    &quot;classes&quot; : 5819,
    &quot;classes.loaded&quot; : 5819,
    &quot;classes.unloaded&quot; : 0,
    &quot;gc.ps_scavenge.count&quot; : 7,
    &quot;gc.ps_scavenge.time&quot; : 54,
    &quot;gc.ps_marksweep.count&quot; : 1,
    &quot;gc.ps_marksweep.time&quot; : 44,
    &quot;httpsessions.max&quot; : -1,
    &quot;httpsessions.active&quot; : 0,
    &quot;counter.status.200.root&quot; : 1,
    &quot;gauge.response.root&quot; : 37.0
}
</code></pre>

<p><strong>In order to gather custom metrics, we have support for ‘gauges’, that is, single value snapshots of data, and ‘counters’ i.e. incrementing/decrementing metrics.</strong></p>

<p>Let’s implement our own custom metrics into the <u>/metrics</u> endpoint. For example, we’ll customize the login flow to record a successful and failed login attempt:</p>

<pre><code class="language-java">@Service
public class LoginServiceImpl {
 
    private final CounterService counterService;
     
    public LoginServiceImpl(CounterService counterService) {
        this.counterService = counterService;
    }
     
    public boolean login(String userName, char[] password) {
        boolean success;
        if (userName.equals(&quot;admin&quot;) &amp;&amp; &quot;secret&quot;.toCharArray().equals(password)) {
            counterService.increment(&quot;counter.login.success&quot;);
            success = true;
        }
        else {
            counterService.increment(&quot;counter.login.failure&quot;);
            success = false;
        }
        return success;
    }
}
</code></pre>

<p>Here’s what the output might look like:</p>

<pre><code class="language-json">{
    ...
    &quot;counter.login.success&quot; : 105,
    &quot;counter.login.failure&quot; : 12,
    ...
}
</code></pre>

<p>Note that login attempts and other security related events are available out of the box in Actuator as audit events.</p>

<h3 id="toc_13"><strong>3.6. Creating A New Endpoint</strong></h3>

<p><strong>Besides using the existing endpoints provided by Spring Boot, we could also create an entirely new one.</strong></p>

<p>Firstly, we’d need to have the new endpoint implement the <u>Endpoint<T></u> interface:</p>

<pre><code class="language-java">@Component
public class CustomEndpoint implements Endpoint&lt;List&lt;String&gt;&gt; {
     
    @Override
    public String getId() {
        return &quot;customEndpoint&quot;;
    }
 
    @Override
    public boolean isEnabled() {
        return true;
    }
 
    @Override
    public boolean isSensitive() {
        return true;
    }
 
    @Override
    public List&lt;String&gt; invoke() {
        // Custom logic to build the output
        List&lt;String&gt; messages = new ArrayList&lt;String&gt;();
        messages.add(&quot;This is message 1&quot;);
        messages.add(&quot;This is message 2&quot;);
        return messages;
    }
}
</code></pre>

<p>In order to access this new endpoint, its <u>id</u> is used to map it, i.e. we could exercise it hitting <u>/customEndpoint</u>.</p>

<p>Output:</p>

<pre><code class="language-json">[ &quot;This is message 1&quot;, &quot;This is message 2&quot; ]
</code></pre>

<h3 id="toc_14"><strong>3.7. Further Customization</strong></h3>

<p>For security purposes, we might choose to expose the actuator endpoints over a non-standard port – the <u>management.port</u> property can easily be used to configure that.</p>

<p>Also, as we already mentioned, in 1.x. Actuator configures its own security model, based on Spring Security but independent from the rest of the application.<br/>
Hence, we can change the <u>management.address</u> property to restrict where the endpoints can be accessed from over the network:</p>

<pre><code class="language-properties">#port used to expose actuator
management.port=8081 
 
#CIDR allowed to hit actuator
management.address=127.0.0.1 
 
#Whether security should be enabled or disabled altogether
management.security.enabled=false
</code></pre>

<p>Besides, all the built-in endpoints except <u>/info</u> are sensitive by default. If the application is using Spring Security – we can secure these endpoints by defining the default security properties – username, password, and role – in the application.properties file:</p>

<pre><code class="language-properties">security.user.name=admin
security.user.password=secret
management.security.role=SUPERUSER
</code></pre>

<h2 id="toc_15"><strong>4. Spring Boot 2.x Actuator</strong></h2>

<p>In 2.x Actuator keeps its fundamental intent, but simplifies its model, extends its capabilities and incorporate better defaults.</p>

<p>Firstly, this version becomes technology agnostic. Also, it simplifies its security model by merging it with the application one.</p>

<p>Lastly, among the various changes, it’s important to keep in mind that some of them are breaking. This includes HTTP request/responses as well as Java APIs.</p>

<p>Furthermore, the latest version supports now the CRUD model, as opposed to the old RW (read/write) model.</p>

<h3 id="toc_16"><strong>4.1. Technology Support</strong></h3>

<p>With its second major version, Actuator is now technology-agnostic whereas in 1.x it was tied to MVC, therefore to the Servlet API.</p>

<p>In 2.x Actuator defines its model, pluggable and extensible without relying on MVC for this.</p>

<p><strong>Hence, with this new model, we’re able to take advantage of MVC as well as WebFlux as an underlying web technology.</strong></p>

<p>Moreover, forthcoming technologies could be added by implementing the right adapters.</p>

<p>Lastly, JMX remains supported to expose endpoints without any additional code.</p>

<h3 id="toc_17"><strong>4.2. Important Changes</strong></h3>

<p>Unlike in previous versions, <strong>Actuator comes with most endpoints disabled</strong>.</p>

<p>Thus, the only two available by default are <u>/health</u> and <u>/info</u>.</p>

<p>Would we want to enable all of them, we could set <u>management.endpoints.web.exposure.include=*.</u> Alternatively, we could list endpoints which should be enabled.</p>

<p><strong>Actuator now shares the security config with the regular App security rules. Hence, the security model is dramatically simplified.</strong></p>

<p>Therefore, to tweak Actuator security rules, we could just add an entry for <u>/actuator/**</u>:</p>

<pre><code class="language-java">@Bean
public SecurityWebFilterChain securityWebFilterChain(
  ServerHttpSecurity http) {
    return http.authorizeExchange()
      .pathMatchers(&quot;/actuator/**&quot;).permitAll()
      .anyExchange().authenticated()
      .and().build();
}
</code></pre>

<p>We can find further details on the <a href="https://docs.spring.io/spring-boot/docs/2.0.x/actuator-api/html/">brand new Actuator official docs</a>.</p>

<p>Also, <strong>by default, all Actuator endpoints are now placed under the <u>/actuator</u> path_._</strong></p>

<p>Same as in the previous version, we can tweak this path, using the new property <u>management.endpoints.web.base-path.</u></p>

<h3 id="toc_18"><strong>4.3. Predefined Endpoints</strong></h3>

<p>Let’s have a look at some available endpoints, most of them were available in 1.x already.</p>

<p>Nonetheless, <strong>some endpoints have been added, some removed and some have been restructured:</strong></p>

<ul>
<li>  <u>/auditevents –</u> lists security audit-related events such as user login/logout. Also, we can filter by principal or type among others fields</li>
<li>  <u>/beans – r</u>eturns all available beans in our <u>BeanFactory</u>. Unlike <u>/auditevents</u>, it doesn’t support filtering</li>
<li>  <u>/conditions –</u> formerly known as /_autoconfig_, builds a report of conditions around auto-configuration</li>
<li>  <u>/configprops –</u> allows us to fetch all <u>@ConfigurationProperties</u> beans</li>
<li>  <u>/env –</u> returns the current environment properties. Additionally, we can retrieve single properties</li>
<li>  <u>/flyway –</u> provides details about our Flyway database migrations</li>
<li>  <u>/health –</u> summarises the health status of our application</li>
<li>  <u>/heapdump –</u> builds and returns a heap dump from the JVM used by our application</li>
<li>  <u>/info –</u> returns general information. It might be custom data, build information or details about the latest commit</li>
<li>  <u>/liquibase – b</u>ehaves like <u>/flyway</u> but for Liquibase</li>
<li>  <u>/logfile –</u> returns ordinary application logs</li>
<li>  <u>/loggers –</u> enables us to query and modify the logging level of our application</li>
<li>  <u>/metrics –</u> details metrics of our application. This might include generic metrics as well as custom ones</li>
<li>  <u>/prometheus –</u> returns metrics like the previous one, but formatted to work with a Prometheus server</li>
<li>  <u>/scheduledtasks –</u> provides details about every scheduled task within our application</li>
<li>  <u>/sessions –</u> lists HTTP sessions given we are using Spring Session</li>
<li>  <u>/shutdown –</u> performs a graceful shutdown of the application</li>
<li>  <u>/threaddump –</u> dumps the thread information of the underlying JVM</li>
</ul>

<h3 id="toc_19"><strong>4.4. Health Indicators</strong></h3>

<p>Just like in the previous version, we can add custom indicators easily. Opposite to other APIs, the abstractions for creating custom health endpoints remain unchanged. However, <strong>a new interface <u>ReactiveHealthIndicator</u> has been added to implement reactive health checks</strong>.</p>

<p>Let’s have a look at a simple custom reactive health check:</p>

<pre><code class="language-java">@Component
public class DownstreamServiceHealthIndicator implements ReactiveHealthIndicator {
 
    @Override
    public Mono&lt;Health&gt; health() {
        return checkDownstreamServiceHealth().onErrorResume(
          ex -&gt; Mono.just(new Health.Builder().down(ex).build())
        );
    }
 
    private Mono&lt;Health&gt; checkDownstreamServiceHealth() {
        // we could use WebClient to check health reactively
        return Mono.just(new Health.Builder().up().build());
    }
}
</code></pre>

<p><strong>A handy feature of health indicators is that we can aggregate them as part of a hierarchy.</strong> Hence, following the previous example, we could group all downstream services under a <u>downstream-</u>_services_ category. This category would be healthy as long as every nested <u>service</u> was reachable.</p>

<p>Composite health checks are present in 1.x through <u>CompositeHealthIndicator.</u> Also, in 2.x we could use <u>CompositeReactiveHealthIndicator</u> for its reactive counterpart.</p>

<p>Unlike in Spring Boot 1.x, the <u>endpoints.</u><id>._sensitive_ flag has been removed. To hide the complete health report, we can take advantage of the new <u>management.endpoint.health.show-details.</u> This flag is false by default.</p>

<h3 id="toc_20"><strong>4.5. Metrics in Spring Boot 2</strong></h3>

<p><strong>In Spring Boot 2.0, the in-house metrics were replaced with Micrometer support.</strong> Thus, we can expect breaking changes. If our application was using metric services such as <u>GaugeService or CounterService</u> they will no longer be available.</p>

<p>Instead, we’re expected to interact with <a href="/micrometer">Micrometer</a> directly. In Spring Boot 2.0, we’ll get a bean of type <u>MeterRegistry</u> autoconfigured for us.</p>

<p>Furthermore, Micrometer is now part of Actuator’s dependencies. Hence, we should be good to go as long as the Actuator dependency is in the classpath.</p>

<p>Moreover, we’ll get a completely new response from the <u>/metrics</u> endpoint_:_</p>

<pre><code class="language-json">{
  &quot;names&quot;: [
    &quot;jvm.gc.pause&quot;,
    &quot;jvm.buffer.memory.used&quot;,
    &quot;jvm.memory.used&quot;,
    &quot;jvm.buffer.count&quot;,
    // ...
  ]
}
</code></pre>

<p>As we can observe in the previous example, there are no actual metrics as we got in 1.x.</p>

<p>To get the actual value of a specific metric, we can now navigate to the desired metric, i.e., <u>/actuator/metrics/jvm.gc.pause</u> and get a detailed response:</p>

<pre><code class="language-json">{
  &quot;name&quot;: &quot;jvm.gc.pause&quot;,
  &quot;measurements&quot;: [
    {
      &quot;statistic&quot;: &quot;Count&quot;,
      &quot;value&quot;: 3.0
    },
    {
      &quot;statistic&quot;: &quot;TotalTime&quot;,
      &quot;value&quot;: 7.9E7
    },
    {
      &quot;statistic&quot;: &quot;Max&quot;,
      &quot;value&quot;: 7.9E7
    }
  ],
  &quot;availableTags&quot;: [
    {
      &quot;tag&quot;: &quot;cause&quot;,
      &quot;values&quot;: [
        &quot;Metadata GC Threshold&quot;,
        &quot;Allocation Failure&quot;
      ]
    },
    {
      &quot;tag&quot;: &quot;action&quot;,
      &quot;values&quot;: [
        &quot;end of minor GC&quot;,
        &quot;end of major GC&quot;
      ]
    }
  ]
}
</code></pre>

<p>As we can see, metrics now are much more thorough. Including not only different values but also some associated meta-data.</p>

<h3 id="toc_21"><strong>4.6. Customizing the <u>/info</u> Endpoint</strong></h3>

<p>The <u>/info</u> endpoint remains unchanged. <strong>As before, we can add git details using the Maven or Gradle respective dependency</strong>:</p>

<pre><code class="language-xml">&lt;dependency&gt;
    &lt;groupId&gt;pl.project13.maven&lt;/groupId&gt;
    &lt;artifactId&gt;git-commit-id-plugin&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>

<p>Likewise, <strong>we could also include build information including name, group, and version using the Maven or Gradle plugin:</strong></p>

<pre><code class="language-xml">&lt;plugin&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
    &lt;executions&gt;
        &lt;execution&gt;
            &lt;goals&gt;
                &lt;goal&gt;build-info&lt;/goal&gt;
            &lt;/goals&gt;
        &lt;/execution&gt;
    &lt;/executions&gt;
&lt;/plugin&gt;
</code></pre>

<h3 id="toc_22"><strong>4.7. Creating a Custom Endpoint</strong></h3>

<p>As we pointed out previously, we can create custom endpoints. However, Spring Boot 2 has redesigned the way to achieve this to support the new technology-agnostic paradigm.</p>

<p><strong>Let’s create an Actuator endpoint to query, enable and disable feature flags in our application</strong>:</p>

<pre><code class="language-java">@Component
@Endpoint(id = &quot;features&quot;)
public class FeaturesEndpoint {
 
    private Map&lt;String, Feature&gt; features = new ConcurrentHashMap&lt;&gt;();
 
    @ReadOperation
    public Map&lt;String, Feature&gt; features() {
        return features;
    }
 
    @ReadOperation
    public Feature feature(@Selector String name) {
        return features.get(name);
    }
 
    @WriteOperation
    public void configureFeature(@Selector String name, Feature feature) {
        features.put(name, feature);
    }
 
    @DeleteOperation
    public void deleteFeature(@Selector String name) {
        features.remove(name);
    }
 
    public static class Feature {
        private Boolean enabled;
 
        // [...] getters and setters 
    }
 
}
</code></pre>

<p>To get the endpoint, we need a bean. In our example, we’re using <u>@Component</u> for this. Also, we need to decorate this bean with <u>@Endpoint</u>.</p>

<p>The path of our endpoint is determined by the <u>id</u> parameter of <u>@Endpoint</u>, in our case, it’ll route requests to <u>/actuator/features.</u></p>

<p>Once ready, we can start defining operations using:</p>

<ul>
<li>  <u>@ReadOperation –</u> it’ll map to HTTP <u>GET</u></li>
<li>  <u>@WriteOperation</u> – it’ll map to HTTP <u>POST</u></li>
<li>  <u>@DeleteOperation</u> – it’ll map to HTTP <u>DELETE</u></li>
</ul>

<p>When we run the application with the previous endpoint in our application, Spring Boot will register it.</p>

<p>A quick way to verify this would be checking the logs:</p>

<pre><code class="language-log">[...].WebFluxEndpointHandlerMapping: Mapped &quot;{[/actuator/features/{name}],
  methods=[GET],
  produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}&quot;
[...].WebFluxEndpointHandlerMapping : Mapped &quot;{[/actuator/features],
  methods=[GET],
  produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}&quot;
[...].WebFluxEndpointHandlerMapping : Mapped &quot;{[/actuator/features/{name}],
  methods=[POST],
  consumes=[application/vnd.spring-boot.actuator.v2+json || application/json]}&quot;
[...].WebFluxEndpointHandlerMapping : Mapped &quot;{[/actuator/features/{name}],
  methods=[DELETE]}&quot;[...]
</code></pre>

<p>In the previous logs, we can see how WebFlux is exposing our new endpoint. Would we switch to MVC, It’ll simply delegate on that technology without having to change any code.</p>

<p>Also, we have a few important considerations to keep in mind with this new approach:</p>

<ul>
<li>  There are no dependencies with MVC</li>
<li>  All the metadata present as methods before (_sensitive, enabled…)_ no longer exists. We can, however, enable or disable the endpoint using <u>@Endpoint(id = “features”, enableByDefault = false)</u></li>
<li>  Unlike in 1.x, there is no need to extend a given interface anymore</li>
<li>  In contrast with the old Read/Write model, now we can define <u>DELETE</u> operations using <u>@DeleteOperation</u></li>
</ul>

<h3 id="toc_23"><strong>4.8. Extending Existing Endpoints</strong></h3>

<p>Let’s imagine we want to make sure the production instance of our application is never a <u>SNAPSHOT</u> version. We decided to do this by changing the HTTP status code of the Actuator endpoint that returns this information, i.e., <u>/info.</u> If our app happened to be a <u>SNAPSHOT</u>. We would get a different <u>HTTP</u> status code.</p>

<p><strong>We can easily extend the behavior of a predefined endpoint using the <u>@EndpointExtension</u> annotations</strong>, or its more concrete specializations <u>@EndpointWebExtension</u> or <u>@EndpointJmxExtension:</u></p>

<pre><code class="language-java">@Component
@EndpointWebExtension(endpoint = InfoEndpoint.class)
public class InfoWebEndpointExtension {
 
    private InfoEndpoint delegate;
 
    // standard constructor
 
    @ReadOperation
    public WebEndpointResponse&lt;Map&gt; info() {
        Map&lt;String, Object&gt; info = this.delegate.info();
        Integer status = getStatus(info);
        return new WebEndpointResponse&lt;&gt;(info, status);
    }
 
    private Integer getStatus(Map&lt;String, Object&gt; info) {
        // return 5xx if this is a snapshot
        return 200;
    }
}
</code></pre>

<h3 id="toc_24"><strong>4.9. Enable All Endpoints</strong></h3>

<p><strong>In order to access the actuator endpoints using HTTP, we need to both enable and expose them</strong>. By default, all endpoints but <u>/shutdown</u> are enabled.  Only the _/health_ and <u>/info</u> endpoints are exposed by default.</p>

<p>We need to add the following configuration to expose all endpoints :</p>

<pre><code class="language-properties">management.endpoints.web.exposure.include=*
</code></pre>

<p>To explicitly enable a specific endpoint (for example _/shutdown), _we use:</p>

<pre><code class="language-properties">management.endpoint.shutdown.enabled=true
</code></pre>

<p>To expose all enabled endpoints except one (for example <u>/loggers</u>), we use:</p>

<pre><code class="language-properties">management.endpoints.web.exposure.include=*
management.endpoints.web.exposure.exclude=loggers
</code></pre>

<h2 id="toc_25"><strong>5. Summary</strong></h2>

<p>In this article, we talked about Spring Boot Actuator. We started defining what Actuator means and what it does for us.</p>

<p>Next, we focused on Actuator for the current Spring Boot version, 1.x. discussing how to use it, tweak it an extend it.</p>

<p>Then, we discussed Actuator in Spring Boot 2. We focused on what’s new, and we took advantage of WebFlux to expose our endpoint.</p>

<p>Also, we talked about the important security changes that we can find in this new iteration. We discussed some popular endpoints and how they have changed as well.</p>

<p>Lastly, we demonstrated how to customize and extend Actuator.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DAGOR：微信微服务过载控制系统]]></title>
    <link href="http://panlw.github.io/15431218451169.html"/>
    <updated>2018-11-25T12:57:25+08:00</updated>
    <id>http://panlw.github.io/15431218451169.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://www.infoq.cn/article/bAveV*B7GTH123tLwyDR">https://www.infoq.cn/article/bAveV*B7GTH123tLwyDR</a></p>

<p>论文导读《Overload control for scaling WeChat microservices》</p>
</blockquote>

<p>喜欢这篇论文有两个理由。首先，我们对微信的后端已经有了一些见解，其次，作者分享了已经在微信生产环境中运行了五年的过载控制系统 DAGOR 的设计方案。这个系统的设计专门考虑到了微服务架构的特殊性。如果你希望为自己的微服务制定策略，那么你就很有必要看看这篇文章。</p>

<h2 id="toc_0">微信</h2>

<p>此时的微信后端由 3000 多个移动服务组成，包括即时消息、社交网络、移动支付和第三方授权。平台每天的外部请求达到了 10<sup>10</sup> 到 10<sup>11</sup> 个。每个这样的请求都可以触发更多内部的微服务请求，从整体来看，微信后端需要每秒处理数亿个请求。</p>

<blockquote>
<p>微信微服务系统的 3000 多项服务运行在 20,000 多台机器上，随着微信的广泛普及，这些数字在不断增加… 随着微信的不断发展，微服务系统一直在快速进行服务更新迭代。从 2018 年的 3 月到 5 月，微信微服务系统平均每天发生近千次的变更。</p>
</blockquote>

<p>微信将微服务分为 “入门跳板” 服务（接收外部请求的前端服务）、“共享跳板”服务（中间层协调服务）和“基本服务”（不再向其他服务发出请求的服务，也就是充当请求的接收器）。</p>

<p><img src="https://static001.geekbang.org/resource/image/0a/cb/0a8bf3edf8d6522c66d8eaf8ecdaf5cb.jpeg" alt=""/></p>

<p>在典型的一天，峰值请求率约为日常平均值的 3 倍。在一年中的某些时候（例如中国农历新年期间），峰值工作负载可以上升到日常平均值的 10 倍。</p>

<h2 id="toc_1">基于大规模微服务平台的过载控制挑战</h2>

<blockquote>
<p>过载控制对于大规模在线应用程序来说至关重要，这些应用程序需要在不可预测的负载激增的情况下实现 24×7 服务可用性。</p>
</blockquote>

<p>传统的过载控制机制是为具有少量服务组件、相对狭窄的 “前门” 和普通依赖关系的系统而设计的。</p>

<blockquote>
<p>现代在线服务在架构和依赖性方面正变得越来越复杂，远远超出了传统过载控制的设计目标。</p>
</blockquote>

<ul>
<li>  由于发送到微信后端的服务请求没有单一的入口点，因此传统的全局入口点（网关）集中负载监控方法并不适用。</li>
<li>  特定请求的服务调用图可能依赖于特定于请求的数据和服务参数，即使对于相同类型的请求也是如此。因此，当特定服务出现过载时，很难确定应该限制哪些类型的请求以缓解这种情况。</li>
<li>  过多的请求中止浪费了计算资源，并由于高延迟而影响了用户体验。</li>
<li>  由于服务 DAG 极其复杂，而且在不断演化，导致有效的跨服务协调的维护成本和系统开销过高。</li>
</ul>

<p>由于一个服务可能会向它所依赖的服务发出多个请求，并且还可能向多个后端服务发出请求，因此我们必须特别注意过载控制。作者发明了一个术语，叫作后续过载，用于描述调用多个过载服务或多次调用单个过载服务的情况。</p>

<blockquote>
<p>后续过载给有效的过载控制带来了挑战。当服务过载时随机执行减载可以让系统维持饱和的吞吐量。但后续过载可能会超预期大大降低系统吞吐量…</p>
</blockquote>

<p>试想一个简单的场景，其中服务 A 两次调用服务 B，如果 B 开始拒绝一半传入的请求，那么 A 的成功概率就降到了 0.25。</p>

<h2 id="toc_2">DAGOR 概览</h2>

<p>微信的过载控制系统叫作 DAGOR。它旨在为所有服务提供过载控制，因此具有服务无关性。由于集中式全局协调成本过高，因此过载控制是以单台机器的粒度运行的。不过，它使用了一种用于处理后续过载所需的轻量级机器间协作协议。最后，当发生过载不得不进行减载时，DAGOR 应该尽可能维持服务的成功率，并尽量减少在失败的服务任务上花费过多的计算资源（例如 CPU、I/O）。</p>

<p>我们需要完成两个基本的任务：过载检测，并在检测到过载之后决定如何处理它。</p>

<h3 id="toc_3">过载检测</h3>

<p>对于过载检测，DAGOR 使用了待处理队列中的请求的平均等待时间（即排队时间）。排队时间可以抵消调用图中较低的延迟所带来的影响（与请求处理时间相比）。即使本地服务器本身没有过载，请求处理时间也会增加。DAGOR 使用基于窗口的监控，每个窗口是一秒钟，或者 2000 个请求，以先达到者为准。</p>

<blockquote>
<p>对于过载检测，假设 WeChat 中每个服务任务的默认超时为 500 毫秒，那么用于表示服务器过载的平均请求排队时间的阈值被设置为 20 毫秒。这种配置已经在微信业务系统中应用了五年多，微信业务活动的稳健性已经证实了这种配置的有效性。</p>
</blockquote>

<h3 id="toc_4">准入控制</h3>

<p>一旦检测到过载，我们就必须决定如何处理它，或者直接丢弃某些请求。我们发现，并非所有的请求都是平等的：</p>

<blockquote>
<p>微信的操作日志显示，当微信支付和即时消息遇到服务不可用时，用户针对微信支付服务的投诉是针对即时消息服务的 100 倍。</p>
</blockquote>

<p>为了以服务无关的方式处理这个问题，每个请求在首次进入系统时都会被赋予业务优先级。这个优先级会随所有下游请求一起流动。用户请求的业务优先级由请求的操作类型来决定。虽然有数百个入口点，但只有几十个具有显式优先级，其他入口点具有默认（较低）优先级。优先级保留在复制的哈希表中。</p>

<p><img src="https://static001.geekbang.org/resource/image/bc/4c/bc4b78e56b4a92639ce10fd396d4024c.jpeg" alt=""/></p>

<p>当过载控制被设置为业务优先级 n 时，将删除所有级别为 n + 1 的请求。这对于混合工作负载来说非常有效，但是假设我们有大量的付款请求，所有这些请求都具有相同的优先级（例如 p）。系统将出现过载，这个时候将过载阈值降至 p-1。一旦过载消失，过载阈值再次增加到 p，于是我们又处于过载状态。要在相同优先级的请求发生过载时避免这种翻转，我们需要比业务优先级更细粒度的控制级别。</p>

<p>微信提供了一个很好的解决方案，它增加了基于用户 ID 的第二层准入控制。</p>

<blockquote>
<p>入口服务通过以用户 ID 作为参数的哈希函数来动态生成用户优先级。每个入口服务每小时更改其哈希函数。因此，来自同一用户的请求很可能在一小时内被分配了相同的用户优先级，但在几小时内被分配了不同的用户优先级。</p>
</blockquote>

<p>这提供了一种公平性，同时还在相对长的时间段内为个人用户提供一致的体验。它还有助于解决后续过载问题，因为具有高优先级的用户请求更有可能在整个调用图中得到处理。</p>

<p>通过组合业务优先级和用户优先级，可以为每个业务优先级提供 128 个用户优先级准入级别。</p>

<p><img src="https://static001.geekbang.org/resource/image/81/eb/81479fb13d05e85029328fa5c103c2eb.jpeg" alt=""/></p>

<blockquote>
<p>由于每个业务优先级的准入级别具有 128 个用户优先级，因此复合准入级别数量可以达到数万。复合准入级别的调整是按照用户优先级的颗粒进行的。</p>
</blockquote>

<p>这里有个有趣的问题，为什么使用会话 ID 代替用户 ID 就不起作用：在遇到糟糕的服务时，这样会导致用户登录登出，那么在过载问题之上又会多出来一个用户登录问题！</p>

<p>DAGOR 为每个服务器维护一个请求的直方图，用于跟踪超过准入优先级的请求的大致分布情况。当在窗口期间检测到过载时，DAGOR 移动到第一个桶，将使预期负载减少 5％。如果没有发生过载，它会移动到第一个桶，将使预期负载增加 1％。</p>

<p>服务器将准入优先级嵌入到发送给上游服务器的每个响应消息中。通过这种方式，上游服务器获知下游服务的当前准入控制设置，可以在发送请求之前对请求执行本地准入控制。</p>

<p>DAGOR 过载控制系统的端到端视图如下所示：</p>

<p><img src="https://static001.geekbang.org/resource/image/f1/7d/f1a8fe2308f879035cc7fed07705497d.jpeg" alt=""/></p>

<h2 id="toc_5">实验</h2>

<p>DAGOR 在微信的生产环境已经运作五年了，这是对它的设计可行性的最好证明。但我们并没有为学术论文提供必要的图表，所以我们进行了一组模拟实验。下面的图表突出显示了基于排队时间而非响应时间的过载控制的好处。在发生后续过载的情况下，这些好处最为明显（图 (b)）。</p>

<p><img src="https://static001.geekbang.org/resource/image/57/a8/578fd9b784abfe540ae573ea5c0f3ea8.jpeg" alt=""/></p>

<p>与 CoDel 和 SEDA 相比，在进行一次后续调用时，DAGOR 的请求成功率提高了 50％，随后出现过载。后续请求数量越多，好处就越大：</p>

<p><img src="https://static001.geekbang.org/resource/image/0d/08/0d1dc62116c14ea4bcdc542319ee6308.jpeg" alt=""/></p>

<p>最后，在公平性方面，CoDel 在压力下更倾向于使用较小的扇出服务，而 DAGOR 在各种请求中表现出大致相同的成功率。</p>

<p><img src="https://static001.geekbang.org/resource/image/c1/ef/c10084f90897f17fdf95dbac3318d7ef.jpeg" alt=""/></p>

<h2 id="toc_6">三个经验教训</h2>

<p>即使你不使用 DAGOR，作者还是为你总结了三个宝贵的经验教训：</p>

<ul>
<li>  大规模微服务架构中的过载控制必须在每个服务中实现分散和自治；</li>
<li>  过载控制应该要考虑到各种反馈机制（例如 DAGOR 的协作准入控制），而不是仅仅依赖于开环启发式；</li>
<li>  应该通过分析实际工作负载来了解过载控制设计。</li>
</ul>

<p>论文地址：<a href="https://www.cs.columbia.edu/%7Eruigu/papers/socc18-final100.pdf">https://www.cs.columbia.edu/~ruigu/papers/socc18-final100.pdf</a></p>

<p>关于微信的微服务架构，微信后台基础架构负责人许家滔曾在 2016 年 ArchSummit 北京站做过分享，想要进一步了解可以查看我们整理的<a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650997325&amp;idx=1&amp;sn=04bf17ba4ee4d506483b0ccda4a8d123&amp;chksm=bdbefa1e8ac97308a9ccb7b93ac4d479fc2b5e81518a95390374b7be42b9af9a082985f0cc5c&amp;scene=27#wechat_redirect">文章</a>。</p>

<p>英文原文：<a href="https://blog.acolyer.org/2018/11/16/overload-control-for-scaling-wechat-microservices/">https://blog.acolyer.org/2018/11/16/overload-control-for-scaling-wechat-microservices/</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spring Boot cache with Redis]]></title>
    <link href="http://panlw.github.io/15425204595510.html"/>
    <updated>2018-11-18T13:54:19+08:00</updated>
    <id>http://panlw.github.io/15425204595510.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://medium.com/@MatthewFTech/spring-boot-cache-with-redis-56026f7da83a">https://medium.com/@MatthewFTech/spring-boot-cache-with-redis-56026f7da83a</a></p>
</blockquote>

<p>Each of us met the situation when application was working slowly. Even the best code will fall on knees at high load. Caching can be fast and relatively cheap way to increase performance and reduce response time.<br/>
<img src="media/15425204595510/15425206105845.jpg" alt=""/></p>

<p>In simple words caching is one of performance strategies when you must face with long running services. Invocation result can be placed inside fast <br/>
in-memory storage and used next time without expensive method execution. Following the green flow, you may notice if we found requested data in cache (called <u>“cache hit”)</u> we save time and resources. Red flow represents worst scenario (called <u>“cache miss”</u>) when cache doesn’t store expected data and you need to load and recalculate it from the beginning with an extra trip to cache which increases response time. So how it works underneath?<br/>
<img src="media/15425204595510/15425206180856.jpg" alt=""/></p>

<p>With big simplification when new data arrives, they are placed in the first empty bucket, but when cache is full, cleaning process is performed according to selected eviction algorithm. Some data are safe, they are used very often or meet other conditions for chosen algorithm. Rest of the data are candidates to remove. In ideal world cache will evict only old data until it found place for new ones. With Spring and Redis we will try to build simple app and consider how different factors can impact on our caching layer.</p>

<p></section></p>

<p><section></p>

<hr/>

<h3 id="toc_0">A Piece of Code</h3>

<p>A little bit of background. Lets imagine that you work with social site where users are able to create own content, top most read posts are good candidates to find place in cache. Post structure will look more and less like code below, pretty simple but enough in this case.</p>

<pre><code class="language-java">public class Post implements Serializable {

    private String id;
    private String title;
    private String description;
    private String image;
    private int shares;
    private Author author;
    //getters setters and constructors
}
</code></pre>

<blockquote>
<p><a href="https://gist.github.com/matthewfrank/2030aad230ca40d6c6a6e25e2d83ef7c#file-postchunk-java">https://gist.github.com/matthewfrank/2030aad230ca40d6c6a6e25e2d83ef7c#file-postchunk-java</a></p>
</blockquote>

<h4 id="toc_1">Configuration and dependencies</h4>

<p>Spring need <code>spring-boot-started-data-redis</code> as cache dependency. <br/>
Basic configuration can be set from <a href="https://docs.spring.io/spring-boot/docs/current/reference/html/common-application-properties.html">properties</a> level.</p>

<pre>spring.cache.type=redis
spring.redis.host=192.168.99.100
spring.redis.port=6379</pre>

<h4 id="toc_2">Cache abstraction</h4>

<p>Spring Framework provides an abstraction layer with set of annotations for caching support and can work together with various cache implementation like Redis, EhCache, Hazelcast, Infinispan and <a href="http://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-caching.html">many more</a>. Loose coupling is always very welcome :)</p>

<p><strong>@Cacheable — </strong>Fulfill cache after method execution, next invocation with the same arguments will be omitted and result will be loaded from cache. Annotation provide useful feature called conditional caching. In some case no all data should be cached e.g. you want store in memory only most popular posts.</p>

<pre><code class="language-java">@Cacheable(value = &quot;post-single&quot;, key = &quot;#id&quot;, unless = &quot;#result.shares &lt; 500&quot;)
@GetMapping(&quot;/{id}&quot;)
public Post getPostByID(@PathVariable String id) throws PostNotFoundException {
    log.info(&quot;get post with id {}&quot;, id);
    return postService.getPostByID(id);
}
@Cacheable(value = &quot;post-top&quot;)
@GetMapping(&quot;/top&quot;)
public List&lt;Post&gt; getTopPosts() {
    return postService.getTopPosts();
}
</code></pre>

<blockquote>
<p><a href="https://gist.github.com/matthewfrank/7e67901a2122b23df3ddd5d6f7864305#file-cacheablecontrollermethods-java">https://gist.github.com/matthewfrank/7e67901a2122b23df3ddd5d6f7864305#file-cacheablecontrollermethods-java</a></p>
</blockquote>

<p><strong>@CachePut — </strong>Annotation allows to update entry in cache and support same options like Cacheable annotation. Code below updates post and return it for cache provider to change entry with new value.</p>

<pre><code class="language-java">@CachePut(value = &quot;post-single&quot;, key = &quot;#post.id&quot;)
@PutMapping(&quot;/update&quot;)
public Post updatePostByID(@RequestBody Post post) throws PostNotFoundException {
    log.info(&quot;update post with id {}&quot;, post.getId());
    postService.updatePost(post);
    return post;
}
</code></pre>

<blockquote>
<p><a href="https://gist.github.com/matthewfrank/fdc85c0c04be6697cce39eebddc05eb0#file-cacheputcontrollermethod-java">https://gist.github.com/matthewfrank/fdc85c0c04be6697cce39eebddc05eb0#file-cacheputcontrollermethod-java</a></p>
</blockquote>

<p><strong>@CacheEvict — </strong>Remove entry from cache, can be both conditional and global to all entries in specific cache.</p>

<pre><code class="language-java">@CacheEvict(value = &quot;post-single&quot;, key = &quot;#id&quot;)
@DeleteMapping(&quot;/delete/{id}&quot;)
public void deletePostByID(@PathVariable String id) throws PostNotFoundException {
    log.info(&quot;delete post with id {}&quot;, id);
    postService.deletePost(id);
}

@CacheEvict(value = &quot;post-top&quot;)
@GetMapping(&quot;/top/evict&quot;)
public void evictTopPosts() {
    log.info(&quot;Evict post-top&quot;);
}
</code></pre>

<blockquote>
<p><a href="https://gist.github.com/matthewfrank/84797873672c189823d59eb61c0b37fb#file-cacheevictcontrollermethod-java">https://gist.github.com/matthewfrank/84797873672c189823d59eb61c0b37fb#file-cacheevictcontrollermethod-java</a></p>
</blockquote>

<p><strong>@EnableCaching — </strong>Annotation ensure that post processor will check all beans trying to find demarcated methods and will create proxy to intercept all invocations.</p>

<p><strong>@Caching — </strong>Aggregate multiple annotations of the same type when e.g. you need to use different conditions and caches.</p>

<p><strong>@CacheConfig — </strong>Class level annotation allows to specify global values for annotations like cache name or key generator.</p>

<h3 id="toc_3">Redis cache</h3>

<p>Redis is popular open source in-memory data store used as a database, message broker and cache, for now only last use-case is important for us. <br/>
Download official Redis image from docker hub typing <code>docker pull redis</code> after this command new image should be present in your local repository (type <code>docker images</code> to check it). Of course it can installed on bare metal but for our case docker internal overhead is negligible. Standard Redis image will work well as storage but for cache we should consider few important things like maximum memory, eviction algorithms and durability.</p>

<p><strong>Maximum memory — </strong>By default redis have no memory limits on 64-bit systems and 3 GB on 32-bit systems. Large memory can contain more data and increase hit ratio, one of the most important metrics but at a certain limit of memory the hit rate will be at the same level.</p>

<p><strong>Eviction algorithms — </strong>When cache size reaches the memory limit, old data is removed to make place for new one. Access pattern is a keyword when you thinking about eviction policies, each strategy will be good for specific case:</p>

<ul>
<li>  <strong>Last Recently Used (LRU)</strong> track when key was used last time. So probably it will be still used in future, but what if it was only <u>‘one shot’</u> before long idle time? Key will be stored to next eviction cycle.</li>
<li>  <strong>Least Frequently Used (LFU)[Available from Redis 4.0]</strong> will count how many times key was used. The most popular keys will survive eviction cycle. Problem appears when key was used very often some time ago. Another key starts to being requested but it still have smaller counter and will be candidate to eviction (Redis team solved problem with long lived keys by decreasing counter after some period of time).</li>
</ul>

<p><strong>Durability — </strong>For some reasons you may want to persist your cache. After startup, cache is initially empty, it will be useful to fulfill it with snapshot data in case of recovery after outage. Redis support three types of persistence:</p>

<ul>
<li>  <strong>RDB</strong> point-in-time snapshots after specific interval of time or amount of writes. Rare snapshots should not harm performance but it will be a good task trying to find balance between snapshot interval and to avoid serving obsolete data after outage.</li>
<li>  <strong>AOF</strong> create persistence logs with every write operation. If you consider this level of durability, you should read about different fsync policies under <code>appendfsync</code> configuration parameter.</li>
<li>  Both RDB and AOF.</li>
</ul>

<p>Every additional fork or other operations like <u>fsync()</u> will consume power, <br/>
if warming-up is not important, disable all persistence options.</p>

<p></section></p>

<p><section></p>

<hr/>

<h4 id="toc_4">Redis configuration</h4>

<p>All aspects mentioned before are configurable from <a href="http://download.redis.io/redis-stable/redis.conf">redis.conf</a> level. Its up to you how much memory you will allocate and what type of eviction algorithm you will choose. Eviction policy can be switched on the fly, but Redis will need some time to collect all keys metadata proper for chosen algorithm.</p>

<pre>**#memory limit up to 128MB (up to you)**
maxmemory 128mb
**#remove the last recently used (LRU) keys first**
maxmemory-policy allkeys-lru
**#eviction precision (up to you)**
`maxmemory-samples 10`</pre>

<p></section></p>

<p><section></p>

<hr/>

<h4 id="toc_5">Metrics worth to track</h4>

<p><strong>Hit/miss ratio — </strong>Describes cache efficiency and give us relevant information about rightness of our approach. Low hit ratio is a signal to reflect on nature of stored data. It’s easy to fall into the trap of premature optimization at the early stages of project when you can only guess relying on experience what data should be cached.</p>

<pre>**λ:** redis-cli info stats
**...
keyspace_hits:142   #Successful lookups
keyspace_misses:26  #Failed lookups
...**</pre>

<p>Redis delivers information about amount of lookups, ratio can be calculated using below formula.</p>

<pre>**hit_ratio ** =(keyspace_hits)/(keyspace_hits + keyspace_misses)
**miss_ratio =** (keyspace_misses)/(keyspace_hits + keyspace_misses)</pre>

<p><strong>Latency —</strong> Maximum delay between request and respond. First place where we can find if something bad happen with your cache. Many factors can impact on latency like VM overhead, slow commands or I/O operations.</p>

<pre>**λ:** redis-cli --latency -h 127.0.0.1 -p 6379
min: 0, max: 16, avg: 0.15 (324531 samples)...</pre>

<p><strong>Fragmentation Ratio —</strong> Redis will always use more memory than you declared in <code>maxmemory</code> for himself and e.g. to keeps keys metadata but high ratio can be first signal of memory fragmentation.</p>

<ul>
<li>  ratio &lt;1.0 —Memory allocator need more than you can give him. Old data will be swapped to disk what occurs resources consumption and increase latency. (In case of cache usage, swapping should be disabled)</li>
<li>  ratio &gt; ~1.5 — Operation system slice memory segment into pieces, Redis use more physical memory than he requested for.</li>
</ul>

<pre>**λ:** redis-cli info memory
**...** used_memory_human:41.22M
**...**
used_memory_rss_human:50.01M
...
**mem_fragmentation_ratio:1.21 #used_memory_rss/used_memory
...**</pre>

<p><strong>Evicted keys — </strong>When cache size exceeds <code>maxmemory</code> limit Redis removes data using chosen eviction policy. Constantly fast growing value can be signal of insufficient cache size.</p>

<pre>**λ:** redis-cli info stats
**...
evicted_keys:14 #14 keys removed since operation time
...**</pre>

<p></section></p>

<p><section></p>

<hr/>

<h4 id="toc_6">Run and Test</h4>

<p>Docker knowledge will be advantage. In short words, commands below will download redis image and run container with custom configuration. Container will run as daemon, last two command runs bash and redis-cli.</p>

<pre>**λ:** docker pull redis
**λ:** docker run -p 6379:6379 -d --name redis-cache       
   \-v /usr/configs:/usr/redis_config 
   \redis redis-server /usr/redis_config/redis.conf
**λ:** docker exec -it redis-cache /bin/bash
**λ:** redis-cli</pre>

<p>Example application is available on GitHub <a href="https://github.com/matthewfrank/spring-boot-redis-cache">here</a>.</p>

<pre>**λ:** for /l %N in (1 1 5) do
   \curl -o /dev/null -s -w "%{time_total}\n"  
   \localhost:8080/post/IDX001
...
**1.012 #cache miss**
0.009 **#cache hit**
0.009 **#cache hit**
0.011 **#cache hit**
0.010 **#cache hit**</pre>

<p>It’s good time to check what is exactly stored by Redis. For more human readable key name, please check other <code>RedisSerializer.java</code> implementations.</p>

<pre>**λ:** redis-cli keys * **#Or more efficient scan 0 instead of** **keys ***
1) "post-single:\xac\xed\x00\x05t\x00\x06IDX001"</pre>

<p>Application work like we expected, after first invocation cache is fulfilled for given key, next request use cached data.</p>

<hr/>

<h4 id="toc_7">Traps</h4>

<p><strong>Stale data.</strong> High dynamic data become outdated very quickly. Without update or expiration mechanism cache will serve obsolete content.</p>

<p><strong>Large memory is not always equal bigger hit ratio.</strong> When you reach specific amount of memory, hit ratio will not increase. In this place many things will depend on eviction policies and nature of data.</p>

<p><strong>Premature optimization.</strong> Let test your services without cache, may be your fears will proved groundless, <u>“Premature optimization is the root of all evil”.</u></p>

<p><strong>Hiding bad performance.</strong> Cache is not always an answer for slow services, try to optimize them before, because when you put them after cache layer, you will hide possible architectural mistakes.</p>

<p><strong>Don’t share your redis cache.</strong> Redis works on single thread. Other teams may use your store for other extensive purposes. Every data bumps, heavy commands (KEYS, SORT etc.) may block your cache and increase execution time. In case of performance problems, check <a href="https://redis.io/commands/slowlog">SLOWLOG</a> command.</p>

<p><strong>Config param [maxmemory].</strong> If you consider run your cache with snapshot persistence you should use less than half of available memory as <code>maxmemory</code> to avoid memory errors.</p>

<p><strong>Monitoring.</strong> You should monitor your cache, from time to time <a href="https://redis.io/commands/info">INFO</a> will not generate performance issues but <a href="https://redis.io/commands/monitor">MONITOR</a> can reduce the throughput dramatically.</p>

<h4 id="toc_8">Repository</h4>

<p><a href="https://github.com/matthewfrank/spring-boot-redis-cache" title="https://github.com/matthewfrank/spring-boot-redis-cache"><strong>matthewfrank/spring-boot-redis-cache</strong><br/>
<u>Contribute to spring-boot-redis-cache development by creating an account on GitHub.</u>github.com</a><a href="https://github.com/matthewfrank/spring-boot-redis-cache"></a></p>

<h4 id="toc_9">Further Reading</h4>

<ul>
<li>  <a href="https://docs.spring.io/spring/docs/current/spring-framework-reference/html/cache.html">Cache Abstraction in Spring</a></li>
<li>  <a href="https://spring.io/guides/gs/caching/">Caching Data with Spring</a></li>
<li>  <a href="https://redis.io/topics/lru-cache">Using Redis as an LRU cache</a></li>
<li>  <a href="https://redis.io/topics/latency">Redis latency</a></li>
<li>  <a href="https://redis.io/topics/persistence">Redis persistence</a></li>
<li>  <a href="http://oldblog.antirez.com/post/redis-as-LRU-cache.html">Redis as an LRU cache</a></li>
<li><p><a href="http://oldblog.antirez.com/post/redis-persistence-demystified.html">Redis persistence demystified</a></p></li>
<li><p><a href="https://medium.com/tag/redis?source=post">Redis</a></p></li>
<li><p><a href="https://medium.com/tag/spring-boot?source=post">Spring Boot</a></p></li>
<li><p><a href="https://medium.com/tag/cache?source=post">Cache</a></p></li>
<li><p><a href="https://medium.com/tag/java?source=post">Java</a></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[如何使用Journalctl查看和操作Systemd日志]]></title>
    <link href="http://panlw.github.io/15423821810758.html"/>
    <updated>2018-11-16T23:29:41+08:00</updated>
    <id>http://panlw.github.io/15423821810758.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://www.howtoing.com/how-to-use-journalctl-to-view-and-manipulate-systemd-logs/">https://www.howtoing.com/how-to-use-journalctl-to-view-and-manipulate-systemd-logs/</a></p>
</blockquote>

<h3 id="toc_0">介绍</h3>

<p>一些最引人注目的优势<code>systemd</code>是那些参与到流程和系统日志。 当使用其它工具，日志通常分散在整个系统中，由不同的守护程序和进程的处理，并且可以是相当困难的，当它们跨越多个应用来解释。 <code>Systemd</code>试图通过用于记录所有内核提供一个集中管理解决方案，以解决这些问题，并用户级进程。 收集和管理这些日志的系统称为日志。</p>

<p>该杂志是与实施<code>journald</code>守护程序，它处理所有的内核，initrd 服务等。在本指南中产生的消息，我们将讨论如何使用<code>journalctl</code>实用工具，可用于访问和操纵数据在期刊内举行。</p>

<h2 id="toc_1">大概的概念</h2>

<p>其中背后的驱动力的<code>systemd</code>刊物是集中日志的管理，无论哪里的消息发起的。 由于大部分的启动过程和服务管理是由处理<code>systemd</code>过程，是有意义的标准化的日志收集和访问的方式。 该<code>journald</code>守护在轻松和动态操作的二进制格式的所有可用资源，并将它们存储收集数据。</p>

<p>这给了我们一些显着的优势。 通过使用单个实用程序与数据交互，管理员能够根据其需要动态显示日志数据。 这可以像从三个引导之前查看引导数据一样简单，或者从两个相关服务顺序组合日志条目以调试通信问题。</p>

<p>以二进制格式存储日志数据还意味着数据可以根据您当前需要的任意输出格式显示。 例如，对于每天的日志管理您可以用于在标准查看日志<code>syslog</code>格式，但如果你决定以后图表服务中断，可以输出每一个条目作为 JSON 对象，使其耗到你的绘图服务。 由于数据未以纯文本形式写入磁盘，因此在需要不同的按需格式时不需要进行转换。</p>

<p>该<code>systemd</code>期刊既可以与现有使用<code>syslog</code>的实现，也可以更换<code>syslog</code>功能，根据您的需要。 而<code>systemd</code>杂志将涵盖大多数管理员的记录需求，它也可以补充现有的日志机制。 例如，你可能有一个集中的<code>syslog</code> ，你用来编译来自多个服务器的数据服务器，但你也不妨与交错在单个系统上从多个服务日志<code>systemd</code>杂志。 您可以通过组合这些技术来实现这两者。</p>

<h2 id="toc_2">设置系统时间</h2>

<p>使用二进制日志进行日志记录的一个好处是能够以 UTC 或本地时间随意查看日志记录。 默认情况下， <code>systemd</code>将显示在当地时间结果。</p>

<p>因此，在我们开始使用期刊之前，我们将确保时区设置正确。 该<code>systemd</code>套房实际上带有一个工具，称为<code>timedatectl</code> ，可以帮助这一点。</p>

<p>首先，看看什么时区以<code>list-timezones</code>选项：</p>

<pre><code>timedatectl list-timezones
</code></pre>

<p>这将列出您的系统上可用的时区。 如果您发现该位置匹配您的服务器的一个，你可以通过设置<code>set-timezone</code>选项：</p>

<pre><code>sudo timedatectl set-timezone zone

</code></pre>

<p>为了确保您的机器现在使用正确的时间，用<code>timedatectl</code>单独命令，或使用<code>status</code>选项。 显示将是相同的：</p>

<pre><code>timedatectl status
</code></pre>

<pre><code>      Local time: Thu 2015-02-05 14:08:06 EST
  Universal time: Thu 2015-02-05 19:08:06 UTC
        RTC time: Thu 2015-02-05 19:08:06
       Time zone: America/New_York (EST, -0500)
     NTP enabled: no
NTP synchronized: no
 RTC in local TZ: no
      DST active: n/a
</code></pre>

<p>第一行应显示正确的时间。</p>

<h2 id="toc_3">基本日志查看</h2>

<script>(adsbygoogle = window.adsbygoogle || []).push({});</script>

<p>要看到，日志<code>journald</code>守护程序收集，使用<code>journalctl</code>命令。</p>

<p>单独使用时，每一个日记帐分录是系统将寻呼机（通常内显示<code>less</code> ），供您浏览。 最旧的条目将在顶部：</p>

<pre><code>journalctl
</code></pre>

<pre><code>-- Logs begin at Tue 2015-02-03 21:48:52 UTC, end at Tue 2015-02-03 22:29:38 UTC. --
Feb 03 21:48:52 localhost.localdomain systemd-journal[243]: Runtime journal is using 6.2M (max allowed 49.
Feb 03 21:48:52 localhost.localdomain systemd-journal[243]: Runtime journal is using 6.2M (max allowed 49.
Feb 03 21:48:52 localhost.localdomain systemd-journald[139]: Received SIGTERM from PID 1 (systemd).
Feb 03 21:48:52 localhost.localdomain kernel: audit: type=1404 audit(1423000132.274:2): enforcing=1 old_en
Feb 03 21:48:52 localhost.localdomain kernel: SELinux: 2048 avtab hash slots, 104131 rules.
Feb 03 21:48:52 localhost.localdomain kernel: SELinux: 2048 avtab hash slots, 104131 rules.
Feb 03 21:48:52 localhost.localdomain kernel: input: ImExPS/2 Generic Explorer Mouse as /devices/platform/
Feb 03 21:48:52 localhost.localdomain kernel: SELinux:  8 users, 102 roles, 4976 types, 294 bools, 1 sens,
Feb 03 21:48:52 localhost.localdomain kernel: SELinux:  83 classes, 104131 rules

. . .
</code></pre>

<p>你将有可能页和数据页来滚动，它可以是几万或几十万排长队，如果<code>systemd</code>已经在系统上很长一段时间。 这将显示日志数据库中有多少数据可用。</p>

<p>该格式将是熟悉的那些谁是用来标准<code>syslog</code>日志记录。 然而，这实际上收集来自多个来源的数据，比传统<code>syslog</code>实现有能力的。 它包括来自早期引导过程，内核，initrd 和应用程序标准错误的日志。 这些都在期刊上可用。</p>

<p>您可能会注意到，显示的所有时间戳都是本地时间。 这对于每个日志条目都可用，现在我们的本地时间在我们的系统上正确设置。 使用此新信息显示所有日志。</p>

<p>如果你想显示 UTC 时间戳，您可以使用<code>--utc</code>标志：</p>

<pre><code>journalctl --utc
</code></pre>

<h2 id="toc_4">日志按时间过滤</h2>

<p>虽然访问这样大量的数据集合是绝对有用的，但是这样大量的信息可能难以或不可能在精神上检查和处理。 正因为如此，的最重要的特点之一<code>journalctl</code>是其过滤选项。</p>

<h3 id="toc_5">显示当前引导的日志</h3>

<p>最基本的这些，你可能每天都在使用，是<code>-b</code>标志。 这将显示自最近重新引导以来收集的所有日记帐分录。</p>

<pre><code>journalctl -b
</code></pre>

<p>这将帮助您识别和管理与您当前环境相关的信息。</p>

<p>在您不使用此功能，并正在显示的靴子一天以上的情况下，你会看到<code>journalctl</code>插入了一行，看起来像这样每次系统崩溃：</p>

<pre><code>. . .

-- Reboot --

. . .
</code></pre>

<p>这可以用于帮助您将信息逻辑地分为引导会话。</p>

<h3 id="toc_6">过去靴子</h3>

<p>虽然你通常想显示当前引导的信息，但是有时候过去的引导也会有帮助。 该杂志可以节省许多以前的靴子信息，因此<code>journalctl</code>可以做成很容易地显示信息。</p>

<p>某些分发版本可以在默认情况下保存以前的引导信息，而其他分发版则禁用此功能。 要启用持久启动信息，您可以键入以下命令来创建存储日志的目录：</p>

<pre><code>sudo mkdir -p /var/log/journal
</code></pre>

<p>或者，您可以编辑日志配置文件：</p>

<pre><code>sudo nano /etc/systemd/journald.conf
</code></pre>

<p>根据<code>[Journal]</code>部分，设置了<code>Storage=</code>选项 “老大难” 启用日志记录持续：</p>

<p>/etc/systemd/journald.conf</p>

<pre><code>. . .
[Journal]
Storage=persistent

</code></pre>

<p>当保存先前的靴子是您的服务器上启用， <code>journalctl</code>提供了一些命令来帮助你靴子划分的单位工作。 要看到，靴子<code>journald</code>知道，使用<code>--list-boots</code>带选项<code>journalctl</code> ：</p>

<pre><code>journalctl --list-boots
</code></pre>

<pre><code>-2 caf0524a1d394ce0bdbcff75b94444fe Tue 2015-02-03 21:48:52 UTC—Tue 2015-02-03 22:17:00 UTC
-1 13883d180dc0420db0abcb5fa26d6198 Tue 2015-02-03 22:17:03 UTC—Tue 2015-02-03 22:19:08 UTC
 0 bed718b17a73415fade0e4e7f4bea609 Tue 2015-02-03 22:19:12 UTC—Tue 2015-02-03 23:01:01 UTC
</code></pre>

<p>这将为每次引导显示一行。 第一列的偏移量，可以用来很容易地与参考引导的引导<code>journalctl</code> 。 如果需要绝对引用，则引导标识位于第二列。 您可以通过结束时列出的两个时间规格来判断引导会话所指的时间。</p>

<p>要显示来自这些引导的信息，您可以使用第一列或第二列中的信息。</p>

<p>例如，看杂志，从先前的引导，使用<code>-1</code>相对终场前的<code>-b</code>标志：</p>

<pre><code>journalctl -b -1
</code></pre>

<p>您还可以使用引导 ID 从引导回调数据：</p>

<pre><code>journalctl -b caf0524a1d394ce0bdbcff75b94444fe
</code></pre>

<h3 id="toc_7">时间窗口</h3>

<p>通过引导查看日志条目非常有用，通常您可能希望请求与系统引导不匹配的时间窗口。 在处理长时间运行的服务器并且正常运行时间很长时，这可能尤其如此。</p>

<p>可以通过使用任意的时间限制进行过滤<code>--since</code>和<code>--until</code>选项，分别限制之后，或者给定时间之前所显示的那些条目。</p>

<p>时间值可以有多种格式。 对于绝对时间值，应使用以下格式：</p>

<pre><code>YYYY-MM-DD HH:MM:SS
</code></pre>

<p>例如，我们可以看到所有的条目从 2015 年 1 月 10 日下午 5:15 通过键入：</p>

<pre><code>journalctl --since &quot;2015-01-10 17:15:00&quot;
</code></pre>

<p>如果上述格式的组件保持关闭，将应用一些默认值。 例如，如果省略日期，则将假定当前日期。 如果时间组件丢失，将替换 “00:00:00”（午夜）。 秒字段也可以关闭，默认为 “00”：</p>

<pre><code>journalctl --since &quot;2015-01-10&quot; --until &quot;2015-01-11 03:00&quot;
</code></pre>

<p>日志还理解一些相对值和命名快捷方式。 例如，您可以使用 “昨天”，“今天”，“明天” 或“现在”字样。 你通过在一个编号的值前面加上 “-” 或“+”或者在句子结构中使用像“”“这样的词来做相对的时间。</p>

<p>要从昨天获取数据，您可以键入：</p>

<pre><code>journalctl --since yesterday
</code></pre>

<p>如果您收到服务中断的报告，从上午 9:00 开始并持续到一个小时前，您可以键入：</p>

<pre><code>journalctl --since 09:00 --until &quot;1 hour ago&quot;
</code></pre>

<p>正如你所看到的，定义灵活的时间窗口来过滤你想要看到的条目是相对容易的。</p>

<h2 id="toc_8">按消息兴趣过滤</h2>

<script>(adsbygoogle = window.adsbygoogle || []).push({});</script>

<p>我们在上面学习了一些方法，可以使用时间约束来过滤日志数据。 在本节中，我们将讨论如何根据你是什么服务或组件兴趣进行过滤。该<code>systemd</code>杂志提供了多种这样的方法。</p>

<h3 id="toc_9">按单位</h3>

<p>也许过滤的最有效的方法是你感兴趣的单位。我们可以用<code>-u</code>选项以这种方式来过滤。</p>

<p>例如，要查看我们系统上的 Nginx 单元的所有日志，我们可以键入：</p>

<pre><code>journalctl -u nginx.service
</code></pre>

<p>通常，您可能希望按时间进行过滤，以便显示您感兴趣的行。例如，要检查当前服务是如何运行的，您可以键入：</p>

<pre><code>journalctl -u nginx.service --since today
</code></pre>

<p>当您利用期刊的交叉记录从各种单位的能力时，这种类型的重点变得非常有用。 例如，如果您的 Nginx 进程连接到一个 PHP-FPM 单元来处理动态内容，您可以通过指定两个单位按时间顺序合并条目：</p>

<pre><code>journalctl -u nginx.service -u php-fpm.service --since today
</code></pre>

<p>这可以使得更容易发现不同程序和调试系统之间的交互，而不是单个进程。</p>

<h3 id="toc_10">按进程，用户或组 ID</h3>

<p>一些服务产生了各种子进程来做工作。 如果你已经搜索出你感兴趣的过程的确切 PID，你也可以过滤它。</p>

<p>要做到这一点，我们可以通过指定过滤<code>_PID</code>领域。 例如，如果我们感兴趣的 PID 是 8088，我们可以键入：</p>

<pre><code>journalctl _PID=8088
</code></pre>

<p>在其他时间，您可能希望显示从特定用户或组记录的所有条目。 这可以通过完成<code>_UID</code>或<code>_GID</code>滤波器。 例如，如果你的 Web 服务器下运行<code>www-data</code>的用户，你可以找到通过键入用户 ID：</p>

<pre><code>id -u www-data
</code></pre>

<pre><code>33
</code></pre>

<p>之后，您可以使用返回的 ID 过滤日志结果：</p>

<pre><code>journalctl _UID=33 --since today
</code></pre>

<p>该<code>systemd</code>轴颈具有许多领域，可以用于过滤。 其中的一些从该过程被传递的被记录和一些被施加<code>journald</code>用它从系统的日志的时间收集信息。</p>

<p>领先的下划线表示该<code>_PID</code>字段是后一种类型。 日志自动记录和索引正在记录以供以后过滤的进程的 PID。 您可以通过键入以下内容找到所有可用的日记字段：</p>

<pre><code>man systemd.journal-fields
</code></pre>

<p>我们将在本指南中讨论其中的一些。 现在，我们将讨论与这些字段过滤有关的一个更有用的选项。 该<code>-F</code>选项可用于显示所有可用的值对于给定的杂志领域。</p>

<p>例如，看哪个组 ID 的<code>systemd</code>杂志有，你可以输入的条目：</p>

<pre><code>journalctl -F _GID
</code></pre>

<pre><code>32
99
102
133
81
84
100
0
124
87
</code></pre>

<p>这将显示日志为组 ID 字段存储的所有值。 这可以帮助您构建过滤器。</p>

<h3 id="toc_11">按组件路径</h3>

<p>我们还可以通过提供路径位置进行过滤。</p>

<p>如果该路径指向一个可执行文件， <code>journalctl</code>将显示所有涉及问题的可执行文件中的条目。 例如，为了找到那些涉及条目<code>bash</code>可执行文件，您可以键入：</p>

<pre><code>journalctl /usr/bin/bash
</code></pre>

<p>通常，如果一个单元可用于可执行文件，该方法更干净，并提供更好的信息（来自相关子进程的条目等）。 然而，有时，这是不可能的。</p>

<h3 id="toc_12">显示内核消息</h3>

<p>内核消息，这些通常是在发现<code>dmesg</code>输出，可以从日志以及检索。</p>

<p>要只显示这些消息，我们可以添加<code>-k</code>或<code>--dmesg</code>标志来我们的命令：</p>

<pre><code>journalctl -k
</code></pre>

<p>默认情况下，这将显示当前引导的内核消息。 您可以使用前面讨论的正常引导选择标志来指定备用引导。 例如，要从五个靴子之前获取消息，您可以键入：</p>

<pre><code>journalctl -k -b -5
</code></pre>

<h3 id="toc_13">优先级</h3>

<p>系统管理员经常感兴趣的一个过滤器是消息优先级。 虽然在非常详细的级别上记录信息通常是有用的，但是当实际消化可用信息时，低优先级日志可能分散注意力和混乱。</p>

<p>您可以使用<code>journalctl</code>通过显示一个指定的优先级或以上的只有消息<code>-p</code>选项。 这允许您过滤掉较低优先级的邮件。</p>

<p>例如，要仅显示在错误级别或更高级别记录的条目，您可以键入：</p>

<pre><code>journalctl -p err -b
</code></pre>

<p>这将显示标记为错误，重要，警报或紧急情况的所有邮件。 该杂志实现了标准<code>syslog</code>消息级别。 您可以使用优先级名称或其相应的数值。 按照从高到低的顺序，这些是：</p>

<ul>
<li>  0：出现</li>
<li>  1：警报</li>
<li>  2：暴击</li>
<li>  3：错误</li>
<li>  4：警告</li>
<li>  5：通知</li>
<li>  6：info</li>
<li>  7：调试</li>
</ul>

<p>上面的数字或名称可以互换的使用<code>-p</code>选项。 选择优先级将显示标记在指定级别及其上方的消息。</p>

<h2 id="toc_14">修改日志显示</h2>

<p>上面，我们通过过滤展示了入口选择。 还有其他方法，我们可以修改输出。 我们可以调整<code>journalctl</code>显示，以适应各种需求。</p>

<h3 id="toc_15">截断或展开输出</h3>

<p>我们可以调整<code>journalctl</code>告诉它缩小或扩大输出显示数据。</p>

<p>默认情况下， <code>journalctl</code>将显示在寻呼机整个条目，使条目落后关闭到屏幕的右侧。 可以通过按向右箭头键访问此信息。</p>

<p>如果你宁愿输出截断，插入其中，信息已被删除省略号，你可以使用<code>--no-full</code>选项：</p>

<pre><code>journalctl --no-full
</code></pre>

<pre><code>. . .

Feb 04 20:54:13 journalme sshd[937]: Failed password for root from 83.234.207.60...h2
Feb 04 20:54:13 journalme sshd[937]: Connection closed by 83.234.207.60 [preauth]
Feb 04 20:54:13 journalme sshd[937]: PAM 2 more authentication failures; logname...ot
</code></pre>

<p>你也可以走在相反的方向与此并告诉<code>journalctl</code> ，以显示它的所有信息，无论是否包括不可打印的字符。 我们可以用做<code>-a</code>标志：</p>

<pre><code>journalctl -a
</code></pre>

<h3 id="toc_16">输出到标准输出</h3>

<p>缺省情况下， <code>journalctl</code>显示在寻呼机的输出更容易消耗。 如果您在处理文本处理工具的数据计划，但是，你可能希望能够输出到标准输出。</p>

<p>你可以用做<code>--no-pager</code>选项：</p>

<pre><code>journalclt --no-pager
</code></pre>

<p>这可以立即管道到处理实用程序或重定向到磁盘上的文件，根据您的需要。</p>

<h3 id="toc_17">输出格式</h3>

<p>如果你正在处理日记帐分录，如上面提到的，你很可能将有一个更容易的分析数据，如果它是在一个更耗格式。 幸运的是，期刊可以根据需要以各种格式显示。 您可以使用做到这一点<code>-o</code>与格式说明选项。</p>

<p>例如，您可以通过输入以下内容在 JSON 中输出日记帐分录：</p>

<pre><code>journalctl -b -u nginx -o json
</code></pre>

<pre><code>{ &quot;__CURSOR&quot; : &quot;s=13a21661cf4948289c63075db6c25c00;i=116f1;b=81b58db8fd9046ab9f847ddb82a2fa2d;m=19f0daa;t=50e33c33587ae;x=e307daadb4858635&quot;, &quot;__REALTIME_TIMESTAMP&quot; : &quot;1422990364739502&quot;, &quot;__MONOTONIC_TIMESTAMP&quot; : &quot;27200938&quot;, &quot;_BOOT_ID&quot; : &quot;81b58db8fd9046ab9f847ddb82a2fa2d&quot;, &quot;PRIORITY&quot; : &quot;6&quot;, &quot;_UID&quot; : &quot;0&quot;, &quot;_GID&quot; : &quot;0&quot;, &quot;_CAP_EFFECTIVE&quot; : &quot;3fffffffff&quot;, &quot;_MACHINE_ID&quot; : &quot;752737531a9d1a9c1e3cb52a4ab967ee&quot;, &quot;_HOSTNAME&quot; : &quot;desktop&quot;, &quot;SYSLOG_FACILITY&quot; : &quot;3&quot;, &quot;CODE_FILE&quot; : &quot;src/core/unit.c&quot;, &quot;CODE_LINE&quot; : &quot;1402&quot;, &quot;CODE_FUNCTION&quot; : &quot;unit_status_log_starting_stopping_reloading&quot;, &quot;SYSLOG_IDENTIFIER&quot; : &quot;systemd&quot;, &quot;MESSAGE_ID&quot; : &quot;7d4958e842da4a758f6c1cdc7b36dcc5&quot;, &quot;_TRANSPORT&quot; : &quot;journal&quot;, &quot;_PID&quot; : &quot;1&quot;, &quot;_COMM&quot; : &quot;systemd&quot;, &quot;_EXE&quot; : &quot;/usr/lib/systemd/systemd&quot;, &quot;_CMDLINE&quot; : &quot;/usr/lib/systemd/systemd&quot;, &quot;_SYSTEMD_CGROUP&quot; : &quot;/&quot;, &quot;UNIT&quot; : &quot;nginx.service&quot;, &quot;MESSAGE&quot; : &quot;Starting A high performance web server and a reverse proxy server...&quot;, &quot;_SOURCE_REALTIME_TIMESTAMP&quot; : &quot;1422990364737973&quot; }

. . .
</code></pre>

<p>这对于使用实用程序进行解析很有用。 你可以使用<code>json-pretty</code>格式，其传递给消费者 JSON 之前得到的数据结构，更好地处理：</p>

<pre><code>journalctl -b -u nginx -o json-pretty
</code></pre>

<pre><code>{ &quot;__CURSOR&quot; : &quot;s=13a21661cf4948289c63075db6c25c00;i=116f1;b=81b58db8fd9046ab9f847ddb82a2fa2d;m=19f0daa;t=50e33c33587ae;x=e307daadb4858635&quot;, &quot;__REALTIME_TIMESTAMP&quot; : &quot;1422990364739502&quot;, &quot;__MONOTONIC_TIMESTAMP&quot; : &quot;27200938&quot;, &quot;_BOOT_ID&quot; : &quot;81b58db8fd9046ab9f847ddb82a2fa2d&quot;, &quot;PRIORITY&quot; : &quot;6&quot;, &quot;_UID&quot; : &quot;0&quot;, &quot;_GID&quot; : &quot;0&quot;, &quot;_CAP_EFFECTIVE&quot; : &quot;3fffffffff&quot;, &quot;_MACHINE_ID&quot; : &quot;752737531a9d1a9c1e3cb52a4ab967ee&quot;, &quot;_HOSTNAME&quot; : &quot;desktop&quot;, &quot;SYSLOG_FACILITY&quot; : &quot;3&quot;, &quot;CODE_FILE&quot; : &quot;src/core/unit.c&quot;, &quot;CODE_LINE&quot; : &quot;1402&quot;, &quot;CODE_FUNCTION&quot; : &quot;unit_status_log_starting_stopping_reloading&quot;, &quot;SYSLOG_IDENTIFIER&quot; : &quot;systemd&quot;, &quot;MESSAGE_ID&quot; : &quot;7d4958e842da4a758f6c1cdc7b36dcc5&quot;, &quot;_TRANSPORT&quot; : &quot;journal&quot;, &quot;_PID&quot; : &quot;1&quot;, &quot;_COMM&quot; : &quot;systemd&quot;, &quot;_EXE&quot; : &quot;/usr/lib/systemd/systemd&quot;, &quot;_CMDLINE&quot; : &quot;/usr/lib/systemd/systemd&quot;, &quot;_SYSTEMD_CGROUP&quot; : &quot;/&quot;, &quot;UNIT&quot; : &quot;nginx.service&quot;, &quot;MESSAGE&quot; : &quot;Starting A high performance web server and a reverse proxy server...&quot;, &quot;_SOURCE_REALTIME_TIMESTAMP&quot; : &quot;1422990364737973&quot; }

. . .
</code></pre>

<p>以下格式可用于显示：</p>

<ul>
<li>  <strong>猫</strong> ：只显示消息字段本身。</li>
<li>  <strong>出口</strong> ：适合传输或备份二进制格式。</li>
<li>  <strong>JSON：</strong>标准的 JSON，每行一个条目。</li>
<li>  <strong>JSON - 漂亮</strong> ：JSON 格式的更好的人类可读性</li>
<li>  <strong>JSON-SSE：JSON</strong> 格式的输出包装以使兼容添加服务器发送的事件</li>
<li>  <strong>总之</strong> ：默认的<code>syslog</code>风格输出</li>
<li>  <strong>短期 ISO：</strong>增强显示 ISO 8601 挂钟时间标记的默认格式。</li>
<li>  <strong>短单调</strong> ：用单调的时间戳的默认格式。</li>
<li>  <strong>短精确</strong> ：与微秒级精度的默认格式</li>
<li>  <strong>详细</strong> ：显示了可用于进入每一个领域的杂志，包括那些通常隐藏在内部。</li>
</ul>

<p>这些选项允许您以最适合您当前需要的任何格式显示日记帐分录。</p>

<h2 id="toc_18">活动进程监视</h2>

<script>(adsbygoogle = window.adsbygoogle || []).push({});</script>

<p>该<code>journalctl</code>命令模仿多少管理员使用<code>tail</code>监视活动或近期活动。 此功能内置到<code>journalctl</code> ，允许您访问这些功能，而不必管到另一个工具。</p>

<h3 id="toc_19">显示最近的日志</h3>

<p>要显示的记录集量，则可以使用<code>-n</code>选项，它的工作原理完全一样<code>tail -n</code> 。</p>

<p>默认情况下，它将显示最近的 10 个条目：</p>

<pre><code>journalctl -n
</code></pre>

<p>您可以指定希望看到与后一个数字的条目数<code>-n</code> ：</p>

<pre><code>journalctl -n 20
</code></pre>

<h3 id="toc_20">跟随日志</h3>

<p>要积极跟踪日志，因为他们正在写的，你可以使用<code>-f</code>标志。 同样，这个工作，如果你使用有经验，你可能期望<code>tail -f</code> ：</p>

<pre><code>journalctl -f
</code></pre>

<h2 id="toc_21">日志维护</h2>

<p>你可能想知道成本是存储我们迄今为止看到的所有数据。 此外，你可能有兴趣清理一些旧的日志和释放空间。</p>

<h3 id="toc_22">正在查找当前磁盘使用情况</h3>

<p>你可以找到的，该杂志目前使用占用的磁盘空间量<code>--disk-usage</code>标志：</p>

<pre><code>journalctl --disk-usage
</code></pre>

<pre><code>Journals take up 8.0M on disk.
</code></pre>

<h3 id="toc_23">删除旧日志</h3>

<p>如果你想缩小你的日记，你可以在两种不同的方式（可带<code>systemd</code>版 218 及更高版本）。</p>

<p>如果使用<code>--vacuum-size</code>选项，您可以通过指示尺寸的缩小你的日记。 这将删除旧条目，直到磁盘上占用的总日志空间为所请求的大小：</p>

<pre><code>sudo journalctl --vacuum-size=1G
</code></pre>

<p>你可以收缩杂志的另一种方式是提供一个截止时间与<code>--vacuum-time</code>选项。 超过该时间的任何条目将被删除。 这允许您保留在特定时间之后创建的条目。</p>

<p>例如，要保留上一年的条目，您可以键入：</p>

<pre><code>sudo journalctl --vacuum-time=1years
</code></pre>

<h3 id="toc_24">限制日志扩展</h3>

<p>您可以配置服务器以限制日志可占用的空间量。 这可以通过编辑来完成<code>/etc/systemd/journald.conf</code>文件。</p>

<p>以下项目可用于限制日记帐增长：</p>

<ul>
<li>  <strong><code>SystemMaxUse=</code></strong> ：指定可以通过在持久存储轴颈使用的最大的磁盘空间。</li>
<li>  <strong><code>SystemKeepFree=</code></strong>指定的空间添加日记帐分录到持久性存储时，杂志应该把可用的容量。</li>
<li>  <strong><code>SystemMaxFileSize=</code></strong> ：控制大个人日志文件如何被旋转前增长到永久存储。</li>
<li>  <strong><code>RuntimeMaxUse=</code></strong> ：指定可以在易失性存储中使用的最大的磁盘空间（内<code>/run</code>文件系统）。</li>
<li>  <strong><code>RuntimeKeepFree=</code></strong> ：指定的空间量，以留出用于其他用途写入数据时，以非易失性存储被设置（在内<code>/run</code>文件系统）。</li>
<li>  <strong><code>RuntimeMaxFileSize=</code></strong> ：指定一个单独的日志文件可以在易失性存储占用的空间量（内<code>/run</code>旋转前的文件系统）。</li>
</ul>

<p>通过设置这些值，可以控制如何<code>journald</code>消耗你的服务器上保留空间。</p>

<h2 id="toc_25">结论</h2>

<p>正如你所看到的， <code>systemd</code>期刊是用于收集和管理您的系统和应用程序的数据非常有用。 大多数灵活性来自于自动记录的大量元数据和日志的集中式特性。 该<code>journalctl</code>命令可以很容易地利用该杂志的高级功能，并做大量的分析和不同应用程序组件的关系调试。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java 测试]]></title>
    <link href="http://panlw.github.io/15419493182627.html"/>
    <updated>2018-11-11T23:15:18+08:00</updated>
    <id>http://panlw.github.io/15419493182627.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">66、线程的基本状态以及状态之间的关系？</h2>

<p><img src="media/15419493182627/15419493198188.jpg" alt=""/></p>

<h2 id="toc_1">80、事务的ACID是指什么？</h2>

<ul>
<li>原子性(Atomic)：事务中各项操作，要么全做要么全不做，任何一项操作的失败都会导致整个事务的失败；</li>
<li>一致性(Consistent)：事务结束后系统状态是一致的；</li>
<li>隔离性(Isolated)：并发执行的事务彼此无法看到对方的中间状态；</li>
<li>持久性(Durable)：事务完成后所做的改动都会被持久化，即使发生灾难性的失败。通过日志和同步备份可以在故障发生后重建数据。</li>
</ul>

<blockquote>
<p>补充：关于事务，在面试中被问到的概率是很高的，可以问的问题也是很多的。首先需要知道的是，只有存在并发数据访问时才需要事务。当多个事务访问同一数据时，可能会存在5类问题，包括3类数据读取问题（脏读、不可重复读和幻读）和2类数据更新问题（第1类丢失更新和第2类丢失更新）。</p>
</blockquote>

<ul>
<li>脏读（Dirty Read）：A事务读取B事务尚未提交的数据并在此基础上操作，而B事务执行回滚，那么A读取到的数据就是脏数据。</li>
<li>幻读（Phantom Read）：事务A重新执行一个查询，返回一系列符合查询条件的行，发现其中插入了被事务B提交的行。</li>
<li>第1类丢失更新：事务A撤销时，把已经提交的事务B的更新数据覆盖了。</li>
<li>第2类丢失更新：事务A覆盖事务B已经提交的数据，造成事务B所做的操作丢失。</li>
</ul>

<table>
<thead>
<tr>
<th>隔离级别</th>
<th>脏读</th>
<th>不可重复读</th>
<th>幻读</th>
<th>第一类丢失更新</th>
<th>第二类丢失更新</th>
</tr>
</thead>

<tbody>
<tr>
<td>READ UNCOMMITED</td>
<td>允许</td>
<td>允许</td>
<td>允许</td>
<td>不允许</td>
<td>允许</td>
</tr>
<tr>
<td>READ COMMITTED</td>
<td>不允许</td>
<td>允许</td>
<td>允许</td>
<td>不允许</td>
<td>允许</td>
</tr>
<tr>
<td>REPEATABLE READ</td>
<td>不允许</td>
<td>不允许</td>
<td>允许</td>
<td>不允许</td>
<td>不允许</td>
</tr>
<tr>
<td>SERIALIZABLE</td>
<td>不允许</td>
<td>不允许</td>
<td>不允许</td>
<td>不允许</td>
<td>不允许</td>
</tr>
</tbody>
</table>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[用 GitLab CI 进行持续集成]]></title>
    <link href="http://panlw.github.io/15412405379571.html"/>
    <updated>2018-11-03T18:22:17+08:00</updated>
    <id>http://panlw.github.io/15412405379571.html</id>
    <content type="html"><![CDATA[
<pre><code>2016.07.29 Scarletsky
</code></pre>

<blockquote>
<p><a href="https://scarletsky.github.io/2016/07/29/use-gitlab-ci-for-continuous-integration/">https://scarletsky.github.io/2016/07/29/use-gitlab-ci-for-continuous-integration/</a></p>
</blockquote>

<h2 id="toc_0"><a href="#%E7%AE%80%E4%BB%8B" title="简介"></a>简介</h2>

<p>从 GitLab 8.0 开始，GitLab CI 就已经集成在 GitLab 中，我们只要在项目中添加一个 <code>.gitlab-ci.yml</code> 文件，然后添加一个 Runner，即可进行持续集成。 而且随着 GitLab 的升级，GitLab CI 变得越来越强大，本文将介绍如何使用 GitLab CI 进行持续集成。</p>

<h2 id="toc_1"><a href="#%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5" title="一些概念"></a>一些概念</h2>

<p>在介绍 GitLab CI 之前，我们先看看一些持续集成相关的概念。</p>

<h3 id="toc_2"><a href="#Pipeline" title="Pipeline"></a>Pipeline</h3>

<p>一次 Pipeline 其实相当于一次构建任务，里面可以包含多个流程，如安装依赖、运行测试、编译、部署测试服务器、部署生产服务器等流程。<br/>
任何提交或者 Merge Request 的合并都可以触发 Pipeline，如下图所示：</p>

<pre><code>+------------------+           +----------------+
|                  |  trigger  |                |
|   Commit / MR    +----------&gt;+    Pipeline    |
|                  |           |                |
+------------------+           +----------------+
</code></pre>

<h3 id="toc_3"><a href="#Stages" title="Stages"></a>Stages</h3>

<p>Stages 表示构建阶段，说白了就是上面提到的流程。<br/>
我们可以在一次 Pipeline 中定义多个 Stages，这些 Stages 会有以下特点：</p>

<ul>
<li>  所有 Stages 会按照顺序运行，即当一个 Stage 完成后，下一个 Stage 才会开始</li>
<li>  只有当所有 Stages 完成后，该构建任务 (Pipeline) 才会成功</li>
<li>  如果任何一个 Stage 失败，那么后面的 Stages 不会执行，该构建任务 (Pipeline) 失败</li>
</ul>

<p>因此，Stages 和 Pipeline 的关系就是：</p>

<pre><code>+--------------------------------------------------------+
|                                                        |
|  Pipeline                                              |
|                                                        |
|  +-----------+     +------------+      +------------+  |
|  |  Stage 1  |----&gt;|   Stage 2  |-----&gt;|   Stage 3  |  |
|  +-----------+     +------------+      +------------+  |
|                                                        |
+--------------------------------------------------------+
</code></pre>

<h3 id="toc_4"><a href="#Jobs" title="Jobs"></a>Jobs</h3>

<p>Jobs 表示构建工作，表示某个 Stage 里面执行的工作。<br/>
我们可以在 Stages 里面定义多个 Jobs，这些 Jobs 会有以下特点：</p>

<ul>
<li>  相同 Stage 中的 Jobs 会并行执行</li>
<li>  相同 Stage 中的 Jobs 都执行成功时，该 Stage 才会成功</li>
<li>  如果任何一个 Job 失败，那么该 Stage 失败，即该构建任务 (Pipeline) 失败</li>
</ul>

<p>所以，Jobs 和 Stage 的关系图就是：</p>

<pre><code>+------------------------------------------+
|                                          |
|  Stage 1                                 |
|                                          |
|  +---------+  +---------+  +---------+   |
|  |  Job 1  |  |  Job 2  |  |  Job 3  |   |
|  +---------+  +---------+  +---------+   |
|                                          |
+------------------------------------------+
</code></pre>

<h2 id="toc_5"><a href="#GitLab-Runner" title="GitLab Runner"></a>GitLab Runner</h2>

<h3 id="toc_6"><a href="#%E7%AE%80%E4%BB%8B-1" title="简介"></a>简介</h3>

<p>理解了上面的基本概念之后，有没有觉得少了些什么东西 —— 由谁来执行这些构建任务呢？<br/>
答案就是 GitLab Runner 了！</p>

<p>想问为什么不是 GitLab CI 来运行那些构建任务？<br/>
一般来说，构建任务都会占用很多的系统资源 (譬如编译代码)，而 GitLab CI 又是 GitLab 的一部分，如果由 GitLab CI 来运行构建任务的话，在执行构建任务的时候，GitLab 的性能会大幅下降。</p>

<p>GitLab CI 最大的作用是管理各个项目的构建状态，因此，运行构建任务这种浪费资源的事情就交给 GitLab Runner 来做拉！<br/>
因为 GitLab Runner 可以安装到不同的机器上，所以在构建任务运行期间并不会影响到 GitLab 的性能~</p>

<h3 id="toc_7"><a href="#%E5%AE%89%E8%A3%85" title="安装"></a>安装</h3>

<p>安装 GitLab Runner 太简单了，按照着 <a href="https://gitlab.com/gitlab-org/gitlab-ci-multi-runner">官方文档</a> 的教程来就好拉！<br/>
下面是 Debian/Ubuntu/CentOS 的安装方法，其他系统去参考官方文档：</p>

<pre><code># For Debian/Ubuntu
$ curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-ci-multi-runner/script.deb.sh | sudo bash
$ sudo apt-get install gitlab-ci-multi-runner
# For CentOS
$ curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-ci-multi-runner/script.rpm.sh | sudo bash
$ sudo yum install gitlab-ci-multi-runner
</code></pre>

<h3 id="toc_8"><a href="#%E6%B3%A8%E5%86%8C-Runner" title="注册 Runner"></a>注册 Runner</h3>

<p>安装好 GitLab Runner 之后，我们只要启动 Runner 然后和 CI 绑定就可以了：</p>

<ul>
<li>  打开你 GitLab 中的项目页面，在项目设置中找到 runners</li>
<li>  运行 <code>sudo gitlab-ci-multi-runner register</code></li>
<li>  输入 CI URL</li>
<li>  输入 Token</li>
<li>  输入 Runner 的名字</li>
<li>  选择 Runner 的类型，简单起见还是选 Shell 吧</li>
<li>  完成</li>
</ul>

<p>当注册好 Runner 之后，可以用 <code>sudo gitlab-ci-multi-runner list</code> 命令来查看各个 Runner 的状态：</p>

<pre><code>$ sudo gitlab-runner list
Listing configured runners          ConfigFile=/etc/gitlab-runner/config.toml
my-runner                           Executor=shell Token=cd1cd7cf243afb47094677855aacd3 URL=http://mygitlab.com/ci
</code></pre>

<h2 id="toc_9"><a href="#gitlab-ci-yml" title=".gitlab-ci.yml"></a>.gitlab-ci.yml</h2>

<h3 id="toc_10"><a href="#%E7%AE%80%E4%BB%8B-2" title="简介"></a>简介</h3>

<p>配置好 Runner 之后，我们要做的事情就是在项目根目录中添加 <code>.gitlab-ci.yml</code> 文件了。<br/>
当我们添加了 <code>.gitlab-ci.yml</code> 文件后，每次提交代码或者合并 MR 都会自动运行构建任务了。</p>

<p>还记得 Pipeline 是怎么触发的吗？Pipeline 也是通过提交代码或者合并 MR 来触发的！<br/>
那么 Pipeline 和 <code>.gitlab-ci.yml</code> 有什么关系呢？<br/>
其实 <code>.gitlab-ci.yml</code> 就是在定义 Pipeline 而已拉！</p>

<h3 id="toc_11"><a href="#%E5%9F%BA%E6%9C%AC%E5%86%99%E6%B3%95" title="基本写法"></a>基本写法</h3>

<p>我们先来看看 <code>.gitlab-ci.yml</code> 是怎么写的：</p>

<pre><code class="language-yml"># 定义 stages
stages:
  - build
  - test
# 定义 job
job1:
  stage: test
  script:
    - echo &quot;I am job1&quot;
    - echo &quot;I am in test stage&quot;
# 定义 job
job2:
  stage: build
  script:
    - echo &quot;I am job2&quot;
    - echo &quot;I am in build stage&quot;
</code></pre>

<p>写起来很简单吧！用 <code>stages</code> 关键字来定义 Pipeline 中的各个构建阶段，然后用一些非关键字来定义 jobs。<br/>
每个 job 中可以可以再用 <code>stage</code> 关键字来指定该 job 对应哪个 stage。<br/>
job 里面的 <code>script</code> 关键字是最关键的地方了，也是每个 job 中必须要包含的，它表示每个 job 要执行的命令。</p>

<p>回想一下我们之前提到的 Stages 和 Jobs 的关系，然后猜猜上面例子的运行结果？</p>

<pre><code>I am job2
I am in build stage
I am job1
I am in test stage
</code></pre>

<p>根据我们在 <code>stages</code> 中的定义，<code>build</code> 阶段要在 <code>test</code> 阶段之前运行，所以 <code>stage:build</code> 的 jobs 会先运行，之后才会运行 <code>stage:test</code> 的 jobs。</p>

<h3 id="toc_12"><a href="#%E5%B8%B8%E7%94%A8%E7%9A%84%E5%85%B3%E9%94%AE%E5%AD%97" title="常用的关键字"></a>常用的关键字</h3>

<p>下面介绍一些常用的关键字，想要更加详尽的内容请前往 <a href="http://docs.gitlab.com/ce/ci/yaml/README.html">官方文档</a></p>

<h4 id="toc_13"><a href="#stages" title="stages"></a>stages</h4>

<p>定义 Stages，默认有三个 Stages，分别是 <code>build</code>, <code>test</code>, <code>deploy</code>。</p>

<h4 id="toc_14"><a href="#types" title="types"></a>types</h4>

<p><code>stages</code> 的别名。</p>

<h4 id="toc_15"><a href="#before-script" title="before_script"></a>before_script</h4>

<p>定义任何 Jobs 运行前都会执行的命令。</p>

<h4 id="toc_16"><a href="#after-script" title="after_script"></a>after_script</h4>

<blockquote>
<p>要求 GitLab 8.7+ 和 GitLab Runner 1.2+</p>
</blockquote>

<p>定义任何 Jobs 运行完后都会执行的命令。</p>

<h4 id="toc_17"><a href="#variables-amp-amp-Job-variables" title="variables &amp;&amp; Job.variables"></a>variables &amp;&amp; Job.variables</h4>

<blockquote>
<p>要求 GitLab Runner 0.5.0+</p>
</blockquote>

<p>定义环境变量。<br/>
如果定义了 Job 级别的环境变量的话，该 Job 会优先使用 Job 级别的环境变量。</p>

<h4 id="toc_18"><a href="#cache-amp-amp-Job-cache" title="cache &amp;&amp; Job.cache"></a>cache &amp;&amp; Job.cache</h4>

<blockquote>
<p>要求 GitLab Runner 0.7.0+</p>
</blockquote>

<p>定义需要缓存的文件。<br/>
每个 Job 开始的时候，Runner 都会删掉 <code>.gitignore</code> 里面的文件。<br/>
如果有些文件 (如 <code>node_modules/</code>) 需要多个 Jobs 共用的话，我们只能让每个 Job 都先执行一遍 <code>npm install</code>。<br/>
这样很不方便，因此我们需要对这些文件进行缓存。缓存了的文件除了可以跨 Jobs 使用外，还可以跨 Pipeline 使用。</p>

<p>具体用法请查看 <a href="http://docs.gitlab.com/ce/ci/yaml/README.html#cache">官方文档</a>。</p>

<h4 id="toc_19"><a href="#Job-script" title="Job.script"></a>Job.script</h4>

<p>定义 Job 要运行的命令，必填项。</p>

<h4 id="toc_20"><a href="#Job-stage" title="Job.stage"></a>Job.stage</h4>

<p>定义 Job 的 stage，默认为 <code>test</code>。</p>

<h4 id="toc_21"><a href="#Job-artifacts" title="Job.artifacts"></a>Job.artifacts</h4>

<p>定义 Job 中生成的附件。<br/>
当该 Job 运行成功后，生成的文件可以作为附件 (如生成的二进制文件) 保留下来，打包发送到 GitLab，之后我们可以在 GitLab 的项目页面下下载该附件。<br/>
注意，不要把 <code>artifacts</code> 和 <code>cache</code> 混淆了。</p>

<h3 id="toc_22"><a href="#%E5%AE%9E%E7%94%A8%E4%BE%8B%E5%AD%90" title="实用例子"></a>实用例子</h3>

<p>下面给出一个我自己在用的例子：</p>

<pre><code class="language-yml">stages:
  - install_deps
  - test
  - build
  - deploy_test
  - deploy_production
cache:
  key: ${CI_BUILD_REF_NAME}
  paths:
    - node_modules/
    - dist/
# 安装依赖
install_deps:
  stage: install_deps
  only:
    - develop
    - master
  script:
    - npm install
# 运行测试用例
test:
  stage: test
  only:
    - develop
    - master
  script:
    - npm run test
# 编译
build:
  stage: build
  only:
    - develop
    - master
  script:
    - npm run clean
    - npm run build:client
    - npm run build:server
# 部署测试服务器
deploy_test:
  stage: deploy_test
  only:
    - develop
  script:
    - pm2 delete app || true
    - pm2 start app.js --name app
# 部署生产服务器
deploy_production:
  stage: deploy_production
  only:
    - master
  script:
    - bash scripts/deploy/deploy.sh
</code></pre>

<p>上面的配置把一次 Pipeline 分成五个阶段：</p>

<ul>
<li>  安装依赖 (<code>install_deps</code>)</li>
<li>  运行测试 (<code>test</code>)</li>
<li>  编译 (<code>build</code>)</li>
<li>  部署测试服务器 (<code>deploy_test</code>)</li>
<li>  部署生产服务器 (<code>deploy_production</code>)</li>
</ul>

<p>设置 <code>Job.only</code> 后，只有当 develop 分支和 master 分支有提交的时候才会触发相关的 Jobs。<br/>
注意，我这里用 GitLab Runner 所在的服务器作为测试服务器。</p>

<h2 id="toc_23"><a href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99" title="参考资料"></a>参考资料</h2>

<p><a href="https://about.gitlab.com/gitlab-ci/">https://about.gitlab.com/gitlab-ci/</a><br/>
<a href="http://docs.gitlab.com/ce/ci/yaml/README.html">http://docs.gitlab.com/ce/ci/yaml/README.html</a><br/>
<a href="http://docs.gitlab.com/ce/ci/variables/README.html">http://docs.gitlab.com/ce/ci/variables/README.html</a><br/>
<a href="https://gitlab.com/gitlab-org/gitlab-ci-multi-runner">https://gitlab.com/gitlab-org/gitlab-ci-multi-runner</a><br/>
<a href="https://gitlab.com/gitlab-org/gitlab-ci-multi-runner/issues/1232">https://gitlab.com/gitlab-org/gitlab-ci-multi-runner/issues/1232</a><br/>
<a href="http://stackbox.cn/2016-02-gitlab-ci-conf/">http://stackbox.cn/2016-02-gitlab-ci-conf/</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Postfix Flush the Mail Queue]]></title>
    <link href="http://panlw.github.io/15407939597215.html"/>
    <updated>2018-10-29T14:19:19+08:00</updated>
    <id>http://panlw.github.io/15407939597215.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://www.cyberciti.biz/tips/howto-postfix-flush-mail-queue.html">https://www.cyberciti.biz/tips/howto-postfix-flush-mail-queue.html</a></p>
</blockquote>

<p>Traditionally you use the “sendmail -q” command to flush mail queue under Sendmail MTA. Under Postfix MTA, just enter the following command to flush the mail queue:<br/>
<code># postfix flush</code><br/>
OR<br/>
<code># postfix -f</code></p>

<p>To see mail queue, enter:<br/>
<code># mailq</code></p>

<p>To remove all mail from the queue, enter:<br/>
<code># postsuper -d ALL</code></p>

<p>To remove all mails in the deferred queue, enter:<br/>
<code># postsuper -d ALL deferred</code></p>

<h2 id="toc_0">postfix-delete.pl script</h2>

<p>Following script deletes all mail from the mailq which matches the regular expression specified as the first argument (Credit: ??? – I found it on old good newsgroup)</p>

<pre><code class="language-sh">#!/usr/bin/perl

$REGEXP = shift || die &quot;no email-adress given (regexp-style, e.g. bl.*\@yahoo.com)!&quot;;

@data = qx&lt;/usr/sbin/postqueue -p&gt;;
for (@data) {
  if (/^(\w+)(\*|\!)?\s/) {
     $queue_id = $1;
  }
  if($queue_id) {
    if (/$REGEXP/i) {
      $Q{$queue_id} = 1;
      $queue_id = &quot;&quot;;
    }
  }
}

#open(POSTSUPER,&quot;|cat&quot;) || die &quot;couldn&#39;t open postsuper&quot; ;
open(POSTSUPER,&quot;|postsuper -d -&quot;) || die &quot;couldn&#39;t open postsuper&quot; ;

foreach (keys %Q) {
  print POSTSUPER &quot;$_\n&quot;;
};
close(POSTSUPER);
</code></pre>

<p>For example, delete all queued messages from or to the domain called fackspamdomain.com, enter:</p>

<pre><code class="language-sh">./postfix-delete.pl fackspamdomain.com
</code></pre>

<p>Delete all queued messages that contain the word “xyz” in the e-mail address:</p>

<pre><code class="language-sh">./postfix-delete.pl xyz
</code></pre>

<p>Updated for accuracy.</p>

<h2 id="toc_1">Posted by: Vivek Gite</h2>

<p>The author is the creator of nixCraft and a seasoned sysadmin, DevOps engineer, and a trainer for the Linux operating system/Unix shell scripting. Get the <strong>latest tutorials on SysAdmin, Linux/Unix and open source topics via <a href="https://www.cyberciti.biz/atom/atom.xml">RSS/XML feed</a></strong> or <a href="https://www.cyberciti.biz/subscribe-to-weekly-linux-unix-newsletter-for-sysadmin/">weekly email newsletter</a>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Principal id to principal object of request context]]></title>
    <link href="http://panlw.github.io/15407357774714.html"/>
    <updated>2018-10-28T22:09:37+08:00</updated>
    <id>http://panlw.github.io/15407357774714.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://github.com/zhangkaitao/shiro-example/tree/master/shiro-example-chapter23-server">https://github.com/zhangkaitao/shiro-example/tree/master/shiro-example-chapter23-server</a></p>

<p><a href="https://github.com/zhangkaitao/shiro-example/blob/master/shiro-example-chapter23-server/src/sql/shiro-schema.sql">https://github.com/zhangkaitao/shiro-example/blob/master/shiro-example-chapter23-server/src/sql/shiro-schema.sql</a></p>

<p><a href="https://github.com/zhangkaitao/shiro-example/blob/master/shiro-example-chapter23-server/src/sql/shiro-data.sql">https://github.com/zhangkaitao/shiro-example/blob/master/shiro-example-chapter23-server/src/sql/shiro-data.sql</a></p>

<p><a href="https://github.com/zhangkaitao/shiro-example/blob/master/shiro-example-chapter23-server/src/main/resources/spring-config-shiro.xml">https://github.com/zhangkaitao/shiro-example/blob/master/shiro-example-chapter23-server/src/main/resources/spring-config-shiro.xml</a></p>
</blockquote>

<pre><code class="language-java">public class SysUserFilter extends PathMatchingFilter {
    @Autowired
    private UserService userService;
    @Override
    protected boolean onPreHandle(ServletRequest request, ServletResponse response, Object mappedValue) throws Exception {
        String username = (String)SecurityUtils.getSubject().getPrincipal();
        request.setAttribute(Constants.CURRENT_USER, userService.findByUsername(username));
        return true;
    }
}
</code></pre>

<pre><code class="language-java">@Target({ElementType.PARAMETER})
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface CurrentUser {
    String value() default Constants.CURRENT_USER;
}
</code></pre>

<pre><code class="language-java">public class CurrentUserMethodArgumentResolver implements HandlerMethodArgumentResolver {
    @Override
    public boolean supportsParameter(MethodParameter parameter) {
        return parameter.hasParameterAnnotation(CurrentUser.class);
    }
    @Override
    public Object resolveArgument(MethodParameter parameter, ModelAndViewContainer mavContainer, NativeWebRequest webRequest, WebDataBinderFactory binderFactory) throws Exception {
        CurrentUser currentUserAnnotation = parameter.getParameterAnnotation(CurrentUser.class);
        return webRequest.getAttribute(currentUserAnnotation.value(), NativeWebRequest.SCOPE_REQUEST);
    }
}
</code></pre>

<pre><code class="language-java">@Controller
public class IndexController {
    @Autowired
    private ResourceService resourceService;
    @Autowired
    private AuthorizationService authorizationService;
    @RequestMapping(&quot;/&quot;)
    public String index(@CurrentUser User loginUser, Model model) {
        Set&lt;String&gt; permissions = authorizationService.findPermissions(Constants.SERVER_APP_KEY, loginUser.getUsername());
        List&lt;Resource&gt; menus = resourceService.findMenus(permissions);
        model.addAttribute(&quot;menus&quot;, menus);
        return &quot;index&quot;;
    }
}
</code></pre>

<p>因此，可以考虑<code>JWT</code>作为<code>Principal</code>的携带者。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spring 整合 Shiro 使用 EL 表达式]]></title>
    <link href="http://panlw.github.io/15407346259853.html"/>
    <updated>2018-10-28T21:50:25+08:00</updated>
    <id>http://panlw.github.io/15407346259853.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="http://elim.iteye.com/blog/2411557">http://elim.iteye.com/blog/2411557</a></p>
</blockquote>

<p>Shiro 是一个轻量级的权限控制框架，应用非常广泛。本文的重点是介绍 Spring 整合 Shiro，并通过扩展使用 Spring 的 EL 表达式，使 @RequiresRoles 等支持动态的参数。对 Shiro 的介绍则不在本文的讨论范围之内，读者如果有对 shiro 不是很了解的，可以通过其<a href="http://shiro.apache.org/index.html">官方网站</a>了解相应的信息。infoq 上也有一篇文章对 shiro 介绍比较全面的，也是官方推荐的，其地址是 <a href="https://www.infoq.com/articles/apache-shiro">https://www.infoq.com/articles/apache-shiro</a>。</p>

<h2 id="toc_0">Shiro 整合 Spring</h2>

<p>首先需要在你的工程中加入 shiro-spring-xxx.jar，如果是使用 Maven 管理你的工程，则可以在你的依赖中加入以下依赖，笔者这里是选择的当前最新的 1.4.0 版本。</p>

<pre><code class="language-xml">&lt;dependency&gt;
    &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt;
    &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt;
    &lt;version&gt;1.4.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>接下来需要在你的 web.xml 中定义一个 shiroFilter，应用它来拦截所有的需要权限控制的请求，通常是配置为<code>/*</code>。另外该 Filter 需要加入最前面，以确保请求进来后最先通过 shiro 的权限控制。这里的 Filter 对应的 class 配置的是 DelegatingFilterProxy，这是 Spring 提供的一个 Filter 的代理，可以使用 Spring bean 容器中的一个 bean 来作为当前的 Filter 实例，对应的 bean 就会取<code>filter-name</code>对应的那个 bean。所以下面的配置会到 bean 容器中寻找一个名为 shiroFilter 的 bean。</p>

<pre><code class="language-xml">&lt;filter&gt;
    &lt;filter-name&gt;shiroFilter&lt;/filter-name&gt;
    &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt;
    &lt;init-param&gt;
        &lt;param-name&gt;targetFilterLifecycle&lt;/param-name&gt;
        &lt;param-value&gt;true&lt;/param-value&gt;
    &lt;/init-param&gt;
&lt;/filter&gt;

&lt;filter-mapping&gt;
    &lt;filter-name&gt;shiroFilter&lt;/filter-name&gt;
    &lt;url-pattern&gt;/*&lt;/url-pattern&gt;
&lt;/filter-mapping&gt;
</code></pre>

<blockquote>
<p>独立使用 Shiro 时通常会定义一个<code>org.apache.shiro.web.servlet.ShiroFilter</code>来做类似的事。</p>
</blockquote>

<p>接下来就是在 bean 容器中定义我们的 shiroFilter 了。如下我们定义了一个 ShiroFilterFactoryBean，其会产生一个 AbstractShiroFilter 类型的 bean。通过 ShiroFilterFactoryBean 我们可以指定一个 SecurityManager，这里使用的 DefaultWebSecurityManager 需要指定一个 Realm，如果需要指定多个 Realm 则通过 realms 指定。这里简单起见就直接使用基于文本定义的 TextConfigurationRealm。通过 loginUrl 指定登录地址、successUrl 指定登录成功后需要跳转的地址，unauthorizedUrl 指定权限不足时的提示页面。filterChainDefinitions 则定义 URL 与需要使用的 Filter 之间的关系，等号右边的是 filter 的别名，默认的别名都定义在<code>org.apache.shiro.web.filter.mgt.DefaultFilter</code>这个枚举类中。</p>

<pre><code class="language-xml">&lt;bean id=&quot;shiroFilter&quot; class=&quot;org.apache.shiro.spring.web.ShiroFilterFactoryBean&quot;&gt;
    &lt;property /&gt;
    &lt;property /&gt;
    &lt;property /&gt;
    &lt;property /&gt;
    &lt;property &gt;
        &lt;value&gt;
            /admin/** = authc, roles[admin]
            /logout = logout
            # 其它地址都要求用户已经登录了
            /** = authc,logger
        &lt;/value&gt;
    &lt;/property&gt;
&lt;/bean&gt;

&lt;bean id=&quot;securityManager&quot; class=&quot;org.apache.shiro.web.mgt.DefaultWebSecurityManager&quot;&gt;
    &lt;property /&gt;
&lt;/bean&gt;
&lt;bean id=&quot;lifecycleBeanPostProcessor&quot; class=&quot;org.apache.shiro.spring.LifecycleBeanPostProcessor&quot;/&gt;

&lt;!-- 简单起见，这里就使用基于文本的Realm实现 --&gt;
&lt;bean id=&quot;realm&quot; class=&quot;org.apache.shiro.realm.text.TextConfigurationRealm&quot;&gt;
    &lt;property &gt;
        &lt;value&gt;
            user1=pass1,role1,role2
            user2=pass2,role2,role3
            admin=admin,admin
        &lt;/value&gt;
    &lt;/property&gt;
&lt;/bean&gt;
</code></pre>

<p>如果需要在 filterChainDefinitions 定义中使用自定义的 Filter，则可以通过 ShiroFilterFactoryBean 的 filters 指定自定义的 Filter 及其别名映射关系。比如下面这样我们新增了一个别名为 logger 的 Filter，并在 filterChainDefinitions 中指定了<code>/**</code>需要应用别名为 logger 的 Filter。</p>

<pre><code class="language-xml">&lt;bean id=&quot;shiroFilter&quot; class=&quot;org.apache.shiro.spring.web.ShiroFilterFactoryBean&quot;&gt;
    &lt;property /&gt;
    &lt;property /&gt;
    &lt;property /&gt;
    &lt;property /&gt;
    &lt;property &gt;
        &lt;util:map&gt;
            &lt;entry key=&quot;logger&quot;&gt;
                &lt;bean class=&quot;com.elim.chat.shiro.filter.LoggerFilter&quot;/&gt;
            &lt;/entry&gt;
        &lt;/util:map&gt;
    &lt;/property&gt;
    &lt;property &gt;
        &lt;value&gt;
            /admin/** = authc, roles[admin]
            /logout = logout
            # 其它地址都要求用户已经登录了
            /** = authc,logger
        &lt;/value&gt;
    &lt;/property&gt;
&lt;/bean&gt;
</code></pre>

<p>其实我们需要应用的 Filter 别名定义也可以不直接通过 ShiroFilterFactoryBean 的 setFilters() 来指定，而是直接在对应的 bean 容器中定义对应的 Filter 对应的 bean。因为默认情况下，ShiroFilterFactoryBean 会把 bean 容器中的所有的 Filter 类型的 bean 以其 id 为别名注册到 filters 中。所以上面的定义等价于下面这样。</p>

<pre><code class="language-xml">&lt;bean id=&quot;shiroFilter&quot; class=&quot;org.apache.shiro.spring.web.ShiroFilterFactoryBean&quot;&gt;
    &lt;property /&gt;
    &lt;property /&gt;
    &lt;property /&gt;
    &lt;property /&gt;
    &lt;property &gt;
        &lt;value&gt;
            /admin/** = authc, roles[admin]
            /logout = logout
            # 其它地址都要求用户已经登录了
            /** = authc,logger
        &lt;/value&gt;
    &lt;/property&gt;
&lt;/bean&gt;

&lt;bean id=&quot;logger&quot; class=&quot;com.elim.chat.shiro.filter.LoggerFilter&quot;/&gt;
</code></pre>

<p>经过以上几步，Shiro 和 Spring 的整合就完成了，这个时候我们请求工程的任意路径都会要求我们登录，且会自动跳转到<code>loginUrl</code>指定的路径让我们输入用户名 / 密码登录。这个时候我们应该提供一个表单，通过 username 获得用户名，通过 password 获得密码，然后提交登录请求的时候请求需要提交到<code>loginUrl</code>指定的地址，但是请求方式需要变为 POST。登录时使用的用户名 / 密码是我们在 TextConfigurationRealm 中定义的用户名 / 密码，基于我们上面的配置则可以使用 user1/pass1、admin/admin 等。登录成功后就会跳转到<code>successUrl</code>参数指定的地址了。如果我们是使用 user1/pass1 登录的，则我们还可以试着访问一下<code>/admin/index</code>，这个时候会因为权限不足跳转到<code>unauthorized.jsp</code>。</p>

<h2 id="toc_1">启用基于注解的支持</h2>

<p>基本的整合需要我们把 URL 需要应用的权限控制都定义在 ShiroFilterFactoryBean 的 filterChainDefinitions 中。这有时候会没那么灵活。Shiro 为我们提供了整合 Spring 后可以使用的注解，它允许我们在需要进行权限控制的 Class 或 Method 上加上对应的注解以定义访问 Class 或 Method 需要的权限，如果是定义中 Class 上的，则表示调用该 Class 中所有的方法都需要对应的权限（注意需要是外部调用，这是动态代理的局限）。要使用这些注解我们需要在 Spring 的 bean 容器中添加下面两个 bean 定义，这样才能在运行时根据注解定义来判断用户是否拥有对应的权限。这是通过 Spring 的 AOP 机制来实现的，关于 Spring Aop 如果有不是特别了解的，可以参考笔者写在 iteye 的<a href="http://www.iteye.com/blogs/subjects/springaop">《Spring Aop 介绍专栏》</a>。下面的两个 bean 定义，<code>AuthorizationAttributeSourceAdvisor</code>是定义了一个 Advisor，其会基于 Shiro 提供的注解配置的方法进行拦截，校验权限。<code>DefaultAdvisorAutoProxyCreator</code>则是提供了为标注有 Shiro 提供的权限控制注解的 Class 创建代理对象，并在拦截到目标方法调用时应用<code>AuthorizationAttributeSourceAdvisor</code>的功能。当拦截到了用户的一个请求，而该用户没有对应方法或类上标注的权限时，将抛出<code>org.apache.shiro.authz.AuthorizationException</code>异常。</p>

<pre><code class="language-xml">&lt;bean class=&quot;org.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator&quot; 
    depends-on=&quot;lifecycleBeanPostProcessor&quot;/&gt;
&lt;bean class=&quot;org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor&quot;&gt;
    &lt;property /&gt;
&lt;/bean&gt;
</code></pre>

<blockquote>
<p>如果我们的 bean 容器中已经定义了<code>&lt;aop:config/&gt;</code>或<code>&lt;aop:aspectj-autoproxy/&gt;</code>，则可以不再定义<code>DefaultAdvisorAutoProxyCreator</code>。因为前面两种情况都会自动添加与<code>DefaultAdvisorAutoProxyCreator</code>类似的 bean。关于<code>DefaultAdvisorAutoProxyCreator</code>的更多介绍也可以参考笔者的 <a href="/blog/2398725">Spring Aop 自动创建代理对象的原理</a>这篇博客。</p>
</blockquote>

<p>Shiro 提供的权限控制注解如下：</p>

<ul>
<li>  RequiresAuthentication：需要用户在当前会话中是被认证过的，即需要通过用户名 / 密码登录过，不包括 RememberMe 自动登录。</li>
<li>  RequiresUser：需要用户是被认证过的，可以是在本次会话中通过用户名 / 密码登录认证，也可以是通过 RememberMe 自动登录。</li>
<li>  RequiresGuest：需要用户是未登录的。</li>
<li>  RequiresRoles：需要用户拥有指定的角色。</li>
<li>  RequiresPermissions：需要用户拥有指定的权限。</li>
</ul>

<p>前面三个都很好理解，而后面两个是类似的。笔者这里拿 @RequiresPermissions 来做个示例。首先我们把上面定义的 Realm 改一下，给 role 添加权限。这样我们的 user1 将拥有 perm1、perm2 和 perm3 的权限，而 user2 将拥有 perm1、perm3 和 perm4 的权限。</p>

<pre><code class="language-xml">&lt;bean id=&quot;realm&quot; class=&quot;org.apache.shiro.realm.text.TextConfigurationRealm&quot;&gt;
    &lt;property &gt;
        &lt;value&gt;
            user1=pass1,role1,role2
            user2=pass2,role2,role3
            admin=admin,admin
        &lt;/value&gt;
    &lt;/property&gt;
    &lt;property &gt;
        &lt;value&gt;
            role1=perm1,perm2
            role2=perm1,perm3
            role3=perm3,perm4
        &lt;/value&gt;
    &lt;/property&gt;
&lt;/bean&gt;
</code></pre>

<p><code>@RequiresPermissions</code>可以添加在方法上，用来指定调用该方法时需要拥有的权限。下面的代码我们就指定了在访问<code>/perm1</code>时必须拥有<code>perm1</code>这个权限。这个时候 user1 和 user2 都能访问。</p>

<pre><code class="language-java">@RequestMapping(&quot;/perm1&quot;)
@RequiresPermissions(&quot;perm1&quot;)
public Object permission1() {
    return &quot;permission1&quot;;
}
</code></pre>

<p>如果需要指定必须同时拥有多个权限才能访问某个方法，可以把需要指定的权限以数组的形式指定（注解上的数组属性指定单个的时候可以不加大括号，需要指定多个时就需要加大括号）。比如下面这样我们就指定了在访问<code>/perm1AndPerm4</code>时用户必须同时拥有<code>perm1</code>和<code>perm4</code>这两个权限。这时候就只有 user2 可以访问，因为只有它才同时拥有<code>perm1</code>和<code>perm4</code>。</p>

<pre><code class="language-java">@RequestMapping(&quot;/perm1AndPerm4&quot;)
@RequiresPermissions({&quot;perm1&quot;, &quot;perm4&quot;})
public Object perm1AndPerm4() {
    return &quot;perm1AndPerm4&quot;;
}
</code></pre>

<p>当同时指定了多个权限时，默认多个权限之间的关系是与的关系，即需要同时拥有指定的所有的权限。如果只需要拥有指定的多个权限中的一个就可以访问，则我们可以通过<code>logical=Logical.OR</code>指定多个权限之间是或的关系。比如下面这样我们就指定了在访问<code>/perm1OrPerm4</code>时只需要拥有<code>perm1</code>或<code>perm4</code>权限即可，这样 user1 和 user2 都可以访问该方法。</p>

<pre><code class="language-java">@RequestMapping(&quot;/perm1OrPerm4&quot;)
@RequiresPermissions(value={&quot;perm1&quot;, &quot;perm4&quot;}, logical=Logical.OR)
public Object perm1OrPerm4() {
    return &quot;perm1OrPerm4&quot;;
}
</code></pre>

<p>@RequiresPermissions 也可以标注在 Class 上，表示在外部访问 Class 中的方法时都需要有对应的权限。比如下面这样我们在 Class 级别指定了需要拥有权限<code>perm2</code>，而在<code>index()</code>方法上则没有指定需要任何权限，但是我们在访问该方法时还是需要拥有 Class 级别指定的权限。此时将只有 user1 可以访问。</p>

<pre><code class="language-java">@RestController
@RequestMapping(&quot;/foo&quot;)
@RequiresPermissions(&quot;perm2&quot;)
public class FooController {

    @RequestMapping(method=RequestMethod.GET)
    public Object index() {
        Map&lt;String, Object&gt; map = new HashMap&lt;&gt;();
        map.put(&quot;abc&quot;, 123);
        return map;
    }

}
</code></pre>

<p>当 Class 和方法级别都同时拥有 @RequiresPermissions 时，方法级别的拥有更高的优先级，而且此时将只会校验方法级别要求的权限。如下我们在 Class 级别指定了需要<code>perm2</code>权限，而在方法级别指定了需要<code>perm3</code>权限，那么在访问<code>/foo</code>时将只需要拥有<code>perm3</code>权限即可访问到<code>index()</code>方法。所以此时 user1 和 user2 都可以访问<code>/foo</code>。</p>

<pre><code class="language-java">@RestController
@RequestMapping(&quot;/foo&quot;)
@RequiresPermissions(&quot;perm2&quot;)
public class FooController {

    @RequestMapping(method=RequestMethod.GET)
    @RequiresPermissions(&quot;perm3&quot;)
    public Object index() {
        Map&lt;String, Object&gt; map = new HashMap&lt;&gt;();
        map.put(&quot;abc&quot;, 123);
        return map;
    }

}
</code></pre>

<p>但是如果此时我们在 Class 上新增<code>@RequiresRoles(&quot;role1&quot;)</code>指定需要拥有角色 role1, 那么此时访问<code>/foo</code>时需要拥有 Class 上的 role1 和<code>index()</code>方法上<code>@RequiresPermissions(&quot;perm3&quot;)</code>指定的<code>perm3</code>权限。因为<code>RequiresRoles</code>和<code>RequiresPermissions</code>属于不同维度的权限定义，Shiro 在校验的时候都将校验一遍，但是如果 Class 和方法上都拥有同类型的权限控制定义的注解时，则只会以方法上的定义为准。</p>

<pre><code class="language-java">@RestController
@RequestMapping(&quot;/foo&quot;)
@RequiresPermissions(&quot;perm2&quot;)
@RequiresRoles(&quot;role1&quot;)
public class FooController {

    @RequestMapping(method=RequestMethod.GET)
    @RequiresPermissions(&quot;perm3&quot;)
    public Object index() {
        Map&lt;String, Object&gt; map = new HashMap&lt;&gt;();
        map.put(&quot;abc&quot;, 123);
        return map;
    }

}
</code></pre>

<blockquote>
<p>虽然示例中使用的只是<code>RequiresPermissions</code>, 但是其它权限控制注解的用法也是类似的，其它注解的用法请感兴趣的朋友自己实践。</p>
</blockquote>

<h2 id="toc_2">基于注解控制权限的原理</h2>

<p>上面使用<code>@RequiresPermissions</code>我们指定的权限都是静态的，写本文的一个主要目的是介绍一种方法，通过扩展实现来使指定的权限可以是动态的。但是在扩展前我们得知道它底层的工作方式，即实现原理，我们才能进行扩展。所以接下来我们先来看一下 Shiro 整合 Spring 后使用<code>@RequiresPermissions</code>的工作原理。在启用对<code>@RequiresPermissions</code>的支持时我们定义了如下 bean，这是一个 Advisor，其继承自 StaticMethodMatcherPointcutAdvisor，它的方法匹配逻辑是只要 Class 或 Method 上拥有 Shiro 的几个权限控制注解即可，而拦截以后的处理逻辑则是由相应的 Advice 指定。</p>

<pre><code class="language-xml">&lt;bean class=&quot;org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor&quot;&gt;
    &lt;property /&gt;
&lt;/bean&gt;
</code></pre>

<p>以下是 AuthorizationAttributeSourceAdvisor 的源码。我们可以看到在其构造方法中通过<code>setAdvice()</code>指定了 AopAllianceAnnotationsAuthorizingMethodInterceptor 这个 Advice 实现类，这是基于 MethodInterceptor 的实现。</p>

<pre><code class="language-java">public class AuthorizationAttributeSourceAdvisor extends StaticMethodMatcherPointcutAdvisor {

    private static final Logger log = LoggerFactory.getLogger(AuthorizationAttributeSourceAdvisor.class);

    private static final Class&lt;? extends Annotation&gt;[] AUTHZ_ANNOTATION_CLASSES =
            new Class[] {
                    RequiresPermissions.class, RequiresRoles.class,
                    RequiresUser.class, RequiresGuest.class, RequiresAuthentication.class
            };

    protected SecurityManager securityManager = null;

    public AuthorizationAttributeSourceAdvisor() {
        setAdvice(new AopAllianceAnnotationsAuthorizingMethodInterceptor());
    }

    public SecurityManager getSecurityManager() {
        return securityManager;
    }

    public void setSecurityManager(org.apache.shiro.mgt.SecurityManager securityManager) {
        this.securityManager = securityManager;
    }

    public boolean matches(Method method, Class targetClass) {
        Method m = method;

        if ( isAuthzAnnotationPresent(m) ) {
            return true;
        }

        //The &#39;method&#39; parameter could be from an interface that doesn&#39;t have the annotation.
        //Check to see if the implementation has it.
        if ( targetClass != null) {
            try {
                m = targetClass.getMethod(m.getName(), m.getParameterTypes());
                return isAuthzAnnotationPresent(m) || isAuthzAnnotationPresent(targetClass);
            } catch (NoSuchMethodException ignored) {
                //default return value is false.  If we can&#39;t find the method, then obviously
                //there is no annotation, so just use the default return value.
            }
        }

        return false;
    }

    private boolean isAuthzAnnotationPresent(Class&lt;?&gt; targetClazz) {
        for( Class&lt;? extends Annotation&gt; annClass : AUTHZ_ANNOTATION_CLASSES ) {
            Annotation a = AnnotationUtils.findAnnotation(targetClazz, annClass);
            if ( a != null ) {
                return true;
            }
        }
        return false;
    }

    private boolean isAuthzAnnotationPresent(Method method) {
        for( Class&lt;? extends Annotation&gt; annClass : AUTHZ_ANNOTATION_CLASSES ) {
            Annotation a = AnnotationUtils.findAnnotation(method, annClass);
            if ( a != null ) {
                return true;
            }
        }
        return false;
    }

}
</code></pre>

<p>AopAllianceAnnotationsAuthorizingMethodInterceptor 的源码如下。其实现的 MethodInterceptor 接口的 invoke 方法又调用了父类的 invoke 方法。同时我们要看到在其构造方法中创建了一些 AuthorizingAnnotationMethodInterceptor 实现，这些实现才是实现权限控制的核心，待会我们会挑出 PermissionAnnotationMethodInterceptor 实现类来看其具体的实现逻辑。</p>

<pre><code class="language-java">public class AopAllianceAnnotationsAuthorizingMethodInterceptor
        extends AnnotationsAuthorizingMethodInterceptor implements MethodInterceptor {

    public AopAllianceAnnotationsAuthorizingMethodInterceptor() {
        List&lt;AuthorizingAnnotationMethodInterceptor&gt; interceptors =
                new ArrayList&lt;AuthorizingAnnotationMethodInterceptor&gt;(5);

        //use a Spring-specific Annotation resolver - Spring&#39;s AnnotationUtils is nicer than the
        //raw JDK resolution process.
        AnnotationResolver resolver = new SpringAnnotationResolver();
        //we can re-use the same resolver instance - it does not retain state:
        interceptors.add(new RoleAnnotationMethodInterceptor(resolver));
        interceptors.add(new PermissionAnnotationMethodInterceptor(resolver));
        interceptors.add(new AuthenticatedAnnotationMethodInterceptor(resolver));
        interceptors.add(new UserAnnotationMethodInterceptor(resolver));
        interceptors.add(new GuestAnnotationMethodInterceptor(resolver));

        setMethodInterceptors(interceptors);
    }

    protected org.apache.shiro.aop.MethodInvocation createMethodInvocation(Object implSpecificMethodInvocation) {
        final MethodInvocation mi = (MethodInvocation) implSpecificMethodInvocation;

        return new org.apache.shiro.aop.MethodInvocation() {
            public Method getMethod() {
                return mi.getMethod();
            }

            public Object[] getArguments() {
                return mi.getArguments();
            }

            public String toString() {
                return &quot;Method invocation [&quot; + mi.getMethod() + &quot;]&quot;;
            }

            public Object proceed() throws Throwable {
                return mi.proceed();
            }

            public Object getThis() {
                return mi.getThis();
            }
        };
    }

    protected Object continueInvocation(Object aopAllianceMethodInvocation) throws Throwable {
        MethodInvocation mi = (MethodInvocation) aopAllianceMethodInvocation;
        return mi.proceed();
    }

    public Object invoke(MethodInvocation methodInvocation) throws Throwable {
        org.apache.shiro.aop.MethodInvocation mi = createMethodInvocation(methodInvocation);
        return super.invoke(mi);
    }
}
</code></pre>

<p>通过看父类的 invoke 方法实现，最终我们会看到核心逻辑是调用 assertAuthorized 方法，而该方法的实现（源码如下）又是依次判断配置的 AuthorizingAnnotationMethodInterceptor 是否支持当前方法进行权限校验（通过判断 Class 或 Method 上是否拥有其支持的注解），当支持时则会调用其 assertAuthorized 方法进行权限校验，而 AuthorizingAnnotationMethodInterceptor 又会调用 AuthorizingAnnotationHandler 的 assertAuthorized 方法。</p>

<pre><code class="language-java">protected void assertAuthorized(MethodInvocation methodInvocation) throws AuthorizationException {
    //default implementation just ensures no deny votes are cast:
    Collection&lt;AuthorizingAnnotationMethodInterceptor&gt; aamis = getMethodInterceptors();
    if (aamis != null &amp;&amp; !aamis.isEmpty()) {
        for (AuthorizingAnnotationMethodInterceptor aami : aamis) {
            if (aami.supports(methodInvocation)) {
                aami.assertAuthorized(methodInvocation);
            }
        }
    }
}
</code></pre>

<p>接下来我们再回过头来看 AopAllianceAnnotationsAuthorizingMethodInterceptor 的定义的 PermissionAnnotationMethodInterceptor，其源码如下。结合 AopAllianceAnnotationsAuthorizingMethodInterceptor 的源码和 PermissionAnnotationMethodInterceptor 的源码，我们可以看到 PermissionAnnotationMethodInterceptor 中这时候指定了 PermissionAnnotationHandler 和 SpringAnnotationResolver。PermissionAnnotationHandler 是 AuthorizingAnnotationHandler 的一个子类。所以我们最终的权限控制由 PermissionAnnotationHandler 的 assertAuthorized 实现决定。</p>

<pre><code class="language-java">public class PermissionAnnotationMethodInterceptor extends AuthorizingAnnotationMethodInterceptor {

    public PermissionAnnotationMethodInterceptor() {
        super( new PermissionAnnotationHandler() );
    }

    public PermissionAnnotationMethodInterceptor(AnnotationResolver resolver) {
        super( new PermissionAnnotationHandler(), resolver);
    }

}
</code></pre>

<p>接下来我们来看 PermissionAnnotationHandler 的 assertAuthorized 方法实现，其完整代码如下。从实现上我们可以看到其会从 Annotation 中获取配置的权限值，而这里的 Annotation 就是 RequiresPermissions 注解。而且在进行权限校验时都是直接使用的我们定义注解时指定的文本值，待会我们进行扩展时就将从这里入手。</p>

<pre><code class="language-java">public class PermissionAnnotationHandler extends AuthorizingAnnotationHandler {

    public PermissionAnnotationHandler() {
        super(RequiresPermissions.class);
    }

    protected String[] getAnnotationValue(Annotation a) {
        RequiresPermissions rpAnnotation = (RequiresPermissions) a;
        return rpAnnotation.value();
    }

    public void assertAuthorized(Annotation a) throws AuthorizationException {
        if (!(a instanceof RequiresPermissions)) return;

        RequiresPermissions rpAnnotation = (RequiresPermissions) a;
        String[] perms = getAnnotationValue(a);
        Subject subject = getSubject();

        if (perms.length == 1) {
            subject.checkPermission(perms[0]);
            return;
        }
        if (Logical.AND.equals(rpAnnotation.logical())) {
            getSubject().checkPermissions(perms);
            return;
        }
        if (Logical.OR.equals(rpAnnotation.logical())) {
            // Avoid processing exceptions unnecessarily - &quot;delay&quot; throwing the exception by calling hasRole first
            boolean hasAtLeastOnePermission = false;
            for (String permission : perms) if (getSubject().isPermitted(permission)) hasAtLeastOnePermission = true;
            // Cause the exception if none of the role match, note that the exception message will be a bit misleading
            if (!hasAtLeastOnePermission) getSubject().checkPermission(perms[0]);

        }
    }
}
</code></pre>

<p>通过前面的介绍我们知道 PermissionAnnotationHandler 的 assertAuthorized 方法参数的 Annotation 是由 AuthorizingAnnotationMethodInterceptor 在调用 AuthorizingAnnotationHandler 的 assertAuthorized 方法时传递的。其源码如下，从源码中我们可以看到 Annotation 是通过 getAnnotation 方法获得的。</p>

<pre><code class="language-java">public void assertAuthorized(MethodInvocation mi) throws AuthorizationException {
    try {
        ((AuthorizingAnnotationHandler)getHandler()).assertAuthorized(getAnnotation(mi));
    }
    catch(AuthorizationException ae) {
        if (ae.getCause() == null) ae.initCause(new AuthorizationException(&quot;Not authorized to invoke method: &quot; + mi.getMethod()));
        throw ae;
    }         
}
</code></pre>

<p>沿着这个方向走下去，最终我们会找到 SpringAnnotationResolver 的 getAnnotation 方法实现，其实现如下。从下面的代码可以看到，其在寻找注解时是优先寻找 Method 上的，如果在 Method 上没有找到会从当前方法调用的所属 Class 上寻找对应的注解。从这里也可以看到为什么我们之前在 Class 和 Method 上都定义了相同类型的权限控制注解时生效的是 Method 上的，而单独存在的时候就是单独定义的那个生效了。</p>

<pre><code class="language-java">public class SpringAnnotationResolver implements AnnotationResolver {

    public Annotation getAnnotation(MethodInvocation mi, Class&lt;? extends Annotation&gt; clazz) {
        Method m = mi.getMethod();

        Annotation a = AnnotationUtils.findAnnotation(m, clazz);
        if (a != null) return a;

        //The MethodInvocation&#39;s method object could be a method defined in an interface.
        //However, if the annotation existed in the interface&#39;s implementation (and not
        //the interface itself), it won&#39;t be on the above method object.  Instead, we need to
        //acquire the method representation from the targetClass and check directly on the
        //implementation itself:
        Class&lt;?&gt; targetClass = mi.getThis().getClass();
        m = ClassUtils.getMostSpecificMethod(m, targetClass);
        a = AnnotationUtils.findAnnotation(m, clazz);
        if (a != null) return a;
        // See if the class has the same annotation
        return AnnotationUtils.findAnnotation(mi.getThis().getClass(), clazz);
    }
}
</code></pre>

<blockquote>
<p>通过以上的源码阅读，相信读者对于 Shiro 整合 Spring 后支持的权限控制注解的原理已经有了比较深入的理解。上面贴出的源码只是部分笔者认为比较核心的，有想详细了解完整内容的请读者自己沿着笔者提到的思路去阅读完整代码。 了解了这块基于注解进行权限控制的原理后，读者朋友们也可以根据实际的业务需要进行相应的扩展。</p>
</blockquote>

<h2 id="toc_3">使用 Spring EL 表达式</h2>

<p>假设现在内部有下面这样一个接口，其中有一个 query 方法，接收一个参数 type。这里我们简化一点，假设只要接收这么一个参数，然后对应不同的取值时将返回不同的结果。</p>

<pre><code class="language-java">public interface RealService {

    Object query(int type);

}
</code></pre>

<p>这个接口是对外开放的，通过对应的 URL 可以请求到该方法，我们定义了对应的 Controller 方法如下：</p>

<pre><code class="language-java">@RequestMapping(&quot;/service/{type}&quot;)
public Object query(@PathVariable(&quot;type&quot;) int type) {
    return this.realService.query(type);
}
</code></pre>

<p>上面的接口服务在进行查询的时候针对 type 是有权限的，不是每个用户都可以使用每种 type 进行查询的，需要拥有对应的权限才行。所以针对上面的处理器方法我们需要加上权限控制，而且在控制时需要的权限是随着参数 type 动态变的。假设关于 type 的每项权限的定义是 query:type 的形式，比如 type=1 时需要的权限是 query:1，type=2 时需要的权限是 query:2。在没有与 Spring 整合时，我们会如下这样做：</p>

<pre><code class="language-java">@RequestMapping(&quot;/service/{type}&quot;)
public Object query(@PathVariable(&quot;type&quot;) int type) {
    SecurityUtils.getSubject().checkPermission(&quot;query:&quot; + type);
    return this.realService.query(type);
}
</code></pre>

<p>但是与 Spring 整合后，上面的做法耦合性强，我们会更希望通过整合后的注解来进行权限控制。对于上面的场景我们更希望通过<code>@RequiresPermissions</code>来指定需要的权限，但是<code>@RequiresPermissions</code>中定义的权限是静态文本，固定的。它没法满足我们动态的需求。这个时候可能你会想着我们可以把 Controller 处理方法拆分为多个，单独进行权限控制。比如下面这样：</p>

<pre><code class="language-java">@RequestMapping(&quot;/service/1&quot;)
@RequiresPermissions(&quot;query:1&quot;)
public Object service1() {
    return this.realService.query(1);
}

@RequiresPermissions(&quot;query:2&quot;)
@RequestMapping(&quot;/service/2&quot;)
public Object service2() {
    return this.realService.query(2);
}

//...

@RequestMapping(&quot;/service/200&quot;)
@RequiresPermissions(&quot;query:200&quot;)
public Object service200() {
    return this.realService.query(200);
}
</code></pre>

<p>这在 type 的取值范围比较小的时候还可以，但是如果像上面这样可能的取值有 200 种，把它们穷举出来定义单独的处理器方法并进行权限控制就显得有点麻烦了。另外就是如果将来 type 的取值有变动，我们还得添加新的处理器方法。所以最好的办法是让<code>@RequiresPermissions</code>支持动态的权限定义，同时又可以维持静态定义的支持。通过前面的分析我们知道，切入点是 PermissionAnnotationHandler，而它里面是没有提供对权限校验的扩展的。我们如果想对它扩展简单的办法就是把它整体的替换。但是我们需要动态处理的权限是跟方法参数相关的，而 PermissionAnnotationHandler 中是取不到方法参数的，为此我们不能直接替换掉 PermissionAnnotationHandler。PermissionAnnotationHandler 是由 PermissionAnnotationMethodInterceptor 调用的，在其父类 AuthorizingAnnotationMethodInterceptor 的 assertAuthorized 方法中调用 PermissionAnnotationHandler 时是可以获取到方法参数的。为此我们的扩展点就选在 PermissionAnnotationMethodInterceptor 类上，我们也需要把它整体的替换。Spring 的 EL 表达式可以支持解析方法参数值，这里我们选择引入 Spring 的 EL 表达式，在<code>@RequiresPermissions</code>定义权限时可以使用 Spring EL 表达式引入方法参数。同时为了兼顾静态的文本。这里引入 Spring 的 EL 表达式模板。关于 Spring 的 EL 表达式模板可以参考笔者的<a href="/blog/2393611">这篇博文</a>。我们定义自己的 PermissionAnnotationMethodInterceptor，把它继承自 PermissionAnnotationMethodInterceptor，重写 assertAuthoried 方法，方法的实现逻辑参考 PermissionAnnotationHandler 中的逻辑，但是所使用的<code>@RequiresPermissions</code>中的权限定义，是我们使用 Spring EL 表达式基于当前调用的方法作为 EvaluationContext 解析后的结果。以下是我们自己定义的 PermissionAnnotationMethodInterceptor 实现。</p>

<pre><code class="language-java">public class SelfPermissionAnnotationMethodInterceptor extends PermissionAnnotationMethodInterceptor {

    private final SpelExpressionParser parser = new SpelExpressionParser();
    private final ParameterNameDiscoverer paramNameDiscoverer = new DefaultParameterNameDiscoverer();
    private final TemplateParserContext templateParserContext = new TemplateParserContext();

    public SelfPermissionAnnotationMethodInterceptor(AnnotationResolver resolver) {
        super(resolver);
    }

    @Override
    public void assertAuthorized(MethodInvocation mi) throws AuthorizationException {
        Annotation annotation = super.getAnnotation(mi);
        RequiresPermissions permAnnotation = (RequiresPermissions) annotation;
        String[] perms = permAnnotation.value();
        EvaluationContext evaluationContext = new MethodBasedEvaluationContext(null, mi.getMethod(), mi.getArguments(), paramNameDiscoverer);
        for (int i=0; i&lt;perms.length; i++) {
            Expression expression = this.parser.parseExpression(perms[i], templateParserContext);
            //使用Spring EL表达式解析后的权限定义替换原来的权限定义
            perms[i] = expression.getValue(evaluationContext, String.class);
        }
        Subject subject = getSubject();

        if (perms.length == 1) {
            subject.checkPermission(perms[0]);
            return;
        }
        if (Logical.AND.equals(permAnnotation.logical())) {
            getSubject().checkPermissions(perms);
            return;
        }
        if (Logical.OR.equals(permAnnotation.logical())) {
            // Avoid processing exceptions unnecessarily - &quot;delay&quot; throwing the exception by calling hasRole first
            boolean hasAtLeastOnePermission = false;
            for (String permission : perms) if (getSubject().isPermitted(permission)) hasAtLeastOnePermission = true;
            // Cause the exception if none of the role match, note that the exception message will be a bit misleading
            if (!hasAtLeastOnePermission) getSubject().checkPermission(perms[0]);

        }
    }

}
</code></pre>

<p>定义了自己的 PermissionAnnotationMethodInterceptor 后，我们需要替换原来的 PermissionAnnotationMethodInterceptor 为我们自己的 PermissionAnnotationMethodInterceptor。根据前面介绍的 Shiro 整合 Spring 后使用<code>@RequiresPermissions</code>等注解的原理我们知道 PermissionAnnotationMethodInterceptor 是由 AopAllianceAnnotationsAuthorizingMethodInterceptor 指定的，而后者又是由 AuthorizationAttributeSourceAdvisor 指定的。为此我们需要在定义 AuthorizationAttributeSourceAdvisor 时通过显示定义 AopAllianceAnnotationsAuthorizingMethodInterceptor 的方式显示的定义其中的 AuthorizingAnnotationMethodInterceptor，然后把自带的 PermissionAnnotationMethodInterceptor 替换为我们自定义的 SelfAuthorizingAnnotationMethodInterceptor。替换后的定义如下：</p>

<pre><code>&lt;bean class=&quot;org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor&quot;&gt;
    &lt;property /&gt;
    &lt;property &gt;
        &lt;bean class=&quot;org.apache.shiro.spring.security.interceptor.AopAllianceAnnotationsAuthorizingMethodInterceptor&quot;&gt;
            &lt;property &gt;
                &lt;util:list&gt;
                    &lt;bean class=&quot;org.apache.shiro.authz.aop.RoleAnnotationMethodInterceptor&quot;
                        c:resolver-ref=&quot;springAnnotationResolver&quot;/&gt;
                    &lt;!-- 使用自定义的PermissionAnnotationMethodInterceptor --&gt;
                    &lt;bean class=&quot;com.elim.chat.shiro.SelfPermissionAnnotationMethodInterceptor&quot;
                        c:resolver-ref=&quot;springAnnotationResolver&quot;/&gt;
                    &lt;bean class=&quot;org.apache.shiro.authz.aop.AuthenticatedAnnotationMethodInterceptor&quot;
                        c:resolver-ref=&quot;springAnnotationResolver&quot;/&gt;
                    &lt;bean class=&quot;org.apache.shiro.authz.aop.UserAnnotationMethodInterceptor&quot;
                        c:resolver-ref=&quot;springAnnotationResolver&quot;/&gt;
                    &lt;bean class=&quot;org.apache.shiro.authz.aop.GuestAnnotationMethodInterceptor&quot;
                        c:resolver-ref=&quot;springAnnotationResolver&quot;/&gt;
                &lt;/util:list&gt;
            &lt;/property&gt;
        &lt;/bean&gt;
    &lt;/property&gt;
&lt;/bean&gt;

&lt;bean id=&quot;springAnnotationResolver&quot; class=&quot;org.apache.shiro.spring.aop.SpringAnnotationResolver&quot;/&gt;
</code></pre>

<p>为了演示前面示例的动态的权限，我们把角色与权限的关系调整如下，让 role1、role2 和 role3 分别拥有 query:1、query:2 和 query:3 的权限。此时 user1 将拥有 query:1 和 query:2 的权限。</p>

<pre><code class="language-xml">&lt;bean id=&quot;realm&quot; class=&quot;org.apache.shiro.realm.text.TextConfigurationRealm&quot;&gt;
    &lt;property &gt;
        &lt;value&gt;
            user1=pass1,role1,role2
            user2=pass2,role2,role3
            admin=admin,admin
        &lt;/value&gt;
    &lt;/property&gt;
    &lt;property &gt;
        &lt;value&gt;
            role1=perm1,perm2,query:1
            role2=perm1,perm3,query:2
            role3=perm3,perm4,query:3
        &lt;/value&gt;
    &lt;/property&gt;
&lt;/bean&gt;
</code></pre>

<p>此时<code>@RequiresPermissions</code>中指定权限时就可以使用 Spring EL 表达式支持的语法了。因为我们在定义 SelfPermissionAnnotationMethodInterceptor 时已经指定了应用基于模板的表达式解析，此时权限中定义的文本都将作为文本解析，动态的部分默认需要使用<code>#{</code>前缀和<code>}</code>后缀包起来（这个前缀和后缀是可以指定的，但是默认就好）。在动态部分中可以使用<code>#</code>前缀引用变量，基于方法的表达式解析中可以使用参数名或<code>p参数索引</code>的形式引用方法参数。所以上面我们需要动态的权限的 query 方法的<code>@RequiresPermissions</code>定义如下。</p>

<pre><code class="language-java">@RequestMapping(&quot;/service/{type}&quot;)
@RequiresPermissions(&quot;query:#{#type}&quot;)
public Object query(@PathVariable(&quot;type&quot;) int type) {
    return this.realService.query(type);
}
</code></pre>

<p>这样 user1 在访问<code>/service/1</code>和<code>/service/2</code>是 OK 的，但是在访问<code>/service/3</code>和<code>/service/300</code>时会提示没有权限，因为 user1 没有<code>query:3</code>和<code>query:300</code>的权限。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[JDBC 连接报错]]></title>
    <link href="http://panlw.github.io/15404508492344.html"/>
    <updated>2018-10-25T15:00:49+08:00</updated>
    <id>http://panlw.github.io/15404508492344.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">报错信息</h2>

<pre><code class="language-log">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
...
</code></pre>

<h2 id="toc_1">解决办法</h2>

<blockquote>
<p><a href="https://segmentfault.com/q/1010000008813924">https://segmentfault.com/q/1010000008813924</a></p>
</blockquote>

<pre><code>; eclipse.ini
-Djava.net.preferIPv4Stack=true
</code></pre>

<blockquote>
<p>具体错误原因嘛，好像是由于jvm中的配置改变，导致jvm无法通过网络获取部分信息</p>
</blockquote>

<p>自述）确实，刚刚升级 JDK 到 Java 11，然后由切换回 Java 8。</p>

<pre><code class="language-sh"># ~/src/bash/bash_profile.d/lang.sh
...
export JAVA_HOME=$(/usr/libexec/java_home -v 1.8)
#export JAVA_HOME=$(/usr/libexec/java_home -v 11)
...
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Microservices Authentication and Authorization Solutions]]></title>
    <link href="http://panlw.github.io/15403881797785.html"/>
    <updated>2018-10-24T21:36:19+08:00</updated>
    <id>http://panlw.github.io/15403881797785.html</id>
    <content type="html"><![CDATA[
<pre><code>Mina Ayoub, 2018 Apr 25
</code></pre>

<blockquote>
<p><a href="https://medium.com/tech-tajawal/microservice-authentication-and-authorization-solutions-e0e5e74b248a">https://medium.com/tech-tajawal/microservice-authentication-and-authorization-solutions-e0e5e74b248a</a></p>
</blockquote>

<p><img src="media/15403881797785/15403882413338.jpg" alt="" style="width:400px;"/></p>

<p>Microservices Architecture brings many benefits to software applications, including small development teams, shorter development cycles, flexibility in language selection, and enhanced service scalability.</p>

<p>At the same time, many complex problems of distributed systems have also been introduced. One of the challenges is how to implement a flexible, secure and efficient authentication and authorization scheme in the Microservices Architecture. This article will try to conduct a more complete discussion on this issue.</p>

<h3 id="toc_0">Monolithic Application Authentication and Authorization</h3>

<p>It has been confusing to differentiate between authentication and authorization. In fact, it is very simple.</p>

<ul>
<li>  <strong>Authentication</strong>: Refers to verify <strong>_who you are_</strong>, so you need to use username and password for authentication.</li>
<li>  <strong>Authorization</strong>: Refers to <strong>_what you can do_</strong>, for example access, edit or delete permissions to some documents, and this happens after verification passes.</li>
</ul>

<p>In the monolithic architecture, the entire application is a process. In the application, a security module is generally used to implement user authentication and authorization.</p>

<p>When the user logs in, the security module of the application authenticates the identity of the user. After verifying that the user is legitimate, a session is created for the user, and a unique session ID is associated with the session. A session stores login user information such as User name, Role, and Permission. The server returns the Session Id to the client. The client records the Session Id as a cookie and sends it to the application in subsequent requests. The application can then use the Session Id to verify the user’s identity, without having to enter a user name and password for authentication each time.</p>

<p><img src="media/15403881797785/15403882606797.jpg" alt=""/><br/>
Monolithic application user login authentication diagram</p>

<p>When the client accesses the application, Session Id is sent to the application along with the HTTP request. The security module generally processes all received client requests through an authorization interceptor. This interceptor first determines whether the Session Id exists. If the Session Id exists, it knows that the user has logged in. Then, by querying the user rights, it is determined whether the user can execute the request or not.</p>

<p><img src="media/15403881797785/15403882804264.jpg" alt=""/><br/>
Monolithic application user request authorization diagram</p>

<h3 id="toc_1">Microservices authentication and authorization problems</h3>

<p>Under the microservice architecture, an application is split into multiple microservice processes, and each microservice implements the business logic of one module in the original single application. After the application is split, the access request for each microservice needs to be authenticated and authorized. If you reference to the implementation of Monolithic application, you will encounter the following problems:</p>

<ul>
<li>  Authentication and authorization logic needs to be handled in each microservice, and this part of the global logic needs to be implemented repeatedly in each microservice. Although we can use the code base to reuse part of the code, this will in turn cause all micro services to have a dependency on a particular code base and its version, affecting the flexibility of the microservice language/framework selection.</li>
<li>  Microservices should follow the principle of single responsibility. A microservice only handles a single business logic. The global logic of authentication and authorization should not be placed in the microservice implementation.</li>
<li>  HTTP is a stateless protocol. For the server, each time the user’s HTTP request is independent. Stateless means that the server can send client requests to any node in the cluster as needed. The stateless design of HTTP has obvious benefits for load balancing. Because there is no state, user requests can be distributed to any server. For services that do not require authentication, such as browsing news pages, there is no problem. However, many services, such as online shopping and enterprise management systems, need to authenticate the user’s identity. Therefore, it is necessary to save the user’s login status in a manner based on the HTTP protocol so as to prevent the user from needing to perform verification for each request. The traditional way is to use a session on the server side to save the user state. Because the server is stateful, it affects the horizontal expansion of the server.</li>
<li>  The authentication and authorization in the microservices architecture involves scenarios that are more complex, involving users accessing microservice applications, third-party applications accessing microservice applications, and multiple microservice applications accessing each other, and in each scenario, The following authentication and authorization schemes need to be considered to ensure the security of the application.</li>
</ul>

<h3 id="toc_2">Microservices authentication and authorization technical solutions</h3>

<h3 id="toc_3"><strong>1. Distributed Session Management</strong></h3>

<p>In order to make full use of benefits of the microservice architecture and to achieve the scalability and resiliency of the microservices, the microservices are preferably to be stateless.</p>

<p>This solution can be applied through different ways like:</p>

<ul>
<li>  <strong>Sticky session</strong>
Which ensures that all requests from a specific user will be sent to the same server who handled the first request corresponding to that user, thus ensuring that session data is always correct for a certain user. However, this solution depends on the load balancer, and it can only meet the horizontally expanded cluster scenario, but when the load balancer is forced suddenly for any reason to shift users to a different server, all of the user’s session data will be lost.</li>
<li>  <strong>Session replication</strong> Means that each instance saves all session data, and synchronizes through the network. Synchronizing session data causes network bandwidth overhead. As long as the session data changes, the data needs to be synchronized to all other machines. The more instances, the more network bandwidth the synchronization brings.</li>
<li>  <strong>Centralized session storage</strong> Means that when a user accesses a microservice, user data can be obtained from shared session storage, ensuring that all microservices can read the same session data. In some scenarios, this scheme is very good, and the user login status is opaque. It is also a highly available and scalable solution. But the disadvantage of this solution is that shared session storage requires a certain protection mechanism and therefore needs to be accessed through a secure way.</li>
</ul>

<p><img src="media/15403881797785/15403882977955.jpg" alt=""/></p>

<h3 id="toc_4">2. Client Token</h3>

<p>The traditional way is to use a session on the server side to save the user state. Because the server is stateful, it has an impact on the horizontal expansion of the server. It is recommended to use Token to record user login status in the microservice architecture.</p>

<p>The main difference between Token and Session is where the storage is different. Sessions are stored centrally in the server; Tokens are held by the user themselves and are typically stored in the browser in the form of cookies. The Token holds the user’s identity information, and each time the request is sent to the server, the server can therefore determine the identity of the visitor and determine whether it has access to the requested resource.</p>

<p>The Token is used to indicate the user’s identity. Therefore, the content of the Token needs to be encrypted to avoid falsification by the requester or the third party. <a href="https://jwt.io/">JWT (Json Web Token)</a> is an open standard (RFC 7519) that defines the Token format, defines the Token content, encrypts it, and provides lib for various languages.</p>

<p>The structure of JWT Token is very simple and consists of three parts:</p>

<ul>
<li>  <strong>Header</strong> 
header contains type, fixed value JWT. Then the Hash algorithm used by JWT.</li>
</ul>

<pre>{
"typ": "JWT",
"alg": "HS256"
}</pre>

<ul>
<li>  <strong>Payload</strong> 
includes standard information such as the user id, expiration date, and user name. It can also add user roles and user-defined information.</li>
</ul>

<pre>{ 
"id": 123, 
"name": "Mena Meseha",
"is_admin": true,
"expire": 1558213420 
}</pre>

<ul>
<li>  <strong>Signature</strong> 
Token’s signature is used by the client to verify the Token’s identity and also to verify the message wasn’t changed along the way.</li>
</ul>

<pre>HMACSHA256(
  base64UrlEncode(header) + "." +
  base64UrlEncode(payload),
  secret
)</pre>

<p>These three parts are combined using Base64 encoding and become Token strings that are eventually returned to the client, separated by “.”, The token formed by the above example will be like this:</p>

<pre>eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpZCI6MTIzLCJuYW1lIjoiTWVuYSBNZXNlaGEiLCJpc19hZG1pbiI6dHJ1ZSwiZXhwaXJlIjoxNTU4MjEzNDIwfQ.Kmy_2WCPbpg-aKQmiLaKFLxb5d3rOC71DHexncH_AcQ</pre>

<p>By using token for user authentication, The server does not save the user status. The client needs to send the token to the server for authentication every time the client requests it.</p>

<p>The basic flow of user authentication in token mode is as the following diagram:</p>

<p><img src="media/15403881797785/15403883110726.jpg" alt=""/></p>

<h3 id="toc_5"><strong>_3._</strong> Single sign-on</h3>

<p>The idea of ​​single sign-on is simple, that is, users only need to log in to the application once, then they can access all the microservices in the application. This solution means that each user-oriented service must interact with the authentication service like the following diagram:</p>

<p><img src="media/15403881797785/15403883205851.jpg" alt=""/></p>

<p>This can result in a lot of very trivial network traffic, repeated work, and it may cause single point of failure. When there are dozens of micro-applications, the drawbacks of this solution will become more apparent.</p>

<h3 id="toc_6"><strong>4. Client Token with API Gateway</strong></h3>

<p>The authentication process of the user is similar to the basic process of token authentication. The difference is that the API Gateway is added as the entrance of the external request. This scenario means that all requests go through the API gateway, effectively hiding the microservices. On request, the API gateway translates the original user token into an opaque token that only itself can resolve like the following diagram:</p>

<p><img src="media/15403881797785/15403883372753.jpg" alt=""/><br/>
Client Token with API Gateway solution</p>

<p>In this case, logging off is not a problem because the API gateway can revoke the user’s token when it logs out and also it adds an extra protection to Auth Token from being decrypted by hiding it from the client.</p>

<h3 id="toc_7">5. Third-party application access</h3>

<h4 id="toc_8">1. API Token</h4>

<p>The third party uses an application-issued API Token to access the application’s data. The Token is generated by the user in the application and provided for use by third-party applications. In this case, generally only third-party applications are allowed to access the user’s own data of the Token, but not other users’ sensitive private data.</p>

<p>For example, Github provides the Personal API Token function. Users can create a Token in <a href="https://github.com/settings/tokens">Github’s developer settings interface</a>, and then use the Token to access the Github API. When creating a Token, you can set which data the Token can access to the user, such as viewing Repo information, deleting Repo, viewing user information, updating user information, and so on.</p>

<p>Using the API Token to Access the Github API is like the following command:</p>

<pre><code>curl -u menameseha:f3kdfvf8e882424ed0f3bavmvdl88c01acd34eec https://api.github.com/user
</code></pre>

<p>The advantage of using the API Token instead of using the username/password directly to access the API is to reduce the risk of exposing the user’s password, and to reclaim the token’s permissions at any time without having to change the password.</p>

<h4 id="toc_9">2. OAuth</h4>

<p>Some third-party applications need to access data from different users, or integrate data from multiple users. You may consider using OAuth. With OAuth, when a third-party application accesses a service, the application prompts the user to authorize a third-party application to use the corresponding access authority and generates a token for access according to the user’s permissions.</p>

<p>In Github, for example, some third-party applications such as GitBook or Travis CI, are integrated via OAuth and Github. OAuth has different authentication processes for different scenarios. A typical authentication process is shown in the following diagram:</p>

<p><img src="media/15403881797785/15403883519667.jpg" alt=""/><br/>
OAuth authentication process</p>

<blockquote>
<p>In the above example, the resource server and the authorization server are both Github, the client program is GitBook or Travis CI, and the user is a direct user of the client program.</p>
</blockquote>

<p>Someone may wonder why an Authorization Code is used to request Access Token, rather than returning the Access Token to the client directly from the authorization server. The reason why OAuth is designed in this way is to pass through the user agent (browser) during the process of redirecting to the client’s Callback URL. If the Access Token is passed directly, there is a risk of being stolen. <br/>
By using the authorization code, the client directly interacts with the authorization server when applying for the access token, and the authorization server also authorize the client when processing the client’s token request, so it’s prevent others from forging the client’s identity to use the authentication code.</p>

<p>When implementing user authentication of the microservice itself, OAuth may also be used to delegate user authentication of the microservice to a third-party authentication service provider.</p>

<p>The purpose of using OAuth for user authorization of third-party application access and microservices is different. The former is to authorize private data access rights of users in microservices to third-party applications. Microservices are authorization and resource servers in the OAuth architecture. The purpose of the latter is to integrate and utilize the OAuth authentication service provided by a well-known authentication provider, which simplifies the cumbersome registration operation, in this case the microservice act the role of the client in the OAuth architecture.<br/>
Therefore, we need to distinguish between these two different scenarios so as to avoid misunderstandings.</p>

<h3 id="toc_10">6. Mutual Authentication</h3>

<p>In addition to vertical traffic from users and third parties, there is a large amount of horizontal traffic between microservices. These traffic may be in the same local area network or across different data centers. Traffic between these microservices exists by third parties. The danger of sniffing and attacking also requires security controls.</p>

<p>Through mutual SSL, mutual authentication between microservices can be achieved, and data transmission between microservices can be encrypted through TLS. A certificate needs to be generated for each microservice, and the microservices are authenticated with each other’s certificates. In the microservice operating environment, there may be a large number of microservice instances, and the microservice instances often change dynamically, such as adding service instances as the level expands. In this case, creating and distributing certificates for each service becomes very difficult. We can create a private certificate center (Internal PKI/CA) to provide certificate management for various microservices such as issuing, revoking, and updating.</p>

<h3 id="toc_11"><strong>Thinking Summary</strong></h3>

<p>Under the microservice architecture, I prefer to use OAuth and JWT together. OAuth is generally used in third-party access scenarios to manage external permissions, so it is more suitable for integration with API gateways and of course, the underlying Token standard JWT is also possible. The JWT is more lightweight, and for access authentication between microservices it is already sufficient. Of course, from the perspective of capability of implementation, the distributed session for example, can fully meet the requirements in many scenarios. Specifically, how to choose an authentication scheme depends on your actual needs.</p>

]]></content>
  </entry>
  
</feed>
