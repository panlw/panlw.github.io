<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Junkman]]></title>
  <link href="http://panlw.github.io/atom.xml" rel="self"/>
  <link href="http://panlw.github.io/"/>
  <updated>2018-05-28T00:53:11+08:00</updated>
  <id>http://panlw.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[电商平台备战促销季的运维秘诀——高可用服务层]]></title>
    <link href="http://panlw.github.io/15274398936380.html"/>
    <updated>2018-05-28T00:51:33+08:00</updated>
    <id>http://panlw.github.io/15274398936380.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>曹林华 纯洁的微笑 2018/5/26</p>

<p><a href="https://mp.weixin.qq.com/s/5vVXBXkd-Ilh7zk5G6Wxcg">原文地址</a></p>

<p>高可用设计是互联网系统架构的基础之一，以天猫双十二交易数据为例，支付宝峰值支付次数超过 8 万笔。大家设想一下，如果这个时候系统出现不可用的情况，那后果将不可想象。<br/>
而解决这个问题的根本就是服务层的高可用。</p>
</blockquote>

<h2 id="toc_0">什么是服务层</h2>

<p>众所周知，服务层主要用来处理网站业务逻辑的，是大型业务网站的核心。比如下面三个业务系统就是典型的服务层，提供基础服务功能的聚合</p>

<ul>
<li><p>用户中心：主要负责用户注册、登录、获取用户用户信息功能</p></li>
<li><p>交易中心：主要包括正向订单生成、逆向订单、查询、金额计算等功能</p></li>
<li><p>支付中心：主要包括订单支付、收银台、对账等功能</p></li>
</ul>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlialgfhicgwy6oEeNHT1HKDxoPpnY0JKoKLZsfKd0Z7zxcfiaSzf0DdSqA/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<h2 id="toc_1">整体架构</h2>

<p>业务发展初期主要以业务为导向，一般采用 「ALL IN ONE」的架构方式来开发产品，这个阶段用一句话概括就是 「糙猛快」。当发展起来之后就会遇到下面这些问题</p>

<ul>
<li><p>文件大：一个代码文件出现超过 2000 行以上</p></li>
<li><p>耦合性严重：不相关业务都直接堆积在 Serivce 层中</p></li>
<li><p>维护代价高：人员离职后，根本没有人了解里面的业务逻辑</p></li>
<li><p>牵一发动全身：改动少量业务逻辑，需要重新把所有依赖包打包并发布</p></li>
</ul>

<p>遇到这些问题，主要还是通过「拆」来解决</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlR3f0gpEglmktfDdT7JZsPNts8y6PwIDvpoVzYzXjriaLYicTz9sib2IYQ/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<p>具体拆的方式，主要根据业务领域划分单元，进行垂直拆分。拆分开来的好处很明显，主要有以下这些：</p>

<ul>
<li><p>每个业务一个独立的业务模块</p></li>
<li><p>业务间完全解耦</p></li>
<li><p>业务间互不影响</p></li>
<li><p>业务模块独立</p></li>
<li><p>单独开发、上线、运维</p></li>
<li><p>效率高</p></li>
</ul>

<h2 id="toc_2">无状态设计</h2>

<p>对于业务逻辑服务层，一般会设计成无状态化的服务，无状态化也就是服务模块只处理业务逻辑，而无需关心业务请求的上下文信息。所以无状态化的服务器之间是相互平等且独立的。</p>

<p>只有服务变为无状态的时候，故障转移才会变的很轻松。通常故障转移就是在某一个应用服务器不能服务用户请求的时候，通过负责均衡的方式，转移用户请求到其他应用服务器上来进行业务逻辑处理</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlrVLkBKpJZja8eqMrcYwM3Mh3TVhgibEniaQickmJw3W7IiccBmMFm5sZAw/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<h2 id="toc_3">超时设置</h2>

<p>一般网站服务都会有主调服务和被调服务之分。超时设置就是主调服务在调用被调服务的时候，设置一个超时等待时间 Timeout。主调服务发现超时后，就进入超时处理流程。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlBhI38T15JZ8gV0fNRfWcjqFibk2VjNPedQAwdy5DOETk1TV43qic5aVA/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<ol>
<li><p>主调服务 A 调用被调服务 B 时，设置超时等待时间为 3 秒，可能由于 B 服务宕机、网络情况不好或程序 BUG 之类，导致 B 服务不能及时响应 A 服务的调用。</p></li>
<li><p>此时 A 服务在等待 3 秒后，将触发超时逻辑而不再关心 B 服务的回复情况。</p></li>
<li><p>A 服务的超时逻辑可以依据情况而定，比如可以采取重试，对另一个对等的 B 服务去请求，或直接放弃结束这个请求调用。</p></li>
</ol>

<p>超时设置的好处在于当某个服务不可用时，不至于整个系统发生雪崩反应。</p>

<h2 id="toc_4">异步调用</h2>

<p>一般请求调用分为同步与异步两种。同步请求就像打电话，需要实时响应，而异步请求就像发送邮件一样，不需要马上回复。</p>

<p>这两种调用各有优劣，主要看面对哪种业务场景。比如在面对并发性能要求比较高的场景，异步调用就比同步调用有比较大的优势，这就好比一个人不能同时打多个电话，但是可以发送很多邮件。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlrMaqR6L2RGN4ZNKoX60pk0ckuAypBfThcu227jxF2rbN8XENfgbLTg/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<blockquote>
<p>那我们什么时候该采用异步调用？</p>
</blockquote>

<p>其实主要看业务场景，如果业务允许延迟处理，那就采用异步的方式处理</p>

<blockquote>
<p>那我们该怎么实现异步调用呢？</p>
</blockquote>

<p>通常采用队列的方式来实现业务上的延迟处理，比如像订单中心调用配送中心，这种场景下面，业务是能接受延迟处理的。</p>

<p>那消息队列主要有哪些功能呢？</p>

<ul>
<li><p>异步处理 - 增加吞吐量</p></li>
<li><p>削峰填谷 - 提高系统稳定性</p></li>
<li><p>系统解耦 - 业务边界隔离</p></li>
<li><p>数据同步 - 最终一致性保证</p></li>
</ul>

<p>那到底有多少种队列呢？其实主要看处理业务的范围大小</p>

<ul>
<li><p>应用内部 - 采用线程池，比如 Java ThreadPool 中 BlockingQueue 来做任务级别的缓冲与处理</p></li>
<li><p>应用外部 - 比如 RabbitMQ 、ActiveMQ 就是做应用级别的队列，方便进行业务边界隔离与提高吞吐量</p></li>
</ul>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlvaSzVuYpLsyDzfC4AJF2X5Ww7L1iaaQceaRwoSjwhL05oJdibvVMQIrw/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<p>同时，技术上来讲，消息队列一般分为两种模型：Pull VS Push</p>

<ul>
<li><p>Pull 模型：消费者主动请求消息队列，获取队列中的消息。</p></li>
<li><p>Push 模型：消息队列主动推送消息到消费者</p></li>
</ul>

<p>其中 Pull 模式可以控制消费速度，不必担心自己处理不了消息，只需要维护队列中偏移量 Offset。所以对于消费量有限并且推送到队列的生产者不均匀的情况下，采用 Pull 模式比较合适。</p>

<p>Push 比较适合实时性要求比较高的情况，只要生产者消息发送到消息队列中，队列就会主动 Push 消息到消费者，不过这种模式对消费者的能力要求就提高很多，如果出现队列给消费者推送一些不能处理的消息，消费者出现 Exception 情况下，就会再次入队列，造成消费堵塞的情况。</p>

<p>不过互联网业界比较成熟的队列主要以采用 Pull 模式为主，像 Kafka、RabbitMQ（两种方式都支持）、RocketMQ 等</p>

<h2 id="toc_5">幂等</h2>

<blockquote>
<p>什么是幂等设计呢？</p>
</blockquote>

<p>其实很简单，就是一次请求和多个请求的作用是一样的。用数学上的术语，即是 f(x) = f(f(x))。</p>

<p>那我们为什么要做幂等性的设计呢？主要是因为现在的系统都是采用分布式的方式设计系统，在分布式系统中调用一般分为 3 个状态：成功、失败、超时。</p>

<p>如果调用是成功或者失败都不要紧，因为状态是明确和清晰，但是如果出现超时的情况，就不知道请求是成功还是失败的。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdla3ufL6yQt2W78GGKeq8M06eN6dmOWia6bSn1BwHwtdrw0gZf33g66wA/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<p>如果出现这种情况，我们该怎么办呢？一般采取重试的操作，重新请求对应接口。如果请求接口是 Get 操作的话，那到还好，因为请求多次的效果是一样的。但是如果是 Post 、Put 操作的话，就会造成数据不一致，甚至数据覆盖等问题。</p>

<p>举个例子：在支付收银台页面进行支付的时候，因为网络超时的问题导致支付失败，这个时候我们都会再进行一次支付操作，但是当支付成功后，发现你的账户余额被减了 2 次，这个时候心里肯定很不爽，心里都要开始骂娘了…</p>

<p>造成这个问题的关键是：网络超时后，不知道支付是什么状态？成功还是失败呢？所以说幂等性设计是必须的，尤其在电商、金融、银行等对数据要求比较高的行业中。</p>

<p>一般在这种场景下我们该怎么解决呢？</p>

<ol>
<li><p>请求方一般会生产一个唯一性 ID 标识，这个标识可以具有业务一样，比如订单号或者支付流水号，在发起请求时候带上唯一性 ID。</p></li>
<li><p>接收者在收到请求后，第一步通过获取唯一性 ID 来查询接收端是否有对应的记录，如果有的话，就直接将上次请求的结果返回，如果没有的话，就进行操作，并在操作完成后记录到对应的表里</p></li>
</ol>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlCPmk4LiaX77Zhuuv3dgZDc6PR9elSx8CEGQGRfXaeuoWPMlScROZGfA/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<h2 id="toc_6">服务降级</h2>

<p>服务降级主要解决资源不足和访问量过大的问题，比如电商平台在双十一、618 等高峰时候采用部分服务不提供访问，减少对系统的影响。</p>

<p>那降级的方式有哪些呢？</p>

<ul>
<li><p>延迟服务：比如春晚，微信发红包就出现抢到红包，但是账号余额并没有增加，要过几天才能加上去。其实这是微信内部采用延迟服务的方式来保证服务的稳定，通过队列实现记录流水账单</p></li>
<li><p>功能降级：停止不重要的功能是非常有用的方式，把相对不重要的功能暂停掉，让系统释放更多的资源。比如关闭相关文章的推荐、用户的评论功能等等，等高峰过去之后，在把服务恢复回来。</p></li>
<li><p>降低数据一致性：在大促的时候，我们发现页面上不显示真实库存的数据，只显示到底有还是没有库存这两种状态。</p></li>
</ul>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdl8ZZdFwaAib5B4yibFvXffcW1I3libeD2icxf1AaNmfYuyT85pTke7wBU3Q/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<p>刚刚说了降级的方式，那我们操作降级的时候有哪些注意点呢？</p>

<ol>
<li><p>清晰定义降级级别： 比如出现吞吐量超过 X，单位时间内响应时间超过 Y 秒、失败次数超过 Z 次等，这些阈值需要在准备的时候，通过压测的方式来确定。</p></li>
<li><p>梳理业务级别：降级之前，首先需要确定哪些业务是必须有，哪些业务是可以有的，哪些业务是可有可无的。</p></li>
<li><p>降级开关：可以通过接入配置中心（比如携程 Apollo、百度 Disconf ）的方式直接后台降级。但是如果公司没有配置中心的话，可以封装一个 API 接口来切分，不过该 API 接口要做成幂等的方式，同时需要做一些简单的签名，来保证其一定的安全性。</p></li>
</ol>

<h2 id="toc_7">总结</h2>

<p>总结一下今天分享的主要内容</p>

<ul>
<li><p>整体架构：根据业务属性进行垂直拆分，减少项目依赖，单独开发、上线、运维</p></li>
<li><p>无状态设计：应用服务中不能保存用户状态数据，如果有状态就会出现难以扩容、单点等问题</p></li>
<li><p>超时设置：当某个服务不可用时，不至于整个系统发生连锁反应</p></li>
<li><p>异步调用：同步调用改成异步调用，解决远程调用故障或调用超时对系统的影响</p></li>
<li><p>服务降级：牺牲非核心业务，保证核心业务的高可用</p></li>
</ul>

<p>所有好的架构设计首要的原则并不是追求先进，而是合理性，要与公司的业务规模和发展趋势相匹配，任何一个公司，哪怕是现在看来规模非常大的公司，比如 BAT 之类，在一开始，其系统架构也应简单和清晰的。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[面试-线程池的成长之路]]></title>
    <link href="http://panlw.github.io/15274396845490.html"/>
    <updated>2018-05-28T00:48:04+08:00</updated>
    <id>http://panlw.github.io/15274396845490.html</id>
    <content type="html"><![CDATA[
<p>尹吉欢 投稿 纯洁的微笑  3天前</p>

<blockquote>
<p><a href="https://mp.weixin.qq.com/s/5dexEENTqJWXN_17c6Lz6A">原文地址</a></p>
</blockquote>

<h1 id="toc_0"><strong>背景</strong></h1>

<p>相信大家在面试过程中遇到面试官问线程的很多，线程过后就是线程池了。从易到难，都是这么个过程，还有就是确实很多人在工作中接触线程池比较少，最多的也就是创建一个然后往里面提交线程，对于一些经验很丰富的面试官来说，一下就可以问出很多线程池相关的问题，与其被问的晕头转向，还不如好好学习。此时不努力更待何时。</p>

<h1 id="toc_1">什么是线程池？</h1>

<p>线程池是一种多线程处理形式，处理过程中将任务提交到线程池，任务的执行交由线程池来管理。</p>

<p>如果每个请求都创建一个线程去处理，那么服务器的资源很快就会被耗尽，使用线程池可以减少创建和销毁线程的次数，每个工作线程都可以被重复利用，可执行多个任务。</p>

<p>如果用生活中的列子来说明，我们可以把线程池当做一个客服团队，如果同时有 1000 个人打电话进行咨询，按照正常的逻辑那就是需要 1000 个客服接听电话，服务客户。现实往往需要考虑到很多层面的东西，比如：资源够不够，招这么多人需要费用比较多。正常的做法就是招 100 个人成立一个客服中心，当有电话进来后分配没有接听的客服进行服务，如果超出了 100 个人同时咨询的话，提示客户等待，稍后处理，等有客服空出来就可以继续服务下一个客户，这样才能达到一个资源的合理利用，实现效益的最大化。</p>

<h1 id="toc_2">Java 中的线程池种类</h1>

<p><strong>1. newSingleThreadExecutor</strong></p>

<p>创建方式：</p>

<pre><code>ExecutorService pool = Executors.newSingleThreadExecutor();
</code></pre>

<p>一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。</p>

<p>使用方式：</p>

<pre><code>import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class ThreadPool {    public static void main(String[] args) {        ExecutorService pool = Executors.newSingleThreadExecutor();        for (int i = 0; i &lt; 10; i++) {            pool.execute(() -&gt; {                System.out.println(Thread.currentThread().getName() + &quot;\t开始发车啦....&quot;);            });        }    }}
</code></pre>

<p>输出结果如下：</p>

<pre><code>pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....
</code></pre>

<p>从输出的结果我们可以看出，一直只有一个线程在运行。</p>

<p>2.<strong>newFixedThreadPool</strong></p>

<p>创建方式：</p>

<pre><code>ExecutorService pool = Executors.newFixedThreadPool(10);
</code></pre>

<p>创建固定大小的线程池。每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。</p>

<p>使用方式：</p>

<pre><code>import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class ThreadPool {    public static void main(String[] args) {        ExecutorService pool = Executors.newFixedThreadPool(10);        for (int i = 0; i &lt; 10; i++) {            pool.execute(() -&gt; {                System.out.println(Thread.currentThread().getName() + &quot;\t开始发车啦....&quot;);            });        }    }}
</code></pre>

<p>输出结果如下：</p>

<pre><code>pool-1-thread-1    开始发车啦....pool-1-thread-4    开始发车啦....pool-1-thread-3    开始发车啦....pool-1-thread-2    开始发车啦....pool-1-thread-6    开始发车啦....pool-1-thread-7    开始发车啦....pool-1-thread-5    开始发车啦....pool-1-thread-8    开始发车啦....pool-1-thread-9    开始发车啦....pool-1-thread-10 开始发车啦....
</code></pre>

<p><strong>3. newCachedThreadPool</strong></p>

<p>创建方式：</p>

<pre><code>ExecutorService pool = Executors.newCachedThreadPool();
</code></pre>

<p>创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲的线程，当任务数增加时，此线程池又添加新线程来处理任务。</p>

<p>使用方式如上 2 所示。</p>

<p>4.<strong>newScheduledThreadPool</strong></p>

<p>创建方式：</p>

<pre><code>ScheduledExecutorService pool = Executors.newScheduledThreadPool(10);
</code></pre>

<p>此线程池支持定时以及周期性执行任务的需求。</p>

<p>使用方式：</p>

<pre><code>import java.util.concurrent.Executors;import java.util.concurrent.ScheduledExecutorService;import java.util.concurrent.TimeUnit;public class ThreadPool {    public static void main(String[] args) {        ScheduledExecutorService pool = Executors.newScheduledThreadPool(10);        for (int i = 0; i &lt; 10; i++) {            pool.schedule(() -&gt; {                System.out.println(Thread.currentThread().getName() + &quot;\t开始发车啦....&quot;);            }, 10, TimeUnit.SECONDS);        }    }}
</code></pre>

<p>上面演示的是延迟 10 秒执行任务, 如果想要执行周期性的任务可以用下面的方式，每秒执行一次</p>

<pre><code>//pool.scheduleWithFixedDelay也可以pool.scheduleAtFixedRate(() -&gt; {                System.out.println(Thread.currentThread().getName() + &quot;\t开始发车啦....&quot;);}, 1, 1, TimeUnit.SECONDS);
</code></pre>

<p>5.<strong>newWorkStealingPool</strong></p>

<p>newWorkStealingPool 是 jdk1.8 才有的，会根据所需的并行层次来动态创建和关闭线程，通过使用多个队列减少竞争，底层用的 ForkJoinPool 来实现的。ForkJoinPool 的优势在于，可以充分利用多 cpu，多核 cpu 的优势，把一个任务拆分成多个 “小任务”，把多个“小任务” 放到多个处理器核心上并行执行；当多个 “小任务” 执行完成之后，再将这些执行结果合并起来即可。</p>

<h1 id="toc_3">说说线程池的拒绝策略</h1>

<p>当请求任务不断的过来，而系统此时又处理不过来的时候，我们需要采取的策略是拒绝服务。RejectedExecutionHandler 接口提供了拒绝任务处理的自定义方法的机会。在 ThreadPoolExecutor 中已经包含四种处理策略。</p>

<ul>
<li>  AbortPolicy 策略：该策略会直接抛出异常，阻止系统正常工作。</li>
</ul>

<pre><code>public static class AbortPolicy implements RejectedExecutionHandler {    public AbortPolicy() { }    public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {        throw new RejectedExecutionException(&quot;Task &quot; + r.toString() +                                                 &quot; rejected from &quot; +                                                 e.toString());    }}
</code></pre>

<ul>
<li>  CallerRunsPolicy 策略：只要线程池未关闭，该策略直接在调用者线程中，运行当前的被丢弃的任务。</li>
</ul>

<pre><code>public static class CallerRunsPolicy implements RejectedExecutionHandler {    public CallerRunsPolicy() { }    public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {        if (!e.isShutdown()) {                r.run();        }    }}
</code></pre>

<ul>
<li>  DiscardOleddestPolicy 策略： 该策略将丢弃最老的一个请求，也就是即将被执行的任务，并尝试再次提交当前任务。</li>
</ul>

<pre><code>public static class DiscardOldestPolicy implements RejectedExecutionHandler {    public DiscardOldestPolicy() { }    public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {        if (!e.isShutdown()) {            e.getQueue().poll();            e.execute(r);        }    }}
</code></pre>

<ul>
<li>  DiscardPolicy 策略：该策略默默的丢弃无法处理的任务，不予任何处理。</li>
</ul>

<pre><code>public static class DiscardPolicy implements RejectedExecutionHandler {    public DiscardPolicy() { }    public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {    }}
</code></pre>

<p>除了 JDK 默认为什么提供的四种拒绝策略，我们可以根据自己的业务需求去自定义拒绝策略，自定义的方式很简单，直接实现 RejectedExecutionHandler 接口即可</p>

<p>比如 Spring integration 中就有一个自定义的拒绝策略 CallerBlocksPolicy，将任务插入到队列中，直到队列中有空闲并插入成功的时候，否则将根据最大等待时间一直阻塞，直到超时。</p>

<pre><code>package org.springframework.integration.util;import java.util.concurrent.BlockingQueue;import java.util.concurrent.RejectedExecutionException;import java.util.concurrent.RejectedExecutionHandler;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;import org.apache.commons.logging.Log;import org.apache.commons.logging.LogFactory;public class CallerBlocksPolicy implements RejectedExecutionHandler {    private static final Log logger = LogFactory.getLog(CallerBlocksPolicy.class);    private final long maxWait;    /**     * @param maxWait The maximum time to wait for a queue slot to be     * available, in milliseconds.     */    public CallerBlocksPolicy(long maxWait) {        this.maxWait = maxWait;    }    @Override    public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) {        if (!executor.isShutdown()) {            try {                BlockingQueue&lt;Runnable&gt; queue = executor.getQueue();                if (logger.isDebugEnabled()) {                    logger.debug(&quot;Attempting to queue task execution for &quot; + this.maxWait + &quot; milliseconds&quot;);                }                if (!queue.offer(r, this.maxWait, TimeUnit.MILLISECONDS)) {                    throw new RejectedExecutionException(&quot;Max wait time expired to queue task&quot;);                }                if (logger.isDebugEnabled()) {                    logger.debug(&quot;Task execution queued&quot;);                }            }            catch (InterruptedException e) {                Thread.currentThread().interrupt();                throw new RejectedExecutionException(&quot;Interrupted&quot;, e);            }        }        else {            throw new RejectedExecutionException(&quot;Executor has been shut down&quot;);        }    }}
</code></pre>

<p>定义好之后如何使用呢？光定义没用的呀，一定要用到线程池中呀，可以通过下面的方式自定义线程池，指定拒绝策略。</p>

<pre><code>BlockingQueue&lt;Runnable&gt; workQueue = new ArrayBlockingQueue&lt;&gt;(100);ThreadPoolExecutor executor = new ThreadPoolExecutor(    10, 100, 10, TimeUnit.SECONDS, workQueue, new CallerBlocksPolicy());
</code></pre>

<h1 id="toc_4">execute 和 submit 的区别？</h1>

<p>在前面的讲解中，我们执行任务是用的 execute 方法，除了 execute 方法，还有一个 submit 方法也可以执行我们提交的任务。</p>

<p>这两个方法有什么区别呢？分别适用于在什么场景下呢？我们来做一个简单的分析。</p>

<p>execute 适用于不需要关注返回值的场景，只需要将线程丢到线程池中去执行就可以了</p>

<pre><code>public class ThreadPool {    public static void main(String[] args) {        ExecutorService pool = Executors.newFixedThreadPool(10);        pool.execute(() -&gt; {            System.out.println(Thread.currentThread().getName() + &quot;\t开始发车啦....&quot;);        });    }}
</code></pre>

<p>submit 方法适用于需要关注返回值的场景，submit 方法的定义如下：</p>

<pre><code>public interface ExecutorService extends Executor {　　...　　&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task);　　&lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result);　　Future&lt;?&gt; submit(Runnable task);　　...}
</code></pre>

<p>其子类 AbstractExecutorService 实现了 submit 方法, 可以看到无论参数是 Callable 还是 Runnable，最终都会被封装成 RunnableFuture，然后再调用 execute 执行。</p>

<pre><code>    /**     * @throws RejectedExecutionException {@inheritDoc}     * @throws NullPointerException       {@inheritDoc}     */    public Future&lt;?&gt; submit(Runnable task) {        if (task == null) throw new NullPointerException();        RunnableFuture&lt;Void&gt; ftask = newTaskFor(task, null);        execute(ftask);        return ftask;    }    /**     * @throws RejectedExecutionException {@inheritDoc}     * @throws NullPointerException       {@inheritDoc}     */    public &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result) {        if (task == null) throw new NullPointerException();        RunnableFuture&lt;T&gt; ftask = newTaskFor(task, result);        execute(ftask);        return ftask;    }    /**     * @throws RejectedExecutionException {@inheritDoc}     * @throws NullPointerException       {@inheritDoc}     */    public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) {        if (task == null) throw new NullPointerException();        RunnableFuture&lt;T&gt; ftask = newTaskFor(task);        execute(ftask);        return ftask;    }
</code></pre>

<p>下面我们来看看这三个方法分别如何去使用：</p>

<p><strong>submit(Callable<t style="max-width: 100%;box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: break-word !important;"> task);</t></strong></p>

<pre><code>public class ThreadPool {    public static void main(String[] args) throws Exception {        ExecutorService pool = Executors.newFixedThreadPool(10);        Future&lt;String&gt; future = pool.submit(new Callable&lt;String&gt;() {            @Override            public String call() throws Exception {                return &quot;Hello&quot;;            }        });        String result = future.get();        System.out.println(result);    }}
</code></pre>

<p><strong>submit(Runnable task, T result);</strong></p>

<pre><code>public class ThreadPool {    public static void main(String[] args) throws Exception {        ExecutorService pool = Executors.newFixedThreadPool(10);        Data data = new Data();        Future&lt;Data&gt; future = pool.submit(new MyRunnable(data), data);        String result = future.get().getName();        System.out.println(result);    }}class Data {    String name;    public String getName() {        return name;    }    public void setName(String name) {        this.name = name;    }}class MyRunnable implements Runnable {    private Data data;    public MyRunnable(Data data) {        this.data = data;    }    @Override    public void run() {        data.setName(&quot;yinjihuan&quot;);    }}
</code></pre>

<p><strong>Future submit(Runnable task);</strong><br/>
直接 submit 一个 Runnable 是拿不到返回值的，返回值就是 null.</p>

<h1 id="toc_5">五种线程池的使用场景</h1>

<ul>
<li><p>newSingleThreadExecutor：一个单线程的线程池，可以用于需要保证顺序执行的场景，并且只有一个线程在执行。</p></li>
<li><p>newFixedThreadPool：一个固定大小的线程池，可以用于已知并发压力的情况下，对线程数做限制。</p></li>
<li><p>newCachedThreadPool：一个可以无限扩大的线程池，比较适合处理执行时间比较小的任务。</p></li>
<li><p>newScheduledThreadPool：可以延时启动，定时启动的线程池，适用于需要多个后台线程执行周期任务的场景。</p></li>
<li><p>newWorkStealingPool：一个拥有多个任务队列的线程池，可以减少连接数，创建当前可用 cpu 数量的线程来并行执行。</p></li>
</ul>

<h1 id="toc_6">线程池的关闭</h1>

<p>关闭线程池可以调用 shutdownNow 和 shutdown 两个方法来实现</p>

<p><strong>shutdownNow：对正在执行的任务全部发出 interrupt()，停止执行，对还未开始执行的任务全部取消，并且返回还没开始的任务列表</strong></p>

<pre><code>public class ThreadPool {    public static void main(String[] args) throws Exception {        ExecutorService pool = Executors.newFixedThreadPool(1);        for (int i = 0; i &lt; 5; i++) {            System.err.println(i);            pool.execute(() -&gt; {                try {                    Thread.sleep(30000);                    System.out.println(&quot;--&quot;);                } catch (Exception e) {                    e.printStackTrace();                }            });        }        Thread.sleep(1000);        List&lt;Runnable&gt; runs = pool.shutdownNow();    }}
</code></pre>

<p>上面的代码模拟了立即取消的场景，往线程池里添加 5 个线程任务，然后 sleep 一段时间，线程池只有一个线程，如果此时调用 shutdownNow 后应该需要中断一个正在执行的任务和返回 4 个还未执行的任务，控制台输出下面的内容：</p>

<pre><code>01234[fs.ThreadPool$$Lambda$1/990368553@682a0b20, fs.ThreadPool$$Lambda$1/990368553@682a0b20, fs.ThreadPool$$Lambda$1/990368553@682a0b20, fs.ThreadPool$$Lambda$1/990368553@682a0b20]java.lang.InterruptedException: sleep interrupted    at java.lang.Thread.sleep(Native Method)    at fs.ThreadPool.lambda$0(ThreadPool.java:15)    at fs.ThreadPool$$Lambda$1/990368553.run(Unknown Source)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    at java.lang.Thread.run(Thread.java:745)
</code></pre>

<p><strong>shutdown：当我们调用 shutdown 后，线程池将不再接受新的任务，但也不会去强制终止已经提交或者正在执行中的任务</strong></p>

<pre><code>public class ThreadPool {    public static void main(String[] args) throws Exception {        ExecutorService pool = Executors.newFixedThreadPool(1);        for (int i = 0; i &lt; 5; i++) {            System.err.println(i);            pool.execute(() -&gt; {                try {                    Thread.sleep(30000);                    System.out.println(&quot;--&quot;);                } catch (Exception e) {                    e.printStackTrace();                }            });        }        Thread.sleep(1000);        pool.shutdown();        pool.execute(() -&gt; {            try {                Thread.sleep(30000);                System.out.println(&quot;--&quot;);            } catch (Exception e) {                e.printStackTrace();            }        });    }}
</code></pre>

<p>上面的代码模拟了正在运行的状态，然后调用 shutdown，接着再往里面添加任务，肯定是拒绝添加的，请看输出结果：</p>

<pre><code>01234Exception in thread &quot;main&quot; java.util.concurrent.RejectedExecutionException: Task fs.ThreadPool$$Lambda$2/1747585824@3d075dc0 rejected from java.util.concurrent.ThreadPoolExecutor@214c265e[Shutting down, pool size = 1, active threads = 1, queued tasks = 4, completed tasks = 0]    at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)    at fs.ThreadPool.main(ThreadPool.java:24)
</code></pre>

<p>还有一些业务场景下需要知道线程池中的任务是否全部执行完成，当我们关闭线程池之后，可以用 isTerminated 来判断所有的线程是否执行完成，千万不要用 isShutdown，isShutdown 只是返回你是否调用过 shutdown 的结果。</p>

<pre><code>public class ThreadPool {    public static void main(String[] args) throws Exception {        ExecutorService pool = Executors.newFixedThreadPool(1);        for (int i = 0; i &lt; 5; i++) {            System.err.println(i);            pool.execute(() -&gt; {                try {                    Thread.sleep(3000);                    System.out.println(&quot;--&quot;);                } catch (Exception e) {                    e.printStackTrace();                }            });        }        Thread.sleep(1000);        pool.shutdown();        while(true){              if(pool.isTerminated()){                  System.out.println(&quot;所有的子线程都结束了！&quot;);                  break;              }              Thread.sleep(1000);            }      }}
</code></pre>

<h1 id="toc_7">自定义线程池</h1>

<p>在实际的使用过程中，大部分我们都是用 Executors 去创建线程池直接使用，如果有一些其他的需求，比如指定线程池的拒绝策略，阻塞队列的类型，线程名称的前缀等等，我们可以采用自定义线程池的方式来解决。</p>

<p>如果只是简单的想要改变线程名称的前缀的话可以自定义 ThreadFactory 来实现，在 Executors.new… 中有一个 ThreadFactory 的参数，如果没有指定则用的是 DefaultThreadFactory。</p>

<p>自定义线程池核心在于创建一个 ThreadPoolExecutor 对象，指定参数</p>

<p>下面我们看下 ThreadPoolExecutor 构造函数的定义：</p>

<pre><code>public ThreadPoolExecutor(int corePoolSize,                              int maximumPoolSize,                              long keepAliveTime,                              TimeUnit unit,                              BlockingQueue&lt;Runnable&gt; workQueue,                              ThreadFactory threadFactory,                              RejectedExecutionHandler handler) ;
</code></pre>

<ul>
<li><p>corePoolSize<br/>
线程池大小，决定着新提交的任务是新开线程去执行还是放到任务队列中，也是线程池的最最核心的参数。一般线程池开始时是没有线程的，只有当任务来了并且线程数量小于 corePoolSize 才会创建线程。</p></li>
<li><p>maximumPoolSize<br/>
最大线程数，线程池能创建的最大线程数量。</p></li>
<li><p>keepAliveTime<br/>
在线程数量超过 corePoolSize 后，多余空闲线程的最大存活时间。</p></li>
<li><p>unit<br/>
时间单位</p></li>
<li><p>workQueue<br/>
存放来不及处理的任务的队列，是一个 BlockingQueue。</p></li>
<li><p>threadFactory<br/>
生产线程的工厂类，可以定义线程名，优先级等。</p></li>
<li><p>handler<br/>
拒绝策略，当任务来不及处理的时候，如何处理, 前面有讲解。</p></li>
</ul>

<p>了解上面的参数信息后我们就可以定义自己的线程池了，我这边用 ArrayBlockingQueue 替换了 LinkedBlockingQueue，指定了队列的大小，当任务超出队列大小之后使用 CallerRunsPolicy 拒绝策略处理。</p>

<p>这样做的好处是严格控制了队列的大小，不会出现一直往里面添加任务的情况，有的时候任务处理的比较慢，任务数量过多会占用大量内存，导致内存溢出。</p>

<p>当然你也可以在提交到线程池的入口进行控制，比如用 CountDownLatch, Semaphore 等。</p>

<pre><code>/** * 自定义线程池&lt;br&gt; * 默认的newFixedThreadPool里的LinkedBlockingQueue是一个无边界队列，如果不断的往里加任务，最终会导致内存的不可控&lt;br&gt; * 增加了有边界的队列，使用了CallerRunsPolicy拒绝策略 * @author yinjihuan * */public class FangjiaThreadPoolExecutor {    private static ExecutorService executorService = newFixedThreadPool(50);    private static ExecutorService newFixedThreadPool(int nThreads) {        return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS,                new ArrayBlockingQueue&lt;Runnable&gt;(10000), new DefaultThreadFactory(), new CallerRunsPolicy());    }    public static void execute(Runnable command) {        executorService.execute(command);    }    public static void shutdown() {        executorService.shutdown();    }    static class DefaultThreadFactory implements ThreadFactory {        private static final AtomicInteger poolNumber = new AtomicInteger(1);        private final ThreadGroup group;        private final AtomicInteger threadNumber = new AtomicInteger(1);        private final String namePrefix;        DefaultThreadFactory() {            SecurityManager s = System.getSecurityManager();            group = (s != null) ? s.getThreadGroup() :                                  Thread.currentThread().getThreadGroup();            namePrefix = &quot;FSH-pool-&quot; +                          poolNumber.getAndIncrement() +                         &quot;-thread-&quot;;        }        public Thread newThread(Runnable r) {            Thread t = new Thread(group, r,                                  namePrefix + threadNumber.getAndIncrement(),                                  0);            if (t.isDaemon())                t.setDaemon(false);            if (t.getPriority() != Thread.NORM_PRIORITY)                t.setPriority(Thread.NORM_PRIORITY);            return t;        }    }}
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[不理解Zookeeper一致性原理，谈何异地多活改造]]></title>
    <link href="http://panlw.github.io/15274395979385.html"/>
    <updated>2018-05-28T00:46:37+08:00</updated>
    <id>http://panlw.github.io/15274395979385.html</id>
    <content type="html"><![CDATA[
<p>原创： 陈东明 DBAplus社群 <em>1周前</em></p>

<p><strong>作者介绍</strong><br/>
*<strong><em>陈东明</em></strong>*，饿了么北京技术中心架构组负责人，负责饿了么的产品线架构设计及基础架构研发工作。曾任百度架构师，负责百度即时通讯产品的架构设计。具有丰富的大规模系统构建和基础架构的研发经验，善于复杂业务需求下的大并发、分布式系统设计和持续优化。<br/>
在2017年饿了么做异地多活建设之时，我的团队承担了Zookeeper的异地多活改造。在此期间，我听到了关于Zookeeper一致性的两种不同说法：</p>

<ul>
<li>一种说法是Zookeeper是最终一致性，由于多副本，以及保证大多数成功的Zab协议，当一个客户端进程写入一个新值，另一个客户端进程不能保证马上就会读到，但能保证最终会读到这个值；</li>
<li>另一种说法是Zookeeper的Zab协议类似于Paxos协议，并且提供了强一致性。</li>
</ul>

<p>每当听到这两种说法，我都想纠正一下——不对，Zookeeper是顺序一致性（sequential consistency）。但解释起来比较复杂，需要一篇长文来说明，于是就有了本文，下面就和大家一起讨论下我的看法。</p>

<p>*<strong><em>什么是sequetial consistency</em></strong>*</p>

<p>*<strong><em>从Zookeeper的文档中我们可以看到，里面明确写出它的一致性是sequential consistency。</em></strong>*（详细参见Zookeeper官方文档[1]）</p>

<p>那么，什么是sequential consistency呢？</p>

<p>sequential consistency是Lamport在1979年首次提出的。（详细参见他的这篇论文：<em>How to make a multiprocessor computer that correctly executes multiprocess programs</em>）</p>

<p>论文中定义，当满足下面这个条件时就是sequential consistency：</p>

<p>“<br/>
<em>the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.</em></p>

<p>这段英文定义很晦涩（这是Lamport大神的一贯风格，严谨但晦涩，Paxos协议也是如此），我第一次看到时的感觉是：“这是什么鬼？”为啥每个英文单词我都认识，但就是不知道他在说什么？第一次看到这句话和我有同感的小伙伴留个言吧~</p>

<p>后文我再把这段英文定义翻译成中文，现在我们先看看这篇论文的标题和定义中出现的一个关键词，来说明一下sequential consistency的应用范围。</p>

<p>论文的标题和定义中包含<strong>“multiprocessor</strong>”****这个词，multiprocessor是多核处理器的意思。从这个关键字来看，sequential consistency是用来定义多核处理器和跑在多核处理器上的程序的一个特性。Lamport这篇论文的标题可以翻译成：“如何让具有多核处理器的计算机正确执行多进程程序”，也就是说如果一个多核处理器具有sequential consistency的特性，这个多核处理器就可以正确运行，后面我会解释这个正确运行是什么意思（也就是本文后面讲到的sequential consistency的作用）。</p>

<p>从这个标题我们还可以看出，sequential consistency<strong>应该是个并发编程（concurrent programming）领域的概念</strong>。但我们现在常常在分布式系统领域讨论sequential consistency，比如本文主要讨论的Zookeeper（Zookeeper很明显是一个分布式系统）的一致性。实际上，多核处理器上运行的多个程序，其实也是一种分布式系统（Lamport在他的这篇 <em>Time, Clocks, and the Ordering of Events in a Distributed System</em> 分布式系统的开山之作中也阐述了这个观点）。所以虽然sequential consistency最早在并发编程中提出，但是它可以<strong>应用在分布式系统中</strong>，比如本文讨论的Zookeeper这种分布式存储系统。另外一个比较重要的Linearizability（线性一致性），也是在并发编程中最早提出的，目前也被广泛应用在分布式系统领域中。</p>

<p>接下来我们尝试翻译一下上文那段晦涩的定义。做这段翻译让我找到了上学时做阅读理解的感觉，我先不直接翻译，因为就算我把它翻译成中文，估计很多人还是会有“为啥每个中文字我都懂，还是不知道在说什么”的感觉。</p>

<p>首先，我来解释一下个别的词。首先，<strong>“any execution”</strong>是什么意思？你有多个程序（program）在多核处理器上运行，例如你有两个程序，第一个程序叫P1，它的代码如下： <br/>
P1_write(x);<br/>
P1_read(y);</p>

<p>第二个程序叫P2，代码如下：<br/>
P2_write(u);<br/>
P2_read(v);</p>

<p>从理论上来讲，两个程序运行在两个独立处理器的核上，有多少种执行的可能？我列举其中几种来举例说明。</p>

<p>第一种：<br/>
P1---write(x)--------read(y)--------<br/>
P2-----------write(u)-------read(v)-</p>

<p>第二种：</p>

<p>P1----------write(x)-read(y)--------<br/>
P2--write(u)----------------read(v)-</p>

<p>第三种：<br/>
P1---read(y)----------write(x)------<br/>
P2-----------write(u)---------read(v)-</p>

<p>我们有24种可能的执行顺序，也就是这四个操作任意的排列组合，也就是4!=24。类似第一种和第二种这样的可能性很好理解。为什么会出现像第三种这样的可能的执行呢？那是因为就算在同一个程序中，由于处理会有多级的缓存，以及处理器中coherence的存在，虽然你的程序中是先write后read，在内存中真正生效的顺序，也有可能是先read后write。</p>

<p>其实还会出现类似下面这样的执行，两个操作在两个处理器上同时执行。<br/>
P1--write(x)-read(y)--------<br/>
P2--write(u)--------read(v)-</p>

<p>如果加上同时运行的这种情况，那就有更多种可能性。我的算数不太好，这里就不继续算下去了，因为到底有多少个不重要，重要的是要知道有很多种可能性。那么定义中的“any execution”，就是指任意一种可能的执行，在定义中也可以理解为所有的这些可能的执行。</p>

<p>接着我们再来解释一个词——<strong>“sequential order”</strong>。什么叫sequential order？我们来翻一下英语词典（感觉更像在做阅读理解了）。<br/>
“<br/>
<em>sequential：连续的；相继的；有顺序的</em><br/>
<em>order：命令；顺序；规则；[贸易] 定单</em></p>

<p>sequential order——有顺序的顺序，这又是什么鬼？</p>

<p>其实sequential是有一个接一个的意思，在处理器的这种上下文中，sequential就是指操作（operartion）一个接一个地执行，也就是顺序执行，并且没有重叠。order是指经过一定的调整，让某样东西按照一定的规则变得有序。比如，在算法中的排序算法就是ordering，就是让数组这个东西按照从大到小或从小到大的规则变得有序。<strong>那么sequential order就是指让操作（operation）按照一个接一个这样的规则排列，并且没有重叠。</strong></p>

<p>再说回上文的例子，如果把四个操作，按一个接一个的规则排列，这时就可以得到4！的排列组合个可能的排列（order），同样的，有多少个不重要。</p>

<p>比如：<br/>
P1_write(x);P1_read(y);P2_write(u);P2_read(v);<br/>
P1_read(y);P1_write(x);P2_write(u);P2:read(v);<br/>
P2_write(u);P2_read(v);P1_read(y);P1:write(x);</p>

<p>我这里只列举其中三个，其他的大家可以自己排一下。</p>

<p>重点来了，其实sequential order就是让这四个操作按照一个接一个的顺序执行，并且没有重叠。注意这个排列不是真实的执行，真实的执行是any execution，这里说的是逻辑上的假设，也就是为什么定义有一个as if。</p>

<p>做了这么多的铺垫，下面我们开始翻译定义中的第一句话：</p>

<p>“<br/>
*任意一种可能的执行效果，与所有的处理器上的某一种操作按照顺序排列执行的效果是一样的。 *</p>

<p>注意，some在这里是指“某一种”的意思，不是一些，因为order是单数（真的是在做阅读理解）。</p>

<p>*<strong><em>这句话的意思就是说，一个处理器要满足这个条件，就只能允许满足这个条件的那些可能的执行存在，其他不满足的可能的执行都不会出现。</em></strong>*</p>

<p>从第一句话中我们可以看出，一种多核处理器要想满足sequential consistency，那么多个程序在多个核运行效果“等同”于在一个核上顺序执行所有操作的效果是差不多的。如果这样的话，其实多核的威力基本就消失了。所以无论是从Lamport写这篇论文的1979年，还是现在，没有任何一个现实的多核处理器实现了sequential consistency。</p>

<p>那么，为什么Lamport大神提出这样一个不现实的概念呢？（要注意Lamport写这篇论文时，并没有把它引申到分布式系统领域，就是针对多核处理器、并发编程领域提出的）我们稍后再论述。</p>

<p>这里还要注意的一点是，在我的翻译里用了“效果”一词，但实际上英文原文中用的是“result（结果）”一词。那效果和结果有什么区别吗？我们解释一下什么叫执行结果。</p>

<p>不管是任何真实的执行，还是某种经过顺序排序后的假设执行，程序会产生一定的结果，比如print出来的结果（result）。实际上定义中说的是结果一样。如果定义中用“效果”的话，那这个定义就只是一个定性的定义，如果用“结果”的话，那这个定义就是一个定量的定义。定量的，也就是说可以通过数学证明的。从这点我们可以看出，大神就是不一样，任何理论都是可以通过数学证明为正确的。本文后面还会提到证明的事情，我们这里再卖个关子。</p>

<p>到这里，关于定义的第一句，更准确的翻译就是： </p>

<p>“<br/>
*任意一种可能的执行的结果，与某一种所有的处理器上的操作按照顺序排列执行的结果是一样的。 *</p>

<p>这里我们还要注意一点，结果一样就意味着，如果有人真的要实现一种sequential consistency的多核处理器的话，因为要保证结果一样，所以他是有一定的空间来优化，而不会完全是一个核顺序执行的效果。但是估计这种优化也是非常有限的。</p>

<p>好了，终于把最难的第一句话解释完了，大家可以松口气，第二句就非常简单了。我们还是先解释第二句种出现的一个词——<strong>“sequence”</strong>。刚刚解释过的sequential order是顺序排序（就是按一个接一个排序），其实这是一个动作，动作会产生结果，它的结果产生了一个操作（operation）的队列。第二句中出现的sequence就是指这个操作（operation）的队列。</p>

<p>好，那第二句的翻译就是：</p>

<p>“<br/>
*并且每个独立的处理器的操作，都会按照程序指定的顺序出现在操作队列中。 *</p>

<p>也就是说如果程序里是先write(x);后read(y);，那么只有满足这个顺序的操作队列是符合条件的。这样，我们刚刚说的很多可能的执行就少了很多，这里我也就不计算少了多少，还是那句话，数量不重要，反正是有，而且变少了。</p>

<p>那么第二句话意味着什么？意味着如果一个多核处理器实现了sequential consistency，那这种多核处理器基本上就告别自（缓）行（存）车了。这里我还要继续卖关子，连缓存这种最有效提高处理器性能的优化都没了，大神为什么要提出这个概念？</p>

<p>好了，到这里我们可以把两句翻译合起来，完整看一下：</p>

<p>“<br/>
*任意一种可能的执行的结果，与某一种所有的处理器上的操作按照顺序排列执行的结果是一样的，并且每个独立的处理器的操作，都会按照程序指定的顺序出现在操作队列中。 *</p>

<p>从这个定义中，我们可以看出，此概念的核心就是sequential order，这也就是为什么Lamport老爷子把这种一致性模型称之为sequential consistency。可以说这个命名是非常贴切的，不知道这种贴切对于以英语为母语的人来说是不是更好理解一些，应该不会出现“顺序的顺序是什么鬼”这种情况。如果你看完本文，也觉得sequential很贴切的话，那就说明我讲清楚了。</p>

<p>接下来我们举个具体的例子，再来说明一下： <br/>
execution A<br/>
P0 writex=1-------------------------------<br/>
P1 -------write x=2----------------------<br/>
P2 -----------------read x<mark>1--read x</mark>2<br/>
P3 -----------------read x<mark>1--read x</mark>2</p>

<p>sequetial order: P0_write x=1,P3_read x<mark>1,P4_read x</mark>1,P1_write x=2,P3_read x<mark>2,P4_read x</mark>2</p>

<p>execution B<br/>
P0 write=1-------------------------------<br/>
P1 -------write x=2----------------------<br/>
P2 -----------------read x<mark>2--read x</mark>1<br/>
P3 -----------------read x<mark>2--read x</mark>1</p>

<p>sequetial order: P1_write x=2,P3_read x<mark>2,P4_read x</mark>2,P0_write x=1,P3_read x<mark>1,P4_read x</mark>1</p>

<p>execution C<br/>
P0 write=1-------------------------------<br/>
P1 -------write x=2----------------------<br/>
P2 -----------------read x<mark>1--read x</mark>2<br/>
P3 -----------------read x<mark>2--read x</mark>1</p>

<p>sequetial order: 你找不出一个符合定义中2个条件的一种order。</p>

<p>所以说如果一个多核处理器只允许execution A和B出现，不允许C出现，那么这个多核处理器就是sequetial consistency的。如果它允许C出现，那它就不是sequetial consistency。</p>

<p>到这里，我们已经完整地解释完什么是sequetial consistency。但是，细心的朋友可能会问，如果你的program是多线程的程序怎么办？那我们再把定义中最后的一个细节解释一下：program这个词。</p>

<p>*<strong><em>program</em></strong>*是指可以直接运行在处理器上的指令序列。这个并不是program的严格定义，但是我要指出的是这个program是在操作系统都没有的远古时代就存在的概念，在上文的定义中prgram就是指那个时代的program。</p>

<p>这个program里没有进程、线程的概念，这些概念都是在有了操作系统之后才出现的。因为没有操作系统，也没有内存空间的概念。不像我们现在所说的程序（program），不同的程序有自己独立的内存地址空间。我们这里，内存（memory）对于不同的program来说是shared。另外，需要注意的是program可以用来说明各种程序，不管你是操作系统内核，还是应用程序，都适用。</p>

<p>*<strong><em>sequential consistency</em></strong>*<br/>
*<strong><em>是分布式领域的概念</em></strong>*</p>

<p>刚刚我们说了，sequential consistency虽然是针对并发编程领域提出的，但实际上它是分布式领域的概念，特别是分布式存储系统。在 <em>Distributed system: Principles and Paradigms</em> （作者Andrew S.Tanenbaum, Maarten Van Steen），作者稍微修改了一下Lamport的定义，让这个定义更贴近分布式领域中的概念，我们来看一下作者是怎么改的： </p>

<p>“<br/>
*The result of any execution is the same as if the (read and write) operations by all processes on the data store were executed in some sequential order and the operations of-each individual process appear in this sequence in the order specified by its program. *</p>

<p>作者把processor换成了process，并且加了on the data store这个限定，在Lamport的定义里没有这个限定，其实默认指的是memory（内存）。process就是指进程，以Zookeeper为例，就是指访问Zookeeper的应用进程。program也不是那么底层概念，也是基于操作系统的应用程序了。</p>

<p>*<strong><em>sequential consistency的作用</em></strong>*</p>

<p>好了，下面该揭晓我上面卖的两个关子了。在Lamport的论文中，给出了一个小例子，如下： <br/>
process 1<br/>
 a := 1;<br/>
 if b = 0 then critical section:<br/>
 a := 0<br/>
 else ... fi</p>

<p>process 2<br/>
 b := 1;<br/>
 if a = 0 then critical section:<br/>
 b := 0<br/>
 else ... fi</p>

<p>Lamport在论文中说，如果一种多核处理满足sequential consistency的条件，<strong>那么最多只有一个程序能够进入critical section</strong>。在论文中，Lamport老爷子并没有解释为什么最多只有一个程序能够进入critical section。而是把这个证明留给了论文的读者，就像我们常见的教科书中的课后习题一样。</p>

<p>Lamport老爷子应该是认为这个证明太简单了，不需要花费笔墨来证明它。sequential consistency这篇论文只有不到两页A4纸，是我见过的最短的论文。这是Lamport一贯的做事风格，他的Paxos论文中，有很多细节都是一笔带过的，给读者留下无尽的遐想（瞎想）。</p>

<p>假设现在我们已经证明这个是正确的（虽然我也没去证明，论文给出了两个参考文献用于证明），那这个例子说明了什么呢？</p>

<p>大家也许注意到了，这个例子没有用到任何锁，但它实现了critical section，critical section是一种多线程synchronization 机制。如果多核处理器是sequential consistency的，那么你写的并发程序“天然就是正确的”。</p>

<p>但是处理器的设计者为了追求性能，将保证程序正确的任务丢给程序开发者。只在硬件级别提供了一些fence、cas等指令，基于这些指令操作内核和语言基础库实现了各种synchronization机制，用来保证操作系统的正确性和应用程序的正确性。程序员必须小心谨慎地使用线程和这些synchronization机制，否则就会出各种意想不到的问题。如果你没有debug一个多线程bug连续加班两天，那说明你是大神。</p>

<p>这些指令都是具有更高一致性级别，也就是linearizability，（关于linearizability可以参见我的另外一篇文章《线性一致性（Linearizability）是并发控制的基础》[2]），虽然一致性级别高，但只是个别指令的，处理器整体只是实现了比sequential consistency低很多的一致性级别。所以实现难度大大降低了。</p>

<p>虽然Lamport老爷子的sequential consistency概念在concurrent programming领域中还没有实际意义，但却给我们指出了程序员的天堂在哪里。在程序员的天堂里，没有多（车）线（来）程（车）编（往）程，只要写程序就行。你面试的时候不会再有人问你多线程编程，不会再问你各种锁。</p>

<p>在分布式领域中，sequential consistency更实际一些。Zookeeper就实现了sequential consistency。同理，这应该也是可以证明的，但目前还没发现有Zookeeper社区有任何论文来证明这个。如果你已经明白上面解释的定义，就可以想清楚Zookeeper是sequential consistency。欢迎大家一起来探讨。</p>

<p>*<strong><em>为何Zookeeper要实现</em></strong>*<br/>
*<strong><em>sequential consistency</em></strong>*</p>

<p>实际上，Zookeeper的一致性更复杂一些，Zookeeper的读操作是sequential consistency的，Zookeeper的写操作是linearizability的。关于这个说法，Zookeeper的官方文档中没有写出来，但在社区的邮件组有详细的讨论（邮件组的讨论参见[3]）。</p>

<p>另外，在 <em>Modular Composition of Coordination Services</em> 这篇关于Zookeeper的论文中也有提到这个观点（这篇论文不是Zookeeper的主流论文，但全面分析了Zookeeper的特性，以及Zookeeper跨机房方案，饿了么的Zookeeper异地多活改造也参考了这篇论文中的一些观点），我们可以这么理解Zookeeper，从整体（read操作+write操作）上来说是sequential consistency，写操作实现了linearizability。</p>

<p>通过简单的推理，我们可以得出Lamport论文中的小例子，在Zookeeper中也是成立的。我们可以这样实现分布式锁。但Zookeeper官方推荐的分布式实现方法并没有采用这个方式，而是利用了Zookeeper的linearizability特性实现了分布式锁（关于Zookeeper官方是如何实现分布式锁的，请参见我这篇文章《Zookeeper实现分布式锁和选主》[4]）。</p>

<p>为什么Zookeeper要实现sequential consistency？Zookeeper最核心的功能是用来做coordination service，也就是用来做分布式锁服务，在分布式的环境下，Zookeeper本身怎么做到“天然正确”？没有其他的synchronization机制保证Zookeeper是正确的，所以只要Zookeeper实现了sequential consistency，那它自身就可以保证正确性，从而对外提供锁服务。 </p>

<p>*<strong><em>参考文章：</em></strong>*<br/>
[1]<a href="http://zookeeper.apache.org/doc/r3.4.9/zookeeperProgrammers.html#ch_zkGuarantees">http://zookeeper.apache.org/doc/r3.4.9/zookeeperProgrammers.html#ch_zkGuarantees</a><br/>
[2]<a href="https://blog.csdn.net/cadem/article/details/79932574">https://blog.csdn.net/cadem/article/details/79932574</a><br/>
[3]<a href="http://comments.gmane.org/gmane.comp.java.hadoop.zookeeper.user/5221">http://comments.gmane.org/gmane.comp.java.hadoop.zookeeper.user/5221</a><br/>
[4]<a href="https://blog.csdn.net/cadem/article/details/56289825">https://blog.csdn.net/cadem/article/details/56289825</a></p>

<p><strong>近期热文</strong><br/>
<a href="http://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650767616&amp;idx=1&amp;sn=5ef24be557292923c5ce8c0daa24f2cf&amp;chksm=f3f93495c48ebd83f9a0a70c67abcb5785362606ccdde8751fccd2ba1cf62b43913f2d5026df&amp;scene=21#wechat_redirect">_深入浅出分布式缓存的通用方法_</a><br/>
<a href="http://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650767604&amp;idx=1&amp;sn=c3e41c1f9f645b1db91e90d87cbd6953&amp;chksm=f3f93561c48ebc77bf04cf44c22710d487f16a3e43bb75e4c2d0fe380a80606a3e97f5948b79&amp;scene=21#wechat_redirect">_一文详解消息队列的常见功能场景与使用精髓_</a><br/>
<a href="http://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650767596&amp;idx=1&amp;sn=4c438f4bd2e65d0b9cd92150380d8b18&amp;chksm=f3f93579c48ebc6f7a306c84bdaa9e2b038ed82ccb37440eafb2f5ff99aa1db06bf97f4e71a5&amp;scene=21#wechat_redirect">_MySQL上云后引发的雪崩_</a><br/>
<a href="http://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650767592&amp;idx=1&amp;sn=d2c79676981c63d70d7275486c88abdf&amp;chksm=f3f9357dc48ebc6b50f01ee6c4329df0ea1c4b0d416969911f52805dc3b44ec408f75d250b37&amp;scene=21#wechat_redirect">_方法论与技术栈双管齐下的运维可用性能力建设_</a><br/>
<a href="http://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650767557&amp;idx=1&amp;sn=9a1935096cbe2fbd658dc28c8f52a402&amp;chksm=f3f93550c48ebc462c16e23d3e6941b28fd71ce50c790d8e39188e20f57f4a34b3e01ef22409&amp;scene=21#wechat_redirect">_中小型企业大数据体系建设的核心技术选型_</a></p>

<p><strong>近期活动</strong><br/>
<a href="http://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650767615&amp;idx=1&amp;sn=243dd45953cad2716f389bf25f99a01d&amp;chksm=f3f9356ac48ebc7c4ed927bfa23def9a14f927d08e4e255c026c9587de83a69062cfc4b057ad&amp;scene=21#wechat_redirect">_2018 DAMS中国数据资产管理峰会_</a><br/>
微信扫一扫<br/>
关注该公众号</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[消息队列之 RabbitMQ]]></title>
    <link href="http://panlw.github.io/15274392082175.html"/>
    <updated>2018-05-28T00:40:08+08:00</updated>
    <id>http://panlw.github.io/15274392082175.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://juejin.im/post/5a67f7836fb9a01cb74e8931">原文地址</a></p>
</blockquote>

<p>关于消息队列，从前年开始断断续续看了些资料，想写很久了，但一直没腾出空，近来分别碰到几个朋友聊这块的技术选型，是时候把这块的知识整理记录一下了。</p>

<p>市面上的消息队列产品有很多，比如老牌的 ActiveMQ、RabbitMQ ，目前我看最火的 Kafka ，还有 ZeroMQ ，去年底阿里巴巴捐赠给 Apache 的 RocketMQ ，连 redis 这样的 NoSQL 数据库也支持 MQ 功能。总之这块知名的产品就有十几种，就我自己的使用经验和兴趣只打算谈谈 RabbitMQ、Kafka 和 ActiveMQ ，本文先讲 RabbitMQ ，在此之前先看下消息队列的相关概念。</p>

<h1 id="toc_0">什么叫消息队列</h1>

<p>消息（Message）是指在应用间传送的数据。消息可以非常简单，比如只包含文本字符串，也可以更复杂，可能包含嵌入对象。</p>

<p>消息队列（Message Queue）是一种应用间的通信方式，消息发送后可以立即返回，由消息系统来确保消息的可靠传递。消息发布者只管把消息发布到 MQ 中而不用管谁来取，消息使用者只管从 MQ 中取消息而不管是谁发布的。这样发布者和使用者都不用知道对方的存在。</p>

<h1 id="toc_1">为何用消息队列</h1>

<p>从上面的描述中可以看出消息队列是一种应用间的异步协作机制，那什么时候需要使用 MQ 呢？</p>

<p>以常见的订单系统为例，用户点击【下单】按钮之后的业务逻辑可能包括：扣减库存、生成相应单据、发红包、发短信通知。在业务发展初期这些逻辑可能放在一起同步执行，随着业务的发展订单量增长，需要提升系统服务的性能，这时可以将一些不需要立即生效的操作拆分出来异步执行，比如发放红包、发短信通知等。这种场景下就可以用 MQ ，在下单的主流程（比如扣减库存、生成相应单据）完成之后发送一条消息到 MQ 让主流程快速完结，而由另外的单独线程拉取 MQ 的消息（或者由 MQ 推送消息），当发现 MQ 中有发红包或发短信之类的消息时，执行相应的业务逻辑。</p>

<p>以上是用于业务解耦的情况，其它常见场景包括最终一致性、广播、错峰流控等等。</p>

<h1 id="toc_2">RabbitMQ 特点</h1>

<p>RabbitMQ 是一个由 Erlang 语言开发的 AMQP 的开源实现。</p>

<p>AMQP ：Advanced Message Queue，高级消息队列协议。它是应用层协议的一个开放标准，为面向消息的中间件设计，基于此协议的客户端与消息中间件可传递消息，并不受产品、开发语言等条件的限制。</p>

<p>RabbitMQ 最初起源于金融系统，用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。具体特点包括：</p>

<ol>
<li><p>可靠性（Reliability） RabbitMQ 使用一些机制来保证可靠性，如持久化、传输确认、发布确认。</p></li>
<li><p>灵活的路由（Flexible Routing） 在消息进入队列之前，通过 Exchange 来路由消息的。对于典型的路由功能，RabbitMQ 已经提供了一些内置的 Exchange 来实现。针对更复杂的路由功能，可以将多个 Exchange 绑定在一起，也通过插件机制实现自己的 Exchange 。</p></li>
<li><p>消息集群（Clustering） 多个 RabbitMQ 服务器可以组成一个集群，形成一个逻辑 Broker 。</p></li>
<li><p>高可用（Highly Available Queues） 队列可以在集群中的机器上进行镜像，使得在部分节点出问题的情况下队列仍然可用。</p></li>
<li><p>多种协议（Multi-protocol） RabbitMQ 支持多种消息队列协议，比如 STOMP、MQTT 等等。</p></li>
<li><p>多语言客户端（Many Clients） RabbitMQ 几乎支持所有常用语言，比如 Java、.NET、Ruby 等等。</p></li>
<li><p>管理界面（Management UI） RabbitMQ 提供了一个易用的用户界面，使得用户可以监控和管理消息 Broker 的许多方面。</p></li>
<li><p>跟踪机制（Tracing） 如果消息异常，RabbitMQ 提供了消息跟踪机制，使用者可以找出发生了什么。</p></li>
<li><p>插件机制（Plugin System） RabbitMQ 提供了许多插件，来从多方面进行扩展，也可以编写自己的插件。</p></li>
</ol>

<h1 id="toc_3">RabbitMQ 中的概念</h1>

<h3 id="toc_4">消息模型</h3>

<p>所有 MQ 产品从模型抽象上来说都是一样的过程： 消费者（consumer）订阅某个队列。生产者（producer）创建消息，然后发布到队列（queue）中，最后将消息发送到监听的消费者。</p>

<p><img src="https://user-gold-cdn.xitu.io/2018/1/24/161260568dd200d6?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/></p>

<h3 id="toc_5">RabbitMQ 基本概念</h3>

<p>上面只是最简单抽象的描述，具体到 RabbitMQ 则有更详细的概念需要解释。上面介绍过 RabbitMQ 是 AMQP 协议的一个开源实现，所以其内部实际上也是 AMQP 中的基本概念：</p>

<p><img src="https://user-gold-cdn.xitu.io/2018/1/24/161260568dd66584?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/></p>

<ol>
<li> Message 消息，消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括 routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。</li>
<li> Publisher 消息的生产者，也是一个向交换器发布消息的客户端应用程序。</li>
<li> Exchange 交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。</li>
<li> Binding 绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。</li>
<li> Queue 消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。</li>
<li> Connection 网络连接，比如一个 TCP 连接。</li>
<li> Channel 信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的 TCP 连接内地虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。</li>
<li> Consumer 消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。</li>
<li> Virtual Host 虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。</li>
<li> Broker 表示消息队列服务器实体。</li>
</ol>

<h4 id="toc_6">AMQP 中的消息路由</h4>

<p>AMQP 中消息的路由过程和 Java 开发者熟悉的 JMS 存在一些差别，AMQP 中增加了 Exchange 和 Binding 的角色。生产者把消息发布到 Exchange 上，消息最终到达队列并被消费者接收，而 Binding 决定交换器的消息应该发送到那个队列。</p>

<p><img src="https://user-gold-cdn.xitu.io/2018/1/24/161260568e434217?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/></p>

<h4 id="toc_7">Exchange 类型</h4>

<p>Exchange 分发消息时根据类型的不同分发策略有区别，目前共四种类型：direct、fanout、topic、headers 。headers 匹配 AMQP 消息的 header 而不是路由键，此外 headers 交换器和 direct 交换器完全一致，但性能差很多，目前几乎用不到了，所以直接看另外三种类型：</p>

<ol>
<li> direct <img src="https://user-gold-cdn.xitu.io/2018/1/24/161260568dc498b6?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/> 消息中的路由键（routing key）如果和 Binding 中的 binding key 一致， 交换器就将消息发到对应的队列中。路由键与队列名完全匹配，如果一个队列绑定到交换机要求路由键为 “dog”，则只转发 routing key 标记为“dog” 的消息，不会转发 “dog.puppy”，也不会转发“dog.guard” 等等。它是完全匹配、单播的模式。</li>
<li> fanout <img src="https://user-gold-cdn.xitu.io/2018/1/24/161260568fe5ce35?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/> 每个发到 fanout 类型交换器的消息都会分到所有绑定的队列上去。fanout 交换器不处理路由键，只是简单的将队列绑定到交换器上，每个发送到交换器的消息都会被转发到与该交换器绑定的所有队列上。很像子网广播，每台子网内的主机都获得了一份复制的消息。fanout 类型转发消息是最快的。</li>
<li> topic <img src="https://user-gold-cdn.xitu.io/2018/1/24/161260569051565f?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/> topic 交换器通过模式匹配分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。它将路由键和绑定键的字符串切分成单词，这些单词之间用点隔开。它同样也会识别两个通配符：符号 “#” 和符号“<em>”。# 匹配 0 个或多个单词，</em> 匹配不多不少一个单词。</li>
</ol>

<h1 id="toc_8">RabbitMQ 安装</h1>

<p>一般来说安装 RabbitMQ 之前要安装 Erlang ，可以去 <a href="https://link.juejin.im?target=http%3A%2F%2Fwww.erlang.org%2Fdownloads">Erlang 官网</a>下载。接着去 <a href="https://link.juejin.im?target=https%3A%2F%2Fwww.rabbitmq.com%2Fdownload.html">RabbitMQ 官网</a>下载安装包，之后解压缩即可。根据操作系统不同官网提供了相应的安装说明：<a href="https://link.juejin.im?target=http%3A%2F%2Fwww.rabbitmq.com%2Finstall-windows.html">Windows</a>、<a href="https://link.juejin.im?target=http%3A%2F%2Fwww.rabbitmq.com%2Finstall-debian.html">Debian / Ubuntu</a>、<a href="https://link.juejin.im?target=http%3A%2F%2Fwww.rabbitmq.com%2Finstall-rpm.html">RPM-based Linux</a>、<a href="https://link.juejin.im?target=http%3A%2F%2Fwww.rabbitmq.com%2Finstall-standalone-mac.html">Mac</a></p>

<p>如果是 Mac 用户，个人推荐使用 HomeBrew 来安装，安装前要先更新 brew：</p>

<pre><code>brew update

</code></pre>

<p>接着安装 rabbitmq 服务器：</p>

<pre><code>brew install rabbitmq

</code></pre>

<p>这样 RabbitMQ 就安装好了，安装过程中会自动其所依赖的 Erlang 。</p>

<h1 id="toc_9">RabbitMQ 运行和管理</h1>

<ol>
<li> 启动 启动很简单，找到安装后的 RabbitMQ 所在目录下的 sbin 目录，可以看到该目录下有 6 个以 rabbitmq 开头的可执行文件，直接执行 rabbitmq-server 即可，下面将 RabbitMQ 的安装位置以 . 代替，启动命令就是：</li>
</ol>

<pre><code>./sbin/rabbitmq-server

</code></pre>

<p>启动正常的话会看到一些启动过程信息和最后的 completed with 7 plugins，这也说明启动的时候默认加载了 7 个插件。</p>

<p><img src="https://user-gold-cdn.xitu.io/2018/1/24/16126056ba03d9f0?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/></p>

<ol>
<li> 后台启动 如果想让 RabbitMQ 以守护程序的方式在后台运行，可以在启动的时候加上 -detached 参数：</li>
</ol>

<pre><code>./sbin/rabbitmq-server -detached

</code></pre>

<ol>
<li> 查询服务器状态 sbin 目录下有个特别重要的文件叫 rabbitmqctl ，它提供了 RabbitMQ 管理需要的几乎一站式解决方案，绝大部分的运维命令它都可以提供。 查询 RabbitMQ 服务器的状态信息可以用参数 status ：</li>
</ol>

<pre><code>./sbin/rabbitmqctl status

</code></pre>

<p>该命令将输出服务器的很多信息，比如 RabbitMQ 和 Erlang 的版本、OS 名称、内存等等</p>

<ol>
<li> 关闭 RabbitMQ 节点 我们知道 RabbitMQ 是用 Erlang 语言写的，在 Erlang 中有两个概念：节点和应用程序。节点就是 Erlang 虚拟机的每个实例，而多个 Erlang 应用程序可以运行在同一个节点之上。节点之间可以进行本地通信（不管他们是不是运行在同一台服务器之上）。比如一个运行在节点 A 上的应用程序可以调用节点 B 上应用程序的方法，就好像调用本地函数一样。如果应用程序由于某些原因奔溃，Erlang 节点会自动尝试重启应用程序。 如果要关闭整个 RabbitMQ 节点可以用参数 stop ：</li>
</ol>

<pre><code>./sbin/rabbitmqctl stop

</code></pre>

<p>它会和本地节点通信并指示其干净的关闭，也可以指定关闭不同的节点，包括远程节点，只需要传入参数 -n ：</p>

<pre><code>./sbin/rabbitmqctl -n rabbit@server.example.com stop 

</code></pre>

<p>-n node 默认 node 名称是 rabbit@server ，如果你的主机名是 server.example.com ，那么 node 名称就是 <a href="mailto:rabbit@server.example.com">rabbit@server.example.com</a> 。</p>

<ol>
<li> 关闭 RabbitMQ 应用程序 如果只想关闭应用程序，同时保持 Erlang 节点运行则可以用 stop_app：</li>
</ol>

<pre><code>./sbin/rabbitmqctl stop_app

</code></pre>

<p>这个命令在后面要讲的集群模式中将会很有用。</p>

<ol>
<li> 启动 RabbitMQ 应用程序</li>
</ol>

<pre><code>./sbin/rabbitmqctl start_app

</code></pre>

<ol>
<li> 重置 RabbitMQ 节点</li>
</ol>

<pre><code>./sbin/rabbitmqctl reset

</code></pre>

<p>该命令将清除所有的队列。</p>

<ol>
<li> 查看已声明的队列</li>
</ol>

<pre><code>./sbin/rabbitmqctl list_queues

</code></pre>

<ol>
<li> 查看交换器</li>
</ol>

<pre><code>./sbin/rabbitmqctl list_exchanges

</code></pre>

<p>该命令还可以附加参数，比如列出交换器的名称、类型、是否持久化、是否自动删除：</p>

<pre><code>./sbin/rabbitmqctl list_exchanges name type durable auto_delete

</code></pre>

<ol>
<li> 查看绑定</li>
</ol>

<pre><code>./sbin/rabbitmqctl list_bindings

</code></pre>

<h1 id="toc_10">Java 客户端访问</h1>

<p>RabbitMQ 支持多种语言访问，以 Java 为例看下一般使用 RabbitMQ 的步骤。</p>

<ol>
<li> maven 工程的 pom 文件中添加依赖</li>
</ol>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt;
    &lt;artifactId&gt;amqp-client&lt;/artifactId&gt;
    &lt;version&gt;4.1.0&lt;/version&gt;
&lt;/dependency&gt;

</code></pre>

<ol>
<li> 消息生产者</li>
</ol>

<pre><code>package org.study.rabbitmq;
import com.rabbitmq.client.Channel;
import com.rabbitmq.client.Connection;
import com.rabbitmq.client.ConnectionFactory;
import java.io.IOException;
import java.util.concurrent.TimeoutException;
public class Producer {

    public static void main(String[] args) throws IOException, TimeoutException {
        //创建连接工厂
        ConnectionFactory factory = new ConnectionFactory();
        factory.setUsername(&quot;guest&quot;);
        factory.setPassword(&quot;guest&quot;);
        //设置 RabbitMQ 地址
        factory.setHost(&quot;localhost&quot;);
        //建立到代理服务器到连接
        Connection conn = factory.newConnection();
        //获得信道
        Channel channel = conn.createChannel();
        //声明交换器
        String exchangeName = &quot;hello-exchange&quot;;
        channel.exchangeDeclare(exchangeName, &quot;direct&quot;, true);

        String routingKey = &quot;hola&quot;;
        //发布消息
        byte[] messageBodyBytes = &quot;quit&quot;.getBytes();
        channel.basicPublish(exchangeName, routingKey, null, messageBodyBytes);

        channel.close();
        conn.close();
    }
}

</code></pre>

<ol>
<li> 消息消费者</li>
</ol>

<pre><code>package org.study.rabbitmq;
import com.rabbitmq.client.*;
import java.io.IOException;
import java.util.concurrent.TimeoutException;
public class Consumer {

    public static void main(String[] args) throws IOException, TimeoutException {
        ConnectionFactory factory = new ConnectionFactory();
        factory.setUsername(&quot;guest&quot;);
        factory.setPassword(&quot;guest&quot;);
        factory.setHost(&quot;localhost&quot;);
        //建立到代理服务器到连接
        Connection conn = factory.newConnection();
        //获得信道
        final Channel channel = conn.createChannel();
        //声明交换器
        String exchangeName = &quot;hello-exchange&quot;;
        channel.exchangeDeclare(exchangeName, &quot;direct&quot;, true);
        //声明队列
        String queueName = channel.queueDeclare().getQueue();
        String routingKey = &quot;hola&quot;;
        //绑定队列，通过键 hola 将队列和交换器绑定起来
        channel.queueBind(queueName, exchangeName, routingKey);

        while(true) {
            //消费消息
            boolean autoAck = false;
            String consumerTag = &quot;&quot;;
            channel.basicConsume(queueName, autoAck, consumerTag, new DefaultConsumer(channel) {
                @Override
                public void handleDelivery(String consumerTag,
                                           Envelope envelope,
                                           AMQP.BasicProperties properties,
                                           byte[] body) throws IOException {
                    String routingKey = envelope.getRoutingKey();
                    String contentType = properties.getContentType();
                    System.out.println(&quot;消费的路由键：&quot; + routingKey);
                    System.out.println(&quot;消费的内容类型：&quot; + contentType);
                    long deliveryTag = envelope.getDeliveryTag();
                    //确认消息
                    channel.basicAck(deliveryTag, false);
                    System.out.println(&quot;消费的消息体内容：&quot;);
                    String bodyStr = new String(body, &quot;UTF-8&quot;);
                    System.out.println(bodyStr);

                }
            });
        }
    }
}

</code></pre>

<ol>
<li> 启动 RabbitMQ 服务器</li>
</ol>

<pre><code>./sbin/rabbitmq-server

</code></pre>

<ol>
<li> 运行 Consumer 先运行 Consumer ，这样当生产者发送消息的时候能在消费者后端看到消息记录。</li>
<li> 运行 Producer 接着运行 Producer , 发布一条消息，在 Consumer 的控制台能看到接收的消息： <img src="https://user-gold-cdn.xitu.io/2018/1/24/16126056b9f84c7d?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/></li>
</ol>

<h1 id="toc_11">RabbitMQ 集群</h1>

<p>RabbitMQ 最优秀的功能之一就是内建集群，这个功能设计的目的是允许消费者和生产者在节点崩溃的情况下继续运行，以及通过添加更多的节点来线性扩展消息通信吞吐量。RabbitMQ 内部利用 Erlang 提供的分布式通信框架 OTP 来满足上述需求，使客户端在失去一个 RabbitMQ 节点连接的情况下，还是能够重新连接到集群中的任何其他节点继续生产、消费消息。</p>

<h3 id="toc_12">RabbitMQ 集群中的一些概念</h3>

<p>RabbitMQ 会始终记录以下四种类型的内部元数据：</p>

<ol>
<li> 队列元数据 包括队列名称和它们的属性，比如是否可持久化，是否自动删除</li>
<li> 交换器元数据 交换器名称、类型、属性</li>
<li> 绑定元数据 内部是一张表格记录如何将消息路由到队列</li>
<li> vhost 元数据 为 vhost 内部的队列、交换器、绑定提供命名空间和安全属性</li>
</ol>

<p>在单一节点中，RabbitMQ 会将所有这些信息存储在内存中，同时将标记为可持久化的队列、交换器、绑定存储到硬盘上。存到硬盘上可以确保队列和交换器在节点重启后能够重建。而在集群模式下同样也提供两种选择：存到硬盘上（独立节点的默认设置），存在内存中。</p>

<p>如果在集群中创建队列，集群只会在单个节点而不是所有节点上创建完整的队列信息（元数据、状态、内容）。结果是只有队列的所有者节点知道有关队列的所有信息，因此当集群节点崩溃时，该节点的队列和绑定就消失了，并且任何匹配该队列的绑定的新消息也丢失了。还好 RabbitMQ 2.6.0 之后提供了镜像队列以避免集群节点故障导致的队列内容不可用。</p>

<p>RabbitMQ 集群中可以共享 user、vhost、exchange 等，所有的数据和状态都是必须在所有节点上复制的，例外就是上面所说的消息队列。RabbitMQ 节点可以动态的加入到集群中。</p>

<p>当在集群中声明队列、交换器、绑定的时候，这些操作会直到所有集群节点都成功提交元数据变更后才返回。集群中有内存节点和磁盘节点两种类型，内存节点虽然不写入磁盘，但是它的执行比磁盘节点要好。内存节点可以提供出色的性能，磁盘节点能保障配置信息在节点重启后仍然可用，那集群中如何平衡这两者呢？</p>

<p>RabbitMQ 只要求集群中至少有一个磁盘节点，所有其他节点可以是内存节点，当节点加入或离开集群时，它们必须要将该变更通知到至少一个磁盘节点。如果只有一个磁盘节点，刚好又是该节点崩溃了，那么集群可以继续路由消息，但不能创建队列、创建交换器、创建绑定、添加用户、更改权限、添加或删除集群节点。换句话说集群中的唯一磁盘节点崩溃的话，集群仍然可以运行，但直到该节点恢复，否则无法更改任何东西。</p>

<h3 id="toc_13">RabbitMQ 集群配置和启动</h3>

<p>如果是在一台机器上同时启动多个 RabbitMQ 节点来组建集群的话，只用上面介绍的方式启动第二、第三个节点将会因为节点名称和端口冲突导致启动失败。所以在每次调用 rabbitmq-server 命令前，设置环境变量 RABBITMQ_NODENAME 和 RABBITMQ_NODE_PORT 来明确指定唯一的节点名称和端口。下面的例子端口号从 5672 开始，每个新启动的节点都加 1，节点也分别命名为 test_rabbit_1、test_rabbit_2、test_rabbit_3。</p>

<p>启动第 1 个节点：</p>

<pre><code>RABBITMQ_NODENAME=test_rabbit_1 RABBITMQ_NODE_PORT=5672 ./sbin/rabbitmq-server -detached

</code></pre>

<p>启动第 2 个节点：</p>

<pre><code>RABBITMQ_NODENAME=test_rabbit_2 RABBITMQ_NODE_PORT=5673 ./sbin/rabbitmq-server -detached

</code></pre>

<p>启动第 2 个节点前建议将 RabbitMQ 默认激活的插件关掉，否则会存在使用了某个插件的端口号冲突，导致节点启动不成功。</p>

<p>现在第 2 个节点和第 1 个节点都是独立节点，它们并不知道其他节点的存在。集群中除第一个节点外后加入的节点需要获取集群中的元数据，所以要先停止 Erlang 节点上运行的 RabbitMQ 应用程序，并重置该节点元数据，再加入并且获取集群的元数据，最后重新启动 RabbitMQ 应用程序。</p>

<p>停止第 2 个节点的应用程序：</p>

<pre><code>./sbin/rabbitmqctl -n test_rabbit_2 stop_app

</code></pre>

<p>重置第 2 个节点元数据：</p>

<pre><code>./sbin/rabbitmqctl -n test_rabbit_2 reset

</code></pre>

<p>第 2 节点加入第 1 个节点组成的集群：</p>

<pre><code>./sbin/rabbitmqctl -n test_rabbit_2 join_cluster test_rabbit_1@localhost

</code></pre>

<p>启动第 2 个节点的应用程序</p>

<pre><code>./sbin/rabbitmqctl -n test_rabbit_2 start_app

</code></pre>

<p>第 3 个节点的配置过程和第 2 个节点类似：</p>

<pre><code>RABBITMQ_NODENAME=test_rabbit_3 RABBITMQ_NODE_PORT=5674 ./sbin/rabbitmq-server -detached

./sbin/rabbitmqctl -n test_rabbit_3 stop_app

./sbin/rabbitmqctl -n test_rabbit_3 reset

./sbin/rabbitmqctl -n test_rabbit_3 join_cluster test_rabbit_1@localhost

./sbin/rabbitmqctl -n test_rabbit_3 start_app

</code></pre>

<h3 id="toc_14">RabbitMQ 集群运维</h3>

<p>停止某个指定的节点，比如停止第 2 个节点：</p>

<pre><code>RABBITMQ_NODENAME=test_rabbit_2 ./sbin/rabbitmqctl stop

</code></pre>

<p>查看节点 3 的集群状态：</p>

<pre><code>./sbin/rabbitmqctl -n test_rabbit_3 cluster_status

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RPC 原理及 RPC 实例分析 - God is a Coder..]]></title>
    <link href="http://panlw.github.io/15274391142573.html"/>
    <updated>2018-05-28T00:38:34+08:00</updated>
    <id>http://panlw.github.io/15274391142573.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://my.oschina.net/hosee/blog/711632">原文地址</a></p>
</blockquote>

<p>在学校期间大家都写过不少程序，比如写个 hello world 服务类，然后本地调用下，如下所示。这些程序的特点是服务消费方和服务提供方是本地调用关系。</p>

<pre><code>public class Test {
     public static void main(String[] args) {
         HelloWorldService helloWorldService = new HelloWorldServiceImpl();
         helloWorldService.sayHello(&quot;test&quot;);
     }
}
</code></pre>

<p>而一旦踏入公司尤其是大型互联网公司就会发现，公司的系统都由成千上万大大小小的服务组成，各服务部署在不同的机器上，由不同的团队负责。</p>

<p>这时就会遇到两个问题：</p>

<ol>
<li> 要搭建一个新服务，免不了需要依赖他人的服务，而现在他人的服务都在远端，怎么调用？</li>
<li> 其它团队要使用我们的新服务，我们的服务该怎么发布以便他人调用？下文将对这两个问题展开探讨。</li>
</ol>

<h2 id="toc_0">1.  如何调用他人的远程服务？</h2>

<p>由于各服务部署在不同机器，服务间的调用免不了网络通信过程，服务消费方每调用一个服务都要写一坨网络通信相关的代码，不仅复杂而且极易出错。</p>

<p>如果有一种方式能让我们像调用本地服务一样调用远程服务，而让调用者对网络通信这些细节透明，那么将大大提高生产力，比如服务消费方在执行 helloWorldService.sayHello(&quot;test&quot;) 时，实质上调用的是远端的服务。这种方式其实就是 RPC（Remote Procedure Call Protocol），在各大互联网公司中被广泛使用，如阿里巴巴的 hsf、dubbo（开源）、Facebook 的 thrift（开源）、Google grpc（开源）、Twitter 的 finagle（开源）等。</p>

<p>要让网络通信细节对使用者透明，我们需要对通信细节进行封装，我们先看下一个 RPC 调用的流程涉及到哪些通信细节：</p>

<p><img src="http://static.oschina.net/uploads/space/2016/0714/102634_AAIe_2243330.png" alt=""/></p>

<ol>
<li> 服务消费方（client）调用以本地调用方式调用服务；</li>
<li> client stub 接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体；</li>
<li> client stub 找到服务地址，并将消息发送到服务端；</li>
<li> server stub 收到消息后进行解码；</li>
<li> server stub 根据解码结果调用本地的服务；</li>
<li> 本地服务执行并将结果返回给 server stub；</li>
<li> server stub 将返回结果打包成消息并发送至消费方；</li>
<li> client stub 接收到消息，并进行解码；</li>
<li> 服务消费方得到最终结果。</li>
</ol>

<p>RPC 的目标就是要 2~8 这些步骤都封装起来，让用户对这些细节透明。</p>

<h3 id="toc_1">1.1 怎么做到透明化远程服务调用？</h3>

<p>怎么封装通信细节才能让用户像以本地调用方式调用远程服务呢？对 java 来说就是使用代理！java 代理有两种方式：</p>

<ol>
<li> jdk 动态代理</li>
<li> 字节码生成</li>
</ol>

<p>尽管字节码生成方式实现的代理更为强大和高效，但代码维护不易，大部分公司实现 RPC 框架时还是选择动态代理方式。</p>

<p>下面简单介绍下动态代理怎么实现我们的需求。我们需要实现 RPCProxyClient 代理类，代理类的 invoke 方法中封装了与远端服务通信的细节，消费方首先从 RPCProxyClient 获得服务提供方的接口，当执行 helloWorldService.sayHello(&quot;test&quot;) 方法时就会调用 invoke 方法。</p>

<pre><code>public class RPCProxyClient implements java.lang.reflect.InvocationHandler{
    private Object obj;

    public RPCProxyClient(Object obj){
        this.obj=obj;
    }

    /**
     * 得到被代理对象;
     */
    public static Object getProxy(Object obj){
        return java.lang.reflect.Proxy.newProxyInstance(obj.getClass().getClassLoader(),
                obj.getClass().getInterfaces(), new RPCProxyClient(obj));
    }

    /**
     * 调用此方法执行
     */
    public Object invoke(Object proxy, Method method, Object[] args)
            throws Throwable {
        //结果参数;
        Object result = new Object();
        // ...执行通信相关逻辑
        // ...
        return result;
    }
}
</code></pre>

<pre><code>public class Test {
     public static void main(String[] args) {
         HelloWorldService helloWorldService = (HelloWorldService)RPCProxyClient.getProxy(HelloWorldService.class);
         helloWorldService.sayHello(&quot;test&quot;);
     }
 }
</code></pre>

<h3 id="toc_2">1.2  怎么对消息进行编码和解码？</h3>

<h4 id="toc_3">1.2.1 确定消息数据结构</h4>

<p>　　上节讲了 invoke 里需要封装通信细节（通信细节再后面几章详细探讨），而通信的第一步就是要确定客户端和服务端相互通信的消息结构。客户端的请求消息结构一般需要包括以下内容：</p>

<p>1）接口名称</p>

<p>　　在我们的例子里接口名是 “HelloWorldService”，如果不传，服务端就不知道调用哪个接口了；</p>

<p>2）方法名</p>

<p>　　一个接口内可能有很多方法，如果不传方法名服务端也就不知道调用哪个方法；</p>

<p>3）参数类型 &amp; 参数值</p>

<p>　　参数类型有很多，比如有 bool、int、long、double、string、map、list，甚至如 struct（class）；以及相应的参数值；</p>

<p>4）超时时间</p>

<p>5）requestID，标识唯一请求 id，在下面一节会详细描述 requestID 的用处。</p>

<p>　　同理服务端返回的消息结构一般包括以下内容。</p>

<p>1）返回值</p>

<p>2）状态 code</p>

<p>3）requestID </p>

<h4 id="toc_4">1.2.2 序列化</h4>

<p>一旦确定了消息的数据结构后，下一步就是要考虑序列化与反序列化了。</p>

<p>什么是序列化？序列化就是将数据结构或对象转换成二进制串的过程，也就是编码的过程。</p>

<p>什么是反序列化？将在序列化过程中所生成的二进制串转换成数据结构或者对象的过程。</p>

<p>为什么需要序列化？转换为二进制串后才好进行网络传输嘛！</p>

<p>为什么需要反序列化？将二进制转换为对象才好进行后续处理！</p>

<p>现如今序列化的方案越来越多，每种序列化方案都有优点和缺点，它们在设计之初有自己独特的应用场景，那到底选择哪种呢？从 RPC 的角度上看，主要看三点：</p>

<ol>
<li> 通用性，比如是否能支持 Map 等复杂的数据结构；</li>
<li> 性能，包括时间复杂度和空间复杂度，由于 RPC 框架将会被公司几乎所有服务使用，如果序列化上能节约一点时间，对整个公司的收益都将非常可观，同理如果序列化上能节约一点内存，网络带宽也能省下不少；</li>
<li> 可扩展性，对互联网公司而言，业务变化飞快，如果序列化协议具有良好的可扩展性，支持自动增加新的业务字段，而不影响老的服务，这将大大提供系统的灵活度。</li>
</ol>

<p>目前互联网公司广泛使用 Protobuf、Thrift、Avro 等成熟的序列化解决方案来搭建 RPC 框架，这些都是久经考验的解决方案。</p>

<h3 id="toc_5">1.3  通信</h3>

<p>消息数据结构被序列化为二进制串后，下一步就要进行网络通信了。目前有两种常用 IO 通信模型：1）BIO；2）<a href="http://my.oschina.net/hosee/blog/615269">NIO</a>。一般 RPC 框架需要支持这两种 IO 模型。</p>

<p>如何实现 RPC 的 IO 通信框架呢？</p>

<ol>
<li> 使用 java nio 方式自研，这种方式较为复杂，而且很有可能出现隐藏 bug，但也见过一些互联网公司使用这种方式；</li>
<li> 基于 mina，mina 在早几年比较火热，不过这些年版本更新缓慢；</li>
<li> 基于 netty，现在很多 RPC 框架都直接基于 netty 这一 IO 通信框架，省力又省心，比如阿里巴巴的 HSF、dubbo，Twitter 的 finagle 等。</li>
</ol>

<h3 id="toc_6">1.4  <strong>消息里为什么要有 requestID？</strong></h3>

<p>如果使用 netty 的话，一般会用 channel.writeAndFlush()方法来发送消息二进制串，这个方法调用后对于整个远程调用 (从发出请求到接收到结果) 来说是一个异步的，即对于当前线程来说，将请求发送出来后，线程就可以往后执行了，至于服务端的结果，是服务端处理完成后，再以消息的形式发送给客户端的。于是这里出现以下两个问题：</p>

<ol>
<li> 怎么让当前线程 “暂停”，等结果回来后，再向后执行？</li>
<li> 如果有多个线程同时进行远程方法调用，这时建立在 client server 之间的 socket 连接上会有很多双方发送的消息传递，前后顺序也可能是随机的，server 处理完结果后，将结果消息发送给 client，client 收到很多消息，怎么知道哪个消息结果是原先哪个线程调用的？</li>
</ol>

<p>如下图所示，线程 A 和线程 B 同时向 client socket 发送请求 requestA 和 requestB，socket 先后将 requestB 和 requestA 发送至 server，而 server 可能将 responseA 先返回，尽管 requestA 请求到达时间更晚。我们需要一种机制保证 responseA 丢给 ThreadA，responseB 丢给 ThreadB。</p>

<p><img src="http://static.oschina.net/uploads/space/2016/0714/104316_FAgB_2243330.png" alt=""/></p>

<p>怎么解决呢？</p>

<ol>
<li> client 线程每次通过 socket 调用一次远程接口前，生成一个唯一的 ID，即 requestID（requestID 必需保证在一个 Socket 连接里面是唯一的），一般常常使用 AtomicLong 从 0 开始累计数字生成唯一 ID；</li>
<li> 将处理结果的回调对象 callback，存放到全局 ConcurrentHashMap 里面 put(requestID, callback)；</li>
<li> 当线程调用 channel.writeAndFlush() 发送消息后，紧接着执行 callback 的 get() 方法试图获取远程返回的结果。在 get() 内部，则使用 synchronized 获取回调对象 callback 的锁，再先检测是否已经获取到结果，如果没有，然后调用 callback 的 wait() 方法，释放 callback 上的锁，让当前线程处于等待状态。</li>
<li> 服务端接收到请求并处理后，将 response 结果（此结果中包含了前面的 requestID）发送给客户端，客户端 socket 连接上专门监听消息的线程收到消息，分析结果，取到 requestID，再从前面的 ConcurrentHashMap 里面 get(requestID)，从而找到 callback 对象，再用 synchronized 获取 callback 上的锁，将方法调用结果设置到 callback 对象里，再调用 callback.notifyAll() 唤醒前面处于等待状态的线程。</li>
</ol>

<pre><code>public Object get() {
        synchronized (this) { // 旋锁
            while (!isDone) { // 是否有结果了
                wait(); //没结果是释放锁，让当前线程处于等待状态
            }
        }
}
</code></pre>

<pre><code>private void setDone(Response res) {
        this.res = res;
        isDone = true;
        synchronized (this) { //获取锁，因为前面wait()已经释放了callback的锁了
            notifyAll(); // 唤醒处于等待的线程
        }
    }
</code></pre>

<h2 id="toc_7">2 如何发布自己的服务？</h2>

<p>如何让别人使用我们的服务呢？有同学说很简单嘛，告诉使用者服务的 IP 以及端口就可以了啊。确实是这样，这里问题的关键在于是自动告知还是人肉告知。</p>

<p>人肉告知的方式：如果你发现你的服务一台机器不够，要再添加一台，这个时候就要告诉调用者我现在有两个 ip 了，你们要轮询调用来实现负载均衡；调用者咬咬牙改了，结果某天一台机器挂了，调用者发现服务有一半不可用，他又只能手动修改代码来删除挂掉那台机器的 ip。现实生产环境当然不会使用人肉方式。</p>

<p>有没有一种方法能实现自动告知，即机器的增添、剔除对调用方透明，调用者不再需要写死服务提供方地址？当然可以，现如今 zookeeper 被广泛用于实现服务自动注册与发现功能！</p>

<p>简单来讲，zookeeper 可以充当一个<code>服务注册表</code>（Service Registry），让多个<code>服务提供者</code>形成一个集群，让<code>服务消费者</code>通过服务注册表获取具体的服务访问地址（ip + 端口）去访问具体的服务提供者。如下图所示：</p>

<p><img src="http://static.oschina.net/uploads/space/2016/0714/105148_gSi2_2243330.png" alt=""/></p>

<p>具体来说，zookeeper 就是个分布式文件系统，每当一个服务提供者部署后都要将自己的服务注册到 zookeeper 的某一路径上: /{service}/{version}/{ip:port}, 比如我们的 HelloWorldService 部署到两台机器，那么 zookeeper 上就会创建两条目录：分别为 / HelloWorldService/1.0.0/100.19.20.01:16888  /HelloWorldService/1.0.0/100.19.20.02:16888。</p>

<p>zookeeper 提供了 “心跳检测” 功能，它会定时向各个服务提供者发送一个请求（实际上建立的是一个 Socket 长连接），如果长期没有响应，服务中心就认为该服务提供者已经“挂了”，并将其剔除，比如 100.19.20.02 这台机器如果宕机了，那么 zookeeper 上的路径就会只剩 / HelloWorldService/1.0.0/100.19.20.01:16888。</p>

<p>服务消费者会去监听相应路径（/HelloWorldService/1.0.0），一旦路径上的数据有任务变化（增加或减少），zookeeper 都会通知服务消费方服务提供者地址列表已经发生改变，从而进行更新。</p>

<p>更为重要的是 zookeeper 与生俱来的容错容灾能力（比如 leader 选举），可以确保服务注册表的高可用性。</p>

<h2 id="toc_8">3.Hadoop 中 <strong>RPC</strong> 实例分析</h2>

<p>ipc.RPC 类中有一些内部类，为了大家对 RPC 类有个初步的印象，就先罗列几个我们感兴趣的分析一下吧：</p>

<p><strong>Invocation</strong> ：用于封装方法名和参数，作为数据传输层。<br/>
<strong>ClientCache</strong> ：用于存储 client 对象，用 socket factory 作为 hash key, 存储结构为 hashMap <SocketFactory, Client>。<br/>
<strong>Invoker</strong> ：是动态代理中的调用实现类，继承了 InvocationHandler.<br/>
<strong>Server</strong> ：是 ipc.Server 的实现类。</p>

<pre><code>    public Object invoke(Object proxy, Method method, Object[] args)
      throws Throwable {
      •••
      ObjectWritable value = (ObjectWritable)
        client.call(new Invocation(method, args), remoteId);
      •••
      return value.get();
    }

</code></pre>

<p>如果你发现这个 invoke() 方法实现的有些奇怪的话，那你就对了。一般我们看到的<a href="http://my.oschina.net/hosee/blog/656945">动态代理</a>的 invoke() 方法中总会有 method.invoke(ac, arg);  这句代码。而上面代码中却没有，这是为什么呢？其实使用 method.invoke(ac, arg); 是在本地 JVM 中调用；而在 hadoop 中，是将数据发送给服务端，服务端将处理的结果再返回给客户端，所以这里的 invoke() 方法必然需要进行网络通信。而网络通信就是下面的这段代码实现的：</p>

<pre><code>ObjectWritable value = (ObjectWritable)
client.call(new Invocation(method, args), remoteId);

</code></pre>

<p>Invocation 类在这里封装了方法名和参数。其实这里网络通信只是调用了 Client 类的 call() 方法。那我们接下来分析一下 ipc.Client 源码吧。和第一章一样，同样是 3 个问题</p>

<ol>
<li> 客户端和服务端的连接是怎样建立的？</li>
<li> 客户端是怎样给服务端发送数据的？</li>
<li> 客户端是怎样获取服务端的返回数据的？</li>
</ol>

<h3 id="toc_9">3.1 客户端和服务端的连接是怎样建立的？</h3>

<pre><code>public Writable call(Writable param, ConnectionId remoteId)  
                       throws InterruptedException, IOException {
    Call call = new Call(param);       //将传入的数据封装成call对象
    Connection connection = getConnection(remoteId, call);   //获得一个连接
    connection.sendParam(call);     // 向服务端发送call对象
    boolean interrupted = false;
    synchronized (call) {
      while (!call.done) {
        try {
          call.wait(); // 等待结果的返回，在Call类的callComplete()方法里有notify()方法用于唤醒线程
        } catch (InterruptedException ie) {
          // 因中断异常而终止，设置标志interrupted为true
          interrupted = true;
        }
      }
      if (interrupted) {
        Thread.currentThread().interrupt();
      }

      if (call.error != null) {
        if (call.error instanceof RemoteException) {
          call.error.fillInStackTrace();
          throw call.error;
        } else { // 本地异常
          throw wrapException(remoteId.getAddress(), call.error);
        }
      } else {
        return call.value; //返回结果数据
      }
    }
  }

</code></pre>

<p>具体代码的作用我已做了注释，所以这里不再赘述。但到目前为止，你依然不知道 RPC 机制底层的网络连接是怎么建立的。分析代码后，我们会发现和网络通信有关的代码只会是下面的两句了：</p>

<pre><code>  Connection connection = getConnection(remoteId, call);   //获得一个连接
  connection.sendParam(call);      // 向服务端发送call对象

</code></pre>

<p>先看看是怎么获得一个到服务端的连接吧，下面贴出 ipc.Client 类中的 getConnection() 方法。</p>

<pre><code>private Connection getConnection(ConnectionId remoteId,
                                   Call call)
                                   throws IOException, InterruptedException {
    if (!running.get()) {
      // 如果client关闭了
      throw new IOException(&quot;The client is stopped&quot;);
    }
    Connection connection;
//如果connections连接池中有对应的连接对象，就不需重新创建了；如果没有就需重新创建一个连接对象。
//但请注意，该//连接对象只是存储了remoteId的信息，其实还并没有和服务端建立连接。
    do {
      synchronized (connections) {
        connection = connections.get(remoteId);
        if (connection == null) {
          connection = new Connection(remoteId);
          connections.put(remoteId, connection);
        }
      }
    } while (!connection.addCall(call)); //将call对象放入对应连接中的calls池，就不贴出源码了
   //这句代码才是真正的完成了和服务端建立连接哦~
    connection.setupIOstreams();
    return connection;
  }

</code></pre>

<p>下面贴出 Client.Connection 类中的 setupIOstreams() 方法：</p>

<pre><code>  private synchronized void setupIOstreams() throws InterruptedException {
   •••
      try {
       •••
        while (true) {
          setupConnection();  //建立连接
          InputStream inStream = NetUtils.getInputStream(socket);     //获得输入流
          OutputStream outStream = NetUtils.getOutputStream(socket);  //获得输出流
          writeRpcHeader(outStream);
          •••
          this.in = new DataInputStream(new BufferedInputStream
              (new PingInputStream(inStream)));   //将输入流装饰成DataInputStream
          this.out = new DataOutputStream
          (new BufferedOutputStream(outStream));   //将输出流装饰成DataOutputStream
          writeHeader();
          // 跟新活动时间
          touch();
          //当连接建立时，启动接受线程等待服务端传回数据，注意：Connection继承了Tread
          start();
          return;
        }
      } catch (IOException e) {
        markClosed(e);
        close();
      }
    }

</code></pre>

<p>再有一步我们就知道客户端的连接是怎么建立的啦，下面贴出 Client.Connection 类中的 setupConnection() 方法：</p>

<pre><code>  private synchronized void setupConnection() throws IOException {
      short ioFailures = 0;
      short timeoutFailures = 0;
      while (true) {
        try {
          this.socket = socketFactory.createSocket(); //终于看到创建socket的方法了
          this.socket.setTcpNoDelay(tcpNoDelay);
         •••
          // 设置连接超时为20s
          NetUtils.connect(this.socket, remoteId.getAddress(), 20000);
          this.socket.setSoTimeout(pingInterval);
          return;
        } catch (SocketTimeoutException toe) {
          /* 设置最多连接重试为45次。
           * 总共有20s*45 = 15 分钟的重试时间。
           */
          handleConnectionFailure(timeoutFailures++, 45, toe);
        } catch (IOException ie) {
          handleConnectionFailure(ioFailures++, maxRetries, ie);
        }
      }
    }

</code></pre>

<p>终于，我们知道了客户端的连接是怎样建立的了，其实就是创建一个普通的 socket 进行通信。</p>

<h3 id="toc_10">3.2 客户端是怎样给服务端发送数据的？ </h3>

<p>下面贴出 Client.Connection 类的 sendParam() 方法吧：</p>

<pre><code>public void sendParam(Call call) {
      if (shouldCloseConnection.get()) {
        return;
      }
      DataOutputBuffer d=null;
      try {
        synchronized (this.out) {
          if (LOG.isDebugEnabled())
            LOG.debug(getName() + &quot; sending #&quot; + call.id);
          //创建一个缓冲区
          d = new DataOutputBuffer();
          d.writeInt(call.id);
          call.param.write(d);
          byte[] data = d.getData();
          int dataLength = d.getLength();
          out.writeInt(dataLength);        //首先写出数据的长度
          out.write(data, 0, dataLength); //向服务端写数据
          out.flush();
        }
      } catch(IOException e) {
        markClosed(e);
      } finally {
        IOUtils.closeStream(d);
      }
    }  

</code></pre>

<h3 id="toc_11">3.3 客户端是怎样获取服务端的返回数据的？ </h3>

<p>下面贴出 Client.Connection 类和 Client.Call 类中的相关方法：</p>

<pre><code>方法一：  
  public void run() {
      •••
      while (waitForWork()) {
        receiveResponse();  //具体的处理方法
      }
      close();
     •••
}

方法二：
private void receiveResponse() {
      if (shouldCloseConnection.get()) {
        return;
      }
      touch();
      try {
        int id = in.readInt();                    // 阻塞读取id
        if (LOG.isDebugEnabled())
          LOG.debug(getName() + &quot; got value #&quot; + id);
          Call call = calls.get(id);    //在calls池中找到发送时的那个对象
        int state = in.readInt();     // 阻塞读取call对象的状态
        if (state == Status.SUCCESS.state) {
          Writable value = ReflectionUtils.newInstance(valueClass, conf);
          value.readFields(in);           // 读取数据
        //将读取到的值赋给call对象，同时唤醒Client等待线程，贴出setValue()代码方法三
          call.setValue(value);              
          calls.remove(id);               //删除已处理的call    
        } else if (state == Status.ERROR.state) {
        •••
        } else if (state == Status.FATAL.state) {
        •••
        }
      } catch (IOException e) {
        markClosed(e);
      }
}

方法三：
public synchronized void setValue(Writable value) {
      this.value = value;
      callComplete();   //具体实现
}
protected synchronized void callComplete() {
      this.done = true;
      notify();         // 唤醒client等待线程
    }

</code></pre>

<p>完成的功能主要是：启动一个处理线程，读取从服务端传来的 call 对象，将 call 对象读取完毕后，唤醒 client 处理线程。就这么简单，客户端就获取了服务端返回的数据了哦~。客户端的源码分析就到这里了哦，下面我们来分析 Server 端的源码吧。</p>

<h3 id="toc_12">3.4 ipc.Server 源码分析</h3>

<p>为了让大家对 ipc.Server 有个初步的了解，我们先分析一下它的几个内部类吧：</p>

<p><strong>Call</strong> ：用于存储客户端发来的请求<br/>
<strong>Listener</strong> ： 监听类，用于监听客户端发来的请求，同时 Listener 内部还有一个静态类，Listener.Reader，当监听器监听到用户请求，便让 Reader 读取用户请求。<br/>
<strong>Responder</strong> ：响应 RPC 请求类，请求处理完毕，由 Responder 发送给请求客户端。<br/>
<strong>Connection</strong> ：连接类，真正的客户端请求读取逻辑在这个类中。<br/>
<strong>Handler</strong> ：请求处理类，会循环阻塞读取 callQueue 中的 call 对象，并对其进行操作。</p>

<pre><code>private void initialize(Configuration conf) throws IOException {
   •••
    // 创建 rpc server
    InetSocketAddress dnSocketAddr = getServiceRpcServerAddress(conf);
    if (dnSocketAddr != null) {
      int serviceHandlerCount =
        conf.getInt(DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_KEY,
                    DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_DEFAULT);
      //获得serviceRpcServer
      this.serviceRpcServer = RPC.getServer(this, dnSocketAddr.getHostName(), 
          dnSocketAddr.getPort(), serviceHandlerCount,
          false, conf, namesystem.getDelegationTokenSecretManager());
      this.serviceRPCAddress = this.serviceRpcServer.getListenerAddress();
      setRpcServiceServerAddress(conf);
}
//获得server
    this.server = RPC.getServer(this, socAddr.getHostName(),
        socAddr.getPort(), handlerCount, false, conf, namesystem
        .getDelegationTokenSecretManager());

   •••
    this.server.start();  //启动 RPC server   Clients只允许连接该server
    if (serviceRpcServer != null) {
      serviceRpcServer.start();  //启动 RPC serviceRpcServer 为HDFS服务的server
    }
    startTrashEmptier(conf);
  }

</code></pre>

<p>查看 Namenode 初始化源码得知：RPC 的 server 对象是通过 ipc.RPC 类的 getServer() 方法获得的。下面咱们去看看 ipc.RPC 类中的 getServer() 源码吧：</p>

<pre><code>public static Server getServer(final Object instance, final String bindAddress, final int port,
                                 final int numHandlers,
                                 final boolean verbose, Configuration conf,
                                 SecretManager&lt;? extends TokenIdentifier&gt; secretManager) 
    throws IOException {
    return new Server(instance, conf, bindAddress, port, numHandlers, verbose, secretManager);
  }

</code></pre>

<p>这时我们发现 getServer()是一个创建 Server 对象的工厂方法，但创建的却是 RPC.Server 类的对象。哈哈，现在你明白了我前面说的 “RPC.Server 是 ipc.Server 的实现类” 了吧。不过 RPC.Server 的构造函数还是调用了 ipc.Server 类的构造函数的，因篇幅所限，就不贴出相关源码了。</p>

<p>初始化 Server 后，Server 端就运行起来了，看看 ipc.Server 的 start() 源码吧：</p>

<pre><code>  /** 启动服务 */
  public synchronized void start() {
    responder.start();  //启动responder
    listener.start();   //启动listener
    handlers = new Handler[handlerCount];

    for (int i = 0; i &lt; handlerCount; i++) {
      handlers[i] = new Handler(i);
      handlers[i].start();   //逐个启动Handler
    }
  }

</code></pre>

<p>分析过 ipc.Client 源码后，我们知道 Client 端的底层通信直接采用了阻塞式 IO 编程，当时我们曾做出猜测：Server 端是不是也采用了阻塞式 IO。现在我们仔细地分析一下吧，如果 Server 端也采用阻塞式 IO，当连接进来的 Client 端很多时，势必会影响 Server 端的性能。hadoop 的实现者们考虑到了这点，所以他们采用了 java  NIO 来实现 Server 端，那 Server 端采用 java NIO 是怎么建立连接的呢？分析源码得知，Server 端采用 Listener 监听客户端的连接，下面先分析一下 Listener 的构造函数吧：</p>

<pre><code>    public Listener() throws IOException {
      address = new InetSocketAddress(bindAddress, port);
      // 创建ServerSocketChannel,并设置成非阻塞式
      acceptChannel = ServerSocketChannel.open();
      acceptChannel.configureBlocking(false);

      // 将server socket绑定到本地端口
      bind(acceptChannel.socket(), address, backlogLength);
      port = acceptChannel.socket().getLocalPort(); 
      // 获得一个selector
      selector= Selector.open();
      readers = new Reader[readThreads];
      readPool = Executors.newFixedThreadPool(readThreads);
      //启动多个reader线程，为了防止请求多时服务端响应延时的问题
      for (int i = 0; i &lt; readThreads; i++) {       
        Selector readSelector = Selector.open();
        Reader reader = new Reader(readSelector);
        readers[i] = reader;
        readPool.execute(reader);
      }
      // 注册连接事件
      acceptChannel.register(selector, SelectionKey.OP_ACCEPT);
      this.setName(&quot;IPC Server listener on &quot; + port);
      this.setDaemon(true);
    }

</code></pre>

<p>在启动 Listener 线程时，服务端会一直等待客户端的连接，下面贴出 Server.Listener 类的 run() 方法：</p>

<pre><code>  public void run() {
     •••
      while (running) {
        SelectionKey key = null;
        try {
          selector.select();
          Iterator&lt;SelectionKey&gt; iter = selector.selectedKeys().iterator();
          while (iter.hasNext()) {
            key = iter.next();
            iter.remove();
            try {
              if (key.isValid()) {
                if (key.isAcceptable())
                  doAccept(key);     //具体的连接方法
              }
            } catch (IOException e) {
            }
            key = null;
          }
        } catch (OutOfMemoryError e) {
       •••         
    }

</code></pre>

<p>下面贴出 Server.Listener 类中 doAccept() 方法中的关键源码吧：</p>

<pre><code>    void doAccept(SelectionKey key) throws IOException,  OutOfMemoryError {
      Connection c = null;
      ServerSocketChannel server = (ServerSocketChannel) key.channel();
      SocketChannel channel;
      while ((channel = server.accept()) != null) { //建立连接
        channel.configureBlocking(false);
        channel.socket().setTcpNoDelay(tcpNoDelay);
        Reader reader = getReader();  //从readers池中获得一个reader
        try {
          reader.startAdd(); // 激活readSelector，设置adding为true
          SelectionKey readKey = reader.registerChannel(channel);//将读事件设置成兴趣事件
          c = new Connection(readKey, channel, System.currentTimeMillis());//创建一个连接对象
          readKey.attach(c);   //将connection对象注入readKey
          synchronized (connectionList) {
            connectionList.add(numConnections, c);
            numConnections++;
          }
        ••• 
        } finally {
//设置adding为false，采用notify()唤醒一个reader,其实代码十三中启动的每个reader都使
//用了wait()方法等待。因篇幅有限，就不贴出源码了。
          reader.finishAdd();
        }
      }
    }

</code></pre>

<p>当 reader 被唤醒，reader 接着执行 doRead() 方法。</p>

<p>下面贴出 Server.Listener.Reader 类中的 doRead() 方法和 Server.Connection 类中的 readAndProcess() 方法源码：</p>

<pre><code>方法一：   
 void doRead(SelectionKey key) throws InterruptedException {
      int count = 0;
      Connection c = (Connection)key.attachment();  //获得connection对象
      if (c == null) {
        return;  
      }
      c.setLastContact(System.currentTimeMillis());
      try {
        count = c.readAndProcess();    // 接受并处理请求  
      } catch (InterruptedException ieo) {
       •••
      }
     •••    
}

方法二：
public int readAndProcess() throws IOException, InterruptedException {
      while (true) {
        •••
        if (!rpcHeaderRead) {
          if (rpcHeaderBuffer == null) {
            rpcHeaderBuffer = ByteBuffer.allocate(2);
          }
         //读取请求头
          count = channelRead(channel, rpcHeaderBuffer);
          if (count &lt; 0 || rpcHeaderBuffer.remaining() &gt; 0) {
            return count;
          }
        // 读取请求版本号  
          int version = rpcHeaderBuffer.get(0);
          byte[] method = new byte[] {rpcHeaderBuffer.get(1)};
        •••  

          data = ByteBuffer.allocate(dataLength);
        }
        // 读取请求  
        count = channelRead(channel, data);

        if (data.remaining() == 0) {
         •••
          if (useSasl) {
         •••
          } else {
            processOneRpc(data.array());//处理请求
          }
        •••
          }
        } 
        return count;
      }
    }

</code></pre>

<p>下面贴出 Server.Connection 类中的 processOneRpc() 方法和 processData() 方法的源码。</p>

<pre><code>方法一：   
 private void processOneRpc(byte[] buf) throws IOException,
        InterruptedException {
      if (headerRead) {
        processData(buf);
      } else {
        processHeader(buf);
        headerRead = true;
        if (!authorizeConnection()) {
          throw new AccessControlException(&quot;Connection from &quot; + this
              + &quot; for protocol &quot; + header.getProtocol()
              + &quot; is unauthorized for user &quot; + user);
        }
      }
}
方法二：
    private void processData(byte[] buf) throws  IOException, InterruptedException {
      DataInputStream dis =
        new DataInputStream(new ByteArrayInputStream(buf));
      int id = dis.readInt();      // 尝试读取id
      Writable param = ReflectionUtils.newInstance(paramClass, conf);//读取参数
      param.readFields(dis);        

      Call call = new Call(id, param, this);  //封装成call
      callQueue.put(call);   // 将call存入callQueue
      incRpcCount();  // 增加rpc请求的计数
    }

</code></pre>

<h2 id="toc_13">4. RPC 与 web service</h2>

<p>RPC：</p>

<p><img src="http://static.oschina.net/uploads/space/2016/0714/114103_HQGm_2243330.png" alt=""/></p>

<p>Web service<img src="http://static.oschina.net/uploads/space/2016/0714/114022_sKwT_2243330.png" alt=""/></p>

<p>web service 接口就是 RPC 中的 stub 组件，规定了 server 能够提供的服务（web service），这在 server 和 client 上是一致的，但是也是跨语言跨平台的。同时，由于 web service 规范中的 WSDL 文件的存在，现在各平台的 web service 框架，都可以基于 WSDL 文件，自动生成 web service 接口 。</p>

<p>其实两者差不多，只是传输的协议不同。</p>

<h2 id="toc_14">Reference：</h2>

<p>1. <a href="http://www.cnblogs.com/LBSer/p/4853234.html">http://www.cnblogs.com/LBSer/p/4853234.html</a><br/>
2. <a href="http://weixiaolu.iteye.com/blog/1504898">http://weixiaolu.iteye.com/blog/1504898</a><br/>
3. <a href="http://kyfxbl.iteye.com/blog/1745550">http://kyfxbl.iteye.com/blog/1745550</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kubernetes API 使用文档]]></title>
    <link href="http://panlw.github.io/15274389377310.html"/>
    <updated>2018-05-28T00:35:37+08:00</updated>
    <id>http://panlw.github.io/15274389377310.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/">原文地址</a></p>
</blockquote>

<h2 id="toc_0">API 概述</h2>

<h4 id="toc_1">※ 资源分类</h4>

<p>这是对 Kubernetes API 及其主要功能提供的基本资源类型的高级概述。</p>

<ul>
<li>  <strong>Workloads</strong>： 用于在集群中管理和运行容器；</li>
<li>  <strong>Discovery &amp; LB</strong>：用于将 Workloads “缝合” 到一个可从外部访问的负载均衡的服务中；</li>
<li>  <strong>Config &amp; Storage</strong>：用于将初始化数据注入到应用程序中，并保留容器外部的数据；</li>
<li>  <strong>Cluster</strong>：用于定义如何配置集群，这些通常仅由集群运营商使用；</li>
<li>  <strong>Metadata</strong>：用于配置群集内其他资源的行为，例如用于扩展 Workloads 的 HorizontalPodAutoscaler</li>
</ul>

<h4 id="toc_2">※ 资源对象</h4>

<p>资源对象通常包含 3 个组件：</p>

<ul>
<li>  <strong>ResourceSpec</strong>：这由用户定义并描述系统所期望的状态，在创建或更新对象时填写；</li>
<li>  <strong>ResourceStatus</strong>：这由服务器填写并报告系统的当前状态，只有 Kubernetes 组件才能填写此内容；</li>
<li>  <strong>Resource ObjectMeta</strong>：这是关于资源的元数据，例如 name / type / api version / annotations / labels，包含可能由后端用户和系统更新的字段（如 annotations）</li>
</ul>

<h4 id="toc_3">※ 资源操作</h4>

<p>大部分资源提供如下操作：</p>

<p><strong>▶ Create</strong></p>

<p>Create 操作会在存储后端创建资源。资源创建后，系统将应用所期望的状态。</p>

<p><strong>▶ Update</strong></p>

<p>更新有 2 种形式：<strong>Replace</strong> 和 <strong>Patch</strong></p>

<p><strong>Replace</strong>：替换资源对象将通过提供的 spec 替换现有的 spec 来更新资源。对于读写操作，这是安全的，因为如果资源在读和写之间被修改，则会发生乐观锁定失败。注意：ResourceStatus 将被系统忽略，并且不会被更新，要更新状态，必须调用特定的状态更新操作。</p>

<p>注意：替换资源对象可能不会立即传播到下游对象。例如，替换 ConfigMap 或 Secret 资源不会导致所有 Pod 看到更改，除非 Pod 在带外重新启动。</p>

<p><strong>Patch</strong>：Patch 会更改特定字段，如何合并更改是每个字段定义的。列表可以被替换或合并，合并列表不会保留排序。</p>

<p><strong>Patch 操作永远不会导致乐观锁定失败，并且最后一次写入会获胜。</strong>如果在更新之前未读取完整状态，或者乐观锁定失败不可取，则建议使用 patch 操作。修补复杂 types、arrays 和 maps 时，如何应用修补程序是以每个字段为基础定义的，并可以替换字段的当前值，也可以将内容合并到当前值中。</p>

<p><strong>▶ Read</strong></p>

<p>读取有 3 种方式：<strong>Get</strong> 、 <strong>List</strong> 和 <strong>Watch</strong></p>

<p><strong>Get</strong>： 按名称检索特定的资源对象；</p>

<p><strong>List</strong>：检索命名空间内特定类型的所有资源对象，并且结果可以限制为与选择器查询结果匹配的资源；</p>

<p><strong>List All Namespaces</strong>：像 List 一样，但是跨所有命名空间检索资源；</p>

<p><strong>Watch</strong>：Watch 将在对象更新时汇出结果，类似于回调，watch 用于响应资源的更改。</p>

<p><strong>▶ Delete</strong></p>

<p>Delete 将删除资源。根据特定的资源，子对象可能会或可能不会被服务器当做垃圾收集，详情请参阅特定资源对象的注释。</p>

<p><strong>▶ 额外操作</strong></p>

<p>资源可以定义特定于该资源类型的附加操作。</p>

<p><strong>Rollback</strong>：将 PodTemplate 回滚到以前的版本，仅适用于某些资源类型；</p>

<p><strong>Read / Write Scale</strong>：读取或更新给定资源的副本数量，仅适用于某些资源类型；</p>

<p><strong>Read / Write Status</strong>：读取或更新资源对象的状态，状态只能通过这些更新操作进行更改。</p>

<h2 id="toc_4">WORKLOADS</h2>

<p>Workloads 资源负责管理和运行集群上的容器。容器（Containers）由控制器（Controllers）通过 Pod 创建，Pods 运行容器并提供环境依赖，如注入到容器中的共享或永久存储卷、配置或加密数据。</p>

<p>最常见的控制器是：</p>

<ul>
<li>  Deployments 无状态持久应用（如 http servers）</li>
<li>  StatefulSets 有状态持久应用（如 database）</li>
<li>  Jobs 运行至完成的应用 （如 batch jobs）</li>
</ul>

<h2 id="toc_5">DISCOVERY &amp; LOAD BALANCING</h2>

<p>Discovery and Load Balancing 负责将 Workloads “缝合” 到一个可从外部访问的负载均衡的服务中。默认情况下，Workloads 只能在群集内访问，它们必须通过 LoadBalancer 或 NodePort Service 暴露到外部。对于开发，可以使用 <code>kubectl proxy</code>命令通过代理 api 主机访问内部可访问的 Workloads 。</p>

<p>常用的资源类型：</p>

<ul>
<li>  Services 提供跨多个 Workload 副本的负载均衡的单个 IP 端点。</li>
<li>  Ingress 提供路由到一个或多个服务的 https(s) 端点</li>
</ul>

<h2 id="toc_6">CONFIG &amp; STORAGE</h2>

<p>Config and Storage 资源负责将数据注入到应用程序中，并保留容器外部的数据。</p>

<p>常用的资源类型：</p>

<ul>
<li>  ConfigMaps 通过环境变量、命令行参数或文件提供注入应用的 K-V 键值对</li>
<li>  Secrets 通过文件提供注入应用的二进制数据</li>
<li>  Volumes 提供容器外部的文件系统。 可能在同一个 Pod 中跨 Container 容器共享，并且其寿命持续超出 Container 或 Pod。</li>
</ul>

<h2 id="toc_7">METADATA</h2>

<p>Metadata resources 负责集群内其他资源的行为。</p>

<p>常用的资源类型：</p>

<ul>
<li>  HorizontalPodAutoscaler 自动缩放 workloads 的副本数量以响应加载</li>
<li>  PodDisruptionBudget 在执行维护时，可以配置给定 workloads 中的多少副本可能同时不可用</li>
<li>  ThirdPartyResource 使用自己的类型扩展 Kubernetes API</li>
<li>  Event 群集中资源生命周期事件的通知</li>
</ul>

<h2 id="toc_8">API 调用方式</h2>

<p><strong>▶ kubectl</strong></p>

<p>示例：创建 Deployment</p>

<pre><code>1.  `$ echo &#39;apiVersion: apps/v1`

2.  `kind: Deployment`

3.  `metadata:`

4.  `name: deployment-example`

5.  `spec:`

6.  `replicas: 3`

7.  `revisionHistoryLimit: 10`

8.  `template:`

9.  `metadata:`

10.  `labels:`

11.  `app: nginx`

12.  `spec:`

13.  `containers:`

14.  `- name: nginx`

15.  `image: nginx:1.10`

16.  `ports:`

17.  `- containerPort: 80`

18.  `&#39; | kubectl create -f -`

</code></pre>

<p>yaml 文件编写的规则拆解如下图：</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/wbiax4xEAl5xuUnZt7eHIfEe51sP9P3DjdKKVzb6BmPcZAQzsLM5ZM6SsD46djF4siaLHADeaQnrT6qOJDGSCkibw/640?wx_fmt=png" alt=""/></p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/wbiax4xEAl5xuUnZt7eHIfEe51sP9P3DjrdW9EOgJy7BickNuoCKaX7R5t1TZHqDF8RN8qSCRzse8nRDO23IA66w/640?wx_fmt=png" alt=""/></p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/wbiax4xEAl5xuUnZt7eHIfEe51sP9P3DjEr3QHuRt71DVQHpNZHibDhFnDibRvT6ghUdMvCOLqSuUIZYO7YXWZyLg/640?wx_fmt=png" alt=""/></p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/wbiax4xEAl5xuUnZt7eHIfEe51sP9P3Djl2MJd16ycvpxFCy7ibcQmaqMVo0ndFic3xlVqvyzwFtl4VTw6y3WuRdg/640?wx_fmt=png" alt=""/></p>

<p>其他资源的创建或其他操作，都可以按照这种方式来操作。</p>

<p><strong>▶ curl</strong></p>

<p>需要使用 <code>kubectl proxy</code></p>

<pre data-initialized="true" data-gclp-id="6" style="box-sizing: border-box;margin-top: 0px;margin-bottom: 0px;padding: 8px 0px 6px;background-color: rgb(241, 239, 238);border-radius: 0px;overflow-y: auto;color: rgb(80, 97, 109);font-size: 10px;line-height: 12px;">

1.  `$ kubectl proxy`

2.  `$ curl -X POST -H 'Content-Type: application/yaml' --data '`

3.  `apiVersion: apps/v1beta1`

4.  `kind: Deployment`

5.  `metadata:`

6.  `name: deployment-example`

7.  `spec:`

8.  `replicas: 3`

9.  `revisionHistoryLimit: 10`

10.  `template:`

11.  `metadata:`

12.  `labels:`

13.  `app: nginx`

14.  `spec:`

15.  `containers:`

16.  `- name: nginx`

17.  `image: nginx:1.10`

18.  `ports:`

19.  `- containerPort: 80`

20.  `' http://127.0.0.1:8001/apis/apps/v1/namespaces/default/deployments` 

</pre>

<p><section class="" powered-by="xiumi.us" style="white-space: normal;box-sizing: border-box;"></p>

<p><section class="" style="box-sizing: border-box;"></p>

<p><section class="" style="font-size: 14px;box-sizing: border-box;"></p>

<p>推荐：<a href="http://mp.weixin.qq.com/s?__biz=MzU0MDEwMjgwNA==&amp;mid=2247484509&amp;idx=1&amp;sn=e8b7f12fb3660d15379e2087c2e0e5c6&amp;chksm=fb3f1da6cc4894b0421d62ebecd16c9de11a3abf919d2c506643d3b864d0357ffc4eec1bfd29&amp;scene=21#wechat_redirect">译：基于注解的控制器：Spring Web/WebFlux 和 测试</a></p>

<p>上一篇：<a href="http://mp.weixin.qq.com/s?__biz=MzU0MDEwMjgwNA==&amp;mid=2247484512&amp;idx=1&amp;sn=4130871a4e6360b4f1b8c1b4dac4b101&amp;chksm=fb3f1d9bcc48948d5430ce9e1cef224c40421241d3a925a891bb1cf2c1753d90db040529848a&amp;scene=21#wechat_redirect">Spring-5-webflux 和阻塞与非阻塞 JDBC</a></p>

<p></section></p>

<p></section></p>

<p></section></p>

<p><section class="" powered-by="xiumi.us" style="white-space: normal;box-sizing: border-box;"></p>

<p><section class="" style="box-sizing: border-box;"></p>

<p><section class="" style="display: inline-block;vertical-align: top;width: 279px;box-sizing: border-box;"></p>

<p><section class="" powered-by="xiumi.us" style="box-sizing: border-box;"></p>

<p><section class="" style="box-sizing: border-box;"></p>

<p><section class="" style="text-align: center;color: rgb(160, 160, 160);font-size: 14px;box-sizing: border-box;"></p>

<p>最好的赞赏</p>

<p>就是你的关注</p>

<p></section></p>

<p></section></p>

<p></section></p>

<p></section></p>

<p><section class="" style="display: inline-block;vertical-align: top;width: 279px;box-sizing: border-box;"></p>

<p><section class="" powered-by="xiumi.us" style="box-sizing: border-box;"></p>

<p><section class="" style="margin-top: 10px;margin-bottom: 10px;text-align: center;box-sizing: border-box;"></p>

<p><section class="" style="max-width: 100%;vertical-align: middle;display: inline-block;box-sizing: border-box;overflow: hidden !important;"><img src="https://mmbiz.qpic.cn/mmbiz_jpg/wbiax4xEAl5zQkzvFqgk7DUAem1u05eybPdhEythAoe3O0FxUHy0tmzgytKI7tJaiaDsEWib43ZFSYmEROK4MNNAQ/640?wx_fmt=jpeg" alt=""/></section></p>

<p></section></p>

<p></section></p>

<p></section></p>

<p></section></p>

<p></section></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[基于 Docker 搭建 MySQL 主从复制 - 秋田君]]></title>
    <link href="http://panlw.github.io/15274384746652.html"/>
    <updated>2018-05-28T00:27:54+08:00</updated>
    <id>http://panlw.github.io/15274384746652.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://my.oschina.net/u/3773384/blog/1810111">原文地址</a></p>
</blockquote>

<pre><code>本篇博文相对简单，因为是初次使用 Docker，MySQL 的主从复制之前也在 Centos 环境下搭建过，但是也忘的也差不多了，因此本次尝试在 Docker 中搭建。根据网上教程走还是踩了一些坑，不过所幸最终搭建成功，因此记录下来，避免以后踩了重复的坑。
</code></pre>

<h2 id="toc_0">搭建环境</h2>

<p>Centos 7.2 64 位</p>

<p>MySQL 5.7.13</p>

<p>Docker 1.13.1</p>

<p>接下来，我们将会在一台服务器上安装 docker，并使用 docker 运行三个 MySQL 容器，分别为一主两从。</p>

<h2 id="toc_1">安装 docker</h2>

<p>执行命令</p>

<pre><code>[root@VM_0_17_centos ~]# yum install docker
</code></pre>

<p>如果有提示，一路 y 下去</p>

<p>安装成功启动 Docker 后，查看版本</p>

<pre><code>[root@VM_0_17_centos ~]# docker version
Client:
 Version:         1.13.1
 API version:     1.26
 Package version: &lt;unknown&gt;
 Go version:      go1.8.3
 Git commit:      774336d/1.13.1
 Built:           Wed Mar  7 17:06:16 2018
 OS/Arch:         linux/amd64

Server:
 Version:         1.13.1
 API version:     1.26 (minimum version 1.12)
 Package version: &lt;unknown&gt;
 Go version:      go1.8.3
 Git commit:      774336d/1.13.1
 Built:           Wed Mar  7 17:06:16 2018
 OS/Arch:         linux/amd64
 Experimental:    false
</code></pre>

<p>出现版本信息，则安装成功</p>

<h2 id="toc_2">启动 Docker</h2>

<p>启动 Docker 并设置为开机自启动</p>

<pre><code>[root@VM_0_17_centos ~]# systemctl  start docker.service
[root@VM_0_17_centos ~]# systemctl  enable docker.service
</code></pre>

<h2 id="toc_3">安装 MySQL</h2>

<p>使用 Docker 拉取 MySQL 镜像</p>

<pre><code>[root@VM_0_17_centos ~]# docker pull mysql:5.7.13
</code></pre>

<h2 id="toc_4">运行主容器</h2>

<pre><code>[root@VM_0_17_centos ~]# docker run --name master -p 3306:3306 -e MYSQL_ROOT_PASSWORD=root -d mysql:5.7.13

</code></pre>

<p>--name 为容器指定名称，这里是 master</p>

<p>-p 将容器的指定端口映射到主机的指定端口，这里是将容器的 3306 端口映射到主机的 3306 端口</p>

<p>-e 设置环境变量，这里是指定 root 账号的密码为 root</p>

<p>-d 后台运行容器，并返回容器 ID</p>

<p>mysql:5.7.13 指定运行的 mysql 版本</p>

<h2 id="toc_5">检验是否启动成功</h2>

<p>docker ps -a 显示所有的容器，包括未运行的</p>

<pre><code>[root@VM_0_17_centos ~]# docker ps -a
ee86c19336f8        mysql:5.7.13        &quot;docker-entrypoint...&quot;   About an hour ago   Up About an hour    0.0.0.0:3306-&gt;3306/tcp   master

</code></pre>

<p>注意，是 UP 状态，表示正在运行中</p>

<p>开放 3306 端口</p>

<pre><code>[root@VM_0_17_centos ~]# firewall-cmd --zone=public --add-port=3306/tcp --permanent
[root@VM_0_17_centos ~]# firewall-cmd --reload

</code></pre>

<p>--permanent 永久开启，避免下次开机需要再次手动开启端口</p>

<p>使用 Navicat 连接测试</p>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/114916_0h3I_3773384.png" alt=""/></p>

<p>MySQL 主容器已经启动成功</p>

<h2 id="toc_6">创建主容器的复制账号</h2>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/120249_0ZQx_3773384.png" alt=""/></p>

<p>使用 Navicat 友好的图像化界面执行 SQL</p>

<pre><code>GRANT REPLICATION SLAVE ON *.* to &#39;backup&#39;@&#39;%&#39; identified by &#39;backup&#39;;
show grants for &#39;backup&#39;@&#39;%&#39;;
</code></pre>

<p>出现如下信息表示授权成功</p>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/121018_Vtxn_3773384.png" alt=""/></p>

<h2 id="toc_7">修改 MySQL 配置环境</h2>

<p>创建配置文件目录</p>

<p>目录结构如下</p>

<p>/usr/local/mysql/master</p>

<p>/usr/local/mysql/slave1</p>

<p>/usr/local/mysql/slave2</p>

<p>拷贝一份 MySQL 配置文件</p>

<pre><code>[root@VM_0_17_centos local]# docker cp master:/etc/mysql/my.cnf /usr/local/mysql/master/my.cnf

</code></pre>

<p>进到 master 目录下，已存在拷贝的 my.cnf</p>

<pre><code>[root@VM_0_17_centos master]# ll
total 4
-rw-r--r-- 1 root root 1801 May 10 10:27 my.cnf

</code></pre>

<p>修改 my.cnf，在 [mysqld] 节点最后加上后保存</p>

<pre><code>log-bin=mysql-bin
server-id=1
</code></pre>

<p>log-bin=mysql-bin 使用 binary logging，mysql-bin 是 log 文件名的前缀</p>

<p>server-id=1 唯一服务器 ID，非 0 整数，不能和其他服务器的 server-id 重复</p>

<p>将修改后的文件覆盖 Docker 中 MySQL 中的配置文件</p>

<pre><code>[root@VM_0_17_centos master]# docker cp /usr/local/mysql/master/my.cnf master:/etc/mysql/my.cnf

</code></pre>

<p>重启 mysql 的 docker , 让配置生效</p>

<pre><code>[root@VM_0_17_centos master]# docker restart master
</code></pre>

<p>启动后，重新测试连接，连接成功表示主容器配置成功</p>

<h2 id="toc_8">运行 MySQL 从容器</h2>

<p>首先运行从容器</p>

<pre><code>[root@VM_0_17_centos ~]# docker run --name slave1 -p 3307:3306 -e MYSQL_ROOT_PASSWORD=root -d mysql:5.7.13

</code></pre>

<p>与主容器相似，拷贝配置文件至 slave1 目录修改后覆盖回 Docker 中</p>

<pre><code>log-bin=mysql-bin
server-id=2
</code></pre>

<p>别忘记，重启 slave1 容器，使配置生效</p>

<h2 id="toc_9">配置主从复制</h2>

<p>使用 Navicat 连接 slave1 后新建查询，执行以下 SQL</p>

<pre><code>CHANGE MASTER TO 
MASTER_HOST=&#39;ip&#39;,
MASTER_PORT=3306,
MASTER_USER=&#39;backup&#39;,
MASTER_PASSWORD=&#39;backup&#39;;

START SLAVE;
</code></pre>

<p>MASTER_HOST 填 Navicat 连接配置中的 ip 应该就可以</p>

<p>MASTER_PORT 主容器的端口</p>

<p>MASTER_USER 同步账号的用户名</p>

<p>MASTER_PASSWORD 同步账号的密码</p>

<h2 id="toc_10">检查是否配置成功</h2>

<pre><code>show slave status;
</code></pre>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/123902_gnvI_3773384.png" alt=""/></p>

<p>Slave_IO_State 如果是 Waiting for master to send event，那么就成功一半了，如果是 Connecting to master，基本就是配置失败了，建议重新检查下配置，具体失败的原因可以查看日志追踪</p>

<pre><code>[root@VM_0_17_centos master]# docker logs slave -f
</code></pre>

<p>我遇到的是 MASTER_USER 和 MASTER_PASSWORD 是否手打输错了，贴出错误日志</p>

<pre><code>2018-05-10T02:57:00.688887Z 11 [ERROR] Slave I/O for channel &#39;&#39;: error connecting to master &#39;bakcup@ip:3306&#39; - retry-time: 60  retries: 2, Error_code: 1045
2018-05-10T02:58:00.690476Z 11 [ERROR] Slave I/O for channel &#39;&#39;: error connecting to master &#39;bakcup@ip:3306&#39; - retry-time: 60  retries: 3, Error_code: 1045
</code></pre>

<p>注意看日志中的 bakcup，解决方法如下</p>

<pre><code>STOP SLAVE;

CHANGE MASTER TO 
MASTER_HOST=&#39;连接Navicat的ip&#39;,
MASTER_PORT=正确的端口,
MASTER_USER=&#39;正确的用户名&#39;,
MASTER_PASSWORD=&#39;正确的密码&#39;;

START SLAVE;
</code></pre>

<p>接着上文，我们说成功一半，并没有说成功了，那么另一半在于 Slave_IO_Running 与 Slave_SQL_Running</p>

<p>如果都是 Yes，那么恭喜你，可以测试主从复制的效果了，如果有一个不是 Yes，一半是重启从容器后，事务回滚引起的，那么给出解决方法如下</p>

<pre><code>stop slave ;
set GLOBAL SQL_SLAVE_SKIP_COUNTER=1;
start slave ;
</code></pre>

<p>执行后，再次观察三个关键字段应该就都没问题了</p>

<p>至此，一主一从已经搭建完成，再添加从实例的方式与上文一致，这里就不在赘述了。</p>

<h2 id="toc_11">测试主从复制</h2>

<p>首先，在主实例中创建一个测试数据库</p>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/130207_7Ty1_3773384.png" alt=""/></p>

<p>打开（刷新）从实例，可见 test 库已存在</p>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/130133_ITNO_3773384.png" alt=""/></p>

<p>在 test 库中创建一个表 t_test，添加一个 id 测试字段</p>

<p>向表中添加几个数据</p>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/130357_IEZg_3773384.png" alt=""/></p>

<p>刷新从库，可见 t_test 表及其中 1、2、3、4 数据已存在</p>

<p>至此，一个具备主从复制的一主两从的 MySQL 就已搭建完成。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Never touch your local /etc/hosts file in OS X again]]></title>
    <link href="http://panlw.github.io/15274377762013.html"/>
    <updated>2018-05-28T00:16:16+08:00</updated>
    <id>http://panlw.github.io/15274377762013.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://gist.github.com/ogrrd/5831371">原文地址</a></p>

<p>To setup your computer to work with *.dev domains, e.g. project.dev, awesome.dev and so on, without having to add to your hosts file each time.</p>
</blockquote>

<h2 id="toc_0">Requirements</h2>

<ul>
<li><a href="http://mxcl.github.io/homebrew/">Homebrew</a></li>
<li>Mountain Lion</li>
</ul>

<h2 id="toc_1">Install</h2>

<pre><code>brew install dnsmasq
</code></pre>

<h2 id="toc_2">Setup</h2>

<h3 id="toc_3">Create config directory</h3>

<pre><code>mkdir -pv $(brew --prefix)/etc/
</code></pre>

<h3 id="toc_4">Setup *.dev</h3>

<pre><code>echo &#39;address=/.dev/127.0.0.1&#39; &gt; $(brew --prefix)/etc/dnsmasq.conf
</code></pre>

<p>You should probably add <code>strict-order</code> to <code>dnsmasq.conf</code> to keep nameserver order of <code>resolv.conf</code> (<a href="https://gist.github.com/drye/5387341">see here</a>).</p>

<h2 id="toc_5">Autostart</h2>

<h3 id="toc_6">Work after reboot</h3>

<pre><code>sudo cp -v $(brew --prefix dnsmasq)/homebrew.mxcl.dnsmasq.plist /Library/LaunchDaemons
</code></pre>

<h3 id="toc_7">Get it going right now</h3>

<pre><code>sudo launchctl load -w /Library/LaunchDaemons/homebrew.mxcl.dnsmasq.plist
</code></pre>

<h2 id="toc_8">Add to resolvers</h2>

<h3 id="toc_9">Create resolver directory</h3>

<pre><code>sudo mkdir -v /etc/resolver
</code></pre>

<h3 id="toc_10">Add your nameserver to resolvers</h3>

<pre><code>sudo bash -c &#39;echo &quot;nameserver 127.0.0.1&quot; &gt; /etc/resolver/dev&#39;
</code></pre>

<h2 id="toc_11">Add local DNS to search order in System Preferences</h2>

<p>System Preferences &gt; Network &gt; Wi-Fi (or whatever you use) &gt; Advanced... &gt; DNS &gt; add 127.0.0.1 to top of the list.</p>

<h2 id="toc_12">Finished</h2>

<p>That&#39;s it! You can run scutil --dns to show all of your current resolvers, and you should see that all requests for a domain ending in .dev will go to the DNS server at 127.0.0.1</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install NGINX with PHP7-FPM on Mac OS X with Homebrew]]></title>
    <link href="http://panlw.github.io/15274361469556.html"/>
    <updated>2018-05-27T23:49:06+08:00</updated>
    <id>http://panlw.github.io/15274361469556.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://gist.github.com/dtomasi/ab76d14338db82ec24a1fc137caff75b">原文地址</a></p>
</blockquote>

<h2 id="toc_0">Install Commandline Tools</h2>

<p><code>xcode-select --install</code></p>

<h2 id="toc_1">Install Homebrew</h2>

<p><code>ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</code></p>

<h4 id="toc_2">Check Installation</h4>

<p><code>brew doctor</code></p>

<h4 id="toc_3">Install brew services</h4>

<p><code>brew tap homebrew/services</code></p>

<h4 id="toc_4">Install bash completion (Optional)</h4>

<p><code>brew install bash-completion</code></p>

<h4 id="toc_5">Update Brew and Packages if allready installed</h4>

<p><code>brew update &amp;&amp; brew upgrade</code></p>

<h4 id="toc_6">Setup Environment</h4>

<p><code>sudo nano ~/.bash_profile</code></p>

<p>Add following lines</p>

<pre><code>  ##
  # Homebrew
  ##
  export PATH=&quot;/usr/local/bin:$PATH&quot;
  export PATH=&quot;/usr/local/sbin:$PATH&quot;
  
  ##
  # Homebrew bash completion
  ##
  if [ -f $(brew --prefix)/etc/bash_completion ]; then
    source $(brew --prefix)/etc/bash_completion
  fi
</code></pre>

<h2 id="toc_7">DNSMasq</h2>

<p>DNSMasq is used to resolve all domains that end with .dev to 127.0.0.1. So you don´t need to touch hosts-File anymore.</p>

<h4 id="toc_8">Install</h4>

<pre><code>brew install dnsmasq
</code></pre>

<h4 id="toc_9">Configure</h4>

<pre><code>curl -L https://gist.githubusercontent.com/dtomasi/ab76d14338db82ec24a1fc137caff75b/raw/550c84393c4c1eef8a3e68bb720df561b5d3f175/dnsmasq.conf -o /usr/local/etc/dnsmasq.conf

sudo curl -L https://gist.githubusercontent.com/dtomasi/ab76d14338db82ec24a1fc137caff75b/raw/550c84393c4c1eef8a3e68bb720df561b5d3f175/dev -o /etc/resolver/dev
</code></pre>

<h4 id="toc_10">Start, Stop and Restart</h4>

<pre><code># Start
sudo brew services start dnsmasq

# Stop
sudo brew services stop dnsmasq

# Restart
sudo brew services restart dnsmasq
</code></pre>

<h4 id="toc_11">Test</h4>

<pre><code>dig testing.a.domain.that.should.point.to.localhost.dev @127.0.0.1
</code></pre>

<h2 id="toc_12">PHP-FPM</h2>

<h4 id="toc_13">Install php70</h4>

<pre><code>  brew tap homebrew/dupes &amp;&amp; \
  brew tap homebrew/php &amp;&amp; \
  brew install --without-apache --with-fpm --with-mysql php70
</code></pre>

<h4 id="toc_14">Configure</h4>

<p><code>sudo nano /usr/local/etc/php/7.0/php-fpm.d/www.conf</code></p>

<pre><code>  user = YOUR_USERNAME
  group = YOUR_GROUP || staff
</code></pre>

<h4 id="toc_15">Testing</h4>

<p>start php-fpm</p>

<p><code>sudo brew services start php70</code></p>

<p>show running processes</p>

<p><code>lsof -Pni4 | grep LISTEN | grep php</code></p>

<h2 id="toc_16">NGINX</h2>

<h4 id="toc_17">Install NGINX</h4>

<pre><code>brew tap homebrew/nginx &amp;&amp; \
brew install nginx
</code></pre>

<h4 id="toc_18">Test Installation</h4>

<pre><code>  ## Start Nginx
  sudo brew services start nginx
  
  ## Check if Nginx is running on default port
  curl -IL http://127.0.0.1:8080
</code></pre>

<p>Output should look like this</p>

<pre><code>HTTP/1.1 200 OK
Server: nginx/1.10.0
Date: Sat, 07 May 2016 07:36:32 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 26 Apr 2016 13:31:24 GMT
Connection: keep-alive
ETag: &quot;571f6dac-264&quot;
Accept-Ranges: bytes
</code></pre>

<h4 id="toc_19">Stop Nginx</h4>

<p><code>sudo brew services stop nginx</code></p>

<h4 id="toc_20">Configure</h4>

<p>Create missing directories</p>

<pre><code>  mkdir -p /usr/local/etc/nginx/sites-available &amp;&amp; \
  mkdir -p /usr/local/etc/nginx/sites-enabled &amp;&amp; \
  mkdir -p /usr/local/etc/nginx/conf.d &amp;&amp; \
  mkdir -p /usr/local/etc/nginx/ssl
</code></pre>

<p>Configure nginx.conf</p>

<pre><code># Remove default
rm /usr/local/etc/nginx/nginx.conf
# Copy mine
curl -L https://gist.githubusercontent.com/dtomasi/ab76d14338db82ec24a1fc137caff75b/raw/c7c99476e6d8bd5b23e814c5593861adb9b54765/nginx.conf -o /usr/local/etc/nginx/nginx.conf
</code></pre>

<p>Start and Test Nginx</p>

<pre><code>  ## Start Nginx
  sudo brew services start nginx
  
  ## Check if Nginx is running on default port
  curl -IL http://localhost

  ## Output should look like this
  HTTP/1.1 200 OK
  Server: nginx/1.10.0
  Date: Sat, 07 May 2016 08:35:57 GMT
  Content-Type: text/html
  Content-Length: 612
  Last-Modified: Tue, 26 Apr 2016 13:31:24 GMT
  Connection: keep-alive
  ETag: &quot;571f6dac-264&quot;
  Accept-Ranges: bytes
</code></pre>

<h2 id="toc_21">Setup SSL</h2>

<p>Create a folder for our SSL certificates and private keys:</p>

<p><code>mkdir -p /usr/local/etc/nginx/ssl</code></p>

<p>Generate 4096 bit RSA keys and the self-sign the certificates in one command:</p>

<p><code>openssl req -new -newkey rsa:4096 -days 365 -nodes -x509 -subj &quot;/C=US/ST=State/L=Town/O=Office/CN=localhost&quot; -keyout /usr/local/etc/nginx/ssl/localhost.key -out /usr/local/etc/nginx/ssl/localhost.crt</code></p>

<h2 id="toc_22">Setup example virtual hosts</h2>

<p>These are working presets. But you need to edit Document-Root</p>

<pre><code>curl -L https://gist.githubusercontent.com/dtomasi/ab76d14338db82ec24a1fc137caff75b/raw/c7c99476e6d8bd5b23e814c5593861adb9b54765/default -o /usr/local/etc/nginx/sites-available/default &amp;&amp; \
curl -L https://gist.githubusercontent.com/dtomasi/ab76d14338db82ec24a1fc137caff75b/raw/c7c99476e6d8bd5b23e814c5593861adb9b54765/default-ssl -o /usr/local/etc/nginx/sites-available/default-ssl
</code></pre>

<p>Activate Virtual Hosts</p>

<pre><code>ln -sfv /usr/local/etc/nginx/sites-available/default /usr/local/etc/nginx/sites-enabled/default
ln -sfv /usr/local/etc/nginx/sites-available/default-ssl /usr/local/etc/nginx/sites-enabled/default-ssl
</code></pre>

<p>Create info.php for testing</p>

<p><code>echo &quot;&lt;?php phpinfo();&quot; &gt; /path/to/your/document/root</code></p>

<p>Test</p>

<pre><code>sudo brew services restart nginx

curl -IL http://localhost/info.php

# Output should look like this
HTTP/1.1 200 OK
Server: nginx/1.10.0
Date: Sat, 07 May 2016 08:40:36 GMT
Content-Type: text/html; charset=UTF-8
Connection: keep-alive
X-Powered-By: PHP/7.0.6
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Alpine Docker Image]]></title>
    <link href="http://panlw.github.io/15273973619516.html"/>
    <updated>2018-05-27T13:02:41+08:00</updated>
    <id>http://panlw.github.io/15273973619516.html</id>
    <content type="html"><![CDATA[
<pre><code class="language-Dockerfile">FROM alpine:3.5
# Install base packages
RUN apk update &amp;&amp; apk add curl bash tree tzdata \
    &amp;&amp; cp -r -f /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \
    &amp;&amp; echo -ne &quot;Alpine Linux 3.4 image. (`uname -rsv`)\n&quot; &gt;&gt; /root/.built
# Define bash as default command
RUN apk --no-cache add openjdk8-jre ttf-droid
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[分布式之消息队列复习精讲]]></title>
    <link href="http://panlw.github.io/15273831746106.html"/>
    <updated>2018-05-27T09:06:14+08:00</updated>
    <id>http://panlw.github.io/15273831746106.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="http://www.cnblogs.com/rjzheng/p/8994962.html">http://www.cnblogs.com/rjzheng/p/8994962.html</a></p>
</blockquote>

<h2 id="toc_0">引言</h2>

<h3 id="toc_1">为什么写这篇文章?</h3>

<p>博主有两位朋友分别是小 A 和小 B:</p>

<ol>
<li> 小 A，工作于传统软件行业 (某社保局的软件外包公司)，每天工作内容就是和产品聊聊需求，改改业务逻辑。再不然就是和运营聊聊天，写几个 SQL，生成下报表。又或者接到客服的通知，某某功能故障了，改改数据，然后下班部署上线。每天过的都是这种生活，技术零成长。</li>
<li> 小 B，工作于某国企，虽然能接触到一些中间件技术。然而，他只会订阅 / 发布消息。通俗点说，就是调调 API。对为什么使用这些中间件啊？如何保证高可用啊？没有充分的认识。</li>
</ol>

<p>庆幸的是两位朋友都很有上进心，于是博主写这篇文章，帮助他们复习一下关于消息队列中间件这块的要点</p>

<h3 id="toc_2">复习要点</h3>

<p>本文大概围绕如下几点进行阐述:</p>

<ol>
<li> 为什么使用消息队列？</li>
<li> 使用消息队列有什么缺点?</li>
<li> 消息队列如何选型?</li>
<li> 如何保证消息队列是高可用的？</li>
<li> 如何保证消息不被重复消费?</li>
<li> 如何保证消费的可靠性传输?</li>
<li> 如何保证消息的顺序性？</li>
</ol>

<p>我们围绕以上七点进行阐述。需要说明一下，本文不是《消息队列从入门到精通》这种课程，因此只是提供一个复习思路，而不是去教你们怎么调用消息队列的 API。建议对消息队列不了解的人，去找点消息队列的博客看看，再看本文，收获更大</p>

<h2 id="toc_3">正文</h2>

<h3 id="toc_4">1、为什么要使用消息队列?</h3>

<p><strong>分析</strong>: 一个用消息队列的人，不知道为啥用，这就有点尴尬。没有复习这点，很容易被问蒙，然后就开始胡扯了。<br/>
<strong>回答</strong>: 这个问题, 咱只答三个最主要的应用场景 (不可否认还有其他的，但是只答三个主要的), 即以下六个字: <strong>解耦、异步、削峰</strong></p>

<h4 id="toc_5">(1) 解耦</h4>

<p><strong>传统模式:</strong><br/>
<img src="media/15273831746106/15273914396502.png" alt=""/></p>

<p>传统模式的缺点：</p>

<ul>
<li>  系统间耦合性太强，如上图所示，系统 A 在代码中直接调用系统 B 和系统 C 的代码，如果将来 D 系统接入，系统 A 还需要修改代码，过于麻烦！</li>
</ul>

<p><strong>中间件模式:</strong><br/>
<img src="media/15273831746106/15273918339281.png" alt=""/></p>

<p>中间件模式的的优点：</p>

<ul>
<li>  将消息写入消息队列，需要消息的系统自己从消息队列中订阅，从而系统 A 不需要做任何修改。</li>
</ul>

<h4 id="toc_6">(2) 异步</h4>

<p><strong>传统模式:</strong></p>

<p><img src="media/15273831746106/15273918558340.png" alt=""/></p>

<p>传统模式的缺点：</p>

<ul>
<li>  一些非必要的业务逻辑以同步的方式运行，太耗费时间。</li>
</ul>

<p><strong>中间件模式:</strong><br/>
<img src="media/15273831746106/15273918690655.png" alt=""/></p>

<p>中间件模式的的优点：</p>

<ul>
<li>  将消息写入消息队列，非必要的业务逻辑以异步的方式运行，加快响应速度</li>
</ul>

<h4 id="toc_7">(3) 削峰</h4>

<p><strong>传统模式</strong><br/>
<img src="media/15273831746106/15273918819389.png" alt=""/></p>

<p>传统模式的缺点：</p>

<ul>
<li>  并发量大的时候，所有的请求直接怼到数据库，造成数据库连接异常</li>
</ul>

<p><strong>中间件模式:</strong><br/>
<img src="media/15273831746106/15273924798559.png" alt=""/></p>

<p>中间件模式的的优点：</p>

<ul>
<li>  系统 A 慢慢的按照数据库能处理的并发量，从消息队列中慢慢拉取消息。在生产中，这个短暂的高峰期积压是允许的。</li>
</ul>

<h3 id="toc_8">2、使用了消息队列会有什么缺点?</h3>

<p><strong>分析</strong>: 一个使用了 MQ 的项目，如果连这个问题都没有考虑过，就把 MQ 引进去了，那就给自己的项目带来了风险。我们引入一个技术，要对这个技术的弊端有充分的认识，才能做好预防。<strong>要记住，不要给公司挖坑！</strong><br/>
<strong>回答</strong>: 回答也很容易，从以下两个个角度来答</p>

<ul>
<li>  <strong>系统可用性降低</strong>: 你想啊，本来其他系统只要运行好好的，那你的系统就是正常的。现在你非要加个消息队列进去，那消息队列挂了，你的系统不是呵呵了。因此，系统可用性降低</li>
<li>  <strong>系统复杂性增加</strong>: 要多考虑很多方面的问题，比如一致性问题、如何保证消息不被重复消费，如何保证保证消息可靠传输。因此，需要考虑的东西更多，系统复杂性增大。</li>
</ul>

<p>但是，我们该用还是要用的。</p>

<h3 id="toc_9">3、消息队列如何选型?</h3>

<p>先说一下，博主只会 ActiveMQ,RabbitMQ,RocketMQ,Kafka，对什么 ZeroMQ 等其他 MQ 没啥理解，因此只能基于这四种 MQ 给出回答。<br/>
<strong>分析</strong>: 既然在项目中用了 MQ，肯定事先要对业界流行的 MQ 进行调研，如果连每种 MQ 的优缺点都没了解清楚，就拍脑袋依据喜好，用了某种 MQ，还是给项目挖坑。如果面试官问:&quot;你为什么用这种 MQ？。&quot; 你直接回答 &quot;领导决定的。&quot; 这种回答就很 LOW 了。<strong>还是那句话，不要给公司挖坑。</strong><br/>
<strong>回答</strong>: 首先，咱先上 <a href="http://activemq.apache.org/">ActiveMQ 的社区</a>，看看该 MQ 的更新频率:</p>

<pre><code>Apache ActiveMQ 5.15.3 Release
Christopher L. Shannon posted on Feb 12, 2018
Apache ActiveMQ 5.15.2 Released
Christopher L. Shannon posted on Oct 23, 2017
Apache ActiveMQ 5.15.0 Released
Christopher L. Shannon posted on Jul 06, 2017
省略以下记录
...
</code></pre>

<p>我们可以看出，ActiveMq 几个月才发一次版本，据说研究重心在他们的下一代产品 Apollo。<br/>
接下来，我们再去 <a href="http://www.rabbitmq.com/">RabbitMQ 的社区</a>去看一下, RabbitMQ 的更新频率</p>

<pre><code>RabbitMQ 3.7.3 release  30 January 2018
RabbitMQ 3.6.15 release  17 January 2018
RabbitMQ 3.7.2 release23 December 2017
RabbitMQ 3.7.1 release21 December 2017
省略以下记录
...
</code></pre>

<p>我们可以看出，RabbitMQ 版本发布比 ActiveMq 频繁很多。至于 RocketMQ 和 kafka 就不带大家看了，总之也比 ActiveMQ 活跃的多。详情，可自行查阅。<br/>
再来一个性能对比表</p>

<table>
<thead>
<tr>
<th>特性</th>
<th>ActiveMQ</th>
<th>RabbitMQ</th>
<th>RocketMQ</th>
<th>kafka</th>
</tr>
</thead>

<tbody>
<tr>
<td>开发语言</td>
<td>java</td>
<td>erlang</td>
<td>java</td>
<td>scala</td>
</tr>
<tr>
<td>单机吞吐量</td>
<td>万级</td>
<td>万级</td>
<td>10 万级</td>
<td>10 万级</td>
</tr>
<tr>
<td>时效性</td>
<td>ms 级</td>
<td>us 级</td>
<td>ms 级</td>
<td>ms 级以内</td>
</tr>
<tr>
<td>可用性</td>
<td>高 (主从架构)</td>
<td>高 (主从架构)</td>
<td>非常高 (分布式架构)</td>
<td>非常高 (分布式架构)</td>
</tr>
<tr>
<td>功能特性</td>
<td>成熟的产品，在很多公司得到应用；有较多的文档；各种协议支持较好</td>
<td>基于 erlang 开发，所以并发能力很强，性能极其好，延时很低; 管理界面较丰富</td>
<td>MQ 功能比较完备，扩展性佳</td>
<td>只支持主要的 MQ 功能，像一些消息查询，消息回溯等功能没有提供，毕竟是为大数据准备的，在大数据领域应用广。</td>
</tr>
</tbody>
</table>

<p>综合上面的材料得出以下两点:<br/>
(1) 中小型软件公司，建议选 RabbitMQ. 一方面，erlang 语言天生具备高并发的特性，而且他的管理界面用起来十分方便。正所谓，成也萧何，败也萧何！他的弊端也在这里，虽然 RabbitMQ 是开源的，然而国内有几个能定制化开发 erlang 的程序员呢？所幸，RabbitMQ 的社区十分活跃，可以解决开发过程中遇到的 bug，这点对于中小型公司来说十分重要。不考虑 rocketmq 和 kafka 的原因是，一方面中小型软件公司不如互联网公司，数据量没那么大，选消息中间件，应首选功能比较完备的，所以 kafka 排除。不考虑 rocketmq 的原因是，rocketmq 是阿里出品，如果阿里放弃维护 rocketmq，中小型公司一般抽不出人来进行 rocketmq 的定制化开发，因此不推荐。<br/>
(2) 大型软件公司，根据具体使用在 rocketMq 和 kafka 之间二选一。一方面，大型软件公司，具备足够的资金搭建分布式环境，也具备足够大的数据量。针对 rocketMQ, 大型软件公司也可以抽出人手对 rocketMQ 进行定制化开发，毕竟国内有能力改 JAVA 源码的人，还是相当多的。至于 kafka，根据业务场景选择，如果有日志采集功能，肯定是首选 kafka 了。具体该选哪个，看使用场景。</p>

<h3 id="toc_10">4、如何保证消息队列是高可用的？</h3>

<p><strong>分析</strong>: 在第二点说过了，引入消息队列后，系统的可用性下降。在生产中，没人使用单机模式的消息队列。因此，作为一个合格的程序员，应该对消息队列的高可用有很深刻的了解。如果面试的时候，面试官问，你们的消息中间件如何保证高可用的？你的回答只是表明自己只会订阅和发布消息，面试官就会怀疑你是不是只是自己搭着玩，压根没在生产用过。<strong>请做一个爱思考，会思考，懂思考的程序员。</strong><br/>
<strong>回答</strong>: 这问题，其实要对消息队列的集群模式要有深刻了解，才好回答。<br/>
以 rcoketMQ 为例，他的集群就有多 master 模式、多 master 多 slave 异步复制模式、多 master 多 slave 同步双写模式。多 master 多 slave 模式部署架构图 (网上找的, 偷个懒，懒得画):<br/>
<img src="https://images.cnblogs.com/cnblogs_com/rjzheng/1202350/o_rocketcluster.png" alt=""/><br/>
其实博主第一眼看到这个图，就觉得和 kafka 好像，只是 NameServer 集群，在 kafka 中是用 zookeeper 代替，都是用来保存和发现 master 和 slave 用的。通信过程如下:<br/>
Producer 与 NameServer 集群中的其中一个节点（随机选择）建立长连接，定期从 NameServer 获取 Topic 路由信息，并向提供 Topic 服务的 Broker Master 建立长连接，且定时向 Broker 发送心跳。Producer 只能将消息发送到 Broker master，但是 Consumer 则不一样，它同时和提供 Topic 服务的 Master 和 Slave 建立长连接，既可以从 Broker Master 订阅消息，也可以从 Broker Slave 订阅消息。<br/>
那么 kafka 呢, 为了对比说明直接上 kafka 的拓补架构图 (也是找的，懒得画)<br/>
<img src="https://images.cnblogs.com/cnblogs_com/rjzheng/1202350/o_kafka.png" alt=""/><br/>
如上图所示，一个典型的 Kafka 集群中包含若干 Producer（可以是 web 前端产生的 Page View，或者是服务器日志，系统 CPU、Memory 等），若干 broker（Kafka 支持水平扩展，一般 broker 数量越多，集群吞吐率越高），若干 Consumer Group，以及一个 Zookeeper 集群。Kafka 通过 Zookeeper 管理集群配置，选举 leader，以及在 Consumer Group 发生变化时进行 rebalance。Producer 使用 push 模式将消息发布到 broker，Consumer 使用 pull 模式从 broker 订阅并消费消息。<br/>
至于 rabbitMQ, 也有普通集群和镜像集群模式，自行去了解，比较简单，两小时即懂。<br/>
要求，在回答高可用的问题时，应该能逻辑清晰的画出自己的 MQ 集群架构或清晰的叙述出来。</p>

<h3 id="toc_11">5、如何保证消息不被重复消费？</h3>

<p><strong>分析</strong>: 这个问题其实换一种问法就是，如何保证消息队列的幂等性? 这个问题可以认为是消息队列领域的基本问题。换句话来说，是在考察你的设计能力，这个问题的回答可以根据具体的业务场景来答，没有固定的答案。<br/>
<strong>回答</strong>: 先来说一下为什么会造成重复消费?<br/>
  其实无论是那种消息队列，造成重复消费原因其实都是类似的。正常情况下，消费者在消费消息时候，消费完毕后，会发送一个确认信息给消息队列，消息队列就知道该消息被消费了，就会将该消息从消息队列中删除。只是不同的消息队列发送的确认信息形式不同, 例如 RabbitMQ 是发送一个 ACK 确认消息，RocketMQ 是返回一个 CONSUME_SUCCESS 成功标志，kafka 实际上有个 offset 的概念，简单说一下 (如果还不懂，出门找一个 kafka 入门到精通教程), 就是每一个消息都有一个 offset，kafka 消费过消息后，需要提交 offset，让消息队列知道自己已经消费过了。那造成重复消费的原因?，就是因为网络传输等等故障，确认信息没有传送到消息队列，导致消息队列不知道自己已经消费过该消息了，再次将该消息分发给其他的消费者。<br/>
  如何解决? 这个问题针对业务场景来答分以下几点<br/>
  (1) 比如，你拿到这个消息做数据库的 insert 操作。那就容易了，给这个消息做一个唯一主键，那么就算出现重复消费的情况，就会导致主键冲突，避免数据库出现脏数据。<br/>
  (2) 再比如，你拿到这个消息做 redis 的 set 的操作，那就容易了，不用解决，因为你无论 set 几次结果都是一样的，set 操作本来就算幂等操作。<br/>
  (3) 如果上面两种情况还不行，上大招。准备一个第三方介质, 来做消费记录。以 redis 为例，给消息分配一个全局 id，只要消费过该消息，将 &lt; id,message &gt; 以 K-V 形式写入 redis。那消费者开始消费前，先去 redis 中查询有没消费记录即可。</p>

<h3 id="toc_12">6、如何保证消费的可靠性传输?</h3>

<p><strong>分析</strong>: 我们在使用消息队列的过程中，应该做到消息不能多消费，也不能少消费。如果无法做到可靠性传输，可能给公司带来千万级别的财产损失。同样的，如果可靠性传输在使用过程中，没有考虑到，这不是给公司挖坑么，你可以拍拍屁股走了，公司损失的钱，谁承担。还是那句话，<strong>认真对待每一个项目，不要给公司挖坑。</strong><br/>
<strong>回答</strong>: 其实这个可靠性传输，每种 MQ 都要从三个角度来分析: 生产者弄丢数据、消息队列弄丢数据、消费者弄丢数据</p>

<h4 id="toc_13">RabbitMQ</h4>

<p><strong>(1) 生产者丢数据</strong><br/>
从生产者弄丢数据这个角度来看，RabbitMQ 提供 transaction 和 confirm 模式来确保生产者不丢消息。<br/>
transaction 机制就是说，发送消息前，开启事物 (channel.txSelect())，然后发送消息，如果发送过程中出现什么异常，事物就会回滚 (channel.txRollback())，如果发送成功则提交事物 (channel.txCommit())。<br/>
然而缺点就是吞吐量下降了。因此，按照博主的经验，生产上用 confirm 模式的居多。一旦 channel 进入 confirm 模式，所有在该信道上面发布的消息都将会被指派一个唯一的 ID(从 1 开始)，一旦消息被投递到所有匹配的队列之后，rabbitMQ 就会发送一个 Ack 给生产者 (包含消息的唯一 ID)，这就使得生产者知道消息已经正确到达目的队列了. 如果 rabiitMQ 没能处理该消息，则会发送一个 Nack 消息给你，你可以进行重试操作。处理 Ack 和 Nack 的代码如下所示（说好不上代码的，偷偷上了）:</p>

<pre><code>channel.addConfirmListener(new ConfirmListener() {  
                @Override  
                public void handleNack(long deliveryTag, boolean multiple) throws IOException {  
                    System.out.println(&quot;nack: deliveryTag = &quot;+deliveryTag+&quot; multiple: &quot;+multiple);  
                }  
                @Override  
                public void handleAck(long deliveryTag, boolean multiple) throws IOException {  
                    System.out.println(&quot;ack: deliveryTag = &quot;+deliveryTag+&quot; multiple: &quot;+multiple);  
                }  
            });  
</code></pre>

<p><strong>(2) 消息队列丢数据</strong><br/>
处理消息队列丢数据的情况，一般是开启持久化磁盘的配置。这个持久化配置可以和 confirm 机制配合使用，你可以在消息持久化磁盘后，再给生产者发送一个 Ack 信号。这样，如果消息持久化磁盘之前，rabbitMQ 阵亡了，那么生产者收不到 Ack 信号，生产者会自动重发。<br/>
那么如何持久化呢，这里顺便说一下吧，其实也很容易，就下面两步<br/>
1、将 queue 的持久化标识 durable 设置为 true, 则代表是一个持久的队列<br/>
2、发送消息的时候将 deliveryMode=2<br/>
这样设置以后，rabbitMQ 就算挂了，重启后也能恢复数据<br/>
<strong>(3) 消费者丢数据</strong><br/>
消费者丢数据一般是因为采用了自动确认消息模式。这种模式下，消费者会自动确认收到信息。这时 rahbitMQ 会立即将消息删除，这种情况下如果消费者出现异常而没能处理该消息，就会丢失该消息。<br/>
至于解决方案，采用手动确认消息即可。</p>

<h4 id="toc_14">kafka</h4>

<p>这里先引一张 kafka Replication 的<a href="http://www.infoq.com/cn/articles/kafka-analysis-part-2/">数据流向图</a><br/>
<img src="https://images.cnblogs.com/cnblogs_com/rjzheng/1202350/o_kafka1png.png" alt=""/><br/>
Producer 在发布消息到某个 Partition 时，先通过 ZooKeeper 找到该 Partition 的 Leader，然后无论该 Topic 的 Replication Factor 为多少（也即该 Partition 有多少个 Replica），Producer 只将该消息发送到该 Partition 的 Leader。Leader 会将该消息写入其本地 Log。每个 Follower 都从 Leader 中 pull 数据。<br/>
针对上述情况，得出如下分析<br/>
<strong>(1) 生产者丢数据</strong><br/>
在 kafka 生产中，基本都有一个 leader 和多个 follwer。follwer 会去同步 leader 的信息。因此，为了避免生产者丢数据，做如下两点配置</p>

<ol>
<li> 第一个配置要在 producer 端设置 acks=all。这个配置保证了，follwer 同步完成后，才认为消息发送成功。</li>
<li> 在 producer 端设置 retries=MAX，一旦写入失败，这无限重试</li>
</ol>

<p><strong>(2) 消息队列丢数据</strong><br/>
针对消息队列丢数据的情况，无外乎就是，数据还没同步，leader 就挂了，这时 zookpeer 会将其他的 follwer 切换为 leader, 那数据就丢失了。针对这种情况，应该做两个配置。</p>

<ol>
<li> replication.factor 参数，这个值必须大于 1，即要求每个 partition 必须有至少 2 个副本</li>
<li> min.insync.replicas 参数，这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系</li>
</ol>

<p>这两个配置加上上面生产者的配置联合起来用，基本可确保 kafka 不丢数据</p>

<p><strong>(3) 消费者丢数据</strong><br/>
这种情况一般是自动提交了 offset，然后你处理程序过程中挂了。kafka 以为你处理好了。再强调一次 offset 是干嘛的<br/>
<strong>offset</strong>：指的是 kafka 的 topic 中的每个消费组消费的下标。简单的来说就是一条消息对应一个 offset 下标，每次消费数据的时候如果提交 offset，那么下次消费就会从提交的 offset 加一那里开始消费。<br/>
比如一个 topic 中有 100 条数据，我消费了 50 条并且提交了，那么此时的 kafka 服务端记录提交的 offset 就是 49(offset 从 0 开始)，那么下次消费的时候 offset 就从 50 开始消费。<br/>
解决方案也很简单，改成手动提交即可。</p>

<h4 id="toc_15">ActiveMQ 和 RocketMQ</h4>

<p>大家自行查阅吧</p>

<h3 id="toc_16">7、如何保证消息的顺序性？</h3>

<p><strong>分析</strong>: 其实并非所有的公司都有这种业务需求，但是还是对这个问题要有所复习。<br/>
<strong>回答</strong>: 针对这个问题，通过某种算法，将需要保持先后顺序的消息放到同一个消息队列中 (kafka 中就是 partition,rabbitMq 中就是 queue)。然后只用一个消费者去消费该队列。<br/>
有的人会问: 那如果为了吞吐量，有多个消费者去消费怎么办？<br/>
这个问题，没有固定回答的套路。比如我们有一个微博的操作，发微博、写评论、删除微博，这三个异步操作。如果是这样一个业务场景，那只要重试就行。比如你一个消费者先执行了写评论的操作，但是这时候，微博都还没发，写评论一定是失败的，等一段时间。等另一个消费者，先执行写评论的操作后，再执行，就可以成功。<br/>
总之，针对这个问题，我的观点是保证入队有序就行，出队以后的顺序交给消费者自己去保证，没有固定套路。</p>

<h2 id="toc_17">总结</h2>

<p>写到这里，希望读者把本文提出的这几个问题，经过深刻的准备后，一般来说，能囊括大部分的消息队列的知识点。如果面试官不问这几个问题怎么办，简单，自己把几个问题讲清楚，突出以下自己考虑的全面性。<br/>
最后，其实我不太提倡这样突击复习，希望大家打好基本功，<strong>做一个爱思考，懂思考，会思考的程序员</strong>。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[权限系统设计模型分析（DAC，MAC，RBAC，ABAC）]]></title>
    <link href="http://panlw.github.io/15264401346282.html"/>
    <updated>2018-05-16T11:08:54+08:00</updated>
    <id>http://panlw.github.io/15264401346282.html</id>
    <content type="html"><![CDATA[
<pre><code>作者：https://www.jianshu.com/u/11c0ebe856b8
发表：2017.12.10 23:16
原文：https://www.jianshu.com/p/ce0944b4a903
</code></pre>

<blockquote>
<p>好久没有更新文章了…… 这一年过得太忙。<br/>
准备一篇个人认为值得拿出来分享的文章真的需要很多时间，如果你喜欢，请评论、点赞让我知道，我会抽更多的时间来更新一些分享给大家，谢谢！</p>
</blockquote>

<p>此篇文章主要尝试将世面上现有的一些权限系统设计做一下简单的总结分析，个人水平有限，如有错误请不吝指出。</p>

<h2 id="toc_0">术语</h2>

<p>这里对后面会用到的词汇做一个说明，老司机请直接翻到<strong>常见设计模式</strong>。</p>

<h3 id="toc_1">用户</h3>

<p>发起操作的主体。</p>

<h3 id="toc_2">对象（Subject）</h3>

<p>指操作所针对的客体对象，比如订单数据或图片文件。</p>

<h3 id="toc_3">权限控制表 (ACL: Access Control List)</h3>

<p>用来描述权限规则或用户和权限之间关系的数据表。</p>

<h3 id="toc_4">权限 (Permission)</h3>

<p>用来指代对某种对象的某一种操作，例如 “添加文章的操作”。</p>

<h3 id="toc_5">权限标识</h3>

<p>权限的代号，例如用 “ARTICLE_ADD” 来指代 “添加文章的操作” 权限。</p>

<h2 id="toc_6">常见设计模式</h2>

<h3 id="toc_7">自主访问控制（DAC: Discretionary Access Control）</h3>

<p>系统会识别用户，然后根据被操作对象（Subject）的权限控制列表（ACL: Access Control List）或者权限控制矩阵（ACL: Access Control Matrix）的信息来决定用户的是否能对其进行哪些操作，例如读取或修改。</p>

<p>而拥有对象权限的用户，又可以将该对象的权限分配给其他用户，所以称之为 “自主（Discretionary）” 控制。</p>

<p>这种设计最常见的应用就是文件系统的权限设计，如微软的 NTFS。</p>

<p>DAC 最大缺陷就是对权限控制比较分散，不便于管理，比如无法简单地将一组文件设置统一的权限开放给指定的一群用户。</p>

<p><img src="https://upload-images.jianshu.io/upload_images/594774-6d77f13cc568797f.png" alt=""/></p>

<h3 id="toc_8">强制访问控制（MAC: Mandatory Access Control）</h3>

<p>MAC 是为了弥补 DAC 权限控制过于分散的问题而诞生的。在 MAC 的设计中，每一个对象都都有一些权限标识，每个用户同样也会有一些权限标识，而用户能否对该对象进行操作取决于双方的权限标识的关系，这个限制判断通常是由系统硬性限制的。比如在影视作品中我们经常能看到特工在查询机密文件时，屏幕提示需要 “无法访问，需要一级安全许可”，这个例子中，文件上就有“一级安全许可” 的权限标识，而用户并不具有。</p>

<p>MAC 非常适合机密机构或者其他等级观念强烈的行业，但对于类似商业服务系统，则因为不够灵活而不能适用。</p>

<p><img src="https://upload-images.jianshu.io/upload_images/594774-f9ce520635dc31b2.png" alt=""/></p>

<blockquote>
<p><a href="https://link.jianshu.com?t=https://www.centos.org/docs/5/html/Deployment_Guide-en-US/sec-mls-ov.html">Red Hat: MLS</a></p>
</blockquote>

<h3 id="toc_9">基于角色的访问控制（RBAC: Role-Based Access Control)</h3>

<p>因为 DAC 和 MAC 的诸多限制，于是诞生了 RBAC，并且成为了迄今为止最为普及的权限设计模型。</p>

<p>RBAC 在用户和权限之间引入了 “角色（Role）” 的概念（暂时忽略 Session 这个概念）：</p>

<p><img src="https://upload-images.jianshu.io/upload_images/594774-b220fc093138a2c7.png" alt=""/></p>

<blockquote>
<p>图片来自 <a href="https://link.jianshu.com?t=http://directory.apache.org/fortress/user-guide/1.3-what-rbac-is.html">Apache Directory</a></p>
</blockquote>

<p>如图所示，每个用户关联一个或多个角色，每个角色关联一个或多个权限，从而可以实现了非常灵活的权限管理。角色可以根据实际业务需求灵活创建，这样就省去了每新增一个用户就要关联一遍所有权限的麻烦。简单来说 RBAC 就是：用户关联角色，角色关联权限。另外，RBAC 是可以模拟出 DAC 和 MAC 的效果的。</p>

<p>例如数据库软件 MongoDB 便是采用 RBAC 模型，对数据库的操作都划分成了权限（<a href="https://link.jianshu.com?t=https://docs.mongodb.com/manual/reference/privilege-actions/">MongoDB 权限文档</a>）：</p>

<table>
<thead>
<tr>
<th>权限标识</th>
<th>说明</th>
</tr>
</thead>

<tbody>
<tr>
<td>find</td>
<td>具有此权限的用户可以运行所有和查询有关的命令，如：aggregate、checkShardingIndex、count 等。</td>
</tr>
<tr>
<td>insert</td>
<td>具有此权限的用户可以运行所有和新建数据有关的命令：insert 和 create 等。</td>
</tr>
<tr>
<td>collStats</td>
<td>具有此权限的用户可以对指定 database 或 collection 执行 collStats 命令。</td>
</tr>
<tr>
<td>viewRole</td>
<td>具有此权限的用户可以查看指定 database 的角色信息。</td>
</tr>
<tr>
<td>…</td>
<td></td>
</tr>
</tbody>
</table>

<p>基于这些权限，MongoDB 提供了一些预定义的角色（<a href="https://link.jianshu.com?t=https://docs.mongodb.com/manual/reference/built-in-roles/">MongoDB 预定义角色文档</a>，用户也可以自己定义角色）：</p>

<table>
<thead>
<tr>
<th>角色</th>
<th>find</th>
<th>insert</th>
<th>collStats</th>
<th>viewRole</th>
<th>…</th>
</tr>
</thead>

<tbody>
<tr>
<td>read</td>
<td>✔</td>
<td></td>
<td>✔</td>
<td></td>
<td>…</td>
</tr>
<tr>
<td>readWrite</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td></td>
<td>…</td>
</tr>
<tr>
<td>dbAdmin</td>
<td>✔</td>
<td></td>
<td>✔</td>
<td></td>
<td>…</td>
</tr>
<tr>
<td>userAdmin</td>
<td></td>
<td></td>
<td></td>
<td>✔</td>
<td>…</td>
</tr>
</tbody>
</table>

<p>最后授予用户不同的角色，就可以实现不同粒度的权限分配了。</p>

<p>目前市面上绝大部分系统在设计权限系统时都采用 RBAC 模型。然而也有的系统错误地实现了 RBAC，他们采用的是判断用户是否具有某个角色而不是判断权限，例如以下代码：</p>

<pre><code>&lt;?php

if ($user-&gt;hasRole(&#39;hr&#39;)) {
    // 执行某种只有“HR”角色才能做的功能，例如给员工涨薪…
    // ...
}

</code></pre>

<p>如果后期公司规定部门经理也可以给员工涨薪，这时就不得不修改代码了。</p>

<p>以上基本就是 RBAC 的核心设计（RBAC Core）。而基于核心概念之上，RBAC 规范还提供了扩展模式。</p>

<h4 id="toc_10">角色继承 (Hierarchical Role)</h4>

<p><img src="https://upload-images.jianshu.io/upload_images/594774-386977fee82f6152.png" alt=""/></p>

<blockquote>
<p>带有角色继承的 RBAC。图片来自 <a href="https://link.jianshu.com?t=http://directory.apache.org/fortress/user-guide/1.3-what-rbac-is.html">Apache Directory</a></p>
</blockquote>

<p>顾名思义，角色继承就是指角色可以继承于其他角色，在拥有其他角色权限的同时，自己还可以关联额外的权限。这种设计可以给角色分组和分层，一定程度简化了权限管理工作。</p>

<h4 id="toc_11">职责分离 (Separation of Duty)</h4>

<p>为了避免用户拥有过多权限而产生利益冲突，例如一个篮球运动员同时拥有裁判的权限（看一眼就给你判犯规狠不狠？），另一种职责分离扩展版的 RBAC 被提出。</p>

<p>职责分离有两种模式：</p>

<ul>
<li>  静态职责分离 (Static Separation of Duty)：用户无法同时被赋予有冲突的角色。</li>
<li>  动态职责分离 (Dynamic Separation of Duty)：用户在一次会话（Session）中不能同时激活自身所拥有的、互相有冲突的角色，只能选择其一。</li>
</ul>

<p><img src="https://upload-images.jianshu.io/upload_images/594774-feb7c1074d151113.png" alt=""/></p>

<blockquote>
<p>静态职责分离。图片来自 <a href="https://link.jianshu.com?t=http://directory.apache.org/fortress/user-guide/1.3-what-rbac-is.html">Apache Directory</a></p>
</blockquote>

<p><img src="https://upload-images.jianshu.io/upload_images/594774-059b93e4209e8fa6.png" alt=""/></p>

<blockquote>
<p>动态职责分离。图片来自 <a href="https://link.jianshu.com?t=http://directory.apache.org/fortress/user-guide/1.3-what-rbac-is.html">Apache Directory</a></p>
</blockquote>

<p>讲了这么多 RBAC，都还只是在用户和权限之间进行设计，并没有涉及到用户和对象之间的权限判断，而在实际业务系统中限制用户能够使用的对象是很常见的需求。例如华中区域的销售没有权限查询华南区域的客户数据，虽然他们都具有销售的角色，而销售的角色拥有查询客户信息的权限。</p>

<p>那么我们应该怎么办呢？</p>

<h4 id="toc_12">用户和对象的权限控制</h4>

<p>在 RBAC 标准中并没有涉及到这个内容（RBAC 基本只能做到对一类对象的控制），但是这里讲几种基于 RBAC 的实现方式。</p>

<p>首先我们看看 PHP 框架 <a href="https://link.jianshu.com?t=http://www.yiiframework.com/wiki/136/getting-to-understand-hierarchical-rbac-scheme/">Yii 1.X 的解决方案</a>（2.X 中代码更为优雅，但 1.X 的示例代码更容易看明白）：</p>

<pre><code>&lt;?php
$auth=Yii::app()-&gt;authManager;

// command-permission
$auth-&gt;createOperation(&#39;listPosts&#39;,&#39;list posts&#39;);     // define a perm: listPosts
$auth-&gt;createOperation(&#39;createPost&#39;,&#39;create a post&#39;); // define a perm: createPost
$auth-&gt;createOperation(&#39;readPost&#39;,&#39;read a post&#39;);     // define a perm: readPost
$auth-&gt;createOperation(&#39;updatePost&#39;,&#39;update a post&#39;); // define a perm: updatePost
$auth-&gt;createOperation(&#39;deletePost&#39;,&#39;delete a post&#39;); // define a perm: deletePost

// qualifier-permission
// define a qualifier on domain(user)
$bizRule=&#39;return Yii::app()-&gt;user-&gt;id==$params[&quot;post&quot;]-&gt;authID;&#39;;

// define a perm with qualifier: listOwnPosts
$perm=$auth-&gt;createTask(&#39;listOwnPosts&#39;, &#39;list posts by author himself&#39;, $bizRule);
$perm-&gt;addChild(&#39;listPosts&#39;); // sub perm

// define a perm with qualifier: updateOwnPost
$perm=$auth-&gt;createTask(&#39;updateOwnPost&#39;,&#39;update a post by author himself&#39;,$bizRule);
$perm-&gt;addChild(&#39;updatePost&#39;); // sub perm

// define a role: reader
$role=$auth-&gt;createRole(&#39;reader&#39;);
$role-&gt;addChild(&#39;readPost&#39;); // sub perm

// define a role: author
$role=$auth-&gt;createRole(&#39;author&#39;);
$role-&gt;addChild(&#39;reader&#39;); // sub role
$role-&gt;addChild(&#39;createPost&#39;); // sub perm
$role-&gt;addChild(&#39;updateOwnPost&#39;); // sub perm

// define a role: editor
$role=$auth-&gt;createRole(&#39;editor&#39;);
$role-&gt;addChild(&#39;reader&#39;); // sub role
$role-&gt;addChild(&#39;updatePost&#39;); // sub perm

// define a role: admin
$role=$auth-&gt;createRole(&#39;admin&#39;);
$role-&gt;addChild(&#39;editor&#39;); // sub role
$role-&gt;addChild(&#39;author&#39;); // sub role
$role-&gt;addChild(&#39;deletePost&#39;); // sub perm
</code></pre>

<p>实现效果：</p>

<p><img src="https://upload-images.jianshu.io/upload_images/594774-fb67d571497fb0b2.gif" alt=""/></p>

<blockquote>
<p>图片来自 <a href="https://link.jianshu.com?t=http://www.yiiframework.com/wiki/136/getting-to-understand-hierarchical-rbac-scheme/">Yii 官方 WiKi</a></p>
</blockquote>

<p>在这个 Yii 的官方例子中，<code>updateOwnPost</code>在判断用户是否具有<code>updatePost</code>权限的基础上更进一步判断了用户是否有权限操作这个特定的对象，并且这个判断逻辑是通过代码设置的，非常灵活。</p>

<p>不过大部分时候我们并不需要这样的灵活程度，会带来额外的开发和维护成本，而另一种基于模式匹配规则的对象权限控制可能更适合。例如判断用户是否对 Id 为 123 的文章具有编辑的权限，代码可能是这样的：</p>

<pre><code>&lt;?php

// 假设articleId是动态获取的
$articleId = 123;

if ($user-&gt;can(&quot;article:edit:{$articleId}&quot;)) {
    // ...
}

</code></pre>

<p>而给用户授权则有多种方式可以选择：</p>

<pre><code>&lt;?php

// 允许用户编辑Id为123的文章
$user-&gt;grant(&#39;article:edit:123&#39;);

// 使用通配符，允许用户编辑所有文章
$user-&gt;grant(&#39;article:edit:*&#39;);

</code></pre>

<p>虽然不及 Yii 方案的灵活，但某些场景下这样就够用了。</p>

<p>如果大家还有更好的方案，欢迎在评论中提出。</p>

<h3 id="toc_13">基于属性的权限验证（ABAC: Attribute-Based Access Control）</h3>

<p>ABAC 被一些人称为是权限系统设计的未来。</p>

<p>不同于常见的将用户通过某种方式关联到权限的方式，ABAC 则是通过动态计算一个或一组属性来是否满足某种条件来进行授权判断（可以编写简单的逻辑）。属性通常来说分为四类：用户属性（如用户年龄），环境属性（如当前时间），操作属性（如读取）和对象属性（如一篇文章，又称资源属性），所以理论上能够实现非常灵活的权限控制，几乎能满足所有类型的需求。</p>

<p>例如规则：“允许所有班主任在上课时间自由进出校门”这条规则，其中，“班主任”是用户的角色属性，“上课时间”是环境属性，“进出”是操作属性，而 “校门” 就是对象属性了。为了实现便捷的规则设置和规则判断执行，ABAC 通常有配置文件（XML、YAML 等）或 DSL 配合规则解析引擎使用。XACML（eXtensible Access Control Markup Language）是 ABAC 的一个实现，但是该设计过于复杂，我还没有完全理解，故不做介绍。</p>

<p>总结一下，ABAC 有如下特点：</p>

<ol>
<li> 集中化管理</li>
<li> 可以按需实现不同颗粒度的权限控制</li>
<li> 不需要预定义判断逻辑，减轻了权限系统的维护成本，特别是在需求经常变化的系统中</li>
<li> 定义权限时，不能直观看出用户和对象间的关系</li>
<li> 规则如果稍微复杂一点，或者设计混乱，会给管理者维护和追查带来麻烦</li>
<li> 权限判断需要实时执行，规则过多会导致性能问题</li>
</ol>

<p>既然 ABAC 这么好，那最流行的为什么还是 RBAC 呢？</p>

<p>我认为主要还是因为大部分系统对权限控制并没有过多的需求，而且 ABAC 的管理相对来说太复杂了。<a href="https://link.jianshu.com?t=http://blog.kubernetes.io/2017/04/rbac-support-in-kubernetes.html">Kubernetes 便因为 ABAC 太难用，在<code>1.8</code>版本里引入了 RBAC 的方案</a>。</p>

<blockquote>
<p>ABAC 有时也被称为 PBAC（Policy-Based Access Control）或 CBAC（Claims-Based Access Control）。</p>
</blockquote>

<h2 id="toc_14">结语</h2>

<p>权限系统设计可谓博大精深，这篇文章只是介绍了一点皮毛。</p>

<p>随着人类在信息化道路上越走越远，权限系统的设计也在不断创新，但目前好像处在了平台期。</p>

<p>可能因为在 RBAC 到 ABAC 之间有着巨大的鸿沟，无法轻易跨越，也可能是一些基于 RBAC 的微创新方案还不够规范化从而做到普及。不过在服务化架构的浪潮下，未来这一块必然有极高的需求，也许巨头们已经开始布局了。</p>

<h2 id="toc_15">参考文档</h2>

<p><a href="https://link.jianshu.com?t=https://support.microsoft.com/en-us/help/949608/changes-to-the-default-ntfs-discretionary-access-control-list-dacl-set">NTFS 文件系统权限</a></p>

<p><a href="https://link.jianshu.com?t=https://docs.oracle.com/cd/E56344_01/html/E53956/rbac-28.html#scrolltoc">Solaris 权限模型</a></p>

<p><a href="https://link.jianshu.com?t=https://baike.baidu.com/item/%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/8545517">百度百科：访问控制</a></p>

<p><a href="https://link.jianshu.com?t=https://www.centos.org/docs/5/html/Deployment_Guide-en-US/sec-mls-ov.html">Red Hat: Multi-Level Security (MLS)</a></p>

<p><a href="https://link.jianshu.com?t=http://files.cnblogs.com/files/Wenzy/an_introduction_to_rbac.pdf">冰云：An Introduction To Role-Based Access Control</a></p>

<p><a href="https://link.jianshu.com?t=https://csrc.nist.gov/projects/role-based-access-control">NIST: Role-Based Access Control</a></p>

<p><a href="https://link.jianshu.com?t=https://docs.mongodb.com/manual/core/authorization/">MongoDB RBAC</a></p>

<p><a href="https://link.jianshu.com?t=https://stackoverflow.com/questions/7770728/group-vs-role-any-real-difference">Stackoverflow: Group vs role Any real difference?</a></p>

<p><a href="https://link.jianshu.com?t=http://www.yiiframework.com/wiki/136/getting-to-understand-hierarchical-rbac-scheme/">Yii: Getting to Understand Hierarchical RBAC Scheme</a></p>

<p><a href="https://link.jianshu.com?t=http://www.informit.com/articles/article.aspx?p=782116">Role-Based Access Control in Computer Security</a></p>

<p><a href="https://link.jianshu.com?t=http://www.cis.syr.edu/%7Ewedu/Teaching/cis643/LectureNotes_New/RBAC.pdf">(Syracuse University: Role-Based Access Control (RBAC)</a></p>

<p><a href="https://link.jianshu.com?t=http://www.yiiframework.com/doc-2.0/guide-security-authorization.html">Yii 2.0 Guide</a></p>

<p><a href="https://link.jianshu.com?t=https://en.wikipedia.org/wiki/Computer_access_control">WIKIPEDIA: Computer access control</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[“Exit Trap” 让你的 Bash 脚本更稳固可靠]]></title>
    <link href="http://panlw.github.io/15263920307765.html"/>
    <updated>2018-05-15T21:47:10+08:00</updated>
    <id>http://panlw.github.io/15263920307765.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://linux.cn/article-9639-1.html">https://linux.cn/article-9639-1.html</a> 2017-02-17<br/>
作者：<a href="http://redsymbol.net/">aaron maxwell</a> 译者：<a href="https://github.com/Dotcra">Dotcra</a> 校对：<a href="https://github.com/wxy">wxy</a><br/>
作者简介：美国加利福尼亚旧金山的作家，软件工程师，企业家。<a href="https://www.amazon.com/d/0692878971">Powerful Python</a> 的作者，他的 <a href="https://powerfulpython.com/blog/">blog</a>。</p>
</blockquote>

<p>有个简单实用的技巧可以让你的 bash 脚本更稳健 -- 确保总是执行必要的收尾工作，哪怕是在发生异常的时候。要做到这一点，秘诀就是 bash 提供的一个叫做 EXIT 的伪信号，你可以 <a href="http://www.gnu.org/software/bash/manual/bashref.html#index-trap">trap</a> 它，当脚本因为任何原因退出时，相应的命令或函数就会执行。我们来看看它是如何工作的。</p>

<p>基本的代码结构看起来像这样：</p>

<pre><code class="language-sh">#!/bin/bash
function finish {
  # 你的收尾代码
}
trap finish EXIT
</code></pre>

<p>你可以把任何你觉得务必要运行的代码放在这个 <code>finish</code> 函数里。一个很好的例子是：创建一个临时目录，事后再删除它。</p>

<pre><code class="language-sh">#!/bin/bash
scratch=$(mktemp -d -t tmp.XXXXXXXXXX)
function finish {
  rm -rf &quot;$scratch&quot;
}
trap finish EXIT
</code></pre>

<p>这样，在你的核心代码中，你就可以在这个 <code>$scratch</code> 目录里下载、生成、操作中间或临时数据了。<sup><a href="http://redsymbol.net/articles/bash-exit-traps/#footnote-1">注 1</a></sup></p>

<pre><code class="language-sh"># 下载所有版本的 linux 内核…… 为了科学研究！
for major in {1..4}; do
  for minor in {0..99}; do
    for patchlevel in {0..99}; do
      tarball=&quot;linux-${major}-${minor}-${patchlevel}.tar.bz2&quot;
      curl -q &quot;http://kernel.org/path/to/$tarball&quot; -o &quot;$scratch/$tarball&quot; || true
      if [ -f &quot;$scratch/$tarball&quot; ]; then
        tar jxf &quot;$scratch/$tarball&quot;
      fi
    done
  done
done
# 整合成单个文件
# 复制到目标位置
cp &quot;$scratch/frankenstein-linux.tar.bz2&quot; &quot;$1&quot;
# 脚本结束， scratch 目录自动被删除
</code></pre>

<p>比较一下如果不用 <code>trap</code> ，你是怎么删除 <code>scratch</code> 目录的：</p>

<pre><code class="language-sh">#!/bin/bash
# 别这样做！

scratch=$(mktemp -d -t tmp.XXXXXXXXXX)

# 在这里插入你的几十上百行代码

# 都搞定了，退出之前把目录删除
rm -rf &quot;$scratch&quot;
</code></pre>

<p>这有什么问题么？很多：</p>

<ul>
<li>  如果运行出错导致脚本提前退出， <code>scratch</code> 目录及里面的内容不会被删除。这会导致资料泄漏，可能引发安全问题。</li>
<li><p>如果这个脚本的设计初衷就是在脚本末尾以前退出，那么你必须手动复制粘贴 <code>rm</code> 命令到每一个出口。</p></li>
<li><p>这也给维护带来了麻烦。如果今后在脚本某处添加了一个 <code>exit</code> ，你很可能就忘了加上删除操作 -- 从而制造潜在的安全漏洞。</p></li>
</ul>

<h3 id="toc_0">无论如何，服务要在线</h3>

<p>另外一个场景： 想象一下你正在运行一些自动化系统运维任务，要临时关闭一项服务，最后这项服务需要重启，而且要万无一失，即使脚本运行出错。那么你可以这样做：</p>

<pre><code class="language-sh">function finish {
  # 重启服务
  sudo /etc/init.d/something start
}
trap finish EXIT
sudo /etc/init.d/something stop
# 主要任务代码

# 脚本结束，执行 finish 函数重启服务
</code></pre>

<p>一个具体的实例：比如 Ubuntu 服务器上运行着 MongoDB ，你要为 crond 写一个脚本来临时关闭服务并做一些日常维护工作。你应该这样写：</p>

<pre><code class="language-sh">function finish {
  # 重启服务
  sudo service mongdb start
}
trap finish EXIT
# 关闭 mongod 服务
sudo service mongdb stop
# （如果 mongod 配置了 fork ，比如 replica set ，你可能需要执行 “sudo killall --wait /usr/bin/mongod”）
</code></pre>

<h3 id="toc_1">控制开销</h3>

<p>有一种情况特别能体现 EXIT <code>trap</code> 的价值：如果你的脚本运行过程中需要初始化一下成本高昂的资源，结束时要确保把它们释放掉。比如你在 AWS (Amazon Web Services) 上工作，要在脚本中创建一个镜像。</p>

<p>（名词解释: 在亚马逊云上的运行的服务器叫 “<a href="http://aws.amazon.com/ec2/">实例</a>”。实例从<ruby>亚马逊机器镜像 <rt>Amazon Machine Image</rt></ruby> 创建而来，通常被称为 “AMI” 或 “镜像” 。AMI 相当于某个特殊时间点的服务器快照。）</p>

<p>我们可以这样创建一个自定义的 AMI ：</p>

<ol>
<li>基于一个基准 AMI 运行一个实例（例如，启动一个服务器）。</li>
<li>在实例中手动或运行脚本来做一些修改。</li>
<li>用修改后的实例创建一个镜像。</li>
<li>如果不再需要这个实例，可以将其删除。</li>
</ol>

<p>最后一步<strong>相当重要</strong>。如果你的脚本没有把实例删除掉，它会一直运行并计费。（到月底你的账单让你大跌眼镜时，恐怕哭都来不及了！）</p>

<p>如果把 AMI 的创建封装在脚本里，我们就可以利用 <code>trap</code> EXIT 来删除实例了。我们还可以用上 EC2 的命令行工具：</p>

<pre><code class="language-sh">#!/bin/bash
# 定义基准 AMI 的 ID
ami=$1
# 保存临时实例的 ID
instance=&#39;&#39;
# 作为 IT 人，让我们看看 scratch 目录的另类用法
scratch=$(mktemp -d -t tmp.XXXXXXXXXX)
function finish {
  if [ -n &quot;$instance&quot; ]; then
    ec2-terminate-instances &quot;$instance&quot;
  fi
  rm -rf &quot;$scratch&quot;
}
trap finish EXIT
# 创建实例，将输出(包含实例 ID )保存到 scratch 目录下的文件里
ec2-run-instances &quot;$ami&quot; &gt; &quot;$scratch/run-instance&quot;
# 提取实例 ID
instance=$(grep &#39;^INSTANCE&#39; &quot;$scratch/run-instance&quot; | cut -f 2)
</code></pre>

<p>脚本执行到这里，实例（EC2 服务器）已经开始运行 <sup><a href="http://redsymbol.net/articles/bash-exit-traps/#footnote-2">注 2</a>。接下来你可以做任何事情：在实例中安装软件，修改配置文件等，然后为最终版本创建一个镜像。实例会在脚本结束时被删除</sup> -- 即使脚本因错误而提前退出。（请确保实例创建成功后再运行业务代码。）</p>

<h3 id="toc_2">更多应用</h3>

<p>这篇文章只讲了些皮毛。我已经使用这个 bash 技巧很多年了，现在还能不时发现一些有趣的用法。你也可以把这个方法应用到你自己的场景中，从而提升你的 bash 脚本的可靠性。</p>

<h3 id="toc_3">尾注</h3>

<ul>
<li>  注 1. <code>mktemp</code> 的选项 <code>-t</code> 在 Linux 上是可选的，在 OS X 上是必需的。带上此选项可以让你的脚本有更好的可移植性。</li>
<li>  注 2. 如果只是为了获取实例 ID ，我们不用创建文件，直接写成 <code>instance=$(ec2-run-instances &quot;$ami&quot; | grep &#39;^INSTANCE&#39; | cut -f 2)</code> 就可以。但把输出写入文件可以记录更多有用信息，便于调试 ，代码可读性也更强。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Guide To NIO2 Asynchronous File Channel]]></title>
    <link href="http://panlw.github.io/15262259555946.html"/>
    <updated>2018-05-13T23:39:15+08:00</updated>
    <id>http://panlw.github.io/15262259555946.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="http://www.baeldung.com/java-nio2-async-file-channel">http://www.baeldung.com/java-nio2-async-file-channel</a><br/>
Last modified: January 12, 2018</p>
</blockquote>

<h3 id="toc_0"><strong>I just announced the new <u>Spring 5</u> modules in REST With Spring:</strong></h3>

<h2 id="toc_1"><strong>1. Overview</strong></h2>

<p>In this article, we are going to explore one of the key additional APIs of the new I/O (NIO2) in Java 7, asynchronous file channel APIs.</p>

<p>If you are new to asynchronous channel APIs in general, we have an introductory article on this site which you can read by following <a href="/java-nio-2-async-channels">this link</a> before proceeding.</p>

<p>You can read more about NIO.2 <a href="/java-nio-2-file-api">file operations</a> and <a href="/java-nio-2-path">path operations</a> as well – understanding these will make this article much easier to follow.</p>

<p>To use the NIO2 asynchronous file channels in our projects, we have to import the <u>java.nio.channels</u> package as it bundles all required classes:</p>

<pre><code class="language-java">import java.nio.channels.*;
</code></pre>

<h2 id="toc_2"><strong>2. The <u>AsynchronousFileChannel</u></strong></h2>

<p>In this section, we will explore how to use the main class that enables us to perform asynchronous operations on files, the <u>AsynchronousFileChannel</u> class. To create an instance of it, we call the static <u>open</u> method:</p>

<pre><code class="language-java">Path filePath = Paths.get(&quot;/path/to/file&quot;);
 
AsynchronousFileChannel fileChannel = AsynchronousFileChannel.open(
  filePath, READ, WRITE, CREATE, DELETE_ON_CLOSE);
</code></pre>

<p>All enum values come from the Sta_ndardOpenOption_.</p>

<p>The first parameter to the open API is a <u>Path</u> object representing the file location. To read more about path operations in NIO2, follow <a href="/java-nio-2-path">this link</a>. The other parameters make up a set specifying options that should be available to the returned file channel.</p>

<p>The asynchronous file channel we have created can be used to perform all known operations on a file. To perform only a subset of the operations, we would specify options for only those. For instance, to only read:</p>

<pre><code class="language-java">Path filePath = Paths.get(&quot;/path/to/file&quot;);
 
AsynchronousFileChannel fileChannel = AsynchronousFileChannel.open(
  filePath, StandardOpenOption.READ);
</code></pre>

<h2 id="toc_3"><strong>3. Reading From a File</strong></h2>

<p>Just like with all asynchronous operations in NIO2, reading a file’s contents can be done in two ways. Using <u>Future</u> and using <u>CompletionHandler</u>. In each case, we use the <u>read</u> API of the returned channel.</p>

<p>Inside the test resources folder of maven or in the source directory if not using maven, let’s create a file called <u>file.txt</u> with only the text <u>baeldung.com</u> at it’s beginning. We will now demonstrate how to read this content.</p>

<h3 id="toc_4"><strong>3.1. The Future Approach</strong></h3>

<p>First, we will see how to read a file asynchronously using the <u>Future</u> class:</p>

<pre><code class="language-java">@Test
public void givenFilePath_whenReadsContentWithFuture_thenCorrect() {
    Path path = Paths.get(
      URI.create(
        this.getClass().getResource(&quot;/file.txt&quot;).toString()));
    AsynchronousFileChannel fileChannel = AsynchronousFileChannel.open(
      path, StandardOpenOption.READ);
 
    ByteBuffer buffer = ByteBuffer.allocate(1024);
 
    Future&lt;Integer&gt; operation = fileChannel.read(buffer, 0);
 
    // run other code as operation continues in background
    operation.get();
       
    String fileContent = new String(buffer.array()).trim();
    buffer.clear();
 
    assertEquals(fileContent, &quot;baeldung.com&quot;);
}
</code></pre>

<p>In the above code, after creating a file channel, we make use of the <u>read</u> API – which takes a <u>ByteBuffer</u> to store the content read from the channel as its first parameter.</p>

<p>The second parameter is a long indicating the position in the file from which to start reading.</p>

<p>The method returns right away whether the file has been read or not.</p>

<p>Next, we can execute any other code as the operation continues in the background. When we are done with executing other code, we can call the <u>get()</u> API which returns right away if the operation already completed as we were executing other code, or else it blocks until the operation completes.</p>

<p>Our assertion indeed proves that the content from the file has been read.</p>

<p>If we had changed the position parameter in the <u>read</u> API call from zero to something else, we would see the effect too. For example, the seventh character in the string <u>baeldung.com</u> is <u>g</u>. So changing the position parameter to 7 would cause the buffer to contain the string <u>g.com</u>.</p>

<h3 id="toc_5"><strong>3.2. The <u>CompletionHandler</u> Approach</strong></h3>

<p>Next, we will see how to read a file’s contents using a <u>CompletionHandler</u> instance:</p>

<pre><code class="language-java">@Test
public void
  givenPath_whenReadsContentWithCompletionHandler_thenCorrect() {
     
    Path path = Paths.get(
      URI.create( this.getClass().getResource(&quot;/file.txt&quot;).toString()));
    AsynchronousFileChannel fileChannel 
      = AsynchronousFileChannel.open(path, StandardOpenOption.READ);
 
    ByteBuffer buffer = ByteBuffer.allocate(1024);
 
    fileChannel.read(
      buffer, 0, buffer, new CompletionHandler&lt;Integer, ByteBuffer&gt;() {
 
        @Override
        public void completed(Integer result, ByteBuffer attachment) {
            // result is number of bytes read
            // attachment is the buffer containing content
        }
        @Override
        public void failed(Throwable exc, ByteBuffer attachment) {
 
        }
    });
}
</code></pre>

<p>In the above code, we use the second variant of the <u>read</u> API. It still takes a <u>ByteBuffer</u> and the start position of the <u>read</u> operation as the first and second parameters respectively. The third parameter is the <u>CompletionHandler</u> instance.</p>

<p>The first generic type of the completion handler is the return type of the operation, in this case, an Integer representing the number of bytes read.</p>

<p>The second is the type of the attachment. We have chosen to attach the buffer such that when the <u>read</u> completes, we can use the content of the file inside the <u>completed</u> callback API.</p>

<p>Semantically speaking, this is not really a valid unit test since we cannot do an assertion inside the <u>completed</u> callback method. However, we do this for the sake of consistency and because we want our code to be as <u>copy-paste-run-</u>able as possible.</p>

<h2 id="toc_6"><strong>4. Writing to a File</strong></h2>

<p>Java NIO2 also allows us to perform write operations on a file. Just as we did with other operations, we can write to a file in two ways. Using <u>Future</u> and using <u>CompletionHandler</u>. In each case, we use the <u>write</u> API of the returned channel.</p>

<p>Creating an <u>AsynchronousFileChannel</u> for writing to a file can be done like this:</p>

<pre><code class="language-java">AsynchronousFileChannel fileChannel
  = AsynchronousFileChannel.open(path, StandardOpenOption.WRITE);
</code></pre>

<h3 id="toc_7"><strong>4.1. Special Considerations</strong></h3>

<p>Notice the option passed to the <u>open</u> API. We can also add another option <u>StandardOpenOption.CREATE</u> if we want the file represented by a <u>path</u> to be created in case it does not already exist. Another common option is <u>StandardOpenOption.APPEND</u> which does not over-write existing content in the file.</p>

<p>We will use the following line for creating our file channel for test purposes:</p>

<pre><code class="language-java">AsynchronousFileChannel fileChannel = AsynchronousFileChannel.open(
  path, WRITE, CREATE, DELETE_ON_CLOSE);
</code></pre>

<p>This way, we will provide any arbitrary path and be sure that the file will be created. After the test exits, the created file will be deleted. To ensure the files created are not deleted after the test exits, you can remove the last option.</p>

<p>To run assertions, we will need to read the file content where possible after writing to them. Let’s hide the logic for reading in a separate method to avoid redundancy:</p>

<pre><code class="language-java">public static String readContent(Path file) {
    AsynchronousFileChannel fileChannel = AsynchronousFileChannel.open(
      file, StandardOpenOption.READ);
 
    ByteBuffer buffer = ByteBuffer.allocate(1024);
 
    Future&lt;Integer&gt; operation = fileChannel.read(buffer, 0);
 
    // run other code as operation continues in background
    operation.get();     
 
    String fileContent = new String(buffer.array()).trim();
    buffer.clear();
    return fileContent;
}
</code></pre>

<h3 id="toc_8"><strong>4.2. The <u>Future</u> Approach</strong></h3>

<p>To write to a file asynchronously using the <u>Future</u> class:</p>

<pre><code class="language-java">@Test
public void
  givenPathAndContent_whenWritesToFileWithFuture_thenCorrect() {
     
    String fileName = UUID.randomUUID().toString();
    Path path = Paths.get(fileName);
    AsynchronousFileChannel fileChannel = AsynchronousFileChannel.open(
      path, WRITE, CREATE, DELETE_ON_CLOSE);
 
    ByteBuffer buffer = ByteBuffer.allocate(1024);
 
    buffer.put(&quot;hello world&quot;.getBytes());
    buffer.flip();
 
    Future&lt;Integer&gt; operation = fileChannel.write(buffer, 0);
    buffer.clear();
 
    //run other code as operation continues in background
    operation.get();
 
    String content = readContent(path);
    assertEquals(&quot;hello world&quot;, content);
}
</code></pre>

<p>Let’s inspect what is happening in the above code. We create a random file name and use it to get a <u>Path</u> object. We use this path to open an asynchronous file channel with the previously mentioned options.</p>

<p>We then put the content we want to write to the file in a buffer and perform the <u>write</u>. We use our helper method to read the contents of the file and indeed confirm that it is what we expect.</p>

<h3 id="toc_9"><strong>4.3. The <u>CompletionHandler</u> Approach</strong></h3>

<p>We can also use the completion handler so that we don’t have to wait for the operation to complete in a while loop:</p>

<pre><code class="language-java">@Test
public void
  givenPathAndContent_whenWritesToFileWithHandler_thenCorrect() {
     
    String fileName = UUID.randomUUID().toString();
    Path path = Paths.get(fileName);
    AsynchronousFileChannel fileChannel = AsynchronousFileChannel.open(
      path, WRITE, CREATE, DELETE_ON_CLOSE);
 
    ByteBuffer buffer = ByteBuffer.allocate(1024);
    buffer.put(&quot;hello world&quot;.getBytes());
    buffer.flip();
 
    fileChannel.write(
      buffer, 0, buffer, new CompletionHandler&lt;Integer, ByteBuffer&gt;() {
 
        @Override
        public void completed(Integer result, ByteBuffer attachment) {
            // result is number of bytes written
            // attachment is the buffer
        }
        @Override
        public void failed(Throwable exc, ByteBuffer attachment) {
 
        }
    });
}
</code></pre>

<p>When we call the write API this time, the only new thing is a third parameter where we pass an anonymous inner class of type <u>CompletionHandler</u>.</p>

<p>When the operation completes, the class calls it’s completed method within which we can define what should happen.</p>

<h2 id="toc_10"><strong>5. Conclusion</strong></h2>

<p>In this article, we have explored some of the most important features of the Asynchronous File Channel APIs of Java NIO2.</p>

<p>To get all code snippets and the full source code for this article, you can visit the <a href="https://github.com/eugenp/tutorials/tree/master/core-java-io">Github project</a>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[飞哥教你使用异步编程提升服务性能]]></title>
    <link href="http://panlw.github.io/15262258657164.html"/>
    <updated>2018-05-13T23:37:45+08:00</updated>
    <id>http://panlw.github.io/15262258657164.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://mp.weixin.qq.com/s/kchPiySDDu1FMF3tuEggiw">https://mp.weixin.qq.com/s/kchPiySDDu1FMF3tuEggiw</a></p>
</blockquote>

<p>作者：肖飞，2011 年加入京东，目前在交易平台，主导交易平台核心系统的架构优化和技术攻关，以及公共技术组件和平台的建设。</p>

<p>庞大复杂的系统通常会采用服务化组件来实现。系统越复杂，组件之间的依赖和调用关系也会越复杂。对于处于底层的基础服务，直接和间接的调用所带来的流量压力非常大。处于中间层的聚合型服务，面对的挑战则是依赖的服务太多，后端个别服务的性能延迟就会影响其吞吐量。性能优化是我们系统稳定性中的重要一环，这其中，调用所依赖的 RPC 服务或后端数据是重点之一。</p>

<p>目前，除了传统 JDBC 这样从 API 到主流驱动实现就是阻塞式的类库之外，其他常用的 RPC/HTTP 服务、MQ、Redis、Mongodb、Kafka 等系统都提供了成熟的基于 NIO 的客户端库，也有相应的异步 API。</p>

<p>但是目前交易平台的大多数中台服务系统，还在习惯性使用着这些库的同步 API，并不能充分的利用 CPU，这也给我们带来了一定的优化空间。从 16 年开始我们在一些核心的但是服务逻辑相对简单的系统中使用异步方式来实现，虽然暂时还做不到完全的异步化，但是也取得了比较好的效果。这篇文章虽然更多是一个简介性质，但是也涵盖了我们在异步编程中需要关注的要点。希望大家能够习惯和拥抱异步编程。</p>

<h2 id="toc_0"><strong>一、相关概念介绍</strong></h2>

<p>同步 (Synchronous)/ 异步 (Asynchronous)，通常是指函数调用中的消息通信的两种不同模式。</p>

<h3 id="toc_1"><strong>1、异步和同步的区别</strong></h3>

<p>函数调用发生时，消息 (参数) 从 caller 传递到 callee，控制权 (指令执行) 从 caller 转移到 callee。调用返回时，控制权从 callee 转移到 caller。两者的区别在于，callee 是否需要等待执行完成才将控制权转移给 caller。</p>

<p>在 RPC 这种更复杂的场景下，本质上并没有不同。</p>

<p>◆ 同步</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/MrFJyDNenF8OUIRCOltzbKCk6yXqAJP3BYu3bYrc81Uak1E6HfspC1tnU2KGnthJwVLAOaqr2OzlfVQhO1LqEg/640?wx_fmt=png" alt=""/></p>

<p>1.callee 执行完成才返回<br/>
2. 返回值即结果</p>

<p>◆ 异步</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/MrFJyDNenF8OUIRCOltzbKCk6yXqAJP3ENZ7lSUyf8z1icwTJr6rwVLiaNGLWfrrFHOa8kSfLQNVd8ch5dZiaoRsw/640?wx_fmt=png" alt=""/></p>

<p>1.callee 不需要执行完成就可返回<br/>
2.caller 要获取结果，需要通过轮询、回调等机制</p>

<h4 id="toc_2">◆ 同步 RPC</h4>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/MrFJyDNenF8OUIRCOltzbKCk6yXqAJP3jPWsYVcxUuMPzdeJ6sichCQjFM9iceIqvic0yHHKg1Oibexuwn7USoA3oA/640?wx_fmt=png" alt=""/></p>

<h4 id="toc_3">◆ 异步 RPC</h4>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/MrFJyDNenF8OUIRCOltzbKCk6yXqAJP33QaY0TSkO7pMFQwLRglBPiadzQBF2SpJxa8RDrGSKfRWCkuyUWXCGMg/640?wx_fmt=png" alt=""/></p>

<p>可以看到，在异步 RPC 的场景下，客户端和服务端用于处理 IO 的 CPU 能得到充分利用，通常只需要远低于 caller 请求数量的线程就可以了，这就是多路复用。</p>

<h3 id="toc_4"><strong>2、callee 执行机制</strong></h3>

<p>上图中 callee 的 background execute 通常是采用池化线程来完成的，比如 ThreadPoolExecutor 或 EventLoop1。</p>

<h3 id="toc_5"><strong>3、caller 获取执行结果</strong></h3>

<p>caller 调用 callee 时，如果需要获取执行结果（消息双向传递），或者获知执行是否完成（消息单向传递无返回值），在异步模式下，主要依靠下面两种机制。</p>

<h4 id="toc_6">◆ 轮询 (Polling)</h4>

<p>比如 Java 的 Future 就提供了 isDone() 这种询问机制。</p>

<pre><code class="language-java">//Caller.java
void call() {
     Future&lt;Void&gt; f = callee.asyncCall(param);
     // do some other things
     while (true) {
          if (f.isDone()) break;
        //do some other things or sleep or timeout
    }
}
</code></pre>

<p>或阻塞版本</p>

<pre><code class="language-java">//Caller.java
void call() {
    Future&lt;Void&gt; f = callee.asyncCall(param);
    // do some other things
    f.get(timeout, TimeUnit.SECONDS);
}
</code></pre>

<p>轮询的控制逻辑在 caller 端。</p>

<h4 id="toc_7">◆ 回调 (Callback)</h4>

<p>caller 设置一个回调函数，供 callee 执行完成后调用这个函数。回调的控制是反转的，通常由 callee 端控制。</p>

<pre><code class="language-java">//Caller.java
void call() {
    callee.asyncCall(param, new AsyncHandler&lt;Response&lt;Message&gt;&gt;() {
        @Override public void handleResponse(Response&lt;Message&gt; response) {
            msg = response.get();
            // process the msg...
        }
    });
    // do some other things
}
</code></pre>

<h3 id="toc_8"><strong>4、异步模式的场景</strong></h3>

<h4 id="toc_9">◆ 阻塞</h4>

<p>阻塞 (Blocking)/ 非阻塞(Non-Blocking) 是用来描述，在等待调用结果时 caller 线程的状态。阻塞，通常意味着 caller 线程不再使用 CPU 时间，处于可被 OS 调度的状态(注意与 Java 线程状态 2 的区别)。 磁盘 IO 和网络 IO 是常见的会引起线程阻塞的场景 3。受制于底层 OS 的同步阻塞式 IO 系统函数，调用 Java OIO(Old blocking IO) API 无疑是会阻塞的。对于 DiskIO，Java NIO2 提供了异步 API。对于 SocketIO，Java NIO2 以及 NIO 框架 Netty，都提供了异步 API。</p>

<blockquote>
<p>◆ Linux 提供了异步 IO 系统函数，只能用于 DiskIO，还有一些限制 4，Java NIO2 AsynchronousFileChannel 内部仍然使用线程池 + 阻塞式 API 的实现。</p>

<p>◆ Linux 为 SocketIO 准备就绪阶段提供了非阻塞式 API(select/poll/epoll)，但是 IO 执行阶段仍然是同步阻塞的，因此主流的 Java NIO 框架的 Reactor 模式内部实现使用了线程池。</p>
</blockquote>

<h4 id="toc_10">◆ 并行</h4>

<p>比如需要调用多个没有依赖关系的服务，或者访问分散在多个存储分片中的数据，如果服务接口或数据访问接口实现了异步 API，那么就很方便实现并行调用，减少总体调用耗时。</p>

<h4 id="toc_11">◆ 速度不匹配</h4>

<p>使用中间队列解偶 caller 和 callee 的速度不匹配问题，削峰填谷。</p>

<h4 id="toc_12">◆ 批量</h4>

<p>使用中间队列解偶 caller 和 callee 的速度不匹配问题，削峰填谷。</p>

<h2 id="toc_13"><strong>二、异步 API 的几种风格</strong></h2>

<h3 id="toc_14"><strong>1、Callback</strong></h3>

<p>这个比较传统，比如 zookeeper 客户端提供的基于回调的异步 API:</p>

<pre><code class="language-java">try {
    zookeeper.create(path, data, acl, createMode, new StringCallback() {
        public void processResult(int rc, String path, Object ctx, String name) {
            if (rc != KeeperException.Code.OK.intValue()) {
                // error handle
            } else {
                // success process
                // 如果需要在成功后再发起基于回调的异步调用，会形成callback hell
            }
        }
    }, ctx);
} catch( Throwable e) {
    // error handle
}
</code></pre>

<blockquote>
<p>◆ Callback 通常是无状态的 </p>

<p>◆ 要获取 Callback 的计算结果，通常需要 closure </p>

<p>◆ 异常处理比较分散 </p>

<p>◆ 在有多个异步调用链的时候，容易造成 Callback hell</p>
</blockquote>

<h3 id="toc_15"><strong>2、Future/Promise</strong></h3>

<p>Promise 是 callee 给 caller 的凭证，代表未完成但承诺完成（成功或失败）的结果。Promise 本身是有状态的，通常由 callee 端维护。其状态转移如下（术语参考 Promise/A + 和 ES6）：</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/MrFJyDNenF8OUIRCOltzbKCk6yXqAJP3dnnIwboxWueXlrTo9wSvNYa6AfRIfwibZobCqckibNSvwyEzLd3GyO9Q/640?wx_fmt=png" alt=""/></p>

<blockquote>
<p>Promise 的状态只能转移一次，因此如果有 callback，那么. then(callback) 或. catch(callback) 也只被执行一次。</p>
</blockquote>

<p>JDK5 的 Future 只能用轮询或者阻塞的方式获取结果，caller 端处理比较繁琐。Guava 的 ListenableFuture，特别是 JDK8 的 CompletableFuture，则是完整实现了 Promise 风格的异步 API。 个人认为 Promise 是更好的 Callback，ListenableFuture 接口只是比 Future 多了一个 void addListener(Runnable, Executor) 方法。 Promise 提供了比 Callback 更易用更清晰的编程模式，尤其是涉及多个异步 API 的串行调用（chaining 或 pipelining )、组合调用（并行、合并）、异常处理等方面有很大的优势。</p>

<blockquote>
<p><strong>引申阅读</strong> </p>

<p>◆ 这篇文章谈到了 Future 和 Promise 的细微区别、相关历史和技术。 </p>

<p>◆ 这里有一些讨论：Aren’t promises just callbacks?,Is there really a fundamental difference between callbacks and Promises?。 </p>

<p>◆ Promise 借鉴了函数式中的一些概念: 从函数式编程到 Promise。 </p>

<p>◆ 这篇文章简要对比了几种语言中的 Promise 框架。</p>
</blockquote>

<h3 id="toc_16"><strong>3、ReactiveX</strong></h3>

<p>其官方网站的介绍</p>

<blockquote>
<p>An API for asynchronous programming with observable streams</p>
</blockquote>

<p>其关键的概念 Observable 比较 Promise 来说：</p>

<p>◆ Promise 代表一个异步计算值，而 Observable 代表着一系列值 (stream)。 </p>

<p>◆ Promise 的值只能产生一次，而 Observable 的事件可以不断产生。因此 Rx 首先流行在前端 UI 场景：事件来源多，数据变化影响多个 UI 组件的变更。</p>

<p>Rx 的学习曲线比 Promise 要高得多，而且目前 Promise 风格的异步编程能够满足我们大部分的服务端开发场景，因此我们这里主要关注 Promise。</p>

<h2 id="toc_17"><strong>三、Promise 在服务端的应用</strong></h2>

<p>下面穿插着以 JDK8 的 CompletableFuture 和 Guava 的 ListenableFuture（适用 JDK6）为例介绍 Promise 的用法。</p>

<h3 id="toc_18"><strong>1、符合 Promise 风格的方法签名</strong></h3>

<p>Promise 风格的方法签名，有个不成文的规则是不抛出异常，因为异常是 Promise 对象本身就能携带的两种状态之一。比如我们想把一个 Callback 风格的异步 API 包装成 Promise 风格的（通常在使用一个较老的类库时需要这样的包装），可以这样：</p>

<pre><code class="language-java">//caller.java
CompletableFuture&lt;String&gt; asyncCall(final String msg) {
    CompletableFuture&lt;String&gt; promise = new CompletableFuture&lt;&gt;();
    try {
        callee.asyncCall(msg, new Callback&lt;String&gt;() {
            public void onSuccess(String r) { promise.complete(r); }
            public void onFail(Throwable t) { promise.completeExceptionally(t); }
        });
    } catch (Throwable e) {
        promise.completeExceptionally(t);
    }
    return promise;
}
</code></pre>

<p>下面是使用 ListenableFuture 的实现异步发送消息的 API。</p>

<pre><code class="language-java">//LocalMessageEngine.java
static class WriteTask {
    // SettableFuture是Guava中一种可设置状态的Promise类型。
    final SettableFuture&lt;Boolean&gt; promise = SettableFuture.create();
    final byte[] message;
    WriteTask(byte[] message) {
        this.message = message;
    }
}
public Producer createProducer() {
    return new Producer() {
        public ListenableFuture&lt;Boolean&gt; asyncProduce(byte[] message) {
            if (!Engine.this.started) {
                // 返回前已完成
                return Futures.immediateFailedFuture(new IllegalStateException(&quot;Message engine was stopped or not started&quot;));
            }

            WriteTask task = new WriteTask(message);
            boolean queued = writeTaskQueue.offer(task);
            if (!queued) {
                task.promise.set(Boolean.FALSE);
            }
            return task.promise;
        }
    };
}
</code></pre>

<p>上面两个例子，描述了如何创建一个 Promise 对象返回给 caller，以及如何在 callee 端 fulfill 或 reject 这个 Promise。你可能会发现，返回给 caller 之前 Promise 是可以处于完成状态的。在继续下面的使用介绍前，先简单的看下 ListenableFuture 和 CompletableFuture 的几个主要 API。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/MrFJyDNenF8OUIRCOltzbKCk6yXqAJP3ibyDlfaV7ZWzC5wCKpCF3S5ia5bicPE3gnEUibA9xWOafp3uOnPiaxrLLhQ/640?wx_fmt=png" alt=""/></p>

<p><strong>2、串行调用</strong></p>

<p>ListenableFuture 没有提供 then 方法，而是通过 Futures 的一系列静态方法来实现 Promise 风格的 API。由于两者有大部分的 API 是可以相互转化的，限于篇幅下面就不全部演示了。Promse 的 callback 就是其 then 或 catch 方法的函数型参数，当 Promise 被 resolve 时执行这个 callback 函数。这个 callback 函数的输入，就是 Promise 的 resolved 值。根据 callback 函数的输出的不同，需要采取不同的 then 方法。</p>

<h4 id="toc_19">◆ callback 无输出</h4>

<p>Futures.addCallback 和 CompletableFuture.thenAccept 接受无输出的 callback。</p>

<pre><code class="language-java">// Guava
ListenableFuture&lt;QueryResult&gt; promise1 = ...;
Futures.addCallback(promise1, new FutureCallback&lt;QueryResult&gt;() {
    public void onSuccess(QueryResult result) {
        storeInLocalCache(result);
    }
    public void onFailure(Throwable t) {
        reportError(t);
    }
});
</code></pre>

<pre><code class="language-java">// CompletableFuture
CompletableFuture&lt;QueryResult&gt; promise1 = ...;
CompletableFuture&lt;Void&gt; promise2 = promise1.thenAccept(result -&gt; storeInLocalCache(result));
return promise2;
//return promise1.thenAccept(result -&gt; storeInLocalCache(result));//thenAccept返回另一个Promise实例
</code></pre>

<p>先忽略异常处理。对比下这种场景下的 ListenableFuture 和 CompletableFuture：前者采取了更传统的 callback 风格，后者则返回一个新的 Promise 实例，callback 计算完毕则 promise2 被 fulfilled，很容易通过 promise2 来获取 callback 执行完毕与否，不需要 closure。</p>

<h4 id="toc_20">◆ callback 输出一个普通计算值</h4>

<p>这种情况下 callback 就是一个转换函数，输入是前一个 Promise 的 fulfilled 值，输出则作为新 Promise 的 fulfilled 值。Futures.transform 和 CompletableFuture.thenApply 接收这样的 callback 函数。</p>

<pre><code class="language-java">// CompletableFuture
CompletableFuture&lt;QueryResult&gt; queryFuture = ...;
CompletableFuture&lt;List&lt;Row&gt;&gt; rowsFuture = queryFuture.thenApply(result -&gt; result.getRows());
return rowsFuture;
</code></pre>

<h4 id="toc_21">◆ callback 输出一个异步计算值，即一个 Promise</h4>

<p>乍一看，这种情况下的输出跟上一种好像没什么区别。但实际上，输出一个 Promise 值和输出一个普通的值有根本的区别。还记得吧，Promise 代表着一个未完成的并且承诺完成的值。通常这种情况下，意味着 callback 里调用了另外一个 Promise 风格的异步 API。比如下面的例子中 indexService.lookUp 和 dataService.read 方法，由于涉及到 IO，都设计为异步 API。</p>

<pre><code class="language-java">//Guava
ListenableFuture&lt;RowKey&gt; rowKeyFuture = indexService.lookUp(query);
AsyncFunction&lt;RowKey, QueryResult&gt; queryFunction = new AsyncFunction&lt;RowKey, QueryResult&gt;() {
    public ListenableFuture&lt;QueryResult&gt; apply(RowKey rowKey) {
        return dataService.read(rowKey);
    }
};
ListenableFuture&lt;QueryResult&gt; queryFuture = Futures.transformAsync(rowKeyFuture, queryFunction);
return queryFuture;
</code></pre>

<p>Futures.transformAsync 和 CompletableFuture.thenCompose 接收这样的 callback 函数。</p>

<p>设想一下，如果某个逻辑中需要调用的多个 Promise 风格的异步方法（比如多个 RPC 调用），并且有先后依赖关系，即上一个方法的执行结果作为下一个方法的输入。就可以用 thenCompose 把他们串起来。</p>

<pre><code class="language-java">//CompletableFuture
CompletableFuture&lt;RPC4Result&gt; promise4 = rpc1.call(input) //promise1
    .thenCompose(rpc1Result -&gt; rpc2.call(rpc1Result)) //promise2
    .thenCompose(rpc2Result -&gt; rpc3.call(rpc2Result)) //promise3
    .thenCompose(rpc3Result -&gt; rpc4.call(rpc3Result)) //promise4
return promise4;
</code></pre>

<p>不要被链式调用给忽悠了，你还是可以正常使用普通的风格。</p>

<p>单纯看来，上述的串行调用场景下使用 Promise 风格的 API 好像只是消除了 Callback hell。那么采用同步 API 就既没有 Callback hell 的问题，又符合数据依赖关系。可是，你会发现，上面的举例中结尾都返回了 Promise，就是说，包含这段代码的方法被设计为异步 API。而使用同步 API，则会强制这个方法的调用者只能使用同步方式调用。</p>

<h3 id="toc_22"><strong>3、并行调用</strong></h3>

<p>异步 API 很适合并行调用。caller 在调用多个没有依赖关系的异步 API 时，可以先依次发起调用而不用等待每个调用真正执行完成，从 callee 的角度来讲，执行是并行的。caller 可以对调用结果进行合并处理，关键是，合并也是异步风格的。</p>

<pre><code class="language-java">//Guava
List&lt;ListenableFuture&lt;QueryResult&gt;&gt; partialPromises = new ArrayList&lt;ListenableFuture&lt;QueryResult&gt;&gt;(nodes.size());
for (Node node : nodes) {
    partialPromises.add(lookupHandler(node).query());
}
ListenableFuture&lt;List&lt;Row&gt;&gt; mergedPromise = Futures.transform(Futures.allAsList(partialPromises), new Function&lt;List&lt;List&lt;Row&gt;&gt;, List&lt;Row&gt;&gt;() {
    @Override public Long apply(List&lt;List&lt;Row&gt;&gt; input) {
        return merge(input);
    }
})
return mergedPromise;
</code></pre>

<p>Futures.allAsList 是并行执行所有的 promises，若有一个 promise 异常完成则尝试 reject 尚未 resolved 的 promise。也可以使用 Futures.successfulAsList，区别在于后者并不会 reject 尚未 resolved 的 promise。CompletableFuture 的对应物是 allOf 和 anyOf。</p>

<h3 id="toc_23"><strong>4、调用编排</strong></h3>

<p>合并结果设计为异步风格的好处在于，很方便做合并、串行混合调用编排，比如某个逻辑中需要调用四个个 RPC 服务 A、B、C、D，其中：A 的输出作为 B、C 的输入，B、C 可并行，B、C 的输出合并后作为 D 的输入。</p>

<pre><code class="language-java">//CompletableFuture
CompletableFuture&lt;AResult&gt; promiseA = rpcA.call(input);
CompletableFuture&lt;DResult&gt; promiseD = promiseA.thenCompose(aResult -&gt; {
    CompletableFuture&lt;BResult&gt; promiseB = rpcB.call(aResult);
    CompletableFuture&lt;CResult&gt; promiseC = rpcC.call(aResult);
    CompletableFuture&lt;MergedResult&gt; mergedPromise = promiseB.thenCombine(promiseC, (bResult, cResult) -&gt; {
        return merge(bResult, cResult)
    });
    return mergedPromise;
}).thenCompose(mergedResult -&gt; rpcD.call(mergedResult));
return promiseD;
</code></pre>

<h3 id="toc_24"><strong>5、异常处理</strong></h3>

<p>上面提到过 Callback 风格的异步 API，异常处理比较分散。而 Promise 风格的异常处理则优雅得多。我们需要记住，异常是 Promise 携带的两种状态之一。那么异常可以作为 callback 函数的输入。</p>

<h4 id="toc_25">◆ 通用异常处理</h4>

<p>Futures.catching 和 CompletableFuture.exceptionally 接收异常值为参数的 callback 函数。</p>

<pre><code class="language-java">//Guava
ListenableFuture&lt;Integer&gt; fetchCounterPromise = ...;
// Falling back to a zero counter in case an exception happens when
// processing the RPC to fetch counters.
ListenableFuture&lt;Integer&gt; faultTolerantPromise = Futures.catching(
    fetchCounterPromise, FetchException.class,
    new Function&lt;FetchException, Integer&gt;() {
        public Integer apply(FetchException e) {
            return 0;
        }
    });
</code></pre>

<p>我们再看一个更复杂的例子。</p>

<pre><code class="language-java">//CompletableFuture
CompletableFuture&lt;FaultTolerantResult&gt; faultTolerantPromise = rpc1.call(input) //promise1
    .thenCompose(rpc1Result -&gt; rpc2.call(rpc1Result)) //promise2
    .thenCompose(rpc2Result -&gt; rpc3.call(rpc2Result)) //promise3
    .thenCompose(rpc3Result -&gt; rpc4.call(rpc3Result)) //promise4
    .exceptionally(err -&gt; {
        // process err
        return faultTolerantValue;
    });  //faultTolerantPromise
return faultTolerantPromise;
</code></pre>

<p>再提醒一遍，不要被链式调用迷惑了。这个例子里面，rpc1/rpc2/rpc3/rpc4 都有可能发生异常，但我们只需要在最后统一处理（返回异常值或转换为一个默认正常值）。比如 rpc2 发生异常，那么 rpc3/rpc4 的逻辑（接收正常值的 callback 函数）都不会执行，但是 rpc2 的异常会传递给 promise3/promise4。</p>

<h4 id="toc_26">◆ 恢复</h4>

<p>假设在读取某个数据存储发生异常，我们需要某种恢复机制，比如读取另一个 backup 的数据存储（某种重试），那么可以使用 Futures.cachingAsync 和 CompletableFuture.handle。</p>

<pre><code class="language-java">//CompletableFuture
public &lt;V&gt; CompletableFuture&lt;V&gt; dispatch(final Command&lt;V&gt; command) {
    final CompletableFuture&lt;V&gt; dispatched = loadbalance.selectHandler().dispatch(command);
    if (maxTries &gt; 0) {
        final AtomicInteger leftTries = new AtomicInteger(maxTries);
        final BiFunction&lt;CompletableFuture&lt;V&gt;, Throwable, CompletableFuture&lt;V&gt;&gt; fallback = new BiFunction&lt;CompletableFuture&lt;V&gt;, Throwable, CompletableFuture&lt;V&gt;&gt;() {
            @Override
            public CompletableFuture&lt;V&gt; apply(CompletableFuture&lt;V&gt; input, Throwable cause) {
                if (cause == null) return input;
                if (cause instanceof RecoverableException &amp;&amp; leftTries.getAndDecrement() &gt; 0) {
                    final CompletableFuture&lt;V&gt; next = loadbalance.selectHandler().dispatch(new Command&lt;V&gt;(command.type, command.args));
                    return next.handle((v, err) -&gt; err).thenCompose(err -&gt; apply(next, err));
                }
                CompletableFuture&lt;V&gt; errFuture = new CompletableFuture&lt;&gt;();
                errFuture.completeExceptionally(cause);
                return errFuture;
            }
        };
        return dispatched.handle((v, err) -&gt; err).thenCompose(err -&gt; fallback.apply(dispatched, err));
    }
    return dispatched;
}
</code></pre>

<h4 id="toc_27">◆ 超时</h4>

<p>Promise 是一个承诺完成（成功或失败）的结果，但是并不承诺完成时间。所以，通常需要一种超时机制，幸运的是 ListenableFuture 和 CompletableFuture 都实现了 Future 接口。</p>

<pre><code class="language-java">CompletableFuture&lt;FaultTolerantResult&gt; faultTolerantPromise = ...;
try {
    FaultTolerantResult result = faultTolerantPromise.get(1000, TimeUnit.SECONDS);
    // process result
} catch (TimeoutException e) {
    //尝试取消执行尚未开始的callback函数
    faultTolerantPromise.cancel(false);` 

}
</code></pre>

<h2 id="toc_28"><strong>四、Promise 异步编程的注意点</strong></h2>

<p>异步编程比同步编程困难。异步编程通常主要解决一小部分问题，比如阻塞。Promise 借鉴了函数式编程的风格，大量的逻辑会分散在各种 callback 函数来实现。因此对于习惯了同步编程的 OO 式或命令式编程风格的开发人员，需要一定的习惯时间。</p>

<p>上面谈到 callee 执行机制的时候，谈到了线程池，那么 callee 计算完成时，callback 函数的执行通常是池中 resolve Promise 的线程执行。但是，如果 caller 在设置 callback 的时候，Promise 已经完成，那么 callback 的执行线程则是 caller 线程。因此，请特别关注 callback 函数的执行线程的差别。请遵循：</p>

<p>◆ callback 尽量轻量 </p>

<p>◆ callback 避免阻塞 </p>

<p>◆ 否则请指定执行线程</p>

<p>如果 callback 执行了大量的计算，甚至执行了阻塞式操作，那么就很有可能阻塞住 Promise 的 resolve 线程，通常这类线程都是极少的，比如执行 IO 的 EventLoop 线程，有可能造成其他 Promise 得不到执行。</p>

<blockquote>
<p>摘自 ListenableFuture 的文档： </p>

<p>Note: For fast, lightweight listeners that would be safe to execute in any thread, consider MoreExecutors.directExecutor. Otherwise, avoid it. Heavyweight directExecutor listeners can cause problems, and these problems can be difficult to reproduce because they depend on timing. For example: </p>

<p>◆ The listener may be executed by the caller of addListener. That caller may be a UI thread or other latency-sensitive thread. This can harm UI responsiveness. </p>

<p>◆ The listener may be executed by the thread that completes this Future. That thread may be an internal system thread such as an RPC network thread. Blocking that thread may stall progress of the whole system. It may even cause a deadlock. </p>

<p>◆ The listener may delay other listeners, even listeners that are not themselves directExecutor listeners.</p>
</blockquote>

<p>我们也的确碰到过使用 MoreExecutors.directExecutor 时，由于编写了太过复杂的 callback 链，导致线程死锁的问题。</p>

<p>CompletableFuture 和 ListenableFuture 都有指定 callback 执行线程的方法：</p>

<pre><code class="language-java">//使用内置的通用ForkJoin线程池
completableFuture.thenAcceptAsync(callback);
//使用指定的线程执行器
completableFuture.thenAcceptAsync(callback, executor);
//使用指定的线程执行器
Futures.transform(input, callback, executor);
</code></pre>

<p>正因为异步编程的复杂性，因此目前我们也尽量在业务逻辑相对简单的应用上进行异步化改造。后续，我们也会评估 Quasar 等协程框架。</p>

<p>完~</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spring Boot で WebSocket (STOMPではなく、TextWebSocketHandler)を試してみる]]></title>
    <link href="http://panlw.github.io/15262240665064.html"/>
    <updated>2018-05-13T23:07:46+08:00</updated>
    <id>http://panlw.github.io/15262240665064.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>2017-05-10<br/>
<a href="http://blog.enjoyxstudy.com/entry/2017/05/10/000000">http://blog.enjoyxstudy.com/entry/2017/05/10/000000</a></p>
</blockquote>

<p>こないだSTOMP over WebSocketを試してみましたが、今度はSTOMPを使わずに、<code>TextWebSocketHandler</code>を使ったWebSocketを試してみます。題材も同じくチャットです。</p>

<ul>
<li>  <a href="https://github.com/onozaty/sandbox-spring-boot/tree/master/spring-boot-websocket">sandbox-spring-boot/spring-boot-websocket at master · onozaty/sandbox-spring-boot · GitHub</a></li>
</ul>

<p>ルームの情報をどのように渡そうか悩みました。コードを書き始めるまでは、WebSocketで接続する先にヘッダとかで渡せばいいかなと思っていましたが、調べてみたら渡せなかったので、やもえずURLのクエリパラメータとして渡すようにしました。</p>

<h2 id="toc_0">コード</h2>

<p>テキストでのやり取りを行うので、<code>TextWebSocketHandler</code> を利用します。<code>TextWebSocketHandler</code>の各メソッドをoverrideして必要な処理を実装するだけです。</p>

<p>ルームの情報は、URLのクエリとしてクライアントから送っているので、接続が確立したタイミング(<code>afterConnectionEstablished</code>)にて、ルーム毎に<code>WebSocketSession</code>を保持するようにします。 メッセージを受け取ったら、自分のルームと同じ<code>WebSocketSession</code>に対して、メッセージを送るだけです。</p>

<pre><code class="language-java">package com.example;

import java.util.Set;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.CopyOnWriteArraySet;

import org.springframework.stereotype.Component;
import org.springframework.web.socket.CloseStatus;
import org.springframework.web.socket.TextMessage;
import org.springframework.web.socket.WebSocketSession;
import org.springframework.web.socket.handler.TextWebSocketHandler;

@Component
public class ChatHandler extends TextWebSocketHandler {

    private ConcurrentHashMap&lt;String, Set&lt;WebSocketSession&gt;&gt; roomSessionPool = new ConcurrentHashMap&lt;&gt;();

    @Override
    public void afterConnectionEstablished(WebSocketSession session) throws Exception {

        String roomName = session.getUri().getQuery();

        roomSessionPool.compute(roomName, (key, sessions) -&gt; {

            if (sessions == null) {
                sessions = new CopyOnWriteArraySet&lt;&gt;();
            }
            sessions.add(session);

            return sessions;
        });
    }

    @Override
    protected void handleTextMessage(WebSocketSession session, TextMessage message) throws Exception {

        String roomName = session.getUri().getQuery();

        for (WebSocketSession roomSession : roomSessionPool.get(roomName)) {
            roomSession.sendMessage(message);
        }
    }

    @Override
    public void afterConnectionClosed(WebSocketSession session, CloseStatus status) throws Exception {

        String roomName = session.getUri().getQuery();

        roomSessionPool.compute(roomName, (key, sessions) -&gt; {

            sessions.remove(session);
            if (sessions.isEmpty()) {
                // 1件もない場合はMapからクリア
                sessions = null;
            }

            return sessions;
        });
    }
}
</code></pre>

<p>WebSocketの設定として、URLとHandlerを紐付けます。</p>

<pre><code class="language-java">package com.example;

import org.springframework.context.annotation.Configuration;
import org.springframework.web.socket.config.annotation.EnableWebSocket;
import org.springframework.web.socket.config.annotation.WebSocketConfigurer;
import org.springframework.web.socket.config.annotation.WebSocketHandlerRegistry;

import lombok.AllArgsConstructor;

@Configuration
@EnableWebSocket
@AllArgsConstructor
public class WebSocketConfig implements WebSocketConfigurer {

    private final ChatHandler chatHandler;

    @Override
    public void registerWebSocketHandlers(WebSocketHandlerRegistry registry) {
        registry.addHandler(chatHandler, &quot;/endpoint&quot;);
    }

}
</code></pre>

<p>クライアント側では、下記のようなコードになりました。</p>

<pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;ja&quot;&gt;
&lt;head&gt;
&lt;meta charset=&quot;UTF-8&quot; /&gt;
&lt;title&gt;チャット&lt;/title&gt;
&lt;link rel=&quot;stylesheet&quot; href=&quot;/webjars/bootstrap/3.3.7/css/bootstrap.min.css&quot; /&gt;
&lt;link rel=&quot;stylesheet&quot; href=&quot;/webjars/bootstrap/3.3.7/css/bootstrap-theme.min.css&quot; /&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;div class=&quot;container&quot;&gt;
    &lt;h2&gt;チャット&lt;/h2&gt;
    &lt;div class=&quot;form-horizontal&quot;&gt;
      &lt;div class=&quot;form-group&quot;&gt;
        &lt;label for=&quot;roomName&quot; class=&quot;col-sm-2 control-label&quot;&gt;ルーム&lt;/label&gt;
        &lt;div class=&quot;col-sm-2&quot;&gt;
          &lt;input id=&quot;roomName&quot; type=&quot;text&quot; class=&quot;form-control&quot; value=&quot;example&quot; /&gt;
        &lt;/div&gt;
        &lt;div class=&quot;col-sm-3&quot;&gt;
          &lt;button id=&quot;connectButton&quot; type=&quot;button&quot; class=&quot;btn btn-default&quot;&gt;接続&lt;/button&gt;
          &lt;button id=&quot;disconnectButton&quot; class=&quot;btn btn-default&quot;&gt;切断&lt;/button&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;form-group&quot;&gt;
        &lt;label for=&quot;message&quot; class=&quot;col-sm-2 control-label&quot;&gt;メッセージ&lt;/label&gt;
        &lt;div class=&quot;col-sm-4&quot;&gt;
          &lt;input id=&quot;message&quot; type=&quot;text&quot; class=&quot;form-control&quot; /&gt;
        &lt;/div&gt;
        &lt;div class=&quot;col-sm-2&quot;&gt;
          &lt;button id=&quot;sendButton&quot; type=&quot;button&quot; class=&quot;btn btn-default&quot;&gt;送信&lt;/button&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-sm-4 col-sm-offset-2&quot;&gt;
          &lt;ul id=&quot;messageList&quot; class=&quot;list-unstyled&quot;&gt;
          &lt;/ul&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;script src=&quot;/webjars/jquery/1.12.4/jquery.min.js&quot;&gt;&lt;/script&gt;
  &lt;script src=&quot;/webjars/bootstrap/3.3.7/js/bootstrap.min.js&quot;&gt;&lt;/script&gt;
  &lt;script&gt;
    $(function() {
      var endpoint = &#39;ws://&#39; + location.host + &#39;/endpoint&#39;;
      var webSocket = null;

      $(&#39;#connectButton&#39;).click(function() {

        $(&quot;#messageList&quot;).empty();

        webSocket = new WebSocket(endpoint + &#39;?&#39; + encodeURIComponent($(&#39;#roomName&#39;).val()));
        webSocket.onopen = function() {
          $(&#39;#roomName&#39;).prop(&#39;disabled&#39;, true);
          $(&#39;#connectButton&#39;).prop(&#39;disabled&#39;, true);
          $(&#39;#disconnectButton&#39;).prop(&#39;disabled&#39;, false);
        };
        webSocket.onclose = function() {
        };
        webSocket.onmessage = function(message) {
          $(&#39;#messageList&#39;).prepend($(&#39;&lt;li&gt;&#39;).text(message.data));
        };
        webSocket.onerror = function() {
          alert(&#39;エラーが発生しました。&#39;);
        };
      });

      $(&#39;#disconnectButton&#39;).click(function() {

        webSocket.close();
        webSocket = null;

        $(&#39;#roomName&#39;).prop(&#39;disabled&#39;, false);
        $(&#39;#connectButton&#39;).prop(&#39;disabled&#39;, false);
        $(&#39;#disconnectButton&#39;).prop(&#39;disabled&#39;, true);
      });

      $(&#39;#sendButton&#39;).click(function() {
        if (!webSocket) {
          alert(&#39;未接続です。&#39;);
          return;
        }

        webSocket.send($(&#39;#message&#39;).val());
      });
    });
  &lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>STOMPの時のほうが、いろいろシンプルに書けるので、ブラウザがクライアントならば、STOMPを使わない理由はないかなと思っています。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Learn to visualize data with this free D3.js course]]></title>
    <link href="http://panlw.github.io/15248848757950.html"/>
    <updated>2018-04-28T11:07:55+08:00</updated>
    <id>http://panlw.github.io/15248848757950.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://medium.freecodecamp.org/learn-to-visualize-data-with-this-free-d3-js-course-2433b060f9dc">https://medium.freecodecamp.org/learn-to-visualize-data-with-this-free-d3-js-course-2433b060f9dc</a></p>

<p>Per Harald Borgen, 2018/4/13</p>
</blockquote>

<p>D3.js is a JavaScript library which allows you to bring data to life using HTML, SVG, and CSS. Learning it will give you super powers when it comes to extracting value from data, as you’ll basically be able to create any visualization you can think of.</p>

<p>However, it’s not the easiest library to learn, so getting started can be a bit tricky. That’s why we’ve teamed up with web developer and instructor <a href="https://medium.com/@sohaib.nehal">Sohaib Nehal</a> and created a <a href="https://scrimba.com/g/gd3js">free full-length course on it.</a> Throughout the course, Sohaib will give you a soft introduction to the powerful library.</p>

<p>Let’s have a look at how it’s laid out!</p>

<h3 id="toc_0">The content</h3>

<p>The course consists of 10 screencasts which in total last less than an hour. It starts off with the most basic concepts, like selection, manipulation, data loading, and more. This lays the ground work for the various visualizations you’ll learn to create throughout the rest of the course.</p>

<h4 id="toc_1">#1: Course introduction</h4>

<p><img src="https://cdn-images-1.medium.com/freeze/max/30/1*QTASftirCvIEkkzu09ZNcw.png?q=20" alt=""/><img src="https://cdn-images-1.medium.com/max/800/1*QTASftirCvIEkkzu09ZNcw.png" alt=""/><img src="https://cdn-images-1.medium.com/max/800/1*QTASftirCvIEkkzu09ZNcw.png" alt=""/></p>

<p>As usual with Scrimba courses, it begins with a quick walk-through of the course content, along with an intro to D3.js and the instructor.</p>

<h4 id="toc_2">#2: Selection and Manipulation</h4>

<p><img src="https://cdn-images-1.medium.com/freeze/max/30/1*H-7cY_zluQqHuYdvNMbFGw.png?q=20" alt=""/><img src="https://cdn-images-1.medium.com/max/800/1*H-7cY_zluQqHuYdvNMbFGw.png" alt=""/><img src="https://cdn-images-1.medium.com/max/800/1*H-7cY_zluQqHuYdvNMbFGw.png" alt=""/></p>

<p>The first thing you need to learn is how to select and manipulate DOM elements with D3.js. The library is actually pretty powerful in terms of manipulating the DOM, so you could theoretically use it as a <a href="https://blog.webkid.io/replacing-jquery-with-d3/">replacement for jQuery.</a></p>

<h4 id="toc_3">#3: Data Loading and Binding</h4>

<p><img src="https://cdn-images-1.medium.com/freeze/max/30/1*5sEb4D4exhT8YZnpts-T9w.png?q=20" alt=""/><img src="https://cdn-images-1.medium.com/max/800/1*5sEb4D4exhT8YZnpts-T9w.png" alt=""/><img src="https://cdn-images-1.medium.com/max/800/1*5sEb4D4exhT8YZnpts-T9w.png" alt=""/></p>

<p>As you’re going to create visuializations, it’s important to learn how to load data in and also how to bind it to the DOM. So in this lecture you’ll learn that.</p>

<h4 id="toc_4">#4: Creating a simple bar chart</h4>

<p><img src="https://cdn-images-1.medium.com/freeze/max/30/1*Jm03LA1t_o3-GKjt84MLrA.png?q=20" alt=""/><img src="https://cdn-images-1.medium.com/max/800/1*Jm03LA1t_o3-GKjt84MLrA.png" alt=""/><img src="https://cdn-images-1.medium.com/max/800/1*Jm03LA1t_o3-GKjt84MLrA.png" alt=""/></p>

<p>In the third lecture, you’ll learn how to build your very first visualization: a simple bar chart. The reason we’re introducing you to building stuff so early on is because it’s much more fun to create visualizations than simply talking about theory. So we think you’ll enjoy this lesson.</p>

<h4 id="toc_5">#5: Creating labels</h4>

<p><img src="https://cdn-images-1.medium.com/freeze/max/30/1*sDp-GORp42nSv5xEuddOcw.png?q=20" alt=""/><img src="https://cdn-images-1.medium.com/max/800/1*sDp-GORp42nSv5xEuddOcw.png" alt=""/><img src="https://cdn-images-1.medium.com/max/800/1*sDp-GORp42nSv5xEuddOcw.png" alt=""/></p>

<p>The next step is to add labels to the bar chart, as you often would want to do this in real-life. This is a short and simple lecture. Here, I’d recommend you to play around with the positions of the labels, as that’s a simple and fun way of interacting with the code.</p>

<h4 id="toc_6">#6: Scales</h4>

<p>Scales are a critical concept in D3. They allow you to map your data to other relevant ranges, for example the amount of space you have available. So in this lecture, you’ll learn about the <code>scaleLinear()</code> method:</p>

<pre><code class="language-js">var yScale = d3.scaleLinear()
    .domain([0, d3.max(dataset)])
    .range([0, svgHeight]);
</code></pre>

<h4 id="toc_7">#7: Axes</h4>

<p><img src="https://cdn-images-1.medium.com/freeze/max/30/1*nag8GxIZpnUrvtfM9HaYNg.png?q=20" alt=""/><img src="https://cdn-images-1.medium.com/max/800/1*nag8GxIZpnUrvtfM9HaYNg.png" alt=""/><img src="https://cdn-images-1.medium.com/max/800/1*nag8GxIZpnUrvtfM9HaYNg.png" alt=""/></p>

<p>Axes are an integral part of any chart, and D3 provides you a few simple methods for creating them. This lesson builds upon the last one, as it takes advantage of scales when creating the axes. It also sets you up for understanding the super-cool line chart you’ll learn in the final screencast of the course.</p>

<h4 id="toc_8">#8: Creating SVG elements</h4>

<p><img src="https://cdn-images-1.medium.com/freeze/max/30/1*FZdi_TA96EMc0B8I-Tt6Cg.png?q=20" alt=""/><img src="https://cdn-images-1.medium.com/max/800/1*FZdi_TA96EMc0B8I-Tt6Cg.png" alt=""/><img src="https://cdn-images-1.medium.com/max/800/1*FZdi_TA96EMc0B8I-Tt6Cg.png" alt=""/></p>

<p>Even though you’ve already created SVG elements previously in the course, it’s such an important concept that it deserves its own lecture. In it, you’ll learn about the <code>&lt;rect&gt;</code>, <code>&lt;circle&gt;</code> , and <code>&lt;line&gt;</code> elements.</p>

<h4 id="toc_9">#9: Creating a pie chart</h4>

<p><img src="https://cdn-images-1.medium.com/freeze/max/30/1*JvNCACLTK_o7Q1D2AlMVuw.png?q=20" alt=""/><img src="https://cdn-images-1.medium.com/max/800/1*JvNCACLTK_o7Q1D2AlMVuw.png" alt=""/><img src="https://cdn-images-1.medium.com/max/800/1*JvNCACLTK_o7Q1D2AlMVuw.png" alt=""/></p>

<p>Pie charts are handy in many cases, so in this lecture you’ll learn how to create one. D3 provides a simple API for doing this, so it shouldn’t be difficult for you at this point.</p>

<h4 id="toc_10">#10: Creating a line chart</h4>

<p><img src="https://cdn-images-1.medium.com/freeze/max/30/1*NSDd3qCL8-xYDsTnOMQ5KA.png?q=20" alt=""/><img src="https://cdn-images-1.medium.com/max/800/1*NSDd3qCL8-xYDsTnOMQ5KA.png" alt=""/><img src="https://cdn-images-1.medium.com/max/800/1*NSDd3qCL8-xYDsTnOMQ5KA.png" alt=""/></p>

<p>Finally, you’ll learn how to create a line chart to visualize the Bitcoin price. To get the data, you’ll use an external API. This project will also tie together a lot of the concepts you’ve learned throughout the course, so it’s a great visualization to end with.</p>

<p>And that’s it! After going through these ten lessons, you should be well set up for starting to use D3.js in your job or for personal projects.</p>

<p>If you reach this point, we’d really appreciate if you’d give <a href="https://medium.com/@sohaib.nehal">Sohaib</a> a shout-out on <a href="https://twitter.com/Sohaib_Nehal">Twitter</a>!</p>

<h3 id="toc_11">The Scrimba format</h3>

<p>Before you leave, let’s also have a quick look at the technology behind the course. It’s built using <a href="http://scrimba.com">Scrimba</a>, an interactive coding screencast tool. A “scrim” looks like normal video, however it’s fully interactive. This means that you can edit the code inside the screencast.</p>

<p>Here’s a gif which explains the concept:<br/>
<img src="https://cdn-images-1.medium.com/max/1000/1*4PWxbgV--7ZHlB-YVqavJg.gif" alt=""/></p>

<p>This is great for when you feel you need to experiment with the code in order to properly understand it, or when you simply want to copy a piece of the code.</p>

<p>So what are you waiting for? Head over to Scrimba and take the free course today!</p>

<p>Thanks for reading! I’m Per Borgen, co-founder of Scrimba. Feel free to connect with me via Twitter.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Modularizing your GraphQL schema code]]></title>
    <link href="http://panlw.github.io/15248846042998.html"/>
    <updated>2018-04-28T11:03:24+08:00</updated>
    <id>http://panlw.github.io/15248846042998.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://dev-blog.apollodata.com/modularizing-your-graphql-schema-code-d7f71d5ed5f2">https://dev-blog.apollodata.com/modularizing-your-graphql-schema-code-d7f71d5ed5f2</a></p>
</blockquote>

<h2 id="toc_0">How to split up your schema definition, resolvers, and Query type into multiple files</h2>

<p><img src="media/15248846042998/15248847656531.png" alt=""/></p>

<p>As your GraphQL application grows from demo, to proof of concept, to production, the complexity of your schema and resolvers will grow in tandem. To organize your code, you’ll want to split up your schema types and the associated resolvers into multiple files.</p>

<p>We get this question frequently since there are a lot of different approaches to splitting up your schema code, and it might seem that you need a complex setup to get a good result. But it turns out that we can arrange our schema and resolver code in separate files with just a few simple JavaScript concepts.</p>

<p>In this post, we present a straightforward method for modularizing schemas built with <code>graphql-tools</code> that you can tweak to match your tastes and your codebase.</p>

<h3 id="toc_1">Schema</h3>

<p>If you’re just starting out and have your entire schema defined in one file, it might look much like the below snippet. Here, we’ll call it <code>schema.js</code>:</p>

<pre><code class="language-js">// schema.js 

const typeDefs = `
  type Query {
    author(id: Int!): Post
    book(id: Int!): Post
  }
  
  type Author {
    id: Int!
    firstName: String
    lastName: String
    books: [Book]
  }
  
  type Book {
    title: String
    author: Author
  }
`;

makeExecutableSchema({
  typeDefs: typeDefs,
  resolvers: {},
});
</code></pre>

<p>Ideally, instead of having everything in one schema definition string, we’d like to put the schema types for <code>Author</code> and <code>Book</code> in files called <code>author.js</code> and <code>book.js</code>.</p>

<p>At the end of the day, the schema definitions we’ve written in the Schema Definition Language (SDL) are just strings. Treating them as such, we have a simple way of importing type definitions across different files: Split up the string into multiple strings that we can combine later. This is what <code>author.js</code> would look like:</p>

<pre><code class="language-js">// author.js
export const typeDef = `
  type Author {
    id: Int!
    firstName: String
    lastName: String
    books: [Book]
  }
`;
</code></pre>

<p>Here’s <code>book.js</code>:</p>

<pre><code class="language-js">// book.js
export const typeDef = `
  type Book {
    title: String
    author: Author
  }
`;
</code></pre>

<p>Finally, we pull it all together in <code>schema.js</code>:</p>

<pre><code class="language-js">// schema.js
import { typeDef as Author } from &#39;./author.js&#39;;
import { typeDef as Book } from &#39;./book.js&#39;;
</code></pre>

<pre><code class="language-js">const Query = `
  type Query {
    author(id: Int!): Post
    book(id: Int!): Post
  }
`;
</code></pre>

<pre><code class="language-js">makeExecutableSchema({
  typeDefs: [ Query, Author, Book ],
  resolvers: {},
});
</code></pre>

<p>We’re not doing anything fancy here: we’re just importing strings that happen to contain some SDL. Note that for convenience you don’t need to combine the strings yourself — <code>makeExecutableSchema</code> can actually accept an array of type definitions directly to facilitate this approach.</p>

<h3 id="toc_2">Resolvers</h3>

<p>Now that we have a way to chop up our schema into logical parts, we want to be able to move each resolver with its associated part of the schema as well. In general, we want to keep resolvers for a certain type in the same file as the schema definition for that type.</p>

<p>Expanding on our previous example, here’s our <code>schema.js</code> file with some resolvers added into the picture:</p>

<pre><code class="language-js">// schema.js
import { typeDef as Author } from &#39;./author.js&#39;;
import { typeDef as Book } from &#39;./book.js&#39;;
</code></pre>

<pre><code class="language-js">const Query = `
  type Query {
    author(id: Int!): Post
    book(id: Int!): Post
  }
`;
</code></pre>

<pre><code class="language-js">const resolvers = {
  Query: {
    author: () =&gt; { ... },
    book: () =&gt; { ... },
  },
  Author: {
    name: () =&gt; { ... },
  },
  Book: {
    title: () =&gt; { ... },
  },
};
</code></pre>

<pre><code class="language-js">makeExecutableSchema({
  typeDefs: [ Query, Author, Book ],
  resolvers,
});
</code></pre>

<p>Just like we split up the schema definition string, we can split up the <code>resolvers</code> object. We can put a piece of it in <code>author.js</code>, another in <code>book.js</code>, and then import them and use the <code>lodash.merge</code> function to put it all together in <code>schema.js</code>.</p>

<p>Here’s what <code>author.js</code> would look like in that case:</p>

<pre><code class="language-js">// author.js
export const typeDef = `
  type Author {
    id: Int!
    firstName: String
    lastName: String
    books: [Book]
  }
`;
</code></pre>

<pre><code class="language-js">export const resolvers = {
  Author: {
    books: () =&gt; { ... },
  }
};
</code></pre>

<p>Here’s <code>book.js</code>:</p>

<pre><code class="language-js">// book.js
export const typeDef = `
  type Book {
    title: String
    author: Author
  }
`;
</code></pre>

<pre><code class="language-js">export const resolvers = {
  Book: {
    author: () =&gt; { ... },
  }
};
</code></pre>

<p>Then, we apply <code>lodash.merge</code> in <code>schema.js</code> to put everything together:</p>

<pre><code class="language-js">import { merge } from &#39;lodash&#39;;
</code></pre>

<pre><code class="language-js">import { 
  typeDef as Author, 
  resolvers as authorResolvers,
} from &#39;./author.js&#39;;
</code></pre>

<pre><code class="language-js">import { 
  typeDef as Book, 
  resolvers as bookResolvers,
} from &#39;./book.js&#39;;
</code></pre>

<pre><code class="language-js">const Query = `
  type Query {
    author(id: Int!): Author
    book(id: Int!): Book
  }
`;
</code></pre>

<pre><code class="language-js">const resolvers = {
  Query: { 
    ...,
  }
};
</code></pre>

<pre><code class="language-js">makeExecutableSchema({
  typeDefs: [ Query, Author, Book ],
  resolvers: merge(resolvers, authorResolvers, bookResolvers),
});
</code></pre>

<p>And that gives us the <code>resolvers</code> structure that we started out with!</p>

<h3 id="toc_3">Extending types in multiple files</h3>

<p>We’re still defining <code>authors</code> and <code>books</code> as top-level fields on <code>Query</code> within <code>schema.js</code>, even though these are logically tied to <code>Author</code> and <code>Book</code> and should live in <code>author.js</code> and <code>book.js</code>.</p>

<p>To accomplish that, we can use type extensions. We can define our existing <code>Query</code> type like this:</p>

<pre><code class="language-js">const Query = `
  type Query {
    _empty: String
  }

  extend type Query {
    author(id: Int!): Author 
  }

  extend type Query {
    book(id: Int!): Book 
  }
`;
</code></pre>

<p>_Note: In the current version of GraphQL, you can’t have an empty type even if you intend to extend it later. So we need to make sure the Query type has at least one field — in this case we can add a fake <u>empty field. Hopefully in future versions it will be possible to have an empty type to be extended later.</u></p>

<p>Basically, the <code>extend</code> keyword lets us add fields to an already-defined type. We can use this keyword in order to define Query fields relevant to those types in <code>book.js</code> and <code>author.js</code>. We should then also move to defining the Query resolvers for those types in the same place.</p>

<p>Here’s what <code>author.js</code> looks like with this approach:</p>

<pre><code class="language-js">// author.js
</code></pre>

<pre><code class="language-js">export const typeDef = `
  extend type Query {
    author(id: Int!): Author
  }
  ```
  
```js
  type Author {
    id: Int!
    firstName: String
    lastName: String
    books: [Book]
  }
`;
</code></pre>

<pre><code class="language-js">export const resolvers = {
  Query: {
    author: () =&gt; { ... },
  },
  Author: {
    books: () =&gt; { ... },
  }
};
</code></pre>

<p>This is what <code>book.js</code> looks like:</p>

<pre><code class="language-js">// book.js
</code></pre>

<pre><code class="language-js">export const typeDef = `
  extend type Query {
    book(id: Int!): Book
  }
  ```
  
```js
  type Book {
    title: String
    author: Author
  }
`;
</code></pre>

<pre><code class="language-js">export const resolvers = {
  Query: {
    book: () =&gt; { ... },
  },
  Book: {
    author: () =&gt; { ... },
  }
};
</code></pre>

<p>Just as before, we put it all together in <code>schema.js</code>:</p>

<pre><code class="language-js">import { merge } from &#39;lodash&#39;;
</code></pre>

<pre><code class="language-js">import { 
  typeDef as Author, 
  resolvers as authorResolvers,
} from &#39;./author.js&#39;;
</code></pre>

<pre><code class="language-js">import { 
  typeDef as Book, 
  resolvers as bookResolvers,
} from &#39;./book.js&#39;;
</code></pre>

<pre><code class="language-js">// If you had Query fields not associated with a
// specific type you could put them here
const Query = `
  type Query {
    _empty: String
  }
`;
</code></pre>

<pre><code class="language-js">const resolvers = {};
</code></pre>

<pre><code class="language-js">makeExecutableSchema({
  typeDefs: [ Query, Author, Book ],
  resolvers: merge(resolvers, authorResolvers, bookResolvers),
});
</code></pre>

<p>Now, the schema and resolver definitions are properly co-located with the associated types!</p>

<h3 id="toc_4">Final tips</h3>

<p>We’ve just been through the mechanics of modularizing your server code. Here are a few additional tips that may be helpful in figuring out how to carve up your codebase:</p>

<ol>
<li> When learning, prototyping or even building a POC, putting your whole schema in a single file is likely fine: There are benefits to be able to go through your whole schema really quickly, or explain it to a coworker.</li>
<li> You can organize your schema and resolvers by feature: for example, putting the stuff related to the checkout system together might make sense in an ecommerce site.</li>
<li> Keep your resolvers in the same file as the schema definition for the fields they implement. This will allow you to reason about your code efficiently.</li>
<li> Wrap your SDL type definitions with a <code>gql</code> tag using <a href="https://github.com/apollographql/graphql-tag">graphql-tag</a>. If you’re using a GraphQL plugin for your editor or formatting your code with Prettier, you’ll be able to get syntax highlighting for SDL within your code editor as long as you prefix it with the <code>gql</code> tag.</li>
</ol>

<p>Now, go forth and organize your code!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Basics of Web Workers - HTML5 Rocks]]></title>
    <link href="http://panlw.github.io/15248840485977.html"/>
    <updated>2018-04-28T10:54:08+08:00</updated>
    <id>http://panlw.github.io/15248840485977.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://www.html5rocks.com/en/tutorials/workers/basics/">https://www.html5rocks.com/en/tutorials/workers/basics/</a></p>

<p>Eric Bidelman, July 26th, 2010</p>
</blockquote>

<h2 id="toc_0">The Problem: JavaScript Concurrency</h2>

<p>There are a number of bottlenecks preventing interesting applications from being ported (say, from server-heavy implementations) to client-side JavaScript. Some of these include browser compatibility, static typing, accessibility, and performance. Fortunately, the latter is quickly becoming a thing of the past as browser vendors rapidly improve the speed of their JavaScript engines.</p>

<p>One thing that&#39;s remained a hindrance for JavaScript is actually the language itself. JavaScript is a single-threaded environment, meaning multiple scripts cannot run at the same time. As an example, imagine a site that needs to handle UI events, query and process large amounts of API data, and manipulate the DOM. Pretty common, right? Unfortunately all of that can&#39;t be simultaneous due to limitations in browsers&#39; JavaScript runtime. Script execution happens within a single thread.</p>

<p>Developers mimic &#39;concurrency&#39; by using techniques like <code>setTimeout()</code>, <code>setInterval()</code>, <code>XMLHttpRequest</code>, and event handlers. Yes, all of these features run asynchronously, but non-blocking doesn&#39;t necessarily mean concurrency. Asynchronous events are processed after the current executing script has yielded. The good news is that HTML5 gives us something better than these hacks!</p>

<h2 id="toc_1">Introducing Web Workers: Bring Threading to JavaScript</h2>

<p>The <a href="http://www.whatwg.org/specs/web-workers/current-work/">Web Workers</a> specification defines an API for spawning background scripts in your web application. Web Workers allow you to do things like fire up long-running scripts to handle computationally intensive tasks, but without blocking the UI or other scripts to handle user interactions. They&#39;re going to help put and end to that nasty &#39;unresponsive script&#39; dialog that we&#39;ve all come to love:</p>

<p><img src="/static/images/screenshots/workers/unresponsive_script.gif" alt="Unresponsive script dialog" title="Unresponsive script dialog"/> Common unresponsive script dialog.</p>

<p>Workers utilize thread-like message passing to achieve parallelism. They&#39;re perfect for keeping your UI refresh, performant, and responsive for users.</p>

<h3 id="toc_2">Types of Web Workers</h3>

<p>It&#39;s worth noting that the <a href="http://www.whatwg.org/specs/web-workers/current-work/">specification</a> discusses two kinds of Web Workers, <a href="http://www.whatwg.org/specs/web-workers/current-work/#dedicated-workers-and-the-worker-interface">Dedicated Workers</a> and <a href="http://www.whatwg.org/specs/web-workers/current-work/#sharedworker">Shared Workers</a>. This article will only cover dedicated workers and I&#39;ll refer to them as &#39;web workers&#39; or &#39;workers&#39; throughout.</p>

<h2 id="toc_3">Getting Started</h2>

<p>Web Workers run in an isolated thread. As a result, the code that they execute needs to be contained in a separate file. But before we do that, the first thing to do is create a new <code>Worker</code> object in your main page. The constructor takes the name of the worker script:</p>

<pre><code class="language-js">var worker = new Worker(&#39;task.js&#39;);
</code></pre>

<p>If the specified file exists, the browser will spawn a new worker thread, which is downloaded asynchronously. The worker will not begin until the file has completely downloaded and executed. If the path to your worker returns an 404, the worker will fail silently.</p>

<p>After creating the worker, start it by calling the <code>postMessage()</code> method:</p>

<pre><code class="language-js">worker.postMessage(); // Start the worker.
</code></pre>

<h3 id="toc_4">Communicating with a Worker via Message Passing</h3>

<p>Communication between a work and its parent page is done using an event model and the <code>postMessage()</code> method. Depending on your browser/version, <code>postMessage()</code> can accept either a string or JSON object as its single argument. The latest versions of the modern browsers support passing a JSON object.</p>

<p>Below is a example of using a string to pass &#39;Hello World&#39; to a worker in doWork.js. The worker simply returns the message that is passed to it.</p>

<p>Main script:</p>

<pre><code class="language-js">var worker = new Worker(&#39;doWork.js&#39;);

worker.addEventListener(&#39;message&#39;, function(e) {
  console.log(&#39;Worker said: &#39;, e.data);
}, false);

worker.postMessage(&#39;Hello World&#39;); // Send data to our worker.
</code></pre>

<p>doWork.js (the worker):</p>

<pre><code class="language-js">self.addEventListener(&#39;message&#39;, function(e) {
  self.postMessage(e.data);
}, false);
</code></pre>

<p>When <code>postMessage()</code> is called from the main page, our worker handles that message by defining an <code>onmessage</code> handler for the <code>message</code> event. The message payload (in this case &#39;Hello World&#39;) is accessible in <code>Event.data</code>. Although this particular example isn&#39;t very exciting, it demonstrates that <code>postMessage()</code> is also your means for passing data back to the main thread. Convenient!</p>

<p>Messages passed between the main page and workers are copied, not shared. For example, in the next example the &#39;msg&#39; property of the JSON message is accessible in both locations. It appears that the object is being passed directly to the worker even though it&#39;s running in a separate, dedicated space. In actuality, what is happening is that the object is being serialized as it&#39;s handed to the worker, and subsequently, de-serialized on the other end. The page and worker do not share the same instance, so the end result is that a duplicate is created on each pass. Most browsers implement this feature by automatically JSON encoding/decoding the value on either end.</p>

<p>The following is a more complex example that passes messages using JSON objects.</p>

<p>Main script:</p>

<pre><code class="language-js">&lt;button onclick=&quot;sayHI()&quot;&gt;Say HI&lt;/button&gt;
&lt;button onclick=&quot;unknownCmd()&quot;&gt;Send unknown command&lt;/button&gt;
&lt;button onclick=&quot;stop()&quot;&gt;Stop worker&lt;/button&gt;
&lt;output id=&quot;result&quot;&gt;&lt;/output&gt;

&lt;script&gt;
  function sayHI() {
    worker.postMessage({&#39;cmd&#39;: &#39;start&#39;, &#39;msg&#39;: &#39;Hi&#39;});
  }

  function stop() {
    // worker.terminate() from this script would also stop the worker.
    worker.postMessage({&#39;cmd&#39;: &#39;stop&#39;, &#39;msg&#39;: &#39;Bye&#39;});
  }

  function unknownCmd() {
    worker.postMessage({&#39;cmd&#39;: &#39;foobard&#39;, &#39;msg&#39;: &#39;???&#39;});
  }

  var worker = new Worker(&#39;doWork2.js&#39;);

  worker.addEventListener(&#39;message&#39;, function(e) {
    document.getElementById(&#39;result&#39;).textContent = e.data;
  }, false);
&lt;/script&gt;
</code></pre>

<p>doWork2.js:</p>

<pre><code class="language-js">self.addEventListener(&#39;message&#39;, function(e) {
  var data = e.data;
  switch (data.cmd) {
    case &#39;start&#39;:
      self.postMessage(&#39;WORKER STARTED: &#39; + data.msg);
      break;
    case &#39;stop&#39;:
      self.postMessage(&#39;WORKER STOPPED: &#39; + data.msg +
                       &#39;. (buttons will no longer work)&#39;);
      self.close(); // Terminates the worker.
      break;
    default:
      self.postMessage(&#39;Unknown command: &#39; + data.msg);
  };
}, false);
</code></pre>

<p><strong>Note</strong>: There are two ways to stop a worker: by calling <code>worker.terminate()</code> from the main page or by calling <code>self.close()</code> inside of the worker itself.</p>

<p><strong>Example</strong>: Run this worker!</p>

<p><button onclick="sayHI()">Say HI</button> <button onclick="unknownCmd()">Send unknown command</button> <button onclick="stop()">Stop worker</button></p>

<h2 id="toc_5">Transferrable objects</h2>

<p>Most browsers implement the <a href="http://updates.html5rocks.com/2011/09/Workers-ArrayBuffer">structured cloning</a> algorithm, which allows you to pass more complex types in/out of Workers such as <code>File</code>, <code>Blob</code>, <code>ArrayBuffer</code>, and JSON objects. However, when passing these types of data using <code>postMessage()</code>, a copy is still made. Therefore, if you&#39;re passing a large 50MB file (for example), there&#39;s a noticeable overhead in getting that file between the worker and the main thread.</p>

<p>Structured cloning is great, but a copy can take hundreds of milliseconds. To combat the perf hit, you can use <a href="http://www.w3.org/html/wg/drafts/html/master/infrastructure.html#transferable-objects">Transferable Objects</a>.</p>

<p>With Transferable Objects, data is transferred from one context to another. It is zero-copy, which vastly improves the performance of sending data to a Worker. Think of it as pass-by-reference if you&#39;re from the C/C++ world. However, unlike pass-by-reference, the &#39;version&#39; from the calling context is no longer available once transferred to the new context. For example, when transferring an ArrayBuffer from your main app to Worker, the original <code>ArrayBuffer</code> is cleared and no longer usable. Its contents are (quiet literally) transferred to the Worker context.</p>

<p>To use transferrable objects, use a slightly different signature of <code>postMessage()</code>:</p>

<pre><code class="language-js">worker.postMessage(arrayBuffer, [arrayBuffer]);
window.postMessage(arrayBuffer, targetOrigin, [arrayBuffer]);
</code></pre>

<p>The worker case, the first argument is the data and the second is the list of items that should be transferred. The first argument doesn&#39;t have to be an <code>ArrayBuffer</code> by the way. For example, it can be a JSON object:</p>

<pre><code class="language-js">worker.postMessage({data: int8View, moreData: anotherBuffer},
                   [int8View.buffer, anotherBuffer]);
</code></pre>

<p>The important point being: the second argument must be an array of <code>ArrayBuffer</code>s. This is your list of transferrable items.</p>

<p>To see the speed improvement of transferrables, check out this <a href="http://html5-demos.appspot.com/static/workers/transferables/index.html">DEMO</a>. For more information on transferrables, see our <a href="http://updates.html5rocks.com/2011/12/Transferable-Objects-Lightning-Fast">HTML5Rock post</a>.</p>

<h2 id="toc_6">The Worker Environment</h2>

<h3 id="toc_7">Worker Scope</h3>

<p>In the context of a worker, both <code>self</code> and <code>this</code> reference the global scope for the worker. Thus, the previous example could also be written as:</p>

<pre><code class="language-js">addEventListener(&#39;message&#39;, function(e) {
  var data = e.data;
  switch (data.cmd) {
    case &#39;start&#39;:
      postMessage(&#39;WORKER STARTED: &#39; + data.msg);
      break;
    case &#39;stop&#39;:
  ...
}, false);
</code></pre>

<p>Alternatively, you could set the <code>onmessage</code> event handler directly (though <code>addEventListener</code> is always encouraged by JavaScript ninjas).</p>

<pre><code class="language-js">onmessage = function(e) {
  var data = e.data;
  ...
};
</code></pre>

<h3 id="toc_8">Features Available to Workers</h3>

<p>Due to their multi-threaded behavior, web workers only has access to a subset of JavaScript&#39;s features:</p>

<ul>
<li>  The <code>navigator</code> object</li>
<li>  The <code>location</code> object (read-only)</li>
<li>  <code>XMLHttpRequest</code></li>
<li>  <code>setTimeout()/clearTimeout()</code> and <code>setInterval()/clearInterval()</code></li>
<li>  The <a href="/tutorials/appcache/beginner/">Application Cache</a></li>
<li>  Importing external scripts using the <code>importScripts()</code> method</li>
<li>  <a href="#toc-enviornment-subworkers">Spawning other web workers</a></li>
</ul>

<p>Workers do NOT have access to:</p>

<ul>
<li>  The DOM (it&#39;s not thread-safe)</li>
<li>  The <code>window</code> object</li>
<li>  The <code>document</code> object</li>
<li>  The <code>parent</code> object</li>
</ul>

<h3 id="toc_9">Loading External Scripts</h3>

<p>You can load external script files or libraries into a worker with the <code>importScripts()</code> function. The method takes zero or more strings representing the filenames for the resources to import.</p>

<p>This example loads <code>script1.js</code> and <code>script2.js</code> into the worker:</p>

<p>worker.js:</p>

<pre><code class="language-js">importScripts(&#39;script1.js&#39;);
importScripts(&#39;script2.js&#39;);
</code></pre>

<p>Which can also be written as a single import statement:</p>

<pre><code class="language-js">importScripts(&#39;script1.js&#39;, &#39;script2.js&#39;);
</code></pre>

<h3 id="toc_10">Subworkers</h3>

<p>Workers have the ability to spawn child workers. This is great for further breaking up large tasks at runtime. However, subworkers come with a few caveats:</p>

<ul>
<li>  Subworkers must be hosted within the same origin as the parent page.</li>
<li>  URIs within subworkers are resolved relative to their parent worker&#39;s location (as opposed to the main page).</li>
</ul>

<p>Keep in mind most browsers spawn separate processes for each worker. Before you go spawning a worker farm, be cautious about hogging too many of the user&#39;s system resources. One reason for this is that messages passed between main pages and workers are copied, not shared. See <a href="#toc-gettingstarted-workercomm">Communicating with a Worker via Message Passing</a>.</p>

<p>For an sample of how to spawn a subworker, see <a href="http://www.whatwg.org/specs/web-workers/current-work/#delegation">the example</a> in the specification.</p>

<h2 id="toc_11">Inline Workers</h2>

<p>What if you want to create your worker script on the fly, or create a self-contained page without having to create separate worker files? With <code>Blob()</code>, you can &quot;inline&quot; your worker in the same HTML file as your main logic by creating a URL handle to the worker code as a string:</p>

<pre><code class="language-js">var blob = new Blob([&quot;onmessage = function(e) { postMessage(&#39;msg from worker&#39;); }&quot;]);

// Obtain a blob URL reference to our worker &#39;file&#39;.
var blobURL = window.URL.createObjectURL(blob);

var worker = new Worker(blobURL);
worker.onmessage = function(e) {
  // e.data == &#39;msg from worker&#39;
};
worker.postMessage(); // Start the worker.
</code></pre>

<h3 id="toc_12">Blob URLs</h3>

<p>The magic comes with the call to <a href="http://dev.w3.org/2006/webapi/FileAPI/#dfn-createObjectURL"><code>window.URL.createObjectURL()</code></a>. This method creates a simple URL string which can be used to reference data stored in a DOM <code>File</code> or <code>Blob</code> object. For example:</p>

<pre><code class="language-js">blob:http://localhost/c745ef73-ece9-46da-8f66-ebes574789b1
</code></pre>

<p>Blob URLs are unique and last for the lifetime of your application (e.g. until the <code>document</code> is unloaded). If you&#39;re creating many Blob URLs, it&#39;s a good idea to release references that are no longer needed. You can explicitly release a Blob URLs by passing it to <a href="http://dev.w3.org/2006/webapi/FileAPI/#dfn-revokeObjectURL"><code>window.URL.revokeObjectURL()</code></a>:</p>

<pre><code class="language-js">window.URL.revokeObjectURL(blobURL);
</code></pre>

<p>In Chrome, there&#39;s a nice page to view all of the created blob URLs: <code>chrome://blob-internals/</code>.</p>

<h3 id="toc_13">Full Example</h3>

<p>Taking this one step further, we can get clever with how the worker&#39;s JS code is inlined in our page. This technique uses a <code>&lt;script&gt;</code> tag to define the worker:</p>

<pre><code class="language-js">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
  &lt;meta charset=&quot;utf-8&quot; /&gt;
&lt;/head&gt;
&lt;body&gt;

  &lt;div id=&quot;log&quot;&gt;&lt;/div&gt;

  &lt;script id=&quot;worker1&quot; type=&quot;javascript/worker&quot;&gt;
    // This script won&#39;t be parsed by JS engines
    // because its type is javascript/worker.
    self.onmessage = function(e) {
      self.postMessage(&#39;msg from worker&#39;);
    };
    // Rest of your worker code goes here.
  &lt;/script&gt;

  &lt;script&gt;
    function log(msg) {
      // Use a fragment: browser will only render/reflow once.
      var fragment = document.createDocumentFragment();
      fragment.appendChild(document.createTextNode(msg));
      fragment.appendChild(document.createElement(&#39;br&#39;));

      document.querySelector(&quot;#log&quot;).appendChild(fragment);
    }

    var blob = new Blob([document.querySelector(&#39;#worker1&#39;).textContent]);

    var worker = new Worker(window.URL.createObjectURL(blob));
    worker.onmessage = function(e) {
      log(&quot;Received: &quot; + e.data);
    }
    worker.postMessage(); // Start the worker.
  &lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>In my opinion, this new approach is a bit cleaner and more legible. It defines a script tag with <var>id=&quot;worker1&quot;</var> and <var>type=&#39;javascript/worker&#39;</var> (so the browser doesn&#39;t parse the JS). That code is extracted as a string using <code>document.querySelector(&#39;#worker1&#39;).textContent</code> and passed to <code>Blob()</code> to create the file.</p>

<h3 id="toc_14">Loading External Scripts</h3>

<p>When using these techniques to inline your worker code, <code>importScripts()</code> will only work if you supply an absolute URI. If you attempt to pass a relative URI, the browser will complain with a security error. The reason being: the worker (now created from a blob URL) will be resolved with a <code>blob:</code> prefix, while your app will be running from a different (presumably <code>http://</code>) scheme. Hence, the failure will be due to cross origin restrictions.</p>

<p>One way to utilize <code>importScripts()</code> in an inline worker is to &quot;inject&quot; the current url of your main script is running from by passing it to the inline worker and constructing the absolute URL manually. This will insure the external script is imported from the same origin. Assuming your main app is running from <a href="http://example.com/index.html">http://example.com/index.html</a>:</p>

<pre><code class="language-js">...
&lt;script id=&quot;worker2&quot; type=&quot;javascript/worker&quot;&gt;
self.onmessage = function(e) {
  var data = e.data;

  if (data.url) {
    var url = data.url.href;
    var index = url.indexOf(&#39;index.html&#39;);
    if (index != -1) {
      url = url.substring(0, index);
    }
    importScripts(url + &#39;engine.js&#39;);
  }
  ...
};
&lt;/script&gt;
&lt;script&gt;
  var worker = new Worker(window.URL.createObjectURL(bb.getBlob()));
  worker.postMessage(**{url: document.location}**);
&lt;/script&gt;
</code></pre>

<h2 id="toc_15">Handling Errors</h2>

<p>As with any JavaScript logic, you&#39;ll want to handle any errors that are thrown in your web workers. If an error occurs while a worker is executing, the an <code>ErrorEvent</code> is fired. The interface contains three useful properties for figuring out what went wrong: <code>filename</code> - the name of the worker script that caused the error, <code>lineno</code> - the line number where the error occurred, and <code>message</code> - a meaningful description of the error. Here is an example of setting up an <code>onerror</code> event handler to print the properties of the error:</p>

<pre><code class="language-js">&lt;output id=&quot;error&quot; style=&quot;color: red;&quot;&gt;&lt;/output&gt;
&lt;output id=&quot;result&quot;&gt;&lt;/output&gt;

&lt;script&gt;
  function onError(e) {
    document.getElementById(&#39;error&#39;).textContent = [
      &#39;ERROR: Line &#39;, e.lineno, &#39; in &#39;, e.filename, &#39;: &#39;, e.message
    ].join(&#39;&#39;);
  }

  function onMsg(e) {
    document.getElementById(&#39;result&#39;).textContent = e.data;
  }

  var worker = new Worker(&#39;workerWithError.js&#39;);
  worker.addEventListener(&#39;message&#39;, onMsg, false);
  worker.addEventListener(&#39;error&#39;, onError, false);
  worker.postMessage(); // Start worker without a message.
&lt;/script&gt;
</code></pre>

<p><strong>Example</strong>: workerWithError.js tries to perform 1/x, where x is undefined.</p>

<p><button onclick="startErrorWorker()">Run it</button></p>

<p>workerWithError.js:</p>

<pre><code class="language-js">self.addEventListener(&#39;message&#39;, function(e) {
  postMessage(1/x); // Intentional error.
};
</code></pre>

<h2 id="toc_16">A Word on Security</h2>

<h3 id="toc_17">Restrictions with Local Access</h3>

<p>Due to Google Chrome&#39;s security restrictions, workers will not run locally (e.g. from <code>file://</code>) in the latest versions of the browser. Instead, they fail silently! To run your app from the <code>file://</code> scheme, run Chrome with the <code>--allow-file-access-from-files</code> flag set. <strong>NOTE</strong>: It is not recommended to run your primary browser with this flag set. It should only be used for testing purposes and not regular browsing.</p>

<p>Other browsers do not impose the same restriction.</p>

<h3 id="toc_18">Same Origin Considerations</h3>

<p>Worker scripts must be external files with the same scheme as their calling page. Thus, you cannot load a script from a <code>data:</code> URL or <code>javascript:</code> URL, and an <code>https:</code> page cannot start worker scripts that begin with <code>http:</code> URLs.</p>

<h2 id="toc_19">Use Cases</h2>

<p>So what kind app would utilize web workers? Unfortunately, web workers are still relatively new and the majority of samples/tutorials out there involve computing prime numbers. Although that isn&#39;t very interesting, it&#39;s useful for understanding the concepts of web workers. Here are a few more ideas to get your brain churning:</p>

<ul>
<li>  Prefetching and/or caching data for later use</li>
<li>  Code syntax highlighting or other real-time text formatting</li>
<li>  Spell checker</li>
<li>  Analyzing video or audio data</li>
<li>  Background I/O or polling of webservices</li>
<li>  Processing large arrays or humungous JSON responses</li>
<li>  Image filtering in <code>&lt;canvas/&gt;</code></li>
<li>  Updating many rows of a local web database</li>
</ul>

<h2 id="toc_20">Demos</h2>

<ul>
<li>  Example from <a href="http://slides.html5rocks.com/#web-workers">HTML5Rocks slides</a></li>
<li>  <a href="http://htmlfive.appspot.com/static/tracker1.html">Motion tracker</a></li>
<li>  <a href="http://people.mozilla.com/%7Eprouget/demos/worker_and_simulatedannealing/index.xhtml">Simulated Annealing</a></li>
<li>  <a href="http://html5demos.com/worker">HTML5demos sample</a></li>
</ul>

<h2 id="toc_21">References</h2>

<ul>
<li>  <a href="http://www.whatwg.org/specs/web-workers/current-work/">Web Workers</a> specification</li>
<li>  <a href="http://developer.mozilla.org/en/Using_web_workers">&quot;Using web workers&quot;</a> from Mozilla Developer Network</li>
<li>  <a href="http://dev.opera.com/articles/view/web-workers-rise-up/">&quot;Web Workers rise up!&quot;</a> from Dev.Opera</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A tale of Webpack 4 and how to finally configure it in the right way]]></title>
    <link href="http://panlw.github.io/15248829536577.html"/>
    <updated>2018-04-28T10:35:53+08:00</updated>
    <id>http://panlw.github.io/15248829536577.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://hackernoon.com/a-tale-of-webpack-4-and-how-to-finally-configure-it-in-the-right-way-4e94c8e7e5c1">https://hackernoon.com/a-tale-of-webpack-4-and-how-to-finally-configure-it-in-the-right-way-4e94c8e7e5c1</a></p>
</blockquote>

<p>There are a million tutorials online, so you probably have seen a thousand different ways to configure Webpack file. And all of them will be working examples. Why is it so? Webpack itself has been evolving really fast and a lot of loaders and plugins have to keep up. This is a major reason why the configuration files are so different: with a different version combination of the same tools things might work, or break.</p>

<p>Let me just say one thing, and this is my sincere opinion: a lot of people have been complaining about webpack and how cumbersome it is. This is true in many ways, although I have to say with my experience of working with gulp and grunt, you stumble upon the same type of errors there too, meaning that when you use npm modules, it’s inevitable that some versions would be incompatible.</p>

<p>Webpack 4 so far is the popular module bundler that has just undergone a massive update. There is a lot of new things it has to offer, such as zero configuration, reasonable defaults, performance improvement, optimisation tools out of the box.</p>

<p>If you are completely new to webpack, a great way to start would be to read the docs. <a href="https://webpack.js.org/concepts/">Webpack has a pretty nice documentation</a> with many parts explained, so I will go through them very briefly.</p>

<p>Zero config: webpack 4 does not require a configuration file, this is new for the version 4. Webpack kinda grows step by step, so there is no need to do a monstrous configuration from the start.</p>

<p>Performance improvement: webpack 4 is the fastest version of webpack so far.</p>

<p>Reasonable defaults: webpack 4main concepts are <u>entry, output, loaders, plugins</u>. I will not cover these in details, although the difference between loaders and plugins is very vague. It all depends on how library author has implemented it.</p>

<h3 id="toc_0">Core concepts</h3>

<h4 id="toc_1">Entry</h4>

<p>This should be your _.js_ file. Now you will probably see a few configurations where people include _.scss_ or _.css_ file there. This is a major hack and can lead to a lot of unexpected errors. Also sometimes you see an entry with a few _.js_ files. While some solutions allow you to do so, I would say it usually adds more complexity and only do it when you really know why you are doing it.</p>

<h4 id="toc_2">Output</h4>

<p>This is your <u>build/</u> or <u>dist/</u> or <u>wateveryounameit/</u> folder where your end js file will be hosted. This is your end result comprised of modules.</p>

<h4 id="toc_3">Loaders</h4>

<p>They mostly compile or transpile your code, like postcss-loader will go through different plugins. You will see it later.</p>

<h4 id="toc_4">Plugins</h4>

<p>Plugins play a vital role in outputting your code into files.</p>

<h3 id="toc_5">Quickstart</h3>

<p>Create a new directory and move into it:</p>

<pre><code>mkdir webpack-4-tutorial
cd webpack-4-tutorial
</code></pre>

<p>Initialize a package.json :</p>

<pre><code>npm init
</code></pre>

<p>We need to download webpack v4 as a module and webpack-cli to run it from your terminal.</p>

<pre><code>npm install webpack webpack-cli --save-dev
</code></pre>

<p>Make sure you have version 4 installed, if not, you can explicitly specify it in your <u>package.json</u> file. Now open up <u>package.json</u> and add a build script:</p>

<pre><code>&quot;scripts&quot;: {
  &quot;dev&quot;: &quot;webpack&quot;
}
</code></pre>

<p>You will most likely see a warning:</p>

<pre><code>WARNING in configuration

The ‘mode’ option has not been set, webpack will fallback to ‘production’ for this value. Set ‘mode’ option to ‘development’ or ‘production’ to enable defaults for each environment.

You can also set it to ‘none’ to disable any default behavior. Learn more: [https://webpack.js.org/concepts/mode/](https://webpack.js.org/concepts/mode/)
</code></pre>

<h3 id="toc_6">Webpack 4 modes</h3>

<p>You need to edit your script to contain mode flag:</p>

<pre><code>&quot;scripts&quot;: {
 &quot;dev&quot;: &quot;webpack --mode development&quot;
}

ERROR in Entry module not found: Error: Can’t resolve ‘./src’ in ‘~/webpack-4-quickstart’
</code></pre>

<p>This means webpack is looking for a folder _.src/_ with an <u>index.js</u> file. This is a default behaviour for webpack 4 since it requires zero configuration.</p>

<p>Let`s go create a directory with a _.js_ file like this ./src/index.js and put some code there.</p>

<pre><code>console.log(&quot;hello, world&quot;);
</code></pre>

<p>Now run the dev script:</p>

<pre><code>npm run dev
</code></pre>

<p>Now you have a ./dist/main.js directory. This is great since we know our code compiled. But what did just happen?</p>

<blockquote>
<p>By default, webpack requires zero configuration meaning you do not have to fiddle with webpack.config.js to get started using it. Because of that, it had to assume some default behaviour, such that it will always look for ./src folder by default and index.js in it and output to ./dist/main.js main.js is your compiled file with dependencies.</p>
</blockquote>

<p>Having 2 configuration files is a common practice in webpack, especially in big projects. Usually you would have one file for development and one for production. In webpack 4 you have modes: <u>production</u> and <u>development</u>. That eliminates the need for having two files (for medium-sized projects).</p>

<pre><code>&quot;scripts&quot;: {
  &quot;dev&quot;: &quot;webpack --mode development&quot;,
  &quot;build&quot;: &quot;webpack --mode production&quot;
}
</code></pre>

<p>If you paid close attention, you have checked your <u>main.js</u> file and saw it was not minified.</p>

<p><u>I will use build script in this example since it provides a lot of optimisation out of the box, but feel free to use any of them from now on. The core difference between build and dev scripts is how they output files. Build is created for production code. Dev is created for development, meaning that it supports hot module replacement, dev server, and a lot of things that assist your dev work.</u></p>

<p>You can override defaults in npm scripts easily, just use flags:</p>

<pre><code>&quot;scripts&quot;: {
  &quot;dev&quot;: &quot;webpack --mode development ./src/index.js --output ./dist/main.js&quot;,
  &quot;build&quot;: &quot;webpack --mode production ./src/index.js --output ./dist/main.js&quot;
}
</code></pre>

<p>This will override the default option without having to configure anything yet.</p>

<p>As an exercise, try also these flags:</p>

<ul>
<li>  — watch flag for enabling watch mode. It will watch your file changes and recompile every time some file has been updated.</li>
</ul>

<pre><code>&quot;scripts&quot;: {
  &quot;dev&quot;: &quot;webpack --mode development ./src/index.js --output ./dist/main.js --watch&quot;, 
  &quot;build&quot;: &quot;webpack --mode production ./src/index.js --output ./dist/main.js --watch&quot;
}
</code></pre>

<ul>
<li>  — entry flag. Works exactly like output, but rewrites the entry path.</li>
</ul>

<h3 id="toc_7">Transpile your .js code</h3>

<p>Modern JS code is mostly written is ES6, and ES6 is not supported by all the browsers. So you need to transpile it — a fancy word for turn your ES6 code into ES5. You can use babel for that — the most popular tool to transpile things now. Of course, we do not only do it for ES6 code, but for many JS implementations such as TypeScript, React, etc.</p>

<pre><code>npm install babel-core babel-loader babel-preset-env --save-dev
</code></pre>

<p>This is the part when you need to create a config file for babel.</p>

<pre><code>nano .babelrc
</code></pre>

<p>paste there:</p>

<pre><code>{  
  &quot;presets&quot;: [&quot;env&quot;]
}
</code></pre>

<p>We have two options for configuring babel-loader:</p>

<ul>
<li>  using a configuration file webpack.config.js</li>
<li>  using --module-bind in your npm scripts</li>
</ul>

<p>You can technically do a lot with new flags webpack introduces but I would prefer webpack.config.js for simplicity reasons.</p>

<h3 id="toc_8">Configuration file</h3>

<p>Although webpack advertises itself as a zero-configuration platform, it mostly applies to general defaults such as entry and output.</p>

<p>At this point we will create webpack.config.js with the following content:</p>

<pre><code>// webpack v4

const path = require(&#39;path&#39;);

module.exports = {
  entry: { main: &#39;./src/index.js&#39; },
  output: {
    path: path.resolve(__dirname, &#39;dist&#39;),
    filename: &#39;main.js&#39;
  },
  module: {
    rules: [
      {
        test: /\.js$/,
        exclude: /node_modules/,
        use: {
          loader: &quot;babel-loader&quot;
        }
      }
    ]
  }
};
</code></pre>

<p>also we will remove flags from our npm scripts now.</p>

<pre><code>&quot;scripts&quot;: {
  &quot;build&quot;: &quot;webpack --mode production&quot;,
  &quot;dev&quot;: &quot;webpack --mode development&quot;
},
</code></pre>

<p>Now when we run <u>npm run dev</u>it should output us a nice minified _.js_ file into _./dist/main.js_ If not, try re-installing babel-loader.</p>

<blockquote>
<p>The most common pattern of webpack is to use it to compile react application. While this is true, we will not concentrate on React part in this tutorial since I want it to be framework agnostic. Instead, I will show you how to proceed and create your .html and .css configuration.</p>
</blockquote>

<h3 id="toc_9">HTML and CSS imports</h3>

<p>Lets create a small <u>index.html</u> file first in our _./dist_ folder</p>

<pre><code>&lt;html&gt;
  &lt;head&gt;
    &lt;link rel=&quot;stylesheet&quot; href=&quot;style.css&quot;&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div&gt;Hello, world!&lt;/div&gt;
    &lt;script src=&quot;main.js&quot;&gt;&lt;/script&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>As you can see, we are importing here <u>style.css</u> Lets configure it! As we agreed, we ca only have one entry point for webpack. Sow were do we put our css to?</p>

<p>Create a <u>style.css</u> in our _./src_ folder</p>

<pre><code>div {
  color: red;
}
</code></pre>

<p>Do not forget to include it into your .js file:</p>

<pre><code>import &quot;./style.css&quot;;
console.log(&quot;hello, world&quot;);
</code></pre>

<p>In webpack create a new rule for css files:</p>

<pre><code>// webpack v4
const path = require(&#39;path&#39;);
const ExtractTextPlugin = require(&#39;extract-text-webpack-plugin&#39;);

module.exports = {
  entry: { main: &#39;./src/index.js&#39; },
  output: {
    path: path.resolve(__dirname, &#39;dist&#39;),
    filename: &#39;main.js&#39;
  },
  module: {
    rules: [
      {
        test: /\.js$/,
        exclude: /node_modules/,
        use: {
          loader: &quot;babel-loader&quot;
        }
      },
      {
        test: /\.css$/,
        use: ExtractTextPlugin.extract(
          {
            fallback: &#39;style-loader&#39;,
            use: [&#39;css-loader&#39;]
          })
      }
    ]
  }
};
</code></pre>

<p>in terminal run</p>

<pre><code>npm install extract-text-webpack-plugin --save-dev
npm install style-loader css-loader --save-dev
</code></pre>

<p>We need yo use extract text plugin to compile our .css. As you can see, we also added a new rule for .css.</p>

<blockquote>
<p>A quick description of how rules usually work:</p>
</blockquote>

<pre><code>{
  test: /\.YOUR_FILE_EXTENSION$/,
  exclude: /SOMETHING THAT IS THAT EXTENSION BUT SHOULD NOT BE PROCESSED/,
  use: { loader: &quot;loader for your file extension  or a group of loaders&quot; }
}
</code></pre>

<p>We need to use ExtractTextPlugin because webpack be default only understands _.js_ format. ExtractTextPlugin gets your_ .css_ and extracts it into a separate _.css_ file in your _./dist_ directory.</p>

<blockquote>
<p>Spoiler: in certain articles, you will hear that ExtractTextPlugin does not work with webpack 4 but it worked for me :) It proves my point of modules ambiguity in set-up and if it absolutely does not work for you, you can switch to MiniCssExtractPlugin. I will show you how to configure another one later in this article.</p>
</blockquote>

<pre><code>// webpack v4
const path = require(&#39;path&#39;);
const ExtractTextPlugin = require(&#39;extract-text-webpack-plugin&#39;);

module.exports = {
  entry: { main: &#39;./src/index.js&#39; },
  output: {
    path: path.resolve(__dirname, &#39;dist&#39;),
    filename: &#39;main.js&#39;
  },
  module: {
    rules: [
      {
        test: /\.js$/,
        exclude: /node_modules/,
        use: {
          loader: &quot;babel-loader&quot;
        }
      },
      {
        test: /\.css$/,
        use: ExtractTextPlugin.extract(
          {
            fallback: &#39;style-loader&#39;,
            use: [&#39;css-loader&#39;]
          })
      }
    ]
  },
  plugins: [ 
    new ExtractTextPlugin({filename: &#39;style.css&#39;})
  ]
};;
</code></pre>

<p>Since version 4, Webpack 4 has problems with this plugin, so you might run into this error:</p>

<p><a href="https://github.com/webpack-contrib/extract-text-webpack-plugin/issues/701" title="https://github.com/webpack-contrib/extract-text-webpack-plugin/issues/701">Webpack 4 compatibility · Issue #701 · webpack-contrib/extract-text-webpack-plugin<br/>
<u>I&#39;m trying to use this plugin with webpack 4 alpha 5 and getting the following error: Error: Chunk.entrypoints: Use…</u>github.com</a><a href="https://github.com/webpack-contrib/extract-text-webpack-plugin/issues/701"></a></p>

<p>To fix it, you can run</p>

<pre><code>npm install -D extract-text-webpack-plugin@next
</code></pre>

<blockquote>
<p>Pro tip: google errors you get and try to find similar question in Github issues or just ask a question on StackOverflow.</p>
</blockquote>

<p>After that, your css code should compile to _./dist/style.css_</p>

<p>At this point in my package.json my dev dependencies look like this:</p>

<pre><code>&quot;devDependencies&quot;: {
    &quot;babel-core&quot;: &quot;^6.26.0&quot;,
    &quot;babel-loader&quot;: &quot;^7.1.4&quot;,
    &quot;babel-preset-env&quot;: &quot;^1.6.1&quot;,
    &quot;css-loader&quot;: &quot;^0.28.11&quot;,
    &quot;extract-text-webpack-plugin&quot;: &quot;^4.0.0-beta.0&quot;,
    &quot;style-loader&quot;: &quot;^0.20.3&quot;,
    &quot;webpack&quot;: &quot;^4.4.1&quot;,
    &quot;webpack-cli&quot;: &quot;^2.0.12&quot;
  }
</code></pre>

<p>Please, note that another combination might not work since even updating webpack-cli v2.0.12 to 2.0.13 can break it. #justwebpackthings</p>

<p>So now it should output your <u>style.css</u> into _./dist_ folder.</p>

<p><img src="media/15248829536577/15248838758588.png" alt=""/></p>

<h3 id="toc_10">Configure support for SCSS</h3>

<p>It is very common to develop websites with SASS and POSTCSS, they are very helpful. So we will include support for SASS first. Let`s rename our _./src/style.css_ and create another folder to store _.scss_ files in there. Now we need to add support for _.scss_ formatting.</p>

<pre><code>npm install node-sass sass-loader --save-dev
</code></pre>

<p>replace style.scss with _./scss/main.scss_ in your _.js_ file.</p>

<h3 id="toc_11">HTML template</h3>

<p>Now lets create _.html_ file template. Add <u>index.html</u> to _./src_ file with exactly the same structure.</p>

<pre><code>&lt;html&gt;
  &lt;head&gt;
    &lt;link rel=&quot;stylesheet&quot; href=&quot;style.css&quot;&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div&gt;Hello, world!&lt;/div&gt;
    &lt;script src=&quot;main.js&quot;&gt;&lt;/script&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>We will need to use html plugin for this file in order to use it as a template.</p>

<pre><code>npm install html-webpack-plugin --save-dev
</code></pre>

<p>Add it to your webpack file:</p>

<pre><code>// webpack v4
const path = require(&#39;path&#39;);
const ExtractTextPlugin = require(&#39;extract-text-webpack-plugin&#39;);
const HtmlWebpackPlugin = require(&#39;html-webpack-plugin&#39;);

module.exports = {
  entry: { main: &#39;./src/index.js&#39; },
  output: {
    path: path.resolve(__dirname, &#39;dist&#39;),
    filename: &#39;main.js&#39;
  },
  module: {
    rules: [
      {
        test: /\.js$/,
        exclude: /node_modules/,
        use: {
          loader: &quot;babel-loader&quot;
        }
      },
      {
        test: /\.scss$/,
        use: ExtractTextPlugin.extract(
          {
            fallback: &#39;style-loader&#39;,
            use: [&#39;css-loader&#39;, &#39;sass-loader&#39;]
          })
      }
    ]
  },
  plugins: [ 
    new ExtractTextPlugin(
      {filename: &#39;style.css&#39;}
    ),
    new HtmlWebpackPlugin({
      inject: false,
      hash: true,
      template: &#39;./src/index.html&#39;,
      filename: &#39;index.html&#39;
    })
  ]
};
</code></pre>

<p>Now your file from _./src/index.html_ is a template for your final index.html file. To check that everything works, delete every file from _./dist_ folder and the folder itself.</p>

<pre><code>rm -rf /dist
npm run dev
</code></pre>

<p>You will see that _./dist_ folder was created on its own and there are three files: index.html, style.css, script.js.</p>

<h3 id="toc_12">Caching and Hashing</h3>

<p>One of the most common problems in development is implementing caching. It is very important to understand how it works since you want your users to always have the best latest version of your code.</p>

<p>Since this blogpost is mainly about webpack configuration, we will not concentrate on how caching works in details. I will just say that one of the most popular ways to solve caching problems is adding a <u>hash number</u> to asset files, such <u>style.css</u> and <u>script.js</u>. You can read about it <a href="https://developers.google.com/web/fundamentals/performance/webpack/use-long-term-caching#split-the-code-into-routes-and-pages">here</a>. Hashing is needed to teach our browser to only request changed files.</p>

<p>Webpack 4 has a prebuilt functionality for it implemented via <a href="https://webpack.js.org/guides/caching/">chunkhash</a>. It can be done with:</p>

<pre><code>// webpack v4
const path = require(&#39;path&#39;);
const ExtractTextPlugin = require(&#39;extract-text-webpack-plugin&#39;);
const HtmlWebpackPlugin = require(&#39;html-webpack-plugin&#39;);

module.exports = {
  entry: { main: &#39;./src/index.js&#39; },
  output: {
    path: path.resolve(__dirname, &#39;dist&#39;),
    filename: &#39;[name].[chunkhash].js&#39;
  },
  module: {
    rules: [
      {
        test: /\.js$/,
        exclude: /node_modules/,
        use: {
          loader: &quot;babel-loader&quot;
        }
      },
      {
        test: /\.scss$/,
        use: ExtractTextPlugin.extract(
          {
            fallback: &#39;style-loader&#39;,
            use: [&#39;css-loader&#39;, &#39;sass-loader&#39;]
          })
      }
    ]
  },
  plugins: [ 
    new ExtractTextPlugin(
      {filename: &#39;style.[chunkhash].css&#39;, disable: false, allChunks: true}
    ),
    new HtmlWebpackPlugin({
      inject: false,
      hash: true,
      template: &#39;./src/index.html&#39;,
      filename: &#39;index.html&#39;
    }),
  ]
};
</code></pre>

<p>In your_./src/index.html_ file add</p>

<pre><code>&lt;html&gt;
  &lt;head&gt;
    &lt;link rel=&quot;stylesheet&quot; href=&quot;&lt;%=htmlWebpackPlugin.files.chunks.main.css %&gt;&quot;&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div&gt;Hello, world!&lt;/div&gt;
    &lt;script src=&quot;&lt;%= htmlWebpackPlugin.files.chunks.main.entry %&gt;&quot;&gt;&lt;/script&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>This syntax will teach your template to use hashed files. This is a new feature implemented after this issue:</p>

<p><a href="https://github.com/jantimon/html-webpack-plugin/pull/14" title="https://github.com/jantimon/html-webpack-plugin/pull/14">Support for .css and .manifest files and cache busting by jantimon · Pull Request #14 ·…<br/>
<u>This pull request deprecates the usage of {%=o.htmlWebpackPlugin.assets%} and creates a new representation called…</u>github.com</a><a href="https://github.com/jantimon/html-webpack-plugin/pull/14"></a></p>

<p>We will use htmlWebpackPlugin.files.chunks.main pattern described there.</p>

<p>now in our _./dist_ file index.html</p>

<p><img src="media/15248829536577/15248839087986.png" alt=""/></p>

<p>Now if we do not change anything in our _.js_ and. <u>css</u> file and run</p>

<pre><code>npm run dev
</code></pre>

<p>no matter how many times you run it, the numbers in hashes should be identical to each other in both files.</p>

<h3 id="toc_13">Problem with hashing and how to solve it</h3>

<p>Although we have the working implementation here, it is not perfect yet. What if we change some code in our _.scss_ file? Go ahead, change some scss there and run dev script again. Now the new file hash is not generated</p>

<p>What if we add a new console.log to our _.js_ file like this:</p>

<pre><code>import &quot;./style.css&quot;;
console.log(&quot;hello, world&quot;);
console.log(&quot;Hello, world 2&quot;);
</code></pre>

<p>If you run a dev script again, you will see that hash number has been updated in both files.</p>

<p>This issue is known and there is even a stack overflow question about it:</p>

<p><a href="https://stackoverflow.com/questions/44491064/updating-chunkhash-in-both-css-and-js-file-in-webpack" title="https://stackoverflow.com/questions/44491064/updating-chunkhash-in-both-css-and-js-file-in-webpack">Updating chunkhash in both css and js file in webpack<br/>
<u>I have only got the JS file in the output whereas i have used the ExtractTextPlugin to extract the Css file.Both have…</u>stackoverflow.com</a><a href="https://stackoverflow.com/questions/44491064/updating-chunkhash-in-both-css-and-js-file-in-webpack"></a></p>

<h4 id="toc_14">Now how to fix that?</h4>

<p>After trying a lot of plugins that claim they solve this problem I have finally came to two types of solution:</p>

<h4 id="toc_15">Solution 1</h4>

<p>Replace [chukhash] with just [hash] in _.css_ extract plugin. This was one of the solutions to the <a href="https://github.com/webpack-contrib/extract-text-webpack-plugin/issues/763">issue</a>. This appears to be a conflict with webpack 4.3 which introduced a <code>[contenthash]</code> variable of its <a href="https://github.com/webpack/webpack/releases/tag/v4.3.0">own</a>. In conjunction, use this plugin: <a href="https://www.npmjs.com/package/webpack-md5-hash">webpack-md5-hash</a></p>

<p>Now if you make changes to your <u>main.scss</u> file and run dev script, only a new <u>style.css</u> should be generated with a new hash.</p>

<pre><code>// webpack v4
const path = require(&#39;path&#39;);
const ExtractTextPlugin = require(&#39;extract-text-webpack-plugin&#39;);
const HtmlWebpackPlugin = require(&#39;html-webpack-plugin&#39;);
const WebpackMd5Hash = require(&#39;webpack-md5-hash&#39;);

module.exports = {
  entry: { main: &#39;./src/index.js&#39; },
  output: {
    path: path.resolve(__dirname, &#39;dist&#39;),
    filename: &#39;[name].[chunkhash].js&#39;
  },
  module: {
    rules: [
      {
        test: /\.js$/,
        exclude: /node_modules/,
        use: {
          loader: &quot;babel-loader&quot;
        }
      },
      {
        test: /\.scss$/,
        use: ExtractTextPlugin.extract(
          {
            fallback: &#39;style-loader&#39;,
            use: [&#39;css-loader&#39;, &#39;sass-loader&#39;]
          })
      }
    ]
  },
  plugins: [ 
    new ExtractTextPlugin(
      {filename: &#39;style.[hash].css&#39;, disable: false, allChunks: true}
    ),
    new HtmlWebpackPlugin({
      inject: false,
      hash: true,
      template: &#39;./src/index.html&#39;,
      filename: &#39;index.html&#39;
    }),
    new WebpackMd5Hash()
  ]
};
</code></pre>

<p>Now lets test our _.js_ files. Now both files change hash.</p>

<h4 id="toc_16">Solution 2</h4>

<p>There might also be some conflicts still, so now lets try <a href="https://github.com/webpack-contrib/mini-css-extract-plugin">mini-css-extract plugin</a>.</p>

<h3 id="toc_17">Mini-CSS plugin</h3>

<p>The Mini CSS plugin is meant to replace extract-text plugin and provide you with better future compatibility.</p>

<p>I have restructured my webpack file to compile style.css with <a href="https://github.com/webpack-contrib/mini-css-extract-plugin" title="https://github.com/webpack-contrib/mini-css-extract-plugin">/mini-css-extract-plugin</a> and it works for me.</p>

<pre><code>// webpack v4
const path = require(&#39;path&#39;);
// const ExtractTextPlugin = require(&#39;extract-text-webpack-plugin&#39;);
const HtmlWebpackPlugin = require(&#39;html-webpack-plugin&#39;);
const WebpackMd5Hash = require(&#39;webpack-md5-hash&#39;);
const MiniCssExtractPlugin = require(&quot;mini-css-extract-plugin&quot;);

module.exports = {
  entry: { main: &#39;./src/index.js&#39; },
  output: {
    path: path.resolve(__dirname, &#39;dist&#39;),
    filename: &#39;[name].[chunkhash].js&#39;
  },
  module: {
    rules: [
      {
        test: /\.js$/,
        exclude: /node_modules/,
        use: {
          loader: &quot;babel-loader&quot;
        }
      },
      {
        test: /\.scss$/,
        use:  [  &#39;style-loader&#39;, MiniCssExtractPlugin.loader, &#39;css-loader&#39;, &#39;sass-loader&#39;]
      }
    ]
  },
  plugins: [
    // new ExtractTextPlugin(
    //   {filename: &#39;style.[hash].css&#39;, disable: false, allChunks: true }
    // ),
    new MiniCssExtractPlugin({
      filename: &#39;style.[contenthash].css&#39;,
    }),
    new HtmlWebpackPlugin({
      inject: false,
      hash: true,
      template: &#39;./src/index.html&#39;,
      filename: &#39;index.html&#39;
    }),
    new WebpackMd5Hash()
  ]
};
</code></pre>

<blockquote>
<p>Now when I edit <u>main.scss</u> a new hash for style_.css_ is generated. And when I edit css only css hash changes and when I edit _./src/script.js_ only <u>script.js</u> hash changes!</p>
</blockquote>

]]></content>
  </entry>
  
</feed>
