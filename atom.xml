<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Junkman]]></title>
  <link href="http://panlw.github.io/atom.xml" rel="self"/>
  <link href="http://panlw.github.io/"/>
  <updated>2018-06-01T01:09:43+08:00</updated>
  <id>http://panlw.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[深入浅出Netty - EventLoop, EventLoopGroup]]></title>
    <link href="http://panlw.github.io/15277861393795.html"/>
    <updated>2018-06-01T01:02:19+08:00</updated>
    <id>http://panlw.github.io/15277861393795.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>DECEMBER 24 2016 <a href="https://caorong.github.io/2016/12/24/head-first-netty-1/">原文地址</a></p>
</blockquote>

<p>近年来netty 十分流行，至于为何那么流行，因为它性能好，能更有效的利用系统资源。</p>

<p>但是， netty 很容易上手， 但是很多时候却无法理解设计的初衷， 比如为什么会有 EventLoop 这个东东， 本文从历史阐述原因。</p>

<h2 id="toc_0"><a href="#%E5%90%8C%E6%AD%A5%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B" title="同步网络模型"></a>同步网络模型</h2>

<p>首先说说同步模型， 也就是常说的 bio， bio 是基于 stream 的。 网上找到一个图如下</p>

<p><img src="media/15277861393795/15277864485883.jpg" alt=""/></p>

<p>从图可知，对于bio 来说，我的程序何时处理完需要靠 看 datasource 何时发完数据。</p>

<p>这是非常坑爹的， 举个例子， 如果我的服务是一个多线程的 bio 的服务(比如 10个线程)，然后某个黑客建10个 telnet 连上你的server， holding住 不发数据，那么你的服务就down了。</p>

<p>那么如何解决这种问题？ 如果 waiting client 发的数据不阻塞 worker 线程不就行了？</p>

<p>于是出现了 异步网络模型</p>

<h2 id="toc_1"><a href="#%E5%BC%82%E6%AD%A5%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B" title="异步网络模型"></a>异步网络模型</h2>

<p>异步网络模型， 也就是非阻塞IO， 在java 中由于 jdk1.4 之后才被引入所以被称之为 Nio (New IO)</p>

<p>网上同样找到个图</p>

<p><img src="media/15277861393795/15277864586815.jpg" alt=""/></p>

<p>由图可知， 实现异步的原理是 系统帮你给每一个连接(channel) 都维护了一个 <code>buffer</code>。<br/>
将 client 发来的数据暂存到 <code>buffer</code> 中, 待 java 进程需要的时候一次性做一次内存拷贝，在 java 进程中使用。</p>

<p>当然， 写出也是同样的 写到 <code>buffer</code> 中，当 <code>buffer</code> 满 或者 手动调用 <code>flush</code> 写出去。</p>

<p>下面有个简单的 非阻塞 echo server demo， 当读到 q 时关闭连接。</p>

<pre><code class="language-java">ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();

serverSocketChannel.socket().bind(new InetSocketAddress(9999));
serverSocketChannel.configureBlocking(false);

final List&lt;SocketChannel&gt; socketChannelList = Lists.newLinkedList();

new Thread(new Runnable() {
  @Override
  public void run() {
    while (true) {
      // 处理 每个连接是否可读, 这里的逻辑是 读4个字节后切断连接
      for (SocketChannel socketChannel : Lists.newArrayList(socketChannelList)) {
        try {
          ByteBuffer buf = ByteBuffer.allocate(4);
          int readed = socketChannel.read(buf);
          System.out.println(readed);
          System.out.println(Arrays.toString(buf.array()));
          if (readed &gt; 0 &amp;&amp; buf.array()[0] == &#39;q&#39;) {
            // close
            socketChannel.close();
            // remove from list
            socketChannelList.remove(socketChannel);
          }
        } catch (Throwable e) {
          e.printStackTrace();
        }
      }
      try {
        Thread.sleep(100);
      } catch (InterruptedException e) {
      }
    }
  }
}, &quot;server-handler-thread&quot;).start();

// 等待新连接连进来
while (true) {
  SocketChannel socketChannel = serverSocketChannel.accept();
  if (socketChannel != null) {
    socketChannelList.add(socketChannel);
  }
  Thread.sleep(1000);
}
</code></pre>

<p>简单分析下上面的代码，</p>

<ol>
<li> 由于 accept 是非阻塞的， 所以我们要一直轮询 判断是否有新连接进来。</li>
<li> 连接进来后，由于我们不知道 client 何时能发完数据， 所以我们维护了一个列表 <code>socketChannelList</code> 定期去轮询列表，看是否有数据可读。</li>
</ol>

<p>但是后来大家发现， 每当用 nonblocking socket 的时候都要做类似的操作， 于是想把它们提出来，提取成一个通用模块， 让代码实现更容易，</p>

<p>在 <a href="https://en.wikipedia.org/wiki/Berkeley_sockets">berkeley socket</a> 中这个模块被称为 select, poll, epoll. (后面2个是加强版)</p>

<p>而在java中， java 在他们之上抽象成 Selector</p>

<h2 id="toc_2"><a href="#Selector-select-poll-epoll" title="Selector(select, poll, epoll)"></a>Selector(select, poll, epoll)</h2>

<p>既然 select 是一个使用 nonblocking io 的通用封装， 那么在 linux 中， 为什么要有 select, poll, epoll 这么多的选择呢？</p>

<p>我猜测，最早其实只有 select， 然后由于linux 是一个开源， 所以， 后来别人实现了性能更好的 poll, 再之后 epoll 也是这样。</p>

<p>但是, 由于它们都在 linux 2.6.12 之前就已经被实现了， 在github上 都看不到具体提交历史了。。</p>

<p><a href="https://github.com/torvalds/linux/commits/a379f71a30dddbd2e7393624e455ce53c87965d1/fs/select.c?after=Y3Vyc29yOqN59xow3dvS5zk2JORVzlPIeWXRKzY5">select.c</a>,<br/>
<a href="https://github.com/torvalds/linux/commits/5924bbecd0267d87c24110cbe2041b5075173a25/include/linux/poll.h">poll.h</a>,<br/>
<a href="https://github.com/torvalds/linux/commits/5924bbecd0267d87c24110cbe2041b5075173a25/include/linux/eventpoll.h">eventpoll.h</a></p>

<p>我们再来看看 使用了 selector 后的非阻塞 IO 的编码, 同样实现一个 echo server</p>

<pre><code class="language-java">Selector selector = Selector.open();
ServerSocketChannel servChannel = ServerSocketChannel.open();
servChannel.configureBlocking(false);
// 建立一个server socket，到本地端口9999， backlog 1024
servChannel.socket().setReuseAddress(true);
servChannel.socket().bind(new InetSocketAddress(9999), 1024);
// selector 关心 server 上的 ACCEPT 事件
servChannel.register(selector, SelectionKey.OP_ACCEPT); 

while (start) {
  try {
    // 阻塞等待 直到有IO事件可读(系统IO事件队列不为空)
    selector.select();
    // 获取 事件 以及 事件所对应的 channel (client server 的连接)
    Set&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys();
    Iterator&lt;SelectionKey&gt; it = selectedKeys.iterator();
    SelectionKey key = null;
    while (it.hasNext()) {
      key = it.next();
      it.remove();
      try {
        if (key.isValid()) {
          // OP_ACCEPT 事件 表示有个新client 完成了三次握手。连接上了本服务器
          if (key.isAcceptable()) {
            // Accept the new connection
            ServerSocketChannel ssc = (ServerSocketChannel) key.channel();
            SocketChannel sc = ssc.accept();
            sc.configureBlocking(false);
            // 将该连接的可读事件 注册到 selector， 到时候他发起请求的时候，我会收到新事件
            sc.register(selector, SelectionKey.OP_READ);
          }
          // OP_READ 事件 说明 client 发的数据已经发到了系统缓冲区，server 可以去读了。
          if (key.isReadable()) {
            SocketChannel sc = (SocketChannel) key.channel();
            // 分配用户台空间, 将数据从内核态 拷贝到 用户态
            ByteBuffer readBuffer = ByteBuffer.allocate(4);
            int readBytes = sc.read(readBuffer);
            if (readBytes &gt; 0) {
              // 切换读写模式 详见下面的图, 表示自己目前可以读 [position, limit]
              readBuffer.flip();
              byte[] bytes = new byte[readBuffer.remaining()];
              // 将buffer 数据拷贝到 bytes 数组
              // 如果这里只收到一半的数据怎么办？
              String body = new String(bytes, &quot;UTF-8&quot;);
              System.out.println(body);
              // 将 read的数据 写回去
              ByteBuffer writeBuffer = ByteBuffer.allocate(bytes.length);
              writeBuffer.put(bytes);
              writeBuffer.flip();
              sc.write(writeBuffer);
            } else if (readBytes &lt; 0) {
              // 对端链路关闭
              key.cancel();
              sc.close();
            } else
              ;
          }
        }
      } catch (Exception e) {
        if (key != null) {
          key.cancel();
          if (key.channel() != null)
            key.channel().close();
        }
      }
    }
  } catch (Exception e) {
    throw e;
  }
}
</code></pre>

<pre><code class="language-sh">~ ᐅ nc localhost 9999
1
1
2
2
</code></pre>

<p>但是， 很奇怪的是，代码更长了。。。</p>

<p>因为其实真正的 selector 将所有的socket 的时间都封装进去了。</p>

<p>带来的结果是， java 层的代码 只要一个线程就能处理所有了，(我之前的实现需要2个线程)。</p>

<p>当然还有个问题是， 及时用了 selector， 我们依然认为代码可读性非常差， 代码容易写错。</p>

<p>于是， netty 登场了， 他在 selector 之上， 又封装了一层， 让我们先看下 同样实现 netty 的代码。</p>

<h2 id="toc_3"><a href="#netty-echo-server" title="netty echo server"></a>netty echo server</h2>

<pre><code class="language-java">public class Netty4Demo {
  public class EchoHandler extends SimpleChannelInboundHandler {
    @Override
    public void channelRead0(ChannelHandlerContext ctx, Object msg) throws Exception {
      // 为什么 这里可以强转String？
      String in = (String) msg;
      System.out.print(in);
      // 将数据写回
      ctx.writeAndFlush(in);
    }
  }

  public void run() throws Exception {
    EventLoopGroup acceptGroup = new NioEventLoopGroup(1); // 指定 Acceptor 线程池大小
    EventLoopGroup ioGroup = new NioEventLoopGroup(1); // 指定 NIO线程池大小
    try {
      ServerBootstrap b = new ServerBootstrap(); // 创建 ServerBootstrap 对象，他是Netty 用于启动NIO 服务端的辅助启动类，目的是降低服务端的开发复杂度。
      b.group(acceptGroup, ioGrou).channel(
          NioServerSocketChannel.class) // 指定使用 java 的NioServerSocketChannel
          .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { // 创建 IOThread 的 pipeline
            @Override
            public void initChannel(SocketChannel ch) throws Exception {
              ch.pipeline()
                .addLast(new StringDecoder())
                .addLast(new StringEncoder()); // 添加echo Handler
                .addLast(new EchoHandler())
            }
          }).option(ChannelOption.SO_BACKLOG, 128)          // server socket config backlog 设置为 128
          .childOption(ChannelOption.SO_KEEPALIVE, true); // client socket config 设置 keepalive = true
      // 绑定端口，开始接收进来的连接
      ChannelFuture f = b.bind(9999).sync(); // 同步等待绑定本地端口

      // 等待服务器  socket 关闭 。
      // 在这个例子中，这不会发生，但你可以优雅地关闭你的服务器。
      f.channel().closeFuture().sync();
    } finally {
      // 释放两个线程池
      acceptGroup.shutdownGracefully();
      ioGrou.shutdownGracefully();
    }
  }

  public static void main(String[] args) throws Exception {
    new Netty4Demo().run();
  }
}
</code></pre>

<pre><code class="language-sh">~ ᐅ nc localhost 9999
1
1
2
2
3
3
</code></pre>

<p>代码短了一些， 关键是少了很多 if 判断， 代码可读性更好了。</p>

<p>但是， 可读性提高(抽象)的同时， 又带出了了问题， EventLoop 是什么 , EventloopGroup 是什么？ 为什么要有它们？</p>

<p>这里要解释以上的问题， 要先从 线程模型说起</p>

<h2 id="toc_4"><a href="#%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B" title="线程模型"></a>线程模型</h2>

<p>首先是 单线程模型， 如下图</p>

<p><img src="media/15277861393795/15277864778610.jpg" alt=""/></p>

<p>上面直接用 selector 实现的 echo server 就是单线程模型，<br/>
还有类似存在 GIL 问题的语言比如 python 的 tornado 也是 单线程模型。</p>

<p>然后是多线程模型</p>

<p><img src="media/15277861393795/15277864864829.jpg" alt=""/></p>

<p>多线程模型的区别就是 让接收连接 和 对连接读写的处理 分别用不同的线程处理。 比如上面的 echo server demo 就可以被称为一个 多线程模型。<br/>
只不过扩展性差了些。。。</p>

<p>netty 的 EventloopGroup 其实就是线程池， 通过它来配置 接收连接 和 处理连接读写 的线程池大小。</p>

<h2 id="toc_5"><a href="#netty-%E4%B9%8B-EventloopGroup" title="netty 之 EventloopGroup"></a>netty 之 EventloopGroup</h2>

<p><img src="media/15277861393795/15277864950859.jpg" alt=""/></p>

<p>上图就是 EventLoopGroup 的大致模块图。</p>

<p>Boss EventloopGroup 和 worker EventloopGroup 分别 处理 接收连接， 和读写。</p>

<p>而 Eventloop 里则封装了类似上面单线程的 echo server 的模块下面会详细说。</p>

<p>对于开发者来说，主要关心紫色圈出部分。 其余的都已经封装完毕</p>

<p>有几个关键点。</p>

<ol>
<li> Eventloopsize 建议设置为 2 的次方，dispatch 使用位移，更快。</li>
<li> 侦听一个端口，只会绑定到 BossEventLoopGroup 中的一个 Eventloop，所以， BossEventLoopGroup 配置多个也无用。</li>
</ol>

<h2 id="toc_6"><a href="#EventLoop" title="EventLoop"></a>EventLoop</h2>

<p>如果只使用 tcp 和 异步阻塞的话主要关心以下2个 EventLoop (本文也只介绍这2个)</p>

<p>NioEventLoop - 基于java 原生nio</p>

<pre><code>level-triggered （水平触发）

</code></pre>

<p>EpollEventLoop - native jni 直接调用 epoll, only work on linux</p>

<pre><code>edge-triggered （边缘触发）更少的系统调用
C代码，更少GC，更少synchronized
暴露了更多的Socket配置参数

</code></pre>

<h3 id="toc_7"><a href="#%E6%B5%81%E7%A8%8B%E5%9B%BE" title="流程图"></a>流程图</h3>

<p><img src="media/15277861393795/15277865072620.jpg" alt=""/></p>

<p>关键点</p>

<ol>
<li> 整个loop 干的事情就是 <code>select -&gt; processIO -&gt; runAllTask</code></li>
<li> 这是一个死循环</li>
<li> 那么这个loop 如何自己优雅退出？ noway，只能通过外部添加 CloseTask， 比如添加到 MpscQueue</li>
<li> deadline 为 定时任务的触发时间，避免 select 阻塞, 让 定时任务不能及时执行。</li>
<li> 在select 这一步 解决 epollbug</li>
</ol>

<p>关于解决 epoll bug的原理是 应当 “阻塞”的 select 变得不再阻塞。<br/>
所以只需要统计下 select 次数就行了</p>

<p>部分关键代码:</p>

<pre><code class="language-java">for(;;){
    int selectedKeys = selector.select(timeoutMillis); // select with timeout
    selectCnt ++;
    // 我由于 select 阻塞 而等待了 timeoutMillis 毫秒， 说明， 我阻塞了，说明没有bug
    if (time - TimeUnit.MILLISECONDS.toNanos(timeoutMillis) &gt;= currentTimeNanos) {
        selectCnt = 1;
    } else if (SELECTOR_AUTO_REBUILD_THRESHOLD &gt; 0 &amp;&amp;
            selectCnt &gt;= SELECTOR_AUTO_REBUILD_THRESHOLD) {
        // 在小于 timeoutMillis 毫秒的时间内 select 的次数超过了 阀值(512) 次
        rebuildSelector();
        selector = this.selector;

        selector.selectNow();// Select again
        selectCnt = 1;
        break;
    }
}
</code></pre>

<h2 id="toc_8"><a href="#other" title="other"></a>other</h2>

<h3 id="toc_9"><a href="#Selector-wakeup" title="Selector.wakeup()"></a>Selector.wakeup()</h3>

<p>java 的 Selector 在原生的 select api 之上 增加了个 Selector.wakeup()</p>

<p>她的目的是唤醒 阻塞在 <code>select()</code> 的线程, (通过写一个字节)</p>

<p>为什么要唤醒？ 什么时候需要唤醒？</p>

<ol>
<li>注册了新的 channel 或者事件。</li>
<li>channel 关闭， 取消注册。</li>
<li>优先级更高的事件触发（如定时器事件）， 希望及时处理。</li>
</ol>

<h4 id="toc_10"><a href="#%E5%8E%9F%E7%90%86" title="原理"></a>原理</h4>

<p>Linux上利用pipe调用创建一个管道，Windows上则是一个loopback的tcp连接。这是因为win32的管道无法加入select的fd set，将管道或者TCP连接加入select fd set。</p>

<p>wakeup往管道或者连接写入一个字节，阻塞的select因为有I/O事件就绪，立即返回。可见，wakeup的调用开销不可忽视。</p>

<p>之前看到的 coolshell 也分析过 –&gt; Java NIO类库Selector机制解析（<a href="http://blog.csdn.net/haoel/archive/2008/03/27/2224055.aspx">上</a>，<a href="http://blog.csdn.net/haoel/archive/2008/03/27/2224069.aspx">下</a>，<a href="http://blog.csdn.net/haoel/archive/2008/05/04/2379586.aspx">续</a>）</p>

<h1 id="toc_11"><a href="#reference" title="reference"></a>reference</h1>

<p><a href="http://www.infoq.com/cn/articles/netty-threading-model">http://www.infoq.com/cn/articles/netty-threading-model</a></p>

<p><a href="http://calvin1978.blogcn.com/articles/netty-performance.html">http://calvin1978.blogcn.com/articles/netty-performance.html</a></p>

<p><a href="http://calvin1978.blogcn.com/articles/netty-performance2.html">http://calvin1978.blogcn.com/articles/netty-performance2.html</a></p>

<p><a href="http://tech.meituan.com/nio.html">http://tech.meituan.com/nio.html</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Functor and monad examples in plain Java]]></title>
    <link href="http://panlw.github.io/15277856731195.html"/>
    <updated>2018-06-01T00:54:33+08:00</updated>
    <id>http://panlw.github.io/15277856731195.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://www.nurkiewicz.com/2016/06/functor-and-monad-examples-in-plain-java.html">https://www.nurkiewicz.com/2016/06/functor-and-monad-examples-in-plain-java.html</a><br/>
June 23, 2016</p>
</blockquote>

<p>This article was initially an appendix in our <a href="http://amzn.to/28NV8eJ">Reactive Programming with RxJava</a> book. However introduction to monads, albeit very much related to reactive programming, didn&#39;t suit very well. So I decided to take it out and publish separately as a blog post. I am aware that &quot;_my very own, half correct and half complete explanation of monads_&quot; is the new &quot;_Hello, world_&quot; on programming blogs. Yet the article looks at functors and monads from a specific angle of Java data structures and libraries. Thus I thought it&#39;s worthwhile sharing.</p>

<p>RxJava was designed and built on top of very fundamental concepts like <u>functors</u> , <u>monoids</u> and <u>monads</u> . Even though Rx was modeled initially for imperative C# language and we are learning about RxJava, working on top of similarly imperative language, the library has its roots in functional programming. You should not be surprised after you realize how compact RxJava API is. There are pretty much just a handful of core classes, typically immutable, and everything is composed using mostly pure functions.</p>

<p>With a recent rise of functional programming (or functional style), most commonly expressed in modern languages like Scala or Clojure, monads became a widely discussed topic. There is a lot of folklore around them:</p>

<p>A monad is a monoid in the category of endofunctors, what&#39;s the problem?<br/>
<a href="http://james-iry.blogspot.com/2009/05/brief-incomplete-and-mostly-wrong.html">_James Iry_</a></p>

<p>The curse of the monad is that once you get the epiphany, once you understand - &quot;oh that&#39;s what it is&quot; - you lose the ability to explain it to anybody.<br/>
<a href="https://www.youtube.com/watch?v=dkZFtimgAcM">_Douglas Crockford_</a></p>

<p>The vast majority of programmers, especially those without functional programming background, tend to believe monads are some arcane computer science concept, so theoretical that it can not possibly help in their programming career. This negative perspective can be attributed to dozens of articles and blog posts being either too abstract or too narrow. But it turns out that monads are all around us, even is standard Java library, especially since Java Development Kit (JDK) 8 (more on that later). What is absolutely brilliant is that once you understand monads for the first time, suddenly several unrelated classes and abstractions, serving entirely different purposes, become familiar.</p>

<p>Monads generalize various seemingly independent concepts so that learning yet another incarnation of monad takes very little time. For example you do not have to learn how <code>CompletableFuture</code> works in Java 8, once you realize it is a monad, you know precisely how it works and what can you expect from its semantics. And then you hear about RxJava which sounds so much different but because <code>Observable</code> is a monad, there is not much to add. There are numerous other examples of monads you already came across without knowing that. Therefore this section will be a useful refresher even if you fail to actually use RxJava.</p>

<h1 id="toc_0">Functors</h1>

<p>Before we explain what a monad is, let&#39;s explore simpler construct called a <u>functor</u> . A functor is a typed data structure that encapsulates some value(s). From syntactic perspective a functor is a container with the following API:</p>

<pre><code class="language-java">import java.util.function.Function;
 
interface Functor&lt;T&gt; {
     
    &lt;R&gt; Functor&lt;R&gt; map(Function&lt;T, R&gt; f);
     
}
</code></pre>

<p>But mere syntax is not enough to understand what functor is. The only operation that functor provides is <code>map()</code> that takes a function <code>f</code>. This function receives whatever is inside a box, transforms it and wraps the result as-is into a second functor. Please read that carefully. <code>Functor&lt;T&gt;</code> is always an immutable container, thus <code>map</code> never mutates original object it was executed on. Instead it returns the result (or results - be patient) wrapped in a brand new functor, possibly of different type <code>R</code>. Additionally functors should not perform any actions when identity function is applied, that is <code>map(x -&gt; x)</code>. Such a pattern should always return either the same functor or an equal instance.</p>

<p>Often <code>Functor&lt;T&gt;</code> is compared to a box holding instance of <code>T</code> where the only way of interacting with this value is by transforming it. However there is no idiomatic way of unwrapping or escaping from the functor. The value(s) always stay within the context of functor. Why are functors useful? They generalize multiple common idioms like collections, promises, optionals, etc. with a single, uniform API that works across all of them. Let me introduce a couple of functors to make you more fluent with this API:</p>

<pre><code class="language-java">interface Functor&lt;T,F extends Functor&lt;?,?&gt;&gt; {
    &lt;R&gt; F map(Function&lt;T,R&gt; f);
}
 
class Identity&lt;T&gt; implements Functor&lt;T,Identity&lt;?&gt;&gt; {
 
    private final T value;
 
    Identity(T value) { this.value = value; }
 
    public &lt;R&gt; Identity&lt;R&gt; map(Function&lt;T,R&gt; f) {
        final R result = f.apply(value);
        return new Identity&lt;&gt;(result);
    }
     
}
</code></pre>

<p>An extra <code>F</code> type parameter was required to make <code>Identity</code> compile. What you saw in the preceding example was the simplest functor just holding a value. All you can do with that value is transforming it inside <code>map</code> method, but there is no way to extract it. This is considered beyond the scope of pure functor. The only way to interact with functor is by applying sequences of type-safe transformations:</p>

<pre><code class="language-java">Identity&lt;String&gt; idString = new Identity&lt;&gt;(&quot;abc&quot;);
Identity&lt;Integer&gt; idInt = idString.map(String::length);
</code></pre>

<p>Or fluently, just like you compose functions:</p>

<pre><code class="language-java">Identity&lt;byte[]&gt; idBytes = new Identity&lt;&gt;(customer)
        .map(Customer::getAddress)
        .map(Address::street)
        .map((String s) -&gt; s.substring(0, 3))
        .map(String::toLowerCase)
        .map(String::getBytes);
</code></pre>

<p>From this perspective mapping over a functor is not much different than just invoking chained functions:</p>

<pre><code class="language-java">byte[] bytes = customer
        .getAddress()
        .street()
        .substring(0, 3)
        .toLowerCase()
        .getBytes();
</code></pre>

<p>Why would you even bother with such verbose wrapping that not only does not provide any added value, but also is not capable of extracting the contents back? Well, it turns out you can model several other concepts using this raw functor abstraction. For example <code>java.util.Optional&lt;T&gt;</code> starting from Java 8 is a functor with <code>map()</code> method. Let us implement it from scratch:</p>

<pre><code class="language-java">class FOptional&lt;T&gt; implements Functor&lt;T,FOptional&lt;?&gt;&gt; {
 
    private final T valueOrNull;
 
    private FOptional(T valueOrNull) {
        this.valueOrNull = valueOrNull;
    }
 
    public &lt;R&gt; FOptional&lt;R&gt; map(Function&lt;T,R&gt; f) {
        if (valueOrNull == null)
            return empty();
        else
            return of(f.apply(valueOrNull));
    }
 
    public static &lt;T&gt; FOptional&lt;T&gt; of(T a) {
        return new FOptional&lt;T&gt;(a);
    }
 
    public static &lt;T&gt; FOptional&lt;T&gt; empty() {
        return new FOptional&lt;T&gt;(null);
    }
 
}
</code></pre>

<p>Now it becomes interesting. An <code>FOptional&lt;T&gt;</code> functor <u>may</u> hold a value, but just as well it might be empty. It&#39;s a type-safe way of encoding <code>null</code>. There are two ways of constructing <code>FOptional</code> - by supplying a value or creating <code>empty()</code> instance. In both cases, just like with <code>Identity</code>, <code>FOptional</code> is immutable and we can only interact with the value from inside. What differs <code>FOptional</code> is that the transformation function <code>f</code> may not be applied to any value if it is empty. This means functor may not necessarily encapsulate exactly one value of type <code>T</code>. It can just as well wrap arbitrary number of values, just like <code>List</code>... functor:</p>

<pre><code class="language-java">import com.google.common.collect.ImmutableList;
 
class FList&lt;T&gt; implements Functor&lt;T, FList&lt;?&gt;&gt; {
 
    private final ImmutableList&lt;T&gt; list;
 
    FList(Iterable&lt;T&gt; value) {
        this.list = ImmutableList.copyOf(value);
    }
 
    @Override
    public &lt;R&gt; FList&lt;?&gt; map(Function&lt;T, R&gt; f) {
        ArrayList&lt;R&gt; result = new ArrayList&lt;R&gt;(list.size());
        for (T t : list) {
            result.add(f.apply(t));
        }
        return new FList&lt;&gt;(result);
    }
}
</code></pre>

<p>The API remains the same: you take a functor in a transformation <code>T -&gt; R</code> - but the behavior is much different. Now we apply a transformation on each and every item in the <code>FList</code>, declaratively transforming whole list. So if you have a list of <code>customers</code> and you want a list of their streets, it&#39;s as simple as:</p>

<pre><code class="language-java">import static java.util.Arrays.asList;
 
FList&lt;Customer&gt; customers = new FList&lt;&gt;(asList(cust1, cust2));
 
FList&lt;String&gt; streets = customers
        .map(Customer::getAddress)
        .map(Address::street);
</code></pre>

<p>It&#39;s no longer as simple as saying <code>customers.getAddress().street()</code>, you can&#39;t invoke <code>getAddress()</code> on a collection of customers, you must invoke <code>getAddress()</code> on each individual customer and then place it back in a collection. By the way Groovy found this pattern so common that it actually has a syntax sugar for that: <code>customer*.getAddress()*.street()</code>. This operator, known as spread-dot, is actually a <code>map</code> in disguise. Maybe you are wondering why I iterate over <code>list</code> manually inside <code>map</code> rather than using <code>Stream</code>s from Java 8: <code>list.stream().map(f).collect(toList())</code>? Does this ring a bell? What if I told you <code>java.util.stream.Stream&lt;T&gt;</code> in Java is a functor as well? And by the way, also a monad?</p>

<p>Now you should see the first benefits of functors - they abstract away the internal representation and provide consistent, easy to use API over various data structures. As the last example let me introduce <u>promise</u> functor, similar to <code>Future</code>. <code>Promise</code> &quot;promises&quot; that a value will become available one day. It is not yet there, maybe because some background computation was spawned or we are waiting for external event. But it will appear some time in the future. The mechanics of completing a <code>Promise&lt;T&gt;</code> are not interesting, but the functor nature is:</p>

<pre><code class="language-java">Promise&lt;Customer&gt; customer = //...
Promise&lt;byte[]&gt; bytes = customer
        .map(Customer::getAddress)
        .map(Address::street)
        .map((String s) -&gt; s.substring(0, 3))
        .map(String::toLowerCase)
        .map(String::getBytes);
</code></pre>

<p>Looks familiar? That is the point! The implementation of <code>Promise</code> functor is beyond the scope of this article and not even important. Enough to say that we are very close to implementing <code>CompletableFuture</code> from Java 8 and we almost discovered <code>Observable</code> from RxJava. But back to functors. <code>Promise&lt;Customer&gt;</code> does not hold a value of <code>Customer</code> just yet. It promises to have such value in the future. But we can still map over such functor, just like we did with <code>FOptional</code> and <code>FList</code> - the syntax and semantics are exactly the same. The behavior follows what the functor represents. Invoking <code>customer.map(Customer::getAddress)</code> yields <code>Promise&lt;Address&gt;</code> which means <code>map</code> is non-blocking. <code>customer.map()</code> will <u>not</u> wait for the underlying <code>customer</code> promise to complete. Instead it returns another promise, of different type. When upstream promise completes, downstream promise applies a function passed to <code>map()</code> and passes the result downstream. Suddenly our functor allows us to pipeline asynchronous computations in a non-blocking manner. But you do not have to understand or learn that - because <code>Promise</code> is a functor, it must follow syntax and laws.</p>

<p>There are many other great examples of functors, for example representing value or error in a compositional manner. But it is high time to look at monads.</p>

<h1 id="toc_1">From functors to monads</h1>

<p>I assume you understand how functors work and why are they a useful abstraction. But functors are not that universal as one might expect. What happens if your transformation function (the one passed as an argument to <code>map()</code>) returns functor instance rather than simple value? Well, functor is just a value as well, so nothing bad happens. Whatever was returned is placed back in a functor so all behaves consistently. However imagine you have this handy method for parsing <code>String</code>s:</p>

<pre><code class="language-java">FOptional&lt;Integer&gt; tryParse(String s) {
    try {
        final int i = Integer.parseInt(s);
        return FOptional.of(i);
    } catch (NumberFormatException e) {
        return FOptional.empty();
    }
}
</code></pre>

<p>Exceptions are side-effects that undermine type system and functional purity. In pure functional languages there is no place for exceptions, after all we never heard about throwing exceptions during math classes, right? Errors and illegal conditions are represented explicitly using values and wrappers. For example <code>tryParse()</code> takes a <code>String</code> but does not simply return an <code>int</code> or silently throw an exception at runtime. We explicitly tell, through the type system, that <code>tryParse()</code> can fail, there is nothing exceptional or erroneous in having a malformed string. This semi-failure is represented by optional result. Interestingly Java has checked exceptions, the ones that must be declared and handled, so in some sense Java is purer in that regard, it does not hide side-effects. But for better or worse checked exceptions are often discouraged in Java, so let&#39;s get back to <code>tryParse()</code>. It seems useful to compose <code>tryParse</code> with <code>String</code> already wrapped in <code>FOptional</code>:</p>

<pre><code class="language-java">FOptional&lt;String&gt; str = FOptional.of(&quot;42&quot;);
FOptional&lt;FOptional&lt;Integer&gt;&gt; num = str.map(this::tryParse);
</code></pre>

<p>That should not come as a surprise. If <code>tryParse()</code> would return an <code>int</code> you would get <code>FOptional&lt;Integer&gt; num</code>, but because <code>map()</code> function returns <code>FOptional&lt;Integer&gt;</code> itself, it gets wrapped twice into awkward <code>FOptional&lt;FOptional&lt;Integer&gt;&gt;</code>. Please look carefully at the types, you must understand why we got this double wrapper here. Apart from looking horrible, having a functor in functor ruins composition and fluent chaining:</p>

<pre><code class="language-java">FOptional&lt;Integer&gt; num1 = //...
FOptional&lt;FOptional&lt;Integer&gt;&gt; num2 = //...
 
FOptional&lt;Date&gt; date1 = num1.map(t -&gt; new Date(t));
 
//doesn&#39;t compile!
FOptional&lt;Date&gt; date2 = num2.map(t -&gt; new Date(t));
</code></pre>

<p>Here we try to map over the contents of <code>FOptional</code> by turning <code>int</code> into +Date+. Having a function of <code>int -&gt; Date</code> we can easily transform from <code>Functor&lt;Integer&gt;</code> to <code>Functor&lt;Date&gt;</code>, we know how it works. But in case of <code>num2</code> situation becomes complicated. What <code>num2.map()</code> receives as input is no longer an <code>int</code> but an <code>FOoption&lt;Integer&gt;</code> and obviously <code>java.util.Date</code> does not have such a constructor. We broke our functor by double wrapping it. However having a function that returns a functor rather than simple value is so common (like <code>tryParse()</code>) that we can not simply ignore such requirement. One approach is to introduce a special parameterless <code>join()</code> method that &quot;flattens&quot; nested functors:</p>

<pre><code class="language-java">FOptional&lt;Integer&gt; num3 = num2.join()
</code></pre>

<p>It works but because this pattern is so common, special method named <code>flatMap()</code> was introduced. <code>flatMap()</code> is very similar to <code>map</code> but expects the function received as an argument to return a functor - or <u>monad</u> to be precise:</p>

<pre><code class="language-java">interface Monad&lt;T,M extends Monad&lt;?,?&gt;&gt; extends Functor&lt;T,M&gt; {
    M flatMap(Function&lt;T,M&gt; f);
}
</code></pre>

<p>We simply concluded that <code>flatMap</code> is just a syntactic sugar to allow better composition. But <code>flatMap</code> method (often called <code>bind</code> or <code>&gt;&gt;=</code> from Haskell) makes all the difference since it allows complex transformations to be composed in a pure, functional style. If <code>FOptional</code> was an instance of monad, parsing suddenly works as expected:</p>

<pre><code class="language-java">FOptional&lt;String&gt; num = FOptional.of(&quot;42&quot;);
FOptional&lt;Integer&gt; answer = num.flatMap(this::tryParse);
</code></pre>

<p>Monads do not need to implement <code>map</code>, it can be implemented on top of <code>flatMap()</code> easily. As a matter of fact <code>flatMap</code> is the essential operator that enables a whole new universe of transformations. Obviously just like with functors, syntactic compliance is not enough to call some class a monad, the <code>flatMap()</code> operator has to follow monad laws, but they are fairly intuitive like associativity of <code>flatMap()</code> and identity. The latter requires that <code>m(x).flatMap(f)</code> is the same as <code>f(x)</code> for any monad holding a value <code>x</code> and any function <code>f</code>. We are not going to dive too deep into monad theory, instead let&#39;s focus on practical implications. Monads shine when their internal structure is not trivial, for example <code>Promise</code> monad that will hold a value in the future. Can you guess from the type system how <code>Promise</code> will behave in the following program? First all methods that can potentially take some time to complete return a <code>Promise</code>:</p>

<pre><code class="language-java">import java.time.DayOfWeek;
 
 
Promise&lt;Customer&gt; loadCustomer(int id) {
    //...
}
 
Promise&lt;Basket&gt; readBasket(Customer customer) {
    //...
}
 
Promise&lt;BigDecimal&gt; calculateDiscount(Basket basket, DayOfWeek dow) {
    //...
}
</code></pre>

<p>We can now compose these functions as if they were all blocking using monadic operators:</p>

<pre><code class="language-java">Promise&lt;BigDecimal&gt; discount = 
    loadCustomer(42)
        .flatMap(this::readBasket)
        .flatMap(b -&gt; calculateDiscount(b, DayOfWeek.FRIDAY));
</code></pre>

<p>This becomes interesting. <code>flatMap()</code> must preserve monadic type therefor all intermediate objects are <code>Promise</code>s. It is not just about keeping the types in order - preceding program is suddenly fully asynchronous! <code>loadCustomer()</code> returns a <code>Promise</code> so it does not block. <code>readBasket()</code> takes whatever the <code>Promise</code> has (will have) and applies a function returning another <code>Promise</code> and so on and so forth. Basically we built an asynchronous pipeline of computation where the completion of one step in background automatically triggers next step.</p>

<h1 id="toc_2">Exploring <code>flatMap()</code></h1>

<p>It is very common to have two monads and combining the value they enclose together. However both functors and monads do not allow direct access to their internals, which would be impure. Instead we must carefully apply transformation without escaping the monad. Imagine you have two monads and you want to combine them:</p>

<pre><code class="language-java">import java.time.LocalDate;
import java.time.Month;
 
 
Monad&lt;Month&gt; month = //...
Monad&lt;Integer&gt; dayOfMonth = //...
 
Monad&lt;LocalDate&gt; date = month.flatMap((Month m) -&gt;
        dayOfMonth
                .map((int d) -&gt; LocalDate.of(2016, m, d)));
</code></pre>

<p>Please take your time to study the preceding pseudo-code. I don&#39;t use any real monad implementation like <code>Promise</code> or <code>List</code> to emphasize the core concept. We have two independent monads, one of type <code>Month</code> and the other of type <code>Integer</code>. In order to build <code>LocalDate</code> out of them we must build a nested transformation that has access to the internals of both monads. Work through the types, especially making sure you understand why we use <code>flatMap</code> in one place and <code>map()</code> in the other. Think how you would structure this code if you had a third <code>Monad&lt;Year&gt;</code> as well. This pattern of applying a function of two arguments (<code>m</code> and <code>d</code> in our case) is so common that in Haskell there is special helper function called <code>liftM2</code> that does exactly this transformation, implemented on top of <code>map</code> and <code>flatMap</code>. In Java pseudo-syntax it would look somewhat like this:</p>

<pre><code class="language-java">Monad&lt;R&gt; liftM2(Monad&lt;T1&gt; t1, Monad&lt;T2&gt; t2, BiFunction&lt;T1, T2, R&gt; fun) {
    return t1.flatMap((T1 tv1) -&gt;
            t2.map((T2 tv2) -&gt; fun.apply(tv1, tv2))
    );
}
</code></pre>

<p>You don&#39;t have to implement this method for every monad, <code>flatMap()</code> is enough, moreover it works consistently for all monads. <code>liftM2</code> is extremely useful when you consider how it can be used with various monads. For example <code>liftM2(list1, list2, function)</code> will apply <code>function</code> on every possible pair of items from <code>list1</code> and <code>list2</code> (Cartesian product). On the other hand for optionals it will apply a function only when both optionals are non-empty. Even better, for <code>Promise</code> monad a function will be executed asynchronously when both <code>Promise</code>s are completed. This means we just invented a simple synchronization mechanism (<code>join()</code> in fork-join algorithms) of two asynchronous steps.</p>

<p>Another useful operator that we can easily build on top of <code>flatMap()</code> is <code>filter(Predicate&lt;T&gt;)</code> which takes whatever is inside a monad and discards it entirely if it does not meet certain predicate. In a way it is similar to <code>map</code> but rather than 1-to-1 mapping we have 1-to-0-or-1. Again <code>filter()</code> has the same semantics for every monad but quite amazing functionality depending on which monad we actually use. Obviously it allows filtering out certain elements from a list:</p>

<pre><code class="language-java">FList&lt;Customer&gt; vips = 
    customers.filter(c -&gt; c.totalOrders &gt; 1_000);
</code></pre>

<p>But it works just as well e.g. for optionals. In that case we can transform non-empty optional into an empty one if the contents of optional does not meet some criteria. Empty optionals are left intact.</p>

<h1 id="toc_3">From list of monads to monad of list</h1>

<p>Another useful operator that originates from <code>flatMap()</code> is <code>sequence()</code>. You can easily guess what it does simply by looking at type signature:</p>

<pre><code class="language-java">Monad&lt;Iterable&lt;T&gt;&gt; sequence(Iterable&lt;Monad&lt;T&gt;&gt; monads)
</code></pre>

<p>Often we have a bunch of monads of the same type and we want to have a single monad of a list of that type. This might sounds abstract to you, but it is impressively useful. Imagine you wanted to load a few customers from the database concurrently by ID so you used <code>loadCustomer(id)</code> method several times for different IDs, each invocation returning <code>Promise&lt;Customer&gt;</code>. Now you have a list of <code>Promise</code>s but what you really want is a list of customers, e.g. to be displayed in the web browser. <code>sequence()</code> (in RxJava <code>sequence()</code> is called <code>concat()</code> or <code>merge()</code>, depending on use-case) operator is built just for that:</p>

<pre><code class="language-java">FList&lt;Promise&lt;Customer&gt;&gt; custPromises = FList
    .of(1, 2, 3)
    .map(database::loadCustomer);
 
Promise&lt;FList&lt;Customer&gt;&gt; customers = custPromises.sequence();
 
customers.map((FList&lt;Customer&gt; c) -&gt; ...);
</code></pre>

<p>Having an <code>FList&lt;Integer&gt;</code> representing customer IDs we <code>map</code> over it (do you see how it helps that <code>FList</code> is a functor?) by calling <code>database.loadCustomer(id)</code> for each ID. This leads to rather inconvenient list of <code>Promise</code>s. <code>sequence()</code> saves the day, but once again this is not just a syntactic sugar. Preceding code is fully non-blocking. For different kinds of monads <code>sequence()</code> still makes sense, but in a different computational context. For example it can change a <code>FList&lt;FOptional&lt;T&gt;&gt;</code> into <code>FOptional&lt;FList&lt;T&gt;&gt;</code>. And by the way, you can implement <code>sequence()</code> (just like <code>map()</code>) on top of <code>flatMap()</code>.</p>

<p>This is just the tip of the iceberg when it comes to usefulness of <code>flatMap()</code> and monads in general. Despite coming from rather obscure category theory, monads proved to be extremely useful abstraction even in object-oriented programming languages such as Java. Being able to compose functions returning monads is so universally helpful that dozens of unrelated classes follow monadic behavior.</p>

<p>Moreover once you encapsulate data inside monad, it is often hard to get it out explicitly. Such operation is not part of the monad behavior and often leads to non-idiomatic code. For example <code>Promise.get()</code> on <code>Promise&lt;T&gt;</code> can technically return <code>T</code>, but only by blocking, whereas all operators based on <code>flatMap()</code> are non-blocking. Another example is <code>FOptional.get()</code> that can fail because <code>FOptional</code> may be empty. Even <code>FList.get(idx)</code> that peeks particular element from a list sounds awkward because you can replace <code>for</code> loops with <code>map()</code> quite often.</p>

<p>I hope you now understand why monads are so popular these days. Even in object-oriented(-ish) language like Java they are quite useful abstraction.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[我必须得告诉大家的 MySQL 优化原理]]></title>
    <link href="http://panlw.github.io/15277854106004.html"/>
    <updated>2018-06-01T00:50:10+08:00</updated>
    <id>http://panlw.github.io/15277854106004.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://www.jianshu.com/p/d7665192aaaf">原文地址</a></p>
</blockquote>

<p>说起 MySQL 的查询优化，相信大家收藏了一堆奇技淫巧：不能使用<code>SELECT *</code>、不使用 NULL 字段、合理创建索引、为字段选择合适的数据类型..... 你是否真的理解这些优化技巧？是否理解其背后的工作原理？在实际场景下性能真有提升吗？我想未必。因而理解这些优化建议背后的原理就尤为重要，希望本文能让你重新审视这些优化建议，并在实际业务场景下合理的运用。</p>

<h3 id="toc_0">MySQL 逻辑架构</h3>

<p>如果能在头脑中构建一幅 MySQL 各组件之间如何协同工作的架构图，有助于深入理解 MySQL 服务器。下图展示了 MySQL 的逻辑架构图。</p>

<p><img src="https://upload-images.jianshu.io/upload_images/175724-2abdb6fbad8affa0.png" alt=""/></p>

<p>MySQL 逻辑架构整体分为三层，最上层为客户端层，并非 MySQL 所独有，诸如：连接处理、授权认证、安全等功能均在这一层处理。</p>

<p>MySQL 大多数核心服务均在中间这一层，包括查询解析、分析、优化、缓存、内置函数 (比如：时间、数学、加密等函数)。所有的跨存储引擎的功能也在这一层实现：存储过程、触发器、视图等。</p>

<p>最下层为存储引擎，其负责 MySQL 中的数据存储和提取。和 Linux 下的文件系统类似，每种存储引擎都有其优势和劣势。中间的服务层通过 API 与存储引擎通信，这些 API 接口屏蔽了不同存储引擎间的差异。</p>

<h3 id="toc_1">MySQL 查询过程</h3>

<p>我们总是希望 MySQL 能够获得更高的查询性能，最好的办法是弄清楚 MySQL 是如何优化和执行查询的。一旦理解了这一点，就会发现：<strong>很多的查询优化工作实际上就是遵循一些原则让 MySQL 的优化器能够按照预想的合理方式运行而已。</strong></p>

<p>当向 MySQL 发送一个请求的时候，MySQL 到底做了些什么呢？</p>

<p><img src="https://upload-images.jianshu.io/upload_images/175724-cb247a2b90ea9d4d.png" alt=""/></p>

<h4 id="toc_2">客户端 / 服务端通信协议</h4>

<p>MySQL 客户端 / 服务端通信协议是 “半双工” 的：在任一时刻，要么是服务器向客户端发送数据，要么是客户端向服务器发送数据，这两个动作不能同时发生。一旦一端开始发送消息，另一端要接收完整个消息才能响应它，所以我们无法也无须将一个消息切成小块独立发送，也没有办法进行流量控制。</p>

<p>客户端用一个单独的数据包将查询请求发送给服务器，所以当查询语句很长的时候，需要设置<code>max_allowed_packet</code>参数。但是需要注意的是，如果查询实在是太大，服务端会拒绝接收更多数据并抛出异常。</p>

<p>与之相反的是，服务器响应给用户的数据通常会很多，由多个数据包组成。但是当服务器响应客户端请求时，客户端必须完整的接收整个返回结果，而不能简单的只取前面几条结果，然后让服务器停止发送。因而在实际开发中，尽量保持查询简单且只返回必需的数据，减小通信间数据包的大小和数量是一个非常好的习惯，这也是查询中尽量避免使用<code>SELECT *</code>以及加上<code>LIMIT</code>限制的原因之一。</p>

<h4 id="toc_3">查询缓存</h4>

<p>在解析一个查询语句前，如果查询缓存是打开的，那么 MySQL 会检查这个查询语句是否命中查询缓存中的数据。如果当前查询恰好命中查询缓存，在检查一次用户权限后直接返回缓存中的结果。这种情况下，查询不会被解析，也不会生成执行计划，更不会执行。</p>

<p>MySQL 将缓存存放在一个引用表（不要理解成<code>table</code>，可以认为是类似于<code>HashMap</code>的数据结构），通过一个哈希值索引，这个哈希值通过查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息计算得来。所以两个查询在任何字符上的不同（例如：空格、注释），都会导致缓存不会命中。</p>

<p>如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、mysql 库中的系统表，其查询结果<br/>
都不会被缓存。比如函数<code>NOW()</code>或者<code>CURRENT_DATE()</code>会因为不同的查询时间，返回不同的查询结果，再比如包含<code>CURRENT_USER</code>或者<code>CONNECION_ID()</code>的查询语句会因为不同的用户而返回不同的结果，将这样的查询结果缓存起来没有任何的意义。</p>

<p>既然是缓存，就会失效，那查询缓存何时失效呢？MySQL 的查询缓存系统会跟踪查询中涉及的每个表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。正因为如此，在任何的写操作时，MySQL 必须将对应表的所有缓存都设置为失效。如果查询缓存非常大或者碎片很多，这个操作就可能带来很大的系统消耗，甚至导致系统僵死一会儿。而且查询缓存对系统的额外消耗也不仅仅在写操作，读操作也不例外：</p>

<ol>
<li> 任何的查询语句在开始之前都必须经过检查，即使这条 SQL 语句永远不会命中缓存</li>
<li> 如果查询结果可以被缓存，那么执行完成后，会将结果存入缓存，也会带来额外的系统消耗</li>
</ol>

<p>基于此，我们要知道并不是什么情况下查询缓存都会提高系统性能，缓存和失效都会带来额外消耗，只有当缓存带来的资源节约大于其本身消耗的资源时，才会给系统带来性能提升。但要如何评估打开缓存是否能够带来性能提升是一件非常困难的事情，也不在本文讨论的范畴内。如果系统确实存在一些性能问题，可以尝试打开查询缓存，并在数据库设计上做一些优化，比如：</p>

<ol>
<li> 用多个小表代替一个大表，注意不要过度设计</li>
<li> 批量插入代替循环单条插入</li>
<li> 合理控制缓存空间大小，一般来说其大小设置为几十兆比较合适</li>
<li> 可以通过<code>SQL_CACHE</code>和<code>SQL_NO_CACHE</code>来控制某个查询语句是否需要进行缓存</li>
</ol>

<p>最后的忠告是不要轻易打开查询缓存，特别是写密集型应用。如果你实在是忍不住，可以将<code>query_cache_type</code>设置为<code>DEMAND</code>，这时只有加入<code>SQL_CACHE</code>的查询才会走缓存，其他查询则不会，这样可以非常自由地控制哪些查询需要被缓存。</p>

<p>当然查询缓存系统本身是非常复杂的，这里讨论的也只是很小的一部分，其他更深入的话题，比如：缓存是如何使用内存的？如何控制内存的碎片化？事务对查询缓存有何影响等等，读者可以自行阅读相关资料，这里权当抛砖引玉吧。</p>

<h4 id="toc_4">语法解析和预处理</h4>

<p>MySQL 通过关键字将 SQL 语句进行解析，并生成一颗对应的解析树。这个过程解析器主要通过语法规则来验证和解析。比如 SQL 中是否使用了错误的关键字或者关键字的顺序是否正确等等。预处理则会根据 MySQL 规则进一步检查解析树是否合法。比如检查要查询的数据表和数据列是否存在等等。</p>

<h4 id="toc_5">查询优化</h4>

<p>经过前面的步骤生成的语法树被认为是合法的了，并且由优化器将其转化成查询计划。多数情况下，一条查询可以有很多种执行方式，最后都返回相应的结果。优化器的作用就是找到这其中最好的执行计划。</p>

<p>MySQL 使用基于成本的优化器，它尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。在 MySQL 可以通过查询当前会话的<code>last_query_cost</code>的值来得到其计算当前查询的成本。</p>

<pre><code>mysql&gt; select * from t_message limit 10;
...省略结果集

mysql&gt; show status like &#39;last_query_cost&#39;;
+-----------------+-------------+
| Variable_name   | Value       |
+-----------------+-------------+
| Last_query_cost | 6391.799000 |
+-----------------+-------------+

</code></pre>

<p>示例中的结果表示优化器认为大概需要做 6391 个数据页的随机查找才能完成上面的查询。这个结果是根据一些列的统计信息计算得来的，这些统计信息包括：每张表或者索引的页面个数、索引的基数、索引和数据行的长度、索引的分布情况等等。</p>

<p>有非常多的原因会导致 MySQL 选择错误的执行计划，比如统计信息不准确、不会考虑不受其控制的操作成本（用户自定义函数、存储过程）、MySQL 认为的最优跟我们想的不一样（我们希望执行时间尽可能短，但 MySQL 值选择它认为成本小的，但成本小并不意味着执行时间短）等等。</p>

<p>MySQL 的查询优化器是一个非常复杂的部件，它使用了非常多的优化策略来生成一个最优的执行计划：</p>

<ul>
<li>  重新定义表的关联顺序（多张表关联查询时，并不一定按照 SQL 中指定的顺序进行，但有一些技巧可以指定关联顺序）</li>
<li>  优化<code>MIN()</code>和<code>MAX()</code>函数（找某列的最小值，如果该列有索引，只需要查找 B+Tree 索引最左端，反之则可以找到最大值，具体原理见下文）</li>
<li>  提前终止查询（比如：使用 Limit 时，查找到满足数量的结果集后会立即终止查询）</li>
<li>  优化排序（在老版本 MySQL 会使用两次传输排序，即先读取行指针和需要排序的字段在内存中对其排序，然后再根据排序结果去读取数据行，而新版本采用的是单次传输排序，也就是一次读取所有的数据行，然后根据给定的列排序。对于 I/O 密集型应用，效率会高很多）</li>
</ul>

<p>随着 MySQL 的不断发展，优化器使用的优化策略也在不断的进化，这里仅仅介绍几个非常常用且容易理解的优化策略，其他的优化策略，大家自行查阅吧。</p>

<h4 id="toc_6">查询执行引擎</h4>

<p>在完成解析和优化阶段以后，MySQL 会生成对应的执行计划，查询执行引擎根据执行计划给出的指令逐步执行得出结果。整个执行过程的大部分操作均是通过调用存储引擎实现的接口来完成，这些接口被称为<code>handler API</code>。查询过程中的每一张表由一个<code>handler</code>实例表示。实际上，MySQL 在查询优化阶段就为每一张表创建了一个<code>handler</code>实例，优化器可以根据这些实例的接口来获取表的相关信息，包括表的所有列名、索引统计信息等。存储引擎接口提供了非常丰富的功能，但其底层仅有几十个接口，这些接口像搭积木一样完成了一次查询的大部分操作。</p>

<h4 id="toc_7">返回结果给客户端</h4>

<p>查询执行的最后一个阶段就是将结果返回给客户端。即使查询不到数据，MySQL 仍然会返回这个查询的相关信息，比如该查询影响到的行数以及执行时间等等。</p>

<p>如果查询缓存被打开且这个查询可以被缓存，MySQL 也会将结果存放到缓存中。</p>

<p>结果集返回客户端是一个增量且逐步返回的过程。有可能 MySQL 在生成第一条结果时，就开始向客户端逐步返回结果集了。这样服务端就无须存储太多结果而消耗过多内存，也可以让客户端第一时间获得返回结果。需要注意的是，结果集中的每一行都会以一个满足①中所描述的通信协议的数据包发送，再通过 TCP 协议进行传输，在传输过程中，可能对 MySQL 的数据包进行缓存然后批量发送。</p>

<p>回头总结一下 MySQL 整个查询执行过程，总的来说分为 6 个步骤：</p>

<ol>
<li> 客户端向 MySQL 服务器发送一条查询请求</li>
<li> 服务器首先检查查询缓存，如果命中缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段</li>
<li> 服务器进行 SQL 解析、预处理、再由优化器生成对应的执行计划</li>
<li> MySQL 根据执行计划，调用存储引擎的 API 来执行查询</li>
<li> 将结果返回给客户端，同时缓存查询结果</li>
</ol>

<h3 id="toc_8">性能优化建议</h3>

<p>看了这么多，你可能会期待给出一些优化手段，是的，下面会从 3 个不同方面给出一些优化建议。但请等等，还有一句忠告要先送给你：<strong>不要听信你看到的关于优化的 “绝对真理”，包括本文所讨论的内容，而应该是在实际的业务场景下通过测试来验证你关于执行计划以及响应时间的假设</strong>。</p>

<h4 id="toc_9">Scheme 设计与数据类型优化</h4>

<p>选择数据类型只要遵循<strong>小而简单</strong>的原则就好，越小的数据类型通常会更快，占用更少的磁盘、内存，处理时需要的 CPU 周期也更少。越简单的数据类型在计算时需要更少的 CPU 周期，比如，整型就比字符操作代价低，因而会使用整型来存储 ip 地址，使用<code>DATETIME</code>来存储时间，而不是使用字符串。</p>

<p>这里总结几个可能容易理解错误的技巧：</p>

<ol>
<li> 通常来说把可为<code>NULL</code>的列改为<code>NOT NULL</code>不会对性能提升有多少帮助，只是如果计划在列上创建索引，就应该将该列设置为<code>NOT NULL</code>。</li>
<li> 对整数类型指定宽度，比如<code>INT(11)</code>，没有任何卵用。<code>INT</code>使用 32 位（4 个字节）存储空间，那么它的表示范围已经确定，所以<code>INT(1)</code>和<code>INT(20)</code>对于存储和计算是相同的。</li>
<li> <code>UNSIGNED</code>表示不允许负值，大致可以使正数的上限提高一倍。比如<code>TINYINT</code>存储范围是 - 128 ~ 127，而<code>UNSIGNED TINYINT</code>存储的范围却是 0 - 255。</li>
<li> 通常来讲，没有太大的必要使用<code>DECIMAL</code>数据类型。即使是在需要存储财务数据时，仍然可以使用<code>BIGINT</code>。比如需要精确到万分之一，那么可以将数据乘以一百万然后使用<code>BIGINT</code>存储。这样可以避免浮点数计算不准确和<code>DECIMAL</code>精确计算代价高的问题。</li>
<li> <code>TIMESTAMP</code>使用 4 个字节存储空间，<code>DATETIME</code>使用 8 个字节存储空间。因而，<code>TIMESTAMP</code>只能表示 1970 - 2038 年，比<code>DATETIME</code>表示的范围小得多，而且<code>TIMESTAMP</code>的值因时区不同而不同。</li>
<li> 大多数情况下没有使用枚举类型的必要，其中一个缺点是枚举的字符串列表是固定的，添加和删除字符串（枚举选项）必须使用<code>ALTER TABLE</code>（如果只只是在列表末尾追加元素，不需要重建表）。</li>
<li> schema 的列不要太多。原因是存储引擎的 API 工作时需要在服务器层和存储引擎层之间通过行缓冲格式拷贝数据，然后在服务器层将缓冲内容解码成各个列，这个转换过程的代价是非常高的。如果列太多而实际使用的列又很少的话，有可能会导致 CPU 占用过高。</li>
<li> 大表<code>ALTER TABLE</code>非常耗时，MySQL 执行大部分修改表结果操作的方法是用新的结构创建一个张空表，从旧表中查出所有的数据插入新表，然后再删除旧表。尤其当内存不足而表又很大，而且还有很大索引的情况下，耗时更久。当然有一些奇技淫巧可以解决这个问题，有兴趣可自行查阅。</li>
</ol>

<h4 id="toc_10">创建高性能索引</h4>

<p>索引是提高 MySQL 查询性能的一个重要途径，但过多的索引可能会导致过高的磁盘使用率以及过高的内存占用，从而影响应用程序的整体性能。应当尽量避免事后才想起添加索引，因为事后可能需要监控大量的 SQL 才能定位到问题所在，而且添加索引的时间肯定是远大于初始添加索引所需要的时间，可见索引的添加也是非常有技术含量的。</p>

<p>接下来将向你展示一系列创建高性能索引的策略，以及每条策略其背后的工作原理。但在此之前，先了解与索引相关的一些算法和数据结构，将有助于更好的理解后文的内容。</p>

<h5 id="toc_11">索引相关的数据结构和算法</h5>

<p>通常我们所说的索引是指<code>B-Tree</code>索引，它是目前关系型数据库中查找数据最为常用和有效的索引，大多数存储引擎都支持这种索引。使用<code>B-Tree</code>这个术语，是因为 MySQL 在<code>CREATE TABLE</code>或其它语句中使用了这个关键字，但实际上不同的存储引擎可能使用不同的数据结构，比如 InnoDB 就是使用的<code>B+Tree</code>。</p>

<p><code>B+Tree</code>中的 B 是指<code>balance</code>，意为平衡。需要注意的是，B + 树索引并不能找到一个给定键值的具体行，它找到的只是被查找数据行所在的页，接着数据库会把页读入到内存，再在内存中进行查找，最后得到要查找的数据。</p>

<p>在介绍<code>B+Tree</code>前，先了解一下二叉查找树，它是一种经典的数据结构，其左子树的值总是小于根的值，右子树的值总是大于根的值，如下图①。如果要在这课树中查找值为 5 的记录，其大致流程：先找到根，其值为 6，大于 5，所以查找左子树，找到 3，而 5 大于 3，接着找 3 的右子树，总共找了 3 次。同样的方法，如果查找值为 8 的记录，也需要查找 3 次。所以二叉查找树的平均查找次数为 (3 + 3 + 3 + 2 + 2 + 1) / 6 = 2.3 次，而顺序查找的话，查找值为 2 的记录，仅需要 1 次，但查找值为 8 的记录则需要 6 次，所以顺序查找的平均查找次数为：(1 + 2 + 3 + 4 + 5 + 6) / 6 = 3.3 次，因此大多数情况下二叉查找树的平均查找速度比顺序查找要快。</p>

<p><img src="https://upload-images.jianshu.io/upload_images/175724-272c1245eba594f5.png" alt=""/></p>

<p>由于二叉查找树可以任意构造，同样的值，可以构造出如图②的二叉查找树，显然这棵二叉树的查询效率和顺序查找差不多。若想二叉查找数的查询性能最高，需要这棵二叉查找树是平衡的，也即平衡二叉树（AVL 树）。</p>

<p>平衡二叉树首先需要符合二叉查找树的定义，其次必须满足任何节点的两个子树的高度差不能大于 1。显然图②不满足平衡二叉树的定义，而图①是一课平衡二叉树。平衡二叉树的查找性能是比较高的（性能最好的是最优二叉树），查询性能越好，维护的成本就越大。比如图①的平衡二叉树，当用户需要插入一个新的值 9 的节点时，就需要做出如下变动。</p>

<p><img src="https://upload-images.jianshu.io/upload_images/175724-c806af2d9defcbab.png" alt=""/></p>

<p>通过一次左旋操作就将插入后的树重新变为平衡二叉树是最简单的情况了，实际应用场景中可能需要旋转多次。至此我们可以考虑一个问题，平衡二叉树的查找效率还不错，实现也非常简单，相应的维护成本还能接受，为什么 MySQL 索引不直接使用平衡二叉树？</p>

<p>随着数据库中数据的增加，索引本身大小随之增加，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘 I/O 消耗，相对于内存存取，I/O 存取的消耗要高几个数量级。可以想象一下一棵几百万节点的二叉树的深度是多少？如果将这么大深度的一颗二叉树放磁盘上，每读取一个节点，需要一次磁盘的 I/O 读取，整个查找的耗时显然是不能够接受的。那么如何减少查找过程中的 I/O 存取次数？</p>

<p>一种行之有效的解决方法是减少树的深度，将二叉树变为 m 叉树（多路搜索树），而<code>B+Tree</code>就是一种多路搜索树。理解<code>B+Tree</code>时，只需要理解其最重要的两个特征即可：第一，所有的关键字（可以理解为数据）都存储在叶子节点（<code>Leaf Page</code>），非叶子节点（<code>Index Page</code>）并不存储真正的数据，所有记录节点都是按键值大小顺序存放在同一层叶子节点上。其次，所有的叶子节点由指针连接。如下图为高度为 2 的简化了的<code>B+Tree</code>。</p>

<p><img src="https://upload-images.jianshu.io/upload_images/175724-52306456815a0919.png" alt=""/></p>

<p>怎么理解这两个特征？MySQL 将每个节点的大小设置为一个页的整数倍（原因下文会介绍），也就是在节点空间大小一定的情况下，每个节点可以存储更多的内结点，这样每个结点能索引的范围更大更精确。所有的叶子节点使用指针链接的好处是可以进行区间访问，比如上图中，如果查找大于 20 而小于 30 的记录，只需要找到节点 20，就可以遍历指针依次找到 25、30。如果没有链接指针的话，就无法进行区间查找。这也是 MySQL 使用<code>B+Tree</code>作为索引存储结构的重要原因。</p>

<p>MySQL 为何将节点大小设置为页的整数倍，这就需要理解磁盘的存储原理。磁盘本身存取就比主存慢很多，在加上机械运动损耗（特别是普通的机械硬盘），磁盘的存取速度往往是主存的几百万分之一，为了尽量减少磁盘 I/O，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存，预读的长度一般为页的整数倍。</p>

<blockquote>
<p>页是计算机管理存储器的逻辑块，硬件及 OS 往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（许多 OS 中，页的大小通常为 4K）。主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后一起返回，程序继续运行。</p>
</blockquote>

<p>MySQL 巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次 I/O 就可以完全载入。为了达到这个目的，每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了读取一个节点只需一次 I/O。假设<code>B+Tree</code>的高度为 h，一次检索最多需要<code>h-1</code>次 I/O（根节点常驻内存），复杂度 O(h) = O(logmN)。实际应用场景中，M 通常较大，常常超过 100，因此树的高度一般都比较小，通常不超过 3。</p>

<p>最后简单了解下<code>B+Tree</code>节点的操作，在整体上对索引的维护有一个大概的了解，虽然索引可以大大提高查询效率，但维护索引仍要花费很大的代价，因此合理的创建索引也就尤为重要。</p>

<p>仍以上面的树为例，我们假设每个节点只能存储 4 个内节点。首先要插入第一个节点 28，如下图所示。</p>

<p><img src="https://upload-images.jianshu.io/upload_images/175724-a862bb909a8ed6a0.png" alt=""/></p>

<p>接着插入下一个节点 70，在 Index Page 中查询后得知应该插入到 50 - 70 之间的叶子节点，但叶子节点已满，这时候就需要进行也分裂的操作，当前的叶子节点起点为 50，所以根据中间值来拆分叶子节点，如下图所示。</p>

<p><img src="https://upload-images.jianshu.io/upload_images/175724-ce47c776928bc6b8.png" alt=""/></p>

<p>最后插入一个节点 95，这时候 Index Page 和 Leaf Page 都满了，就需要做两次拆分，如下图所示。</p>

<p><img src="https://upload-images.jianshu.io/upload_images/175724-33cee181795b3dcc.png" alt=""/></p>

<p>拆分后最终形成了这样一颗树。</p>

<p><img src="https://upload-images.jianshu.io/upload_images/175724-5289c6ec5d11216e.png" alt=""/></p>

<p><code>B+Tree</code>为了保持平衡，对于新插入的值需要做大量的拆分页操作，而页的拆分需要 I/O 操作，为了尽可能的减少页的拆分操作，<code>B+Tree</code>也提供了类似于平衡二叉树的旋转功能。当 Leaf Page 已满但其左右兄弟节点没有满的情况下，<code>B+Tree</code>并不急于去做拆分操作，而是将记录移到当前所在页的兄弟节点上。通常情况下，左兄弟会被先检查用来做旋转操作。就比如上面第二个示例，当插入 70 的时候，并不会去做页拆分，而是左旋操作。</p>

<p><img src="https://upload-images.jianshu.io/upload_images/175724-bafd2fbc93cf45ae.png" alt=""/></p>

<p>通过旋转操作可以最大限度的减少页分裂，从而减少索引维护过程中的磁盘的 I/O 操作，也提高索引维护效率。需要注意的是，删除节点跟插入节点类似，仍然需要旋转和拆分操作，这里就不再说明。</p>

<h5 id="toc_12">高性能策略</h5>

<p>通过上文，相信你对<code>B+Tree</code>的数据结构已经有了大致的了解，但 MySQL 中索引是如何组织数据的存储呢？以一个简单的示例来说明，假如有如下数据表：</p>

<pre><code>CREATE TABLE People(
    last_name varchar(50) not null,
    first_name varchar(50) not null,
    dob date not null,
    gender enum(`m`,`f`) not null,
    key(last_name,first_name,dob)
);

</code></pre>

<p>对于表中每一行数据，索引中包含了 last_name、first_name、dob 列的值，下图展示了索引是如何组织数据存储的。</p>

<p><img src="https://upload-images.jianshu.io/upload_images/175724-3ba760afbae4a52d.png" alt=""/></p>

<p>可以看到，索引首先根据第一个字段来排列顺序，当名字相同时，则根据第三个字段，即出生日期来排序，正是因为这个原因，才有了索引的 “最左原则”。</p>

<h6 id="toc_13">1、MySQL 不会使用索引的情况：非独立的列</h6>

<p>“独立的列” 是指索引列不能是表达式的一部分，也不能是函数的参数。比如：</p>

<pre><code>select * from where id + 1 = 5

</code></pre>

<p>我们很容易看出其等价于 id = 4，但是 MySQL 无法自动解析这个表达式，使用函数是同样的道理。</p>

<h6 id="toc_14">2、前缀索引</h6>

<p>如果列很长，通常可以索引开始的部分字符，这样可以有效节约索引空间，从而提高索引效率。</p>

<h6 id="toc_15">3、多列索引和索引顺序</h6>

<p>在多数情况下，在多个列上建立独立的索引并不能提高查询性能。理由非常简单，MySQL 不知道选择哪个索引的查询效率更好，所以在老版本，比如 MySQL5.0 之前就会随便选择一个列的索引，而新的版本会采用合并索引的策略。举个简单的例子，在一张电影演员表中，在 actor_id 和 film_id 两个列上都建立了独立的索引，然后有如下查询：</p>

<pre><code>select film_id,actor_id from film_actor where actor_id = 1 or film_id = 1

</code></pre>

<p>老版本的 MySQL 会随机选择一个索引，但新版本做如下的优化：</p>

<pre><code>select film_id,actor_id from film_actor where actor_id = 1  
union all 
select film_id,actor_id from film_actor where film_id = 1 and actor_id &lt;&gt; 1

</code></pre>

<ul>
<li>  当出现多个索引做相交操作时（多个 AND 条件），通常来说一个包含所有相关列的索引要优于多个独立索引。</li>
<li>  当出现多个索引做联合操作时（多个 OR 条件），对结果集的合并、排序等操作需要耗费大量的 CPU 和内存资源，特别是当其中的某些索引的选择性不高，需要返回合并大量数据时，查询成本更高。所以这种情况下还不如走全表扫描。</li>
</ul>

<p>因此<code>explain</code>时如果发现有索引合并（Extra 字段出现<code>Using union</code>），应该好好检查一下查询和表结构是不是已经是最优的，如果查询和表都没有问题，那只能说明索引建的非常糟糕，应当慎重考虑索引是否合适，有可能一个包含所有相关列的多列索引更适合。</p>

<p>前面我们提到过索引如何组织数据存储的，从图中可以看到多列索引时，索引的顺序对于查询是至关重要的，很明显应该把选择性更高的字段放到索引的前面，这样通过第一个字段就可以过滤掉大多数不符合条件的数据。</p>

<blockquote>
<p><strong>索引选择性</strong>是指不重复的索引值和数据表的总记录数的比值，选择性越高查询效率越高，因为选择性越高的索引可以让 MySQL 在查询时过滤掉更多的行。唯一索引的选择性是 1，这是最好的索引选择性，性能也是最好的。</p>
</blockquote>

<p>理解索引选择性的概念后，就不难确定哪个字段的选择性较高了，查一下就知道了，比如：</p>

<pre><code>SELECT * FROM payment where staff_id = 2 and customer_id = 584

</code></pre>

<p>是应该创建<code>(staff_id,customer_id)</code>的索引还是应该颠倒一下顺序？执行下面的查询，哪个字段的选择性更接近 1 就把哪个字段索引前面就好。</p>

<pre><code>select count(distinct staff_id)/count(*) as staff_id_selectivity,
       count(distinct customer_id)/count(*) as customer_id_selectivity,
       count(*) from payment

</code></pre>

<p>多数情况下使用这个原则没有任何问题，但仍然注意你的数据中是否存在一些特殊情况。举个简单的例子，比如要查询某个用户组下有过交易的用户信息：</p>

<pre><code>select user_id from trade where user_group_id = 1 and trade_amount &gt; 0

</code></pre>

<p>MySQL 为这个查询选择了索引<code>(user_group_id,trade_amount)</code>，如果不考虑特殊情况，这看起来没有任何问题，但实际情况是这张表的大多数数据都是从老系统中迁移过来的，由于新老系统的数据不兼容，所以就给老系统迁移过来的数据赋予了一个默认的用户组。这种情况下，通过索引扫描的行数跟全表扫描基本没什么区别，索引也就起不到任何作用。</p>

<p>推广开来说，经验法则和推论在多数情况下是有用的，可以指导我们开发和设计，但实际情况往往会更复杂，实际业务场景下的某些特殊情况可能会摧毁你的整个设计。</p>

<h6 id="toc_16">4、避免多个范围条件</h6>

<p>实际开发中，我们会经常使用多个范围条件，比如想查询某个时间段内登录过的用户：</p>

<pre><code>select user.* from user where login_time &gt; &#39;2017-04-01&#39; and age between 18 and 30;

</code></pre>

<p>这个查询有一个问题：它有两个范围条件，login_time 列和 age 列，MySQL 可以使用 login_time 列的索引或者 age 列的索引，但无法同时使用它们。</p>

<h6 id="toc_17">5、覆盖索引</h6>

<p>如果一个索引包含或者说覆盖所有需要查询的字段的值，那么就没有必要再回表查询，这就称为覆盖索引。覆盖索引是非常有用的工具，可以极大的提高性能，因为查询只需要扫描索引会带来许多好处：</p>

<ul>
<li>  索引条目远小于数据行大小，如果只读取索引，极大减少数据访问量</li>
<li>  索引是有按照列值顺序存储的，对于 I/O 密集型的范围查询要比随机从磁盘读取每一行数据的 IO 要少的多</li>
</ul>

<h6 id="toc_18">6、使用索引扫描来排序</h6>

<p>MySQL 有两种方式可以生产有序的结果集，其一是对结果集进行排序的操作，其二是按照索引顺序扫描得出的结果自然是有序的。如果 explain 的结果中<code>type</code>列的值为<code>index</code>表示使用了索引扫描来做排序。</p>

<p>扫描索引本身很快，因为只需要从一条索引记录移动到相邻的下一条记录。但如果索引本身不能覆盖所有需要查询的列，那么就不得不每扫描一条索引记录就回表查询一次对应的行。这个读取操作基本上是随机 I/O，因此按照索引顺序读取数据的速度通常要比顺序地全表扫描要慢。</p>

<p>在设计索引时，如果一个索引既能够满足排序，又满足查询，是最好的。</p>

<p>只有当索引的列顺序和<code>ORDER BY</code>子句的顺序完全一致，并且所有列的排序方向也一样时，才能够使用索引来对结果做排序。如果查询需要关联多张表，则只有<code>ORDER BY</code>子句引用的字段全部为第一张表时，才能使用索引做排序。<code>ORDER BY</code>子句和查询的限制是一样的，都要满足最左前缀的要求（有一种情况例外，就是最左的列被指定为常数，下面是一个简单的示例），其他情况下都需要执行排序操作，而无法利用索引排序。</p>

<blockquote>
<pre><code>// 最左列为常数，索引：(date,staff_id,customer_id)
select  staff_id,customer_id from demo where date = &#39;2015-06-01&#39; order by staff_id,customer_id

</code></pre>
</blockquote>

<h6 id="toc_19">7、冗余和重复索引</h6>

<p>冗余索引是指在相同的列上按照相同的顺序创建的相同类型的索引，应当尽量避免这种索引，发现后立即删除。比如有一个索引<code>(A,B)</code>，再创建索引<code>(A)</code>就是冗余索引。冗余索引经常发生在为表添加新索引时，比如有人新建了索引<code>(A,B)</code>，但这个索引不是扩展已有的索引<code>(A)</code>。</p>

<p>大多数情况下都应该尽量扩展已有的索引而不是创建新索引。但有极少情况下出现性能方面的考虑需要冗余索引，比如扩展已有索引而导致其变得过大，从而影响到其他使用该索引的查询。</p>

<h6 id="toc_20">8、删除长期未使用的索引</h6>

<p>定期删除一些长时间未使用过的索引是一个非常好的习惯。</p>

<p>关于索引这个话题打算就此打住，最后要说一句，索引并不总是最好的工具，只有当索引帮助提高查询速度带来的好处大于其带来的额外工作时，索引才是有效的。对于非常小的表，简单的全表扫描更高效。对于中到大型的表，索引就非常有效。对于超大型的表，建立和维护索引的代价随之增长，这时候其他技术也许更有效，比如分区表。最后的最后，<strong><code>explain</code>后再提测是一种美德</strong>。</p>

<h4 id="toc_21">特定类型查询优化</h4>

<h5 id="toc_22">优化 COUNT() 查询</h5>

<p><code>COUNT()</code>可能是被大家误解最多的函数了，它有两种不同的作用，其一是统计某个列值的数量，其二是统计行数。统计列值时，要求列值是非空的，它不会统计 NULL。如果确认括号中的表达式不可能为空时，实际上就是在统计行数。最简单的就是当使用<code>COUNT(*)</code>时，并不是我们所想象的那样扩展成所有的列，实际上，它会忽略所有的列而直接统计行数。</p>

<p>我们最常见的误解也就在这儿，在括号内指定了一列却希望统计结果是行数，而且还常常误以为前者的性能会更好。但实际并非这样，如果要统计行数，直接使用<code>COUNT(*)</code>，意义清晰，且性能更好。</p>

<p>有时候某些业务场景并不需要完全精确的<code>COUNT</code>值，可以用近似值来代替，EXPLAIN 出来的行数就是一个不错的近似值，而且执行 EXPLAIN 并不需要真正地去执行查询，所以成本非常低。通常来说，执行<code>COUNT()</code>都需要扫描大量的行才能获取到精确的数据，因此很难优化，MySQL 层面还能做得也就只有覆盖索引了。如果不还能解决问题，只有从架构层面解决了，比如添加汇总表，或者使用 redis 这样的外部缓存系统。</p>

<h5 id="toc_23">优化关联查询</h5>

<p>在大数据场景下，表与表之间通过一个冗余字段来关联，要比直接使用<code>JOIN</code>有更好的性能。如果确实需要使用关联查询的情况下，需要特别注意的是：</p>

<ul>
<li>  确保<code>ON</code>和<code>USING</code>字句中的列上有索引。在创建索引的时候就要考虑到关联的顺序。当表 A 和表 B 用列 c 关联的时候，如果优化器关联的顺序是 A、B，那么就不需要在 A 表的对应列上创建索引。没有用到的索引会带来额外的负担，一般来说，除非有其他理由，只需要在关联顺序中的第二张表的相应列上创建索引（具体原因下文分析）。</li>
<li>  确保任何的<code>GROUP BY</code>和<code>ORDER BY</code>中的表达式只涉及到一个表中的列，这样 MySQL 才有可能使用索引来优化。</li>
</ul>

<p>要理解优化关联查询的第一个技巧，就需要理解 MySQL 是如何执行关联查询的。当前 MySQL 关联执行的策略非常简单，它对任何的关联都执行<strong>嵌套循环关联</strong>操作，即先在一个表中循环取出单条数据，然后在嵌套循环到下一个表中寻找匹配的行，依次下去，直到找到所有表中匹配的行为为止。然后根据各个表匹配的行，返回查询中需要的各个列。</p>

<p>太抽象了？以上面的示例来说明，比如有这样的一个查询：</p>

<pre><code>SELECT A.xx,B.yy 
FROM A INNER JOIN B USING(c)
WHERE A.xx IN (5,6)

</code></pre>

<p>假设 MySQL 按照查询中的关联顺序 A、B 来进行关联操作，那么可以用下面的伪代码表示 MySQL 如何完成这个查询：</p>

<pre><code>outer_iterator = SELECT A.xx,A.c FROM A WHERE A.xx IN (5,6);
outer_row = outer_iterator.next;
while(outer_row) {
    inner_iterator = SELECT B.yy FROM B WHERE B.c = outer_row.c;
    inner_row = inner_iterator.next;
    while(inner_row) {
        output[inner_row.yy,outer_row.xx];
        inner_row = inner_iterator.next;
    }
    outer_row = outer_iterator.next;
}

</code></pre>

<p>可以看到，最外层的查询是根据<code>A.xx</code>列来查询的，<code>A.c</code>上如果有索引的话，整个关联查询也不会使用。再看内层的查询，很明显<code>B.c</code>上如果有索引的话，能够加速查询，因此只需要在关联顺序中的第二张表的相应列上创建索引即可。</p>

<h5 id="toc_24">优化 LIMIT 分页</h5>

<p>当需要分页操作时，通常会使用<code>LIMIT</code>加上偏移量的办法实现，同时加上合适的<code>ORDER BY</code>字句。如果有对应的索引，通常效率会不错，否则，MySQL 需要做大量的文件排序操作。</p>

<p>一个常见的问题是当偏移量非常大的时候，比如：<code>LIMIT 10000 20</code>这样的查询，MySQL 需要查询 10020 条记录然后只返回 20 条记录，前面的 10000 条都将被抛弃，这样的代价非常高。</p>

<p>优化这种查询一个最简单的办法就是尽可能的使用覆盖索引扫描，而不是查询所有的列。然后根据需要做一次关联查询再返回所有的列。对于偏移量很大时，这样做的效率会提升非常大。考虑下面的查询：</p>

<pre><code>SELECT film_id,description FROM film ORDER BY title LIMIT 50,5;

</code></pre>

<p>如果这张表非常大，那么这个查询最好改成下面的样子：</p>

<pre><code>SELECT film.film_id,film.description
FROM film INNER JOIN (
    SELECT film_id FROM film ORDER BY title LIMIT 50,5
) AS tmp USING(film_id);

</code></pre>

<p>这里的延迟关联将大大提升查询效率，让 MySQL 扫描尽可能少的页面，获取需要访问的记录后在根据关联列回原表查询所需要的列。</p>

<p>有时候如果可以使用书签记录上次取数据的位置，那么下次就可以直接从该书签记录的位置开始扫描，这样就可以避免使用<code>OFFSET</code>，比如下面的查询：</p>

<pre><code>SELECT id FROM t LIMIT 10000, 10;
改为：
SELECT id FROM t WHERE id &gt; 10000 LIMIT 10;

</code></pre>

<p>其他优化的办法还包括使用预先计算的汇总表，或者关联到一个冗余表，冗余表中只包含主键列和需要做排序的列。</p>

<h5 id="toc_25">优化 UNION</h5>

<p>MySQL 处理<code>UNION</code>的策略是先创建临时表，然后再把各个查询结果插入到临时表中，最后再来做查询。因此很多优化策略在<code>UNION</code>查询中都没有办法很好的时候。经常需要手动将<code>WHERE</code>、<code>LIMIT</code>、<code>ORDER BY</code>等字句 “下推” 到各个子查询中，以便优化器可以充分利用这些条件先优化。</p>

<p>除非确实需要服务器去重，否则就一定要使用<code>UNION ALL</code>，如果没有<code>ALL</code>关键字，MySQL 会给临时表加上<code>DISTINCT</code>选项，这会导致整个临时表的数据做唯一性检查，这样做的代价非常高。当然即使使用 ALL 关键字，MySQL 总是将结果放入临时表，然后再读出，再返回给客户端。虽然很多时候没有这个必要，比如有时候可以直接把每个子查询的结果返回给客户端。</p>

<h4 id="toc_26">结语</h4>

<p>理解查询是如何执行以及时间都消耗在哪些地方，再加上一些优化过程的知识，可以帮助大家更好的理解 MySQL，理解常见优化技巧背后的原理。希望本文中的原理、示例能够帮助大家更好的将理论和实践联系起来，更多的将理论知识运用到实践中。</p>

<p>其他也没啥说的了，给大家留两个思考题吧，可以在脑袋里想想答案，这也是大家经常挂在嘴边的，但很少有人会思考为什么？</p>

<ol>
<li><p>有非常多的程序员在分享时都会抛出这样一个观点：尽可能不要使用存储过程，存储过程非常不容易维护，也会增加使用成本，应该把业务逻辑放到客户端。既然客户端都能干这些事，那为什么还要存储过程？</p></li>
<li><p><code>JOIN</code>本身也挺方便的，直接查询就好了，为什么还需要视图呢？</p></li>
</ol>

<h4 id="toc_27">参考资料</h4>

<p>[1] <a href="https://link.jianshu.com?t=http://union-click.jd.com/jdc?d=6vMwSD">姜承尧 著；MySQL 技术内幕 - InnoDB 存储引擎；机械工业出版社，2013</a><br/>
[2] <a href="https://link.jianshu.com?t=http://union-click.jd.com/jdc?d=wMESxy">Baron Scbwartz 等著；宁海元 周振兴等译；高性能 MySQL（第三版）; 电子工业出版社， 2013</a><br/>
[3] <a href="https://link.jianshu.com?t=https://segmentfault.com/a/1190000004690721">由 B-/B + 树看 MySQL 索引结构</a></p>

<blockquote>
<p>备注：水平有限，难免疏漏，如果问题请留言<br/>
本文已经同步更新到微信公众号：<strong><a href="https://link.jianshu.com?t=https://mp.weixin.qq.com/misc/getqrcode?fakeid=3267562543&amp;token=1639688523&amp;style=1">轻描淡写 CODE</a> » <a href="https://link.jianshu.com?t=https://mp.weixin.qq.com/s/pCHxwsdMQvAFDdYwGk7ydQ">我必须得告诉大家的 MySQL 优化原理</a></strong></p>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Leaf——美团点评分布式ID生成系统]]></title>
    <link href="http://panlw.github.io/15277850962764.html"/>
    <updated>2018-06-01T00:44:56+08:00</updated>
    <id>http://panlw.github.io/15277850962764.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://tech.meituan.com/MT_Leaf.html">原文地址</a></p>
</blockquote>

<h1 id="toc_0">背景</h1>

<p>在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识。如在美团点评的金融、支付、餐饮、酒店、猫眼电影等产品的系统中，数据日渐增长，对数据分库分表后需要有一个唯一ID来标识一条数据或消息，数据库的自增ID显然不能满足需求；特别一点的如订单、骑手、优惠券也都需要有唯一ID做标识。此时一个能够生成全局唯一ID的系统是非常必要的。概括下来，那业务系统对ID号的要求有哪些呢？</p>

<ol>
<li> 全局唯一性：不能出现重复的ID号，既然是唯一标识，这是最基本的要求。</li>
<li> 趋势递增：在MySQL InnoDB引擎中使用的是聚集索引，由于多数RDBMS使用B-tree的数据结构来存储索引数据，在主键的选择上面我们应该尽量使用有序的主键保证写入性能。</li>
<li> 单调递增：保证下一个ID一定大于上一个ID，例如事务版本号、IM增量消息、排序等特殊需求。</li>
<li> 信息安全：如果ID是连续的，恶意用户的扒取工作就非常容易做了，直接按照顺序下载指定URL即可；如果是订单号就更危险了，竞对可以直接知道我们一天的单量。所以在一些应用场景下，会需要ID无规则、不规则。</li>
</ol>

<p>上述123对应三类不同的场景，3和4需求还是互斥的，无法使用同一个方案满足。</p>

<p>同时除了对ID号码自身的要求，业务还对ID号生成系统的可用性要求极高，想象一下，如果ID生成系统瘫痪，整个美团点评支付、优惠券发券、骑手派单等关键动作都无法执行，这就会带来一场灾难。</p>

<p>由此总结下一个ID生成系统应该做到如下几点：</p>

<ol>
<li> 平均延迟和TP999延迟都要尽可能低；</li>
<li> 可用性5个9；</li>
<li> 高QPS。</li>
</ol>

<h1 id="toc_1">常见方法介绍</h1>

<h3 id="toc_2">UUID</h3>

<p>UUID(Universally Unique Identifier)的标准型式包含32个16进制数字，以连字号分为五段，形式为8-4-4-4-12的36个字符，示例：<code>550e8400-e29b-41d4-a716-446655440000</code>，到目前为止业界一共有5种方式生成UUID，详情见IETF发布的UUID规范 <a href="http://www.ietf.org/rfc/rfc4122.txt">A Universally Unique IDentifier (UUID) URN Namespace</a>。</p>

<p>优点：</p>

<ul>
<li>  性能非常高：本地生成，没有网络消耗。</li>
</ul>

<p>缺点：</p>

<ul>
<li>  不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用。</li>
<li>  信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露，这个漏洞曾被用于寻找梅丽莎病毒的制作者位置。</li>
<li><p>ID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用：</p>

<p>① MySQL官方有明确的建议主键要尽量越短越好[4]，36个字符长度的UUID不符合要求。</p>

<blockquote>
<p>All indexes other than the clustered index are known as secondary indexes. In InnoDB, each record in a secondary index contains the primary key columns for the row, as well as the columns specified for the secondary index. InnoDB uses this primary key value to search for the row in the clustered index. <strong>_If the primary key is long, the secondary indexes use more space, so it is advantageous to have a short primary key_</strong>.</p>
</blockquote>

<p>② 对MySQL索引不利：如果作为数据库主键，在InnoDB引擎下，UUID的无序性可能会引起数据位置频繁变动，严重影响性能。</p></li>
</ul>

<h3 id="toc_3">类snowflake方案</h3>

<p>这种方案大致来说是一种以划分命名空间（UUID也算，由于比较常见，所以单独分析）来生成ID的一种算法，这种方案把64-bit分别划分成多段，分开来标示机器、时间等，比如在snowflake中的64-bit分别表示如下图（图片来自网络）所示：</p>

<p><img src="media/15277850962764/15277851805055.png" alt=""/></p>

<p>41-bit的时间可以表示（1L&lt;&lt;41）/(1000L<em>3600</em>24*365)=69年的时间，10-bit机器可以分别表示1024台机器。如果我们对IDC划分有需求，还可以将10-bit分5-bit给IDC，分5-bit给工作机器。这样就可以表示32个IDC，每个IDC下可以有32台机器，可以根据自身需求定义。12个自增序列号可以表示2<sup>12个ID，理论上snowflake方案的QPS约为409.6w/s，这种分配方式可以保证在任何一个IDC的任何一台机器在任意毫秒内生成的ID都是不同的。</sup></p>

<p>这种方式的优缺点是：</p>

<p>优点：</p>

<ul>
<li>  毫秒数在高位，自增序列在低位，整个ID都是趋势递增的。</li>
<li>  不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的。</li>
<li>  可以根据自身业务特性分配bit位，非常灵活。</li>
</ul>

<p>缺点：</p>

<ul>
<li>  强依赖机器时钟，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态。</li>
</ul>

<h4 id="toc_4">应用举例Mongdb objectID</h4>

<p><a href="https://docs.mongodb.com/manual/reference/method/ObjectId/#description">MongoDB官方文档 ObjectID</a>可以算作是和snowflake类似方法，通过“时间+机器码+pid+inc”共12个字节，通过4+3+2+3的方式最终标识成一个24长度的十六进制字符。</p>

<h3 id="toc_5">数据库生成</h3>

<p>以MySQL举例，利用给字段设置<code>auto_increment_increment</code>和<code>auto_increment_offset</code>来保证ID自增，每次业务使用下列SQL读写MySQL得到ID号。</p>

<pre><code>begin;
REPLACE INTO Tickets64 (stub) VALUES (&#39;a&#39;);
SELECT LAST_INSERT_ID();
commit;

</code></pre>

<p><img src="media/15277850962764/15277851990427.png" alt=""/></p>

<p>这种方案的优缺点如下：</p>

<p>优点：</p>

<ul>
<li>  非常简单，利用现有数据库系统的功能实现，成本小，有DBA专业维护。</li>
<li>  ID号单调自增，可以实现一些对ID有特殊要求的业务。</li>
</ul>

<p>缺点：</p>

<ul>
<li>  强依赖DB，当DB异常时整个系统不可用，属于致命问题。配置主从复制可以尽可能的增加可用性，但是数据一致性在特殊情况下难以保证。主从切换时的不一致可能会导致重复发号。</li>
<li>  ID发号性能瓶颈限制在单台MySQL的读写性能。</li>
</ul>

<p>对于MySQL性能问题，可用如下方案解决：在分布式系统中我们可以多部署几台机器，每台机器设置不同的初始值，且步长和机器数相等。比如有两台机器。设置步长step为2，TicketServer1的初始值为1（1，3，5，7，9，11...）、TicketServer2的初始值为2（2，4，6，8，10...）。这是Flickr团队在2010年撰文介绍的一种主键生成策略（<a href="http://code.flickr.net/2010/02/08/ticket-servers-distributed-unique-primary-keys-on-the-cheap/">Ticket Servers: Distributed Unique Primary Keys on the Cheap</a> ）。如下所示，为了实现上述方案分别设置两台机器对应的参数，TicketServer1从1开始发号，TicketServer2从2开始发号，两台机器每次发号之后都递增2。</p>

<pre><code>TicketServer1:
auto-increment-increment = 2
auto-increment-offset = 1

TicketServer2:
auto-increment-increment = 2
auto-increment-offset = 2

</code></pre>

<p>假设我们要部署N台机器，步长需设置为N，每台的初始值依次为0,1,2...N-1那么整个架构就变成了如下图所示：</p>

<p><img src="media/15277850962764/15277852102924.png" alt=""/></p>

<p>这种架构貌似能够满足性能的需求，但有以下几个缺点：</p>

<ul>
<li>  系统水平扩展比较困难，比如定义好了步长和机器台数之后，如果要添加机器该怎么做？假设现在只有一台机器发号是1,2,3,4,5（步长是1），这个时候需要扩容机器一台。可以这样做：把第二台机器的初始值设置得比第一台超过很多，比如14（假设在扩容时间之内第一台不可能发到14），同时设置步长为2，那么这台机器下发的号码都是14以后的偶数。然后摘掉第一台，把ID值保留为奇数，比如7，然后修改第一台的步长为2。让它符合我们定义的号段标准，对于这个例子来说就是让第一台以后只能产生奇数。扩容方案看起来复杂吗？貌似还好，现在想象一下如果我们线上有100台机器，这个时候要扩容该怎么做？简直是噩梦。所以系统水平扩展方案复杂难以实现。</li>
<li>  ID没有了单调递增的特性，只能趋势递增，这个缺点对于一般业务需求不是很重要，可以容忍。</li>
<li>  数据库压力还是很大，每次获取ID都得读写一次数据库，只能靠堆机器来提高性能。</li>
</ul>

<h1 id="toc_6">Leaf方案实现</h1>

<p>Leaf这个名字是来自德国哲学家、数学家莱布尼茨的一句话：</p>

<blockquote>
<p>There are no two identical leaves in the world</p>

<p>&quot;世界上没有两片相同的树叶&quot;</p>
</blockquote>

<p>综合对比上述几种方案，每种方案都不完全符合我们的要求。所以Leaf分别在上述第二种和第三种方案上做了相应的优化，实现了Leaf-segment和Leaf-snowflake方案。</p>

<h3 id="toc_7">Leaf-segment数据库方案</h3>

<p>第一种Leaf-segment方案，在使用数据库的方案上，做了如下改变：</p>

<ul>
<li>  原方案每次获取ID都得读写一次数据库，造成数据库压力大。改为利用proxy server批量获取，每次获取一个segment(step决定大小)号段的值。用完之后再去数据库获取新的号段，可以大大的减轻数据库的压力。</li>
<li>  各个业务不同的发号需求用biz_tag字段来区分，每个biz-tag的ID获取相互隔离，互不影响。如果以后有性能需求需要对数据库扩容，不需要上述描述的复杂的扩容操作，只需要对biz_tag分库分表就行。</li>
</ul>

<p>数据库表设计如下：</p>

<pre><code>+-------------+--------------+------+-----+-------------------+-----------------------------+
| Field       | Type         | Null | Key | Default           | Extra                       |
+-------------+--------------+------+-----+-------------------+-----------------------------+
| biz_tag     | varchar(128) | NO   | PRI |                   |                             |
| max_id      | bigint(20)   | NO   |     | 1                 |                             |
| step        | int(11)      | NO   |     | NULL              |                             |
| desc        | varchar(256) | YES  |     | NULL              |                             |
| update_time | timestamp    | NO   |     | CURRENT_TIMESTAMP | on update CURRENT_TIMESTAMP |
+-------------+--------------+------+-----+-------------------+-----------------------------+

</code></pre>

<p>重要字段说明：biz_tag用来区分业务，max_id表示该biz_tag目前所被分配的ID号段的最大值，step表示每次分配的号段长度。原来获取ID每次都需要写数据库，现在只需要把step设置得足够大，比如1000。那么只有当1000个号被消耗完了之后才会去重新读写一次数据库。读写数据库的频率从1减小到了1/step，大致架构如下图所示：</p>

<p><img src="media/15277850962764/15277852227891.png" alt=""/></p>

<p>test_tag在第一台Leaf机器上是1~1000的号段，当这个号段用完时，会去加载另一个长度为step=1000的号段，假设另外两台号段都没有更新，这个时候第一台机器新加载的号段就应该是3001~4000。同时数据库对应的biz_tag这条数据的max_id会从3000被更新成4000，更新号段的SQL语句如下：</p>

<pre><code class="language-sql">Begin
UPDATE table SET max_id=max_id+step WHERE biz_tag=xxx
SELECT tag, max_id, step FROM table WHERE biz_tag=xxx
COMMIT
</code></pre>

<p>这种模式有以下优缺点：</p>

<p>优点：</p>

<ul>
<li>  Leaf服务可以很方便的线性扩展，性能完全能够支撑大多数业务场景。</li>
<li>  ID号码是趋势递增的8byte的64位数字，满足上述数据库存储的主键要求。</li>
<li>  容灾性高：Leaf服务内部有号段缓存，即使DB宕机，短时间内Leaf仍能正常对外提供服务。</li>
<li>  可以自定义max_id的大小，非常方便业务从原有的ID方式上迁移过来。</li>
</ul>

<p>缺点：</p>

<ul>
<li>  ID号码不够随机，能够泄露发号数量的信息，不太安全。</li>
<li>  TP999数据波动大，当号段使用完之后还是会hang在更新数据库的I/O上，tg999数据会出现偶尔的尖刺。</li>
<li>  DB宕机会造成整个系统不可用。</li>
</ul>

<h3 id="toc_8">双buffer优化</h3>

<p>对于第二个缺点，Leaf-segment做了一些优化，简单的说就是：</p>

<p>Leaf 取号段的时机是在号段消耗完的时候进行的，也就意味着号段临界点的ID下发时间取决于下一次从DB取回号段的时间，并且在这期间进来的请求也会因为DB号段没有取回来，导致线程阻塞。如果请求DB的网络和DB的性能稳定，这种情况对系统的影响是不大的，但是假如取DB的时候网络发生抖动，或者DB发生慢查询就会导致整个系统的响应时间变慢。</p>

<p>为此，我们希望DB取号段的过程能够做到无阻塞，不需要在DB取号段的时候阻塞请求线程，即当号段消费到某个点时就异步的把下一个号段加载到内存中。而不需要等到号段用尽的时候才去更新号段。这样做就可以很大程度上的降低系统的TP999指标。详细实现如下图所示：</p>

<p><img src="media/15277850962764/15277852373915.png" alt=""/></p>

<p>采用双buffer的方式，Leaf服务内部有两个号段缓存区segment。当前号段已下发10%时，如果下一个号段未更新，则另启一个更新线程去更新下一个号段。当前号段全部下发完后，如果下个号段准备好了则切换到下个号段为当前segment接着下发，循环往复。</p>

<ul>
<li><p>每个biz-tag都有消费速度监控，通常推荐segment长度设置为服务高峰期发号QPS的600倍（10分钟），这样即使DB宕机，Leaf仍能持续发号10-20分钟不受影响。</p></li>
<li><p>每次请求来临时都会判断下个号段的状态，从而更新此号段，所以偶尔的网络抖动不会影响下个号段的更新。</p></li>
</ul>

<h3 id="toc_9">Leaf高可用容灾</h3>

<p>对于第三点“DB可用性”问题，我们目前采用一主两从的方式，同时分机房部署，Master和Slave之间采用<strong>半同步方式[5]</strong>同步数据。同时使用公司Atlas数据库中间件(已开源，改名为<a href="http://tech.meituan.com/dbproxy-introduction.html">DBProxy</a>)做主从切换。当然这种方案在一些情况会退化成异步模式，甚至在<strong>非常极端</strong>情况下仍然会造成数据不一致的情况，但是出现的概率非常小。如果你的系统要保证100%的数据强一致，可以选择使用“类Paxos算法”实现的强一致MySQL方案，如MySQL 5.7前段时间刚刚GA的<a href="https://dev.mysql.com/doc/refman/5.7/en/group-replication.html">MySQL Group Replication</a>。但是运维成本和精力都会相应的增加，根据实际情况选型即可。</p>

<p><img src="media/15277850962764/15277852467563.png" alt=""/></p>

<p>同时Leaf服务分IDC部署，内部的服务化框架是“MTthrift RPC”。服务调用的时候，根据负载均衡算法会优先调用同机房的Leaf服务。在该IDC内Leaf服务不可用的时候才会选择其他机房的Leaf服务。同时服务治理平台OCTO还提供了针对服务的过载保护、一键截流、动态流量分配等对服务的保护措施。</p>

<h1 id="toc_10">Leaf-snowflake方案</h1>

<p>Leaf-segment方案可以生成趋势递增的ID，同时ID号是可计算的，不适用于订单ID生成场景，比如竞对在两天中午12点分别下单，通过订单id号相减就能大致计算出公司一天的订单量，这个是不能忍受的。面对这一问题，我们提供了 Leaf-snowflake方案。</p>

<p><img src="media/15277850962764/15277852551504.png" alt=""/></p>

<p>Leaf-snowflake方案完全沿用snowflake方案的bit位设计，即是“1+41+10+12”的方式组装ID号。对于workerID的分配，当服务集群数量较小的情况下，完全可以手动配置。Leaf服务规模较大，动手配置成本太高。所以使用Zookeeper持久顺序节点的特性自动对snowflake节点配置wokerID。Leaf-snowflake是按照下面几个步骤启动的：</p>

<ol>
<li> 启动Leaf-snowflake服务，连接Zookeeper，在leaf_forever父节点下检查自己是否已经注册过（是否有该顺序子节点）。</li>
<li> 如果有注册过直接取回自己的workerID（zk顺序节点生成的int类型ID号），启动服务。</li>
<li> 如果没有注册过，就在该父节点下面创建一个持久顺序节点，创建成功后取回顺序号当做自己的workerID号，启动服务。</li>
</ol>

<p><img src="media/15277850962764/15277852658209.png" alt=""/></p>

<h3 id="toc_11">弱依赖ZooKeeper</h3>

<p>除了每次会去ZK拿数据以外，也会在本机文件系统上缓存一个workerID文件。当ZooKeeper出现问题，恰好机器出现问题需要重启时，能保证服务能够正常启动。这样做到了对三方组件的弱依赖。一定程度上提高了SLA</p>

<h3 id="toc_12">解决时钟问题</h3>

<p>因为这种方案依赖时间，如果机器的时钟发生了回拨，那么就会有可能生成重复的ID号，需要解决时钟回退的问题。</p>

<p><img src="media/15277850962764/15277852832400.png" alt=""/></p>

<p>参见上图整个启动流程图，服务启动时首先检查自己是否写过ZooKeeper leaf_forever节点：</p>

<ol>
<li> 若写过，则用自身系统时间与leaf_forever/\({self}节点记录时间做比较，若小于leaf_forever/\){self}时间则认为机器时间发生了大步长回拨，服务启动失败并报警。</li>
<li> 若未写过，证明是新服务节点，直接创建持久节点leaf_forever/${self}并写入自身系统时间，接下来综合对比其余Leaf节点的系统时间来判断自身系统时间是否准确，具体做法是取leaf_temporary下的所有临时节点(所有运行中的Leaf-snowflake节点)的服务IP：Port，然后通过RPC请求得到所有节点的系统时间，计算sum(time)/nodeSize。</li>
<li> 若abs( 系统时间-sum(time)/nodeSize ) &lt; 阈值，认为当前系统时间准确，正常启动服务，同时写临时节点leaf_temporary/${self} 维持租约。</li>
<li> 否则认为本机系统时间发生大步长偏移，启动失败并报警。</li>
<li> 每隔一段时间(3s)上报自身系统时间写入leaf_forever/${self}。</li>
</ol>

<p>由于强依赖时钟，对时间的要求比较敏感，在机器工作时NTP同步也会造成秒级别的回退，建议可以直接关闭NTP同步。要么在时钟回拨的时候直接不提供服务直接返回ERROR_CODE，等时钟追上即可。<strong>或者做一层重试，然后上报报警系统，更或者是发现有时钟回拨之后自动摘除本身节点并报警</strong>，如下：</p>

<pre><code class="language-java"> //发生了回拨，此刻时间小于上次发号时间
 if (timestamp &lt; lastTimestamp) {

            long offset = lastTimestamp - timestamp;
            if (offset &lt;= 5) {
                try {
                    //时间偏差大小小于5ms，则等待两倍时间
                    wait(offset &lt;&lt; 1);//wait
                    timestamp = timeGen();
                    if (timestamp &lt; lastTimestamp) {
                       //还是小于，抛异常并上报
                        throwClockBackwardsEx(timestamp);
                      }    
                } catch (InterruptedException e) {  
                   throw  e;
                }
            } else {
                //throw
                throwClockBackwardsEx(timestamp);
            }
        }
 //分配ID
</code></pre>

<p><strong>从上线情况来看，在2017年闰秒出现那一次出现过部分机器回拨，由于Leaf-snowflake的策略保证，成功避免了对业务造成的影响。</strong></p>

<h1 id="toc_13">Leaf现状</h1>

<p>Leaf在美团点评公司内部服务包含金融、支付交易、餐饮、外卖、酒店旅游、猫眼电影等众多业务线。目前Leaf的性能在4C8G的机器上QPS能压测到近5w/s，TP999 1ms，已经能够满足大部分的业务的需求。每天提供亿数量级的调用量，作为公司内部公共的基础技术设施，必须保证高SLA和高性能的服务，我们目前还仅仅达到了及格线，还有很多提高的空间。</p>

<h1 id="toc_14">作者简介</h1>

<p>照东，美团点评基础架构团队成员，主要参与<a href="http://tech.meituan.com/mt-mtrace.html">美团大型分布式链路跟踪系统Mtrace</a>和美团点评分布式ID生成系统Leaf的开发工作。曾就职于阿里巴巴，2016年7月加入美团。</p>

<p>最后做一个招聘广告：如果你对大规模分布式环境下的服务治理、分布式会话链追踪等系统感兴趣，诚挚欢迎投递简历至：zhangjinlu#meituan.com。</p>

<h1 id="toc_15">参考资料</h1>

<ol>
<li> 施瓦茨. 高性能MySQL[M]. 电子工业出版社, 2010:162-171.</li>
<li> <a href="https://zh.wikipedia.org/wiki/%E9%80%9A%E7%94%A8%E5%94%AF%E4%B8%80%E8%AF%86%E5%88%AB%E7%A0%81">维基百科：UUID</a>.</li>
<li> <a href="https://github.com/twitter/snowflake">snowflake</a>.</li>
<li> <a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-index-types.html">MySQL: Clustered and Secondary Indexes</a>.</li>
<li> <a href="https://dev.mysql.com/doc/refman/5.5/en/replication-semisync.html">半同步复制 Semisynchronous Replication</a>.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[蚂蚁通信框架实践]]></title>
    <link href="http://panlw.github.io/15277835170976.html"/>
    <updated>2018-06-01T00:18:37+08:00</updated>
    <id>http://panlw.github.io/15277835170976.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzUzMzU5Mjc1Nw==&amp;mid=2247483812&amp;idx=1&amp;sn=580d9003b01b3c5dab821c04a97b77cb&amp;chksm=faa0ee7ecdd767684e66a7844270ca1d0cc94b05719b39068d161e2e56560d00167efd753859&amp;mpshare=1&amp;scene=23&amp;srcid=0531OaPJuQOHngeGUjgDXvfG%23rd">原文地址</a></p>
</blockquote>

<pre><code>**原创声明**：本文系作者原创，谢绝个人、媒体、公众号或网站未经授权转载，违者追究其法律责任。
</code></pre>

<h2 id="toc_0"><strong>前  言</strong></h2>

<p>互联网领域的通信技术，有各式各样的通信协议可以选择，比如基于 TCP/IP 协议簇的 HTTP(1/2)、SPDY 协议、WebSocket、Google 基于 UDP 的 QUIC 协议等。这些协议，都有完整的报文格式与字段定义，对安全，序列化机制，数据压缩机制，CRC 校验机制等各种通信细节都有较好的设计。能够高效、稳定、且安全地运行在公网环境。</p>

<p>而对于私网环境，比如一个公司的 IDC 内部，如果所有应用的节点间，全部通过标准协议来通信，会有很多问题：比如研发效率方面的影响，我们的研发框架，需要做大量业务数据转化成标准协议的工作；再比如升级兼容性，标准协议的字段众多，版本各异，兼容性也得不到保障；除此还有无用字段的传输，也会造成资源浪费，功能定制也可能不那么灵活。而解决这些问题，比较常见的做法就是自己来设计协议，可以自己来定义字段，制定升级方式，可插拔可开关的特性需求等，我们把这样的协议叫做私有通信协议。</p>

<p>在蚂蚁金服的分布式技术体系下，我们大量的技术产品（非网关类产品），都需要在内网，进行节点间通信。高吞吐、高并发的通信，数量众多的连接管理（C10K 问题），便捷的升级机制，兼容性保障，灵活的线程池模型运用，细致的异常处理与日志埋点等，这些功能都需要在通信协议和实现框架上做文章。本文主要从如下几个方面来对蚂蚁通信框架实践之路进行介绍：</p>

<ol>
<li> 私有通信协议设计</li>
<li> 基础通信功能设计要点分析</li>
<li> 私有通信协议设计举例</li>
<li> 蚂蚁自研通信框架 Bolt</li>
</ol>

<h2 id="toc_1"><strong>私有通信协议设计</strong></h2>

<p>我们的分布式架构，所需要的内部通信模块，采用了私有协议来设计和研发。当然私有协议，也是有很多弊端的，比如在通用性上、公网传输的能力上相比标准协议会有劣势。然而，我们为了最大程度的提升性能，降低成本，提高灵活性与效率，最佳选择还是高度定制化的私有协议：</p>

<ul>
<li>  可以有效地利用协议里的各个字段</li>
<li>  灵活满足各种通信功能需求：比如 CRC 校验，Server Fail-Fast 机制，自定义序列化器</li>
<li>  最大程度满足性能需求：IO 模型与线程模型的灵活运用</li>
</ul>

<p>比如一个典型的 Request-Response 通信场景：</p>

<ol>
<li> 在一个通信节点上，如何把一个请求对象，序列化成字节流，通过怎样的网络传输方式，传递到另一个节点</li>
<li> 在对端的通信节点上，需要高效的读取字节流，并反序列化成原始的请求对象，然后根据请求内容，做一些逻辑处理。处理完成后，响应返回。</li>
<li> 同时，此时要考虑，如何充分利用网络 IO、CPU 以及内存，来保证吞吐和处理效率的最优。</li>
</ol>

<p>文章后面的内容，比较清晰地介绍了这个通信场景的设计与实现方案。</p>

<p><img src="media/15277835170976/15277842914150.jpg" alt=""/><br/>
图1 - 私有协议与必要的功能模块</p>

<p>首先协议设计上，我们需要考虑的几个关键问题：</p>

<p><strong>Protocol</strong></p>

<ul>
<li>  协议应该包括哪些必要字段与主要业务负载字段：协议里设计的每个字段都应该被使用到，避免无效字段；</li>
<li>  需要考虑通信功能特性的支持：比如CRC校验，安全校验，数据压缩机制等；</li>
<li>  需要考虑协议的可扩展性：充分评估现有业务的需求，设计一个通用，扩展性高的协议，避免经常对协议进行修改；</li>
<li>  需要考虑协议的升级机制：毕竟是私有协议，没有长期的验证，字段新增或者修改，是有可能发生的，因此升级机制是必须考虑的；</li>
</ul>

<p><strong>Encoder 与 Decoder</strong></p>

<ul>
<li>  协议相关的编解码方式：私有协议需要有核心的encode与decode过程，并且针对业务负载能支持不同的序列化与反序列化机制。这部分，不同的私有协议，由于字段的差异，核心encode和decode过程是不一样的，因此需要分开考虑</li>
</ul>

<p><strong>Heartbeat</strong></p>

<ul>
<li>  协议相关的心跳触发与处理：不同的协议对心跳的需求，处理逻辑也可能是不同的。因此心跳的触发逻辑，心跳的处理逻辑，也都需要单独考虑。</li>
</ul>

<p><strong>Command 与 Command Handler</strong></p>

<ul>
<li>  可扩展的命令与命令处理器管理
<img src="media/15277835170976/15277843020616.jpg" alt=""/>
图2 - 通信命令设计举例</li>
<li>  负载命令：一般传输的业务的具体数据，比如带着请求参数，响应结果的命令；</li>
<li>  控制命令：一些功能管理命令，心跳命令等，它们通常完成一些复杂的分布式跨节点的协调功能，以此来保证负载命令通信过程的稳定，是必不可少的一部分。</li>
<li>  协议的通信过程，会有各种命令定义，逻辑上，我们把传输业务具体负载的请求对象，叫做负载命令（Payload Command），另一种叫做控制命令（Control Command），比如一些功能管理命令，或者心跳命令。</li>
<li>  定义了通信命令，我们还需要定义命令处理器，用来编写各个命令对应的业务处理逻辑。同时，我们需要保存命令与命令处理器的映射关系，以便在处理阶段，走到正确的处理器。</li>
</ul>

<p>有了私有协议的设计要点，我们接下来分两部分来介绍下实现：基础通信模块与私有协议设计举例。</p>

<p>首先是基础通信功能模块的实现，这部分沉淀了我们的一些优化和最佳实践，可以被不同的私有协议复用。</p>

<h2 id="toc_2"><strong>基础通信功能设计要点分析</strong></h2>

<p>蚂蚁的中间件产品，主要是 Java 语言开发，如果通信产品直接用原生的 Java NIO 接口开发，工作量相当庞大。通常我们会选择一些基础网络编程框架，而在基础网络通信框架上，我们也经历了自研（比如伯岩的 Gecko）、基于 Apache Mina 实现。最终，由于 Netty 在网络编程领域的出色表现，我们逐步切换到了 Netty 上。</p>

<p>Netty 在 2008 年就发布了<code>3.0.0</code> 版本，到现在已经经历了 10 年多的发展。而且从 <code>4.x</code> 之后的版本，把无锁化的设计理念放在第一位，然后针对内存分配，高效的 Queue 队列，高吞吐的超时机制等，做了各种细节优化。同时 Netty 的核心 Committer 与社区非常活跃，如果发现了缺陷能够及时得到修复。所有这些，使得 Netty 性能非常的出色和稳定，成为当下 Java 领域最优秀的网络通信组件。接下来主要介绍我们对 Netty 的学习经验，内部使用上的一些最佳实践。</p>

<h3 id="toc_3">1. 网络 IO 模型与线程模型</h3>

<p><img src="media/15277835170976/15277843394656.jpg" alt=""/><br/>
图3 - Netty与Reactor</p>

<p>如果你对 Java 网络 IO 这个话题感兴趣的话，肯定看过 Doug Lea 的《Scalable IO in Java》，在这个 PPT 里详细介绍了如何使用 Java NIO 的技术来实现 Douglas C. Schmidt 发表的 Reactor 论文里所描述的 IO 模型。针对这个高效的通信模型，Netty 做了非常友好的支持：</p>

<ul>
<li><p><strong>Reactor模型</strong></p>

<ul>
<li><p>我们只需要在初始化 <code>ServerBootstrap</code> 时，提供两个不同的 <code>EventLoopGroup</code> 实例，就实现了 Reactor 的主从模型。我们通常把处理建连事件的线程，叫做 BossGroup，对应 <code>ServerBootstrap</code> 构造方法里的 <code>parentGroup</code> 参数，即我们常说的 Acceptor 线程；处理已创建好的 <code>channel</code>  相关连 IO 事件的线程，叫做 WorkerGroup，对应 <code>ServerBootstrap</code> 构造方法里的 <code>childGroup</code> 参数，即我们常说的 IO 线程。</p></li>
<li><p>最佳实践：通常 <code>bossGroup</code> 只需要设置为 <code>1</code> 即可，因为 <code>ServerSocketChannel</code> 在初始化阶段，只会注册到某一个 <code>eventLoop</code> 上，而这个 <code>eventLoop</code> 只会有一个线程在运行，所以没有必要设置为多线程（什么时候需要多线程呢，可以参考 Norman Maurer 在 StackOverflow 上的这个回答）；而 IO 线程，为了充分利用 CPU，同时考虑减少线上下文切换的开销，通常设置为 CPU 核数的两倍，这也是 Netty 提供的默认值。</p></li>
</ul></li>
<li><p><strong>串行化设计理念</strong></p>

<ul>
<li>  Netty 从 <code>4.x</code> 的版本之后，所推崇的设计理念是串行化处理一个 <code>Channel</code> 所对应的所有 IO 事件和异步任务，单线程处理来规避并发问题。Netty 里的 <code>Channel</code> 在创建后，会通过 <code>EventLoopGroup</code> 注册到某一个 <code>EventLoop</code> 上，之后该 <code>Channel</code> 所有读写事件，以及经由 <code>ChannelPipeline</code> 里各个 <code>Handler</code> 的处理，都是在这一个线程里。一个 <code>Channel</code> 只会注册到一个 <code>EventLoop</code> 上，而一个 <code>EventLoop</code> 可以注册多个 <code>Channel</code> 。所以我们在使用时，也需要尽可能避免使用带锁的实现，能无锁化就无锁。</li>
<li>  最佳实践：<code>Channel</code> 的实现是线程安全的，因此我们通常在运行时，会保存一个 <code>Channel</code> 的引用，同时为了保持 Netty 的无锁化理念，也应该尽可能避免使用带锁的实现，尤其是在 <code>Handler</code> 里的处理逻辑。举个例子：这里会有一个比较特殊的容易死锁的场景，比如在业务线程提交异步任务前需要先抢占某个锁，<code>Handler</code> 里某个异步任务的处理也需要获取同一把锁。如果某一个时刻业务线程先拿到锁 lock1，同时 <code>Handler</code> 里由于事件机制触发了一个异步任务 A，并在业务线程提交异步任务之前，提交到了 <code>EventLoop</code> 的队列里。之后，业务线程提交任务 B，等待 B 执行完成后才能释放锁 lock1；而任务 A 在队列里排在 B 之前，先被执行，执行过程需要获取锁 lock1 才能完成。这样死锁就发生了，与常见的资源竞争不同，而是任务执行权导致的死锁。要规避这类问题，最好的办法就是不要加锁；如果实在需要用锁，需要格外注意 Netty 的线程模型与任务处理机制。</li>
</ul></li>
<li><p><strong>业务处理</strong></p>

<ul>
<li>  IO 密集型的轻计算业务：此时线程的上下文切换消耗，会比 IO 线程的占用消耗更为突出，所以我们通常会建议在 IO 线程来处理请求；</li>
<li>  CPU 密集型的计算业务：比如需要做远程调用，操作 DB 的业务，此时 IO 线程的占用远远超过线程上下文切换的消耗，所以我们就会建议在单独的业务线程池里来处理请求，以此来释放 IO 线程的占用。该模式，也是我们蚂蚁微服务，消息通信等最常使用的模型。该模式在后面的 RPC 协议实现举例部分会详细介绍。</li>
<li>  如文章开头所描述的场景，我们需要合理设计，来将硬件的 IO 能力，CPU 计算能力与内存结合起来，发挥最佳的效果。针对不同的业务类型，我们会选择不同的处理方式</li>
<li>  最佳实践：“Never block the event loop, reduce context-swtiching”，引自Netty committer Norman Maurer，另外阿里 HSF 的作者毕玄也有类似的总结。</li>
</ul></li>
<li><p><strong>其他实践建议</strong></p>

<ul>
<li>  最小化线程池，能复用 <code>EventLoopGroup</code> 的地方尽量复用。比如蚂蚁因为历史原因，有过两版 RPC 协议，在两个协议升级过渡期间，我们会复用 Acceptor 线程与 IO 线程在同一个端口处理不同协议的请求；除此，针对多应用合并部署的场景，我们也会复用 IO 线程防止一个进程开过多的 IO 线程。</li>
<li>  对于无状态的 <code>ChannelHandler</code> ，设置成共享模式。比如我们的事件处理器，RPC 处理器都可以设置为共享，减少不同的 <code>Channel</code> 对应的 <code>ChannelPipeline</code> 里生成的对象个数。</li>
<li>  正确使用 <code>ChannelHandlerContext</code> 的 <code>ctx.write()</code> 与 <code>ctx.channel().write()</code> 方法。前者是从当前 <code>Handler</code> 的下一个 <code>Handler</code> 开始处理，而后者会从 tail 开始处理。大多情况下使用 <code>ctx.write()</code> 即可。</li>
<li>  在使用 <code>Channel</code> 写数据之前，建议使用 <code>isWritable()</code> 方法来判断一下当前 <code>ChannelOutboundBuffer</code> 里的写缓存水位，防止 OOM 发生。不过实践下来，正常的通信过程不太会 OOM，但当网络环境不好，同时传输报文很大时，确实会出现限流的情况。</li>
</ul></li>
</ul>

<h3 id="toc_4">2. 连接管理</h3>

<p>为了提高通信效率，我们需要考虑复用连接，减少 TCP 三次握手的次数，因此需要有连接管理的机制。而在业务的通信场景中，我们还识别到一些不得不走硬负载（比如 LVS VIP）的场景，此时如果只建立单链接，可能会出现负载不均衡的问题，此时需要建立多个连接，来缓解负载不均的问题。我们需要设计一个针对某个连接地址（IP 与 Port 唯一确定的地址）建立特定数目连接的实现，同时保存在一个连接池里。该连接池设计了一个通用的 <code>PoolKey</code>不限定 Key 的类型。</p>

<p>需要注意的是，这里的建连过程，有一个并发问题要解，比如客户端在高并发的调用建连接口时，如何保证建立的连接刚好是所设定的个数呢？为了配合 Netty 的无锁理念，我们也采用一个无锁化的建连过程来实现，利用 <code>ConcurrentHashMap</code> 的 <code>putIfAbsent</code> 接口：</p>

<p><img src="media/15277835170976/15277843573310.jpg" alt=""/><br/>
代码1 - 无锁建连代码</p>

<p>除此，我们的连接管理，还要具备定时断连功能，自动重连功能，自定义连接选择算法功能来适用不同的连接场景。</p>

<ul>
<li>  最佳实践：在 Netty 的 4.0.28.Final#3218 里，提供了一种 <code>ChannelPool</code> 的接口类与默认实现，其中 <code>FixedChannelPool</code> 与我们实现的连接池做的事情一样。而 Netty 采用了更巧妙的方式来规避并发问题，即在初始化 <code>FixedChannelPool</code> 时，就将其关联到某一个 <code>eventLoop</code> 上，后续的建连动作，采用经典的 <code>inEventLoop()</code> 方法来判断，如果不在 <code>eventLoop</code> 线程，则入队等待下次调度。如此规避了并发问题。这个功能，我们目前还没有实践过，后续计划采用这个官方实现重构一版。</li>
</ul>

<h3 id="toc_5">3. 基础通信模型</h3>

<p><img src="media/15277835170976/15277843662971.jpg" alt=""/><br/>
图4 - 几种通信模型</p>

<p>如图所示，我们实现了多种通信接口 <code>oneway</code> ，<code>sync</code> ，<code>future</code> ，<code>callback</code> 。图中都是ping/pong模式的通信，蓝色部分表示线程正在执行任务</p>

<ul>
<li>  可以看到 <code>oneway</code> 不关心响应，请求线程不会被阻塞，但使用时需要注意控制调用节奏，防止压垮接收方；</li>
<li>  <code>sync</code> 调用会阻塞请求线程，待响应返回后才能进行下一个请求。这是最常用的一种通信模型；</li>
<li>  <code>future</code> 调用，在调用过程不会阻塞线程，但获取结果的过程会阻塞线程；</li>
<li>  <code>callback</code> 是真正的异步调用，永远不会阻塞线程，结果处理是在异步线程里执行。</li>
</ul>

<h3 id="toc_6">4. 超时控制</h3>

<p><img src="media/15277835170976/15277843806586.jpg" alt=""/><br/>
图5 - 超时控制模型</p>

<p>除了 <code>oneway</code> 模式，其他三种通信模型都需要进行超时控制，我们同样采用 Netty 里针对超时机制，所设计的高效方案 <code>HashedWheelTimer</code> 。如图所示，其原理是首先在发起调用前，我们会新增一个超时任务 <code>timeoutTask</code> 到 <code>MpscQueue</code> （Netty 实现的一种高效的无锁队列）里，然后在循环里，会不断的遍历 Queue 里的这些超时任务（每次最多10万），针对每个任务，会根据其设置的超时时间，来计算该任务所属于的 <code>bucket</code> 位置与剩余轮数 <code>remainingRounds</code> ，然后加入到对应 <code>bucket</code> 的链表结构里。随着 <code>tick++</code> 的进行，时间在不断的增长，每 <code>tick</code> 8 次，就是 1 个时间轮 <code>round</code>。当对应超时任务的<code>remainingRounds</code>减到 <code>0</code> 时，就是触发这个超时任务的时候，此时再执行其 <code>run()</code> 方法，做超时逻辑处理。</p>

<ul>
<li>  最佳实践：通常一个进程使用一个<code>HashedWheelTimer</code>实例，采用单例模型即可。</li>
</ul>

<h3 id="toc_7">5. 批量解包与批量提交</h3>

<p><img src="media/15277835170976/15277843927772.jpg" alt=""/><br/>
图6 - 批量解包与批量提交</p>

<p>Netty 提供了一个方便的解码工具类 <code>ByteToMessageDecoder</code> ，如图上半部分所示，这个类具备 <code>accumulate</code> 批量解包能力，可以尽可能的从 <code>socket</code> 里读取字节，然后同步调用 <code>decode</code> 方法，解码出业务对象，并组成一个 <code>List</code> 。最后再循环遍历该 <code>List</code> ，依次提交到 <code>ChannelPipeline</code> 进行处理。此处我们做了一个细小的改动，如图下半部分所示，即将提交的内容从单个 <code>command</code> ，改为整个 <code>List</code> 一起提交，如此能减少 <code>pipeline</code> 的执行次数，同时提升吞吐量。这个模式在低并发场景，并没有什么优势，而在高并发场景下对提升吞吐量有不小的性能提升。</p>

<ul>
<li>  最佳实践：<code>ByteToMessageDecoder</code>  因为内部的实现有成员变量，不是无状态的，所以一定不能被设置为 <code>@Sharable</code></li>
</ul>

<h3 id="toc_8">6. 其他有用的功能</h3>

<ul>
<li>  <strong>事件触发与监听机制</strong>

<ul>
<li>  Netty 的 <code>ChannelHandler</code> 完美实现了拦截器模式。在 <code>ChannelHandler</code> 里 <code>hook</code> 了各个IO事件与IO操作的方法，我们可以方便的覆写这些方法，来加一些自定义的逻辑。比如为了把建连，断连事件触发给上层业务，方便做一些准备或者优雅关闭的处理，我们实现一个继承了 <code>ChannelInBoundHandler</code> 与 <code>ChannelOutboundHandler</code> 的处理器，覆盖这些事件所对应的建连与断连方法，然后设计一套业务的 <code>event</code> 感知逻辑即可。</li>
</ul></li>
<li>  <strong>双工通信</strong>

<ul>
<li>  我们知道 TCP 是可以提供全双工的通信能力的。因此，当客户端与服务端建立连接后，我们是可以由服务端发起通信请求，客户端来处理的。而为了支持这个功能，我们只需要把可以复用的 <code>inboundHandler</code> 与 <code>outboundHandler</code> 在 客户端的 <code>Bootstrap</code> 与服务端的 <code>ServerBootstrap</code> 里都注册一遍即可</li>
</ul></li>
</ul>

<p>有了私有协议的设计要点，与基础通信模块的实现，我们来看一个私有协议设计的举例，一种典型的 RPC 特征的通信实现。</p>

<h2 id="toc_9"><strong>私有通信协议举例</strong></h2>

<h3 id="toc_10">1. 通信协议的设计</h3>

<p><img src="media/15277835170976/15277844025910.jpg" alt=""/><br/>
图7 - 协议字段举例</p>

<ul>
<li>  <code>ProtocolCode</code> ：如果一个端口，需要处理多种协议的请求，那么这个字段是必须的。因为需要根据 <code>ProtocolCode</code> 来进入不同的核心编解码器。比如在支付宝，因为曾经使用过基于mina开发的通信框架，当时设计了一版协议。因此，我们在设计新版协议时，需要预留该字段，来适配不同的协议类型。该字段可以在想换协议的时候，方便的进行更换。</li>
<li>  <code>ProtocolVersion</code> ：确定了某一种通信协议后，我们还需要考虑协议的微小调整需求，因此需要增加一个 <code>version</code> 的字段，方便在协议上追加新的字段</li>
</ul>

<p><img src="media/15277835170976/15277844148635.jpg" alt=""/><br/>
图8 - 协议号与版本号的关系</p>

<ul>
<li>  <code>RequestType</code> ：请求类型， 比如<code>request</code> <code>response</code> <code>oneway</code></li>
<li>  <code>CommandCode</code> ：请求命令类型，比如 <code>request</code> 可以分为：负载请求，或者心跳请求。<code>oneway</code> 之所以需要单独设置，是因为在处理响应时，需要做特殊判断，来控制响应是否回传。</li>
<li>  <code>CommandVersion</code> ：请求命令版本号。该字段用来区分请求命令的不同版本。如果修改 <code>Command</code> 版本，不修改协议，那么就是纯粹代码重构的需求；除此情况，<code>Command</code> 的版本升级，往往会同步做协议的升级。</li>
<li>  <code>RequestId</code> ：请求 ID，该字段主要用于异步请求时，保留请求存根使用，便于响应回来时触发回调。另外，在日志打印与问题调试时，也需要该字段。</li>
<li>  <code>Codec</code> ：序列化器。该字段用于保存在做业务的序列化时，使用的是哪种序列化器。通信框架不限定序列化方式，可以方便的扩展。</li>
<li>  <code>Switch</code> ：协议开关，用于一些协议级别的开关控制，比如 CRC 校验，安全校验等。</li>
<li>  <code>Timeout</code> ：超时字段，客户端发起请求时，所设置的超时时间。该字段非常有用，在后面会详细讲解用法。</li>
<li>  <code>ResponseStatus</code> ：响应码。从字段精简的角度，我们不可能每次响应都带上完整的异常栈给客户端排查问题，因此，我们会定义一些响应码，通过编号进行网络传输，方便客户端定位问题。</li>
<li>  <code>ClassLen</code> ：业务请求类名长度</li>
<li>  <code>HeaderLen</code> ：业务请求头长度</li>
<li>  <code>ContentLen</code> ：业务请求体长度</li>
<li>  <code>ClassName</code> ：业务请求类名。需要注意类名传输的时候，务必指定字符集，不要依赖系统的默认字符集。曾经线上的机器，因为运维误操作，默认的字符集被修改，导致字符的传输出现编解码问题。而我们的通信框架指定了默认字符集，因此躲过一劫。</li>
<li>  <code>HeaderContent</code> ：业务请求头</li>
<li>  <code>BodyContent</code> ：业务请求体</li>
<li>  <code>CRC32</code> ：CRC校验码，这也是通信场景里必不可少的一部分，而我们金融业务属性的特征，这个显得尤为重要。</li>
</ul>

<h3 id="toc_11">2. 灵活的反序列化时机控制</h3>

<p>从上面的协议介绍，可以看到协议的基本字段所占用空间是比较小的，目前只有24个字节。协议上的主要负载就是 <code>ClassName</code>  ，<code>HeaderContent</code> ， <code>BodyContent</code> 这三部分。这三部分的序列化和反序列化是整个请求响应里最耗时的部分。在请求发送阶段，在调用 Netty 的写接口之前，会在业务线程先做好序列化，这里没有什么疑问。而在请求接收阶段，反序列化的时机就需要考虑一下了。结合上面提到的最佳实践的网络 IO 模型，请求接收阶段，我们有 IO 线程，业务线程两种线程池。为了最大程度的配合业务特性，保证整体吞吐我们设计了精细的开关来控制反序列化时机：</p>

<p><img src="media/15277835170976/15277844266936.jpg" alt=""/><br/>
图9 - 反序列化与业务处理时序图</p>

<p><img src="media/15277835170976/15277844352308.jpg" alt=""/><br/>
表格1 - 反序列化场景具体介绍</p>

<h3 id="toc_12">3. Server Fail-Fast 机制</h3>

<p><img src="media/15277835170976/15277844481916.jpg" alt=""/><br/>
图10 - Server Fail-Fast机制</p>

<p>在协议里，留意到我们有timeout这个字段，这个是把客户端发起调用时，所设置的超时时间通过协议传到了 Server 端。有了这个，我们就可以实现 Fail-Fast 快速失败的机制。比如当客户端设置超时时间 1s，当请求到达 Server 开始计时 <code>arriveTimeStamp</code> ，到任务被线程调度到开始处理时，记录 <code>startToProcessTimestamp</code> ，二者的差值即请求反序列化与线程池排队的时延，如果这个时间间隔已经超过了 1s，那么请求就没有必要被处理了。这个机制，在服务端出现处理抖动时，对于快速恢复会很有用。</p>

<ul>
<li>  最佳实践：不要依赖跨系统的时钟，因为时钟可能会不一致，跨系统就会出现误差，因此是从请求到达 Server 的那一刻，在 Server 的进程里开始计时。</li>
</ul>

<h3 id="toc_13">4. 用户请求处理器(UserProcessor)</h3>

<p>在通用设计部分，我们提到了命令处理器。而为了方便开发者使用，我们还提供了一个用户请求处理器，即在 RPC 的命令处理器中，再增加一层映射关系，保存的是 业务传输对象的 <code>className</code> 与 <code>UserProcessor</code> 的对应关系。此时服务端只需要简单注册一个 <code>className</code> 对应的processor，并提供一个独立的 <code>executor</code> ，就可以实现在业务线程处理请求了。</p>

<p><img src="media/15277835170976/15277844585373.jpg" alt=""/><br/>
图11 - 命令处理器与用户请求处理器的关系</p>

<p>除此，我们还设计了一个 <code>RemotingContext</code> 用于保存请求处理阶段的一些通信层的关键辅助类或者信息，方便通信框架开发者使用；同时还提供了一个 <code>BizContext</code> ，有选择把通信层的信息暴露给框架使用者，方便框架使用者使用。有了用户请求处理器，以及上下文的传递机制，我们就可以方便的把通信层处理逻辑与业务处理逻辑联动起来，比如一些开关的控制，字段的传递等定制功能：</p>

<ul>
<li>  请求超时处理开关：用于开关 Server Fail-Fast 机制。</li>
<li>  IO 线程业务处理开关：用户可以选择在 IO 线程处理业务请求；或者在业务线程来处理。</li>
<li>  线程池选择器 <code>ExecutorSelector</code> ：用户可以提供多个业务线程池，使用 <code>ExecutorSelector</code> 来实现选择逻辑</li>
<li>  泛化调用的支持：序列化请求与反序列化响应阶段，针对泛化调用，使用特殊的序列化器。而是否开启该功能，需要依赖上下文来传递一些标识。</li>
</ul>

<h3 id="toc_14">5. 其他实现细节</h3>

<ul>
<li><p><strong>可扩展的序列化机制</strong><br/>
针对业务对象里的 <code>HeaderContent</code> 与 <code>BodyContent</code> ，我们提供了用户自定义逻辑：用户可以结合自身的请求内容做定制的序列化和反序列化动作；如果用户没有自定义，那么会默认使用 Bolt 框架当前集成的序列化器，比如 Hessian（默认使用）、FastJson 等。</p></li>
<li><p><strong>埋点与异常处理</strong><br/>
为了精细化请求处理过程，我们会记录请求发送阶段的建连耗时，客户端超时时间，请求到达时间，线程调度等待时间等，然后通过上下文传递机制，连通业务与通信层；同时还会细化各个异常场景，比如请求超时异常，服务端线程池繁忙，序列化异常（请求与响应），反序列化异常（请求与响应）等。有了这些就能方便进行问题排查和快速定位。</p></li>
<li><p><strong>日志打印</strong><br/>
<img src="media/15277835170976/15277844755617.jpg" alt=""/><br/>
图12 - 日志模板</p>

<p>作为通信框架，必要的日志打印也是很重要的。比如可以打印建连与断连的日志，便于排查连接问题；一些关键的异常场景也可以打印出来，方便定位问题；还可以打印一些关键字，来表示程序 BUG，便于框架开发者定位和分析。而打印日志的方式，我们选择依赖日志门面 <code>SLF4J</code> ，然后提供不同的日志实现所需要的配置文件。运行时，根据业务所依赖的日志实现（比如 <code>log4j</code> ， <code>log4j2</code> ， <code>logback</code> 来动态加载日志配置）。同时默认使用异步 <code>logger</code> 来打印日志。</p></li>
</ul>

<h2 id="toc_15"><strong>蚂蚁通信框架-BOLT</strong></h2>

<ul>
<li><p>为了让 Java 程序员，花更多的时间在一些 Productive 的事情上，而不是纠结底层 NIO 的实现，处理难以调试的网络问题，Netty 应运而生</p></li>
<li><p>为了让中间件的开发者，花更多的时间在中间件的特性实现上，而不是重复地一遍遍制造通信框架的轮子，Bolt 应运而生。</p></li>
</ul>

<p>Bolt 即为本文所描述的方法论的一个实践实现，名字取自迪士尼动画，闪电狗。定位是一个基于 Netty 最佳实践过的，通用、高效、稳定的通信框架。我们希望能把这些年，在 RPC，MSG 在网络通信上碰到的问题与解决方案沉淀到这个基础组件里，不断的优化和完善它。让更多的需要网络通信的场景能够统一受益。目前已经运用在了蚂蚁中间件的微服务，消息中心，分布式事务，分布式开关，配置中心等众多产品上。</p>

<p>除了 Bolt 提供的高效通信能力外，还可以方便的进行协议适配的工作。比如蚂蚁内部之前使用的 RPC 协议是 Tr 协议，是基于 Apache Mina 开发的老版本通信框架，由于年久失修，同时性能逐步落伍，我们重新设计了 Bolt 协议，精简以及新增了一些协议字段，同时切换到了 Netty 上。在新老 RPC 协议的切换期间，我们利用 Bolt 进行了协议适配，开发了 BoltTrAdaptor，最大程度的复用基础通信能力，仅仅把协议相关的部分单独实现，以此来保证新老协议调用的兼容性。</p>

<p>针对蚂蚁内部的新老通信框架，我们进行了细致的压测，如下图所示。我们的压测环境是，4 核10G 的虚拟机，千兆网卡，请求与响应包大小 1024 字节，分别压测了四种场景。由压测结果能看出 Bolt -&gt; Bolt 的场景，整体吞吐量最大，平均RT最小，同时对比了 IO ，CPU 使用率等情况，资源整体利用率上也提升很多。</p>

<p><img src="media/15277835170976/15277844880461.jpg" alt=""/><br/>
图13 - 压测TPS数据</p>

<p><img src="media/15277835170976/15277844973704.jpg" alt=""/><br/>
图14 - 压测平均RT数据</p>

<p>Bolt 在实验室里的极限性能压测，采用的是 32 核物理机，万兆网卡的环境，请求和响应 100 字节负载，服务端收到请求后马上返回响应，瓶颈基本就是业务线程池所使用的 <code>ArrayBlockingQueue</code> <code>LinkedBlockingQueue</code> 的性能瓶颈，压力到了十多万，就会出现较大幅度的毛刺和抖动。纯粹为了压测场景，改成使用 <code>SynchronousQueue</code> 后，毛刺减少了很多，基本能稳定在 30W TPS 的处理能力。</p>

<h2 id="toc_16"><strong>写在最后</strong></h2>

<p>近期我们也在准备开源蚂蚁 Bolt 通信框架，主要是吸取 Netty 的开源精神，回馈社区，与社区共建与完善。如果你也有制造通信框架轮子的需求，或者想适配内部的自有或者开源通信协议（比如 Dubbo 等），可以试一下蚂蚁 Bolt 通信框架，敬请期待！我们有很多想法还在实验室里酝酿，还没有落地到生产环境使用。非常欢迎一起来探讨网络通信问题，参与共建。</p>

<p>最后附上蚂蚁中间件的招聘链接，通信是分布式架构体系的基础设施，欢迎有志之士加盟，打造高效、稳定的通信技术。点击左下角的【阅读原文】获取蚂蚁中间件通信组的招聘信息。</p>

<h2 id="toc_17"><strong>参考</strong></h2>

<ol>
<li> Scalable IO in Java, Slides, by Doug Lea, <a href="http://gee.cs.oswego.edu/dl/cpjslides/nio.pdf">http://gee.cs.oswego.edu/dl/cpjslides/nio.pdf</a></li>
<li> Reactor, Thesis, by Douglas C. Schmidt, <a href="http://www.dre.vanderbilt.edu/%7Eschmidt/PDF/reactor-siemens.pdf">http://www.dre.vanderbilt.edu/~schmidt/PDF/reactor-siemens.pdf</a></li>
<li> Hashed and Hierarchical Timing Wheels, Thesis, by George Varghese and Anthony Lauck, <a href="http://www.cs.columbia.edu/%7Enahum/w6998/papers/ton97-timing-wheels.pdf">http://www.cs.columbia.edu/~nahum/w6998/papers/ton97-timing-wheels.pdf</a></li>
<li> Netty Best Practices, Slides, by Norman Maurer, <a href="http://normanmaurer.me/presentations/2014-facebook-eng-netty/slides.html">http://normanmaurer.me/presentations/2014-facebook-eng-netty/slides.html</a></li>
<li> NSF-RPC的优化过程，博客文章，来自毕玄，<a href="http://bluedavy.me/?p=384">http://bluedavy.me/?p=384</a></li>
<li> Netty 源码分析系列 ，博客文章，来自永顺，<a href="https://segmentfault.com/a/1190000007282628">https://segmentfault.com/a/1190000007282628</a></li>
<li> 《Netty权威指南》，书籍，来自李林锋</li>
<li> 《Netty实战》，书籍，来自Norman Maurer等著，何品翻译</li>
</ol>

<h3 id="toc_18"><strong>附文中提到的一些链接地址信息</strong></h3>

<ol>
<li> Gecko: <a href="https://github.com/killme2008/gecko">https://github.com/killme2008/gecko</a></li>
<li> Mina: <a href="http://mina.apache.org/">http://mina.apache.org/</a></li>
<li> Netty: <a href="http://netty.io/">http://netty.io/</a></li>
<li> Stackoverflow - Do we need more than a single thread for boss group?：<a href="https://stackoverflow.com/questions/22280916/do-we-need-more-than-a-single-thread-for-boss-group">https://stackoverflow.com/questions/22280916/do-we-need-more-than-a-single-thread-for-boss-group</a></li>
<li> [#3218] Add ChannelPool abstraction and implementations：<a href="https://github.com/netty/netty/pull/3607">https://github.com/netty/netty/pull/3607</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[揭秘JDBC超时机制]]></title>
    <link href="http://panlw.github.io/15277833297034.html"/>
    <updated>2018-06-01T00:15:29+08:00</updated>
    <id>http://panlw.github.io/15277833297034.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzAxNjM2MTk0Ng==&amp;mid=2247484354&amp;idx=1&amp;sn=6179c839cb309b03421f39b5b65a2374&amp;chksm=9bf4b377ac833a61acd0563fd3a010a18ba7dfd749a4dac933097e153ba99aa322828510e55c&amp;mpshare=1&amp;scene=23&amp;srcid=0531q9DrnbcTctSQuYmLHhtJ%23rd">原文地址</a></p>
</blockquote>

<p>恰当的JDBC超时设置能够有效地减少服务失效的时间。本文将对数据库的各种超时设置及其设置方法做介绍。</p>

<h3 id="toc_0">真实案例：应用服务器在遭到DDos攻击后无法响应</h3>

<p>在遭到DDos攻击后，整个服务都垮掉了。由于第四层交换机不堪重负，网络变得无法连接，从而导致业务系统也无法正常运转。安全组很快屏蔽了所有的DDos攻击，并恢复了网络，但业务系统却还是无法工作。 通过分析系统的thread dump发现，业务系统停在了JDBC API的调用上。20分钟后，系统仍处于WAITING状态，无法响应。30分钟后，系统抛出异常，服务恢复正常。</p>

<blockquote>
<p>为什么我们明明将query timeout设置成了3秒，系统却持续了30分钟的WAITING状态？为什么30分钟后系统又恢复正常了？</p>
</blockquote>

<p>当你对理解了JDBC的超时设置后，就能找到问题的答案。</p>

<h3 id="toc_1">为什么我们要了解JDBC</h3>

<p>当遇到性能问题或系统出错时，业务系统和数据库通常是我们最关心的两个部分。在公司里，这两个部分是交由两个不同的部门来负责的，因此各个部门都会集中精力地在自身领域内寻找问题，这样的话，在业务系统和数据库之间的部分就会成为一个盲区。对于Java应用而言，这个盲区就是DBCP数据库连接池和JDBC，本文将集中介绍JDBC。</p>

<h3 id="toc_2">什么是JDBC</h3>

<p>JDBC是Java应用中用来连接关系型数据库的标准API。Sun公司一共定义了4种类型的JDBC，我们主要使用的是第4种，该类型的Driver完全由Java代码实现，通过使用socket与数据库进行通信。</p>

<p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt=""/></p>

<p>图1 JDBC Type 4.</p>

<p>第4种类型的JDBC通过socket对字节流进行处理，因此也会有一些基本网络操作，类似于<code>HttpClient</code>这种用于网络操作的代码库。当在网络操作中遇到问题的时候，将会消耗大量的cpu资源，并且失去响应超时。如果你之前用过HttpClient，那么你一定遇到过未设置timeout造成的错误。同样，第4种类型的JDBC，若没有合理地设置socket timeout，也会有相同的错误——连接被阻塞。</p>

<p>接下来，就让我们来学习一下如何正确地设置socket timeout，以及需要考虑的问题。</p>

<p>应用与数据库间的timeout层级 </p>

<p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt=""/></p>

<p>图2 Timeout Class.</p>

<p>上图展示了简化后应用与数据库间的timeout层级。（译者注：WAS/BLOC是作者公司的具体应用名称，无需深究） 高级别的timeout依赖于低级别的timeout，只有当低级别的timeout无误时，高级别的timeout才能确保正常。例如，当socket timeout出现问题时，高级别的statement timeout和transaction timeout都将失效。</p>

<p>我们收到的很多评论中提到：</p>

<blockquote>
<p>即使设置了statement timeout，当网络出错时，应用也无法从错误中恢复。</p>
</blockquote>

<p><strong>statement timeout无法处理网络连接失败时的超时，它能做的仅仅是限制statement的操作时间</strong>。网络连接失败时的timeout必须交由JDBC来处理。</p>

<p>JDBC的socket timeout会受到操作系统socket timeout设置的影响，这就解释了为什么在之前的案例中，JDBC连接会在网络出错后阻塞30分钟，然后又奇迹般恢复，即使我们并没有对JDBC的socket timeout进行设置。</p>

<p>DBCP连接池位于图2的左侧，你会发现timeout层级与DBCP是相互独立的。DBCP负责的是数据库连接的创建和管理，并不干涉timeout的处理。当连接在DBCP中创建，或是DBCP发送校验query检查连接有效性的时候，socket timeout将会影响这些过程，但并不直接对应用造成影响。</p>

<p>当在应用中调用DBCP的<code>getConnection()</code>方法时，你可以设置获取数据库连接的超时时间，但是这和JDBC的timeout毫不相关。</p>

<p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt=""/></p>

<p>图3 Timeout for Each Levels.</p>

<h3 id="toc_3">什么是Transaction Timeout？</h3>

<p><strong>transaction timeout</strong>一般存在于框架（Spring, EJB）或应用级。</p>

<p>transaction timeout或许是个相对陌生的概念，简单地说，transaction timeout就是“<code>Statement Timeout * N（需要执行的statement数量） + @（垃圾回收等其他时间）</code>”。transaction timeout用来限制执行statement的总时长。</p>

<p>例如，假设执行一个statement需要0.1秒，那么执行少量statement不会有什么问题，但若是要执行100,000个statement则需要10,000秒（约7个小时）。这时，transaction timeout就派上用场了。EJB CMT (Container Managed Transaction)就是一种典型的实现，它提供了多种方法供开发者选择。但我们并不使用EJB，Spring的transaction timeout设置会更常用一些。在Spring中，你可以使用下面展示的XML或是在源码中使用<code>@Transactional</code>注解来进行设置。</p>

<pre><code>&lt;tx:attributes&gt;  
        &lt;tx:method name=“…” timeout=“3″/&gt;  
&lt;/tx:attributes&gt;
</code></pre>

<p>Spring提供的transaction timeout配置非常简单，它会记录每个事务的开始时间和消耗时间，当特定的事件发生时就会对消耗时间做校验，当超出timeout值时将抛出异常。</p>

<p>Spring中，数据库连接被保存在ThreadLocal里，这被称为<code>事务同步（Transaction Synchronization）</code>，与此同时，事务的开始时间和消耗时间也被保存下来。当使用这种代理连接创建statement时，就会校验事务的消耗时间。</p>

<p>EJB CMT的实现方式与之类似，其结构本身也十分简单。 当你选用的容器或框架并不支持transaction timeout这一特性，你可以考虑自己来实现。transaction timeout并没有标准的API。Lucy框架的1.5和1.6版本都不支持transaction timeout，但是你可以通过使用Spring的Transaction Manager来达到与之同样的效果。 假设某个事务中包含5个statement，每个statement的执行时间是200ms，其他业务逻辑的执行时间是100ms，那么transaction timeout至少应该设置为1,100ms（200 * 5 + 100）。</p>

<p>什么是Statement Timeout？</p>

<p>statement timeout用来限制statement的执行时长，timeout的值通过调用JDBC的java.sql.Statement.setQueryTimeout(int timeout) API进行设置。不过现在开发者已经很少直接在代码中设置，而多是通过框架来进行设置。 以iBatis为例，statement timeout的默认值可以通过sql-map-config.xml中的defaultStatementTimeout 属性进行设置。同时，你还可以设置sqlmap中select，insert，update标签的timeout属性，从而对不同sql语句的超时时间进行独立的配置。 如果你使用的是Lucy1.5或1.6版本，通过设置queryTimeout属性可以在datasource层面对statement timeout进行设置。 statement timeout的具体值需要依据应用本身的特性而定，并没有可供推荐的配置。</p>

<p>JDBC的statement timeout处理过程<br/>
不同的关系型数据库，以及不同的JDBC驱动，其statement timeout处理过程会有所不同。其中，Oracle和MS SQLServer的处理相类似，MySQL和CUBRID类似。</p>

<p><strong>Oracle JDBC Statement的QueryTimeout处理过程</strong></p>

<ol>
<li><p>通过调用Connection的createStatement()方法创建statement</p></li>
<li><p>调用Statement的executeQuery()方法</p></li>
<li><p>statement通过自身connection将query发送给Oracle数据库</p></li>
<li><p>statement在OracleTimeoutPollingThread（每个classloader一个）上进行注册</p></li>
<li><p>达到超时时间</p></li>
<li><p>OracleTimeoutPollingThread调用OracleStatement的cancel()方法</p></li>
<li><p>通过connection向正在执行的query发送cancel消息</p></li>
</ol>

<p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt=""/></p>

<p>图4 Query Timeout Execution Process for Oracle JDBC Statement.</p>

<p><strong>JTDS (MS SQLServer) Statement的QueryTimeout处理过程</strong></p>

<ol>
<li><p>通过调用Connection的createStatement()方法创建statement</p></li>
<li><p>调用Statement的executeQuery()方法</p></li>
<li><p>statement通过自身connection将query发送给MS SqlServer数据库</p></li>
<li><p>statement在TimerThread上进行注册</p></li>
<li><p>达到超时时间</p></li>
<li><p>TimerThread调用JtdsStatement实例中的<code>TsdCore.cancel()</code>方法</p></li>
<li><p>通过ConnectionJDBC向正在执行的query发送cancel消息</p></li>
</ol>

<p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt=""/></p>

<p>图5 QueryTimeout Execution Process for JTDS (MS SQLServer) Statement.</p>

<p><strong>MySQL JDBC Statement的QueryTimeout处理过程（5.0.8）</strong></p>

<ol>
<li><p>通过调用<code>Connection.createStatement()</code>方法创建statement</p></li>
<li><p>调用<code>Statement.executeQuery()</code>方法</p></li>
<li><p>statement通过自身connection将query发送给MySQL数据库</p></li>
<li><p>statement创建一个新的timeout-execution线程用于超时处理</p></li>
<li><p>5.1版本后改为每个connection分配一个timeout-execution线程</p></li>
<li><p>向timeout-execution线程进行注册</p></li>
<li><p>达到超时时间</p></li>
<li><p>timeout-execution线程创建一个和statement配置相同的connection</p></li>
<li><p>使用新创建的connection向超时query发送cancel query（<code>KILL QUERY</code> “connectionId”）</p></li>
</ol>

<p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt=""/></p>

<p>图6 QueryTimeout Execution Process for MySQL JDBC Statement (5.0.8).</p>

<p><strong>CUBRID JDBC Statement的QueryTimeout处理过程</strong></p>

<ol>
<li><p>通过调用Connection的createStatement()方法创建statement</p></li>
<li><p>调用Statement的executeQuery()方法</p></li>
<li><p>statement通过自身connection将query发送给CUBRID数据库</p></li>
<li><p>statement创建一个新的timeout-execution线程用于超时处理</p></li>
<li><p>5.1版本后改为每个connection分配一个timeout-execution线程 6. 向timeout-execution线程进行注册</p></li>
<li><p>达到超时时间</p></li>
<li><p>TimerThread调用JtdsStatement实例中的TsdCore.cancel()方法</p></li>
<li><p>timeout-execution线程创建一个和statement配置相同的connection</p></li>
<li><p>使用新创建的connection向超时query发送cancel消息</p></li>
</ol>

<p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt=""/></p>

<p>图7 QueryTimeout Execution Process for CUBRID JDBC Statement.</p>

<p>什么是JDBC的socket timeout？</p>

<p>第4种类型的JDBC使用socket与数据库连接，数据库并不对应用与数据库间的连接超时进行处理。 JDBC的socket timeout在数据库被突然停掉或是发生网络错误（由于设备故障等原因）时十分重要。由于TCP/IP的结构原因，socket没有办法探测到网络错误，因此应用也无法主动发现数据库连接断开。如果没有设置socket timeout的话，应用在数据库返回结果前会无期限地等下去，这种连接被称为dead connection。 为了避免dead connections，socket必须要有超时配置。socket timeout可以通过JDBC设置，socket timeout能够避免应用在发生网络错误时产生无休止等待的情况，缩短服务失效的时间。</p>

<p>不推荐使用socket timeout来限制statement的执行时长，因此socket timeout的值必须要高于statement timeout，否则，socket timeout将会先生效，这样statement timeout就变得毫无意义，也无法生效。</p>

<p>下面展示了socket timeout的两个设置项，不同的JDBC驱动其配置方式会有所不同。</p>

<ul>
<li><p>socket连接时的timeout：通过Socket.connect(SocketAddress endpoint, int timeout)设置</p></li>
<li><p>socket读写时的timeout：通过Socket.setSoTimeout(int timeout)设置</p></li>
</ul>

<p>通过查看CUBRID，MySQL，MS SQL Server (JTDS)和Oracle的JDBC驱动源码，我们发现所有的驱动内部都是使用上面的2个API来设置socket timeout的。</p>

<p>下面是不同驱动的socket timeout配置方式。</p>

<p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt=""/></p>

<ul>
<li><p><code>connectTimeout</code>和<code>socketTimeout</code>的默认值为0时，timeout不生效。</p></li>
<li><p>除了调用DBCP的API以外，还可以通过properties属性进行配置。</p></li>
</ul>

<p>通过properties属性进行配置时，需要传入key为“connectionProperties”的键值对，value的格式为“[propertyName=property;]*”。下面是iBatis中的properties配置。<br/>
Xml代码</p>

<pre><code>&lt;transactionManager type=“JDBC”&gt;  
  &lt;dataSource type=“com.nhncorp.lucy.db.DbcpDSFactory”&gt;  
     ….  
     &lt;property name=“connectionProperties” value=“oracle.net.CONNECT_TIMEOUT=6000;oracle.jdbc.ReadTimeout=6000″/&gt;   
  &lt;/dataSource&gt;  
&lt;/transactionManager&gt;
</code></pre>

<p>操作系统的socket timeout配置</p>

<p>如果不设置socket timeout或connect timeout，应用多数情况下是无法发现网络错误的。因此，当网络错误发生后，在连接重新连接成功或成功接收到数据之前，应用会无限制地等下去。但是，通过本文开篇处的实际案例我们发现，30分钟后应用的连接问题奇迹般的解决了，这是因为操作系统同样能够对socket timeout进行配置。公司的Linux服务器将socket timeout设置为了30分钟，从而会在操作系统的层面对网络连接做校验，因此即使JDBC的socket timeout设置为0，由网络错误造成的数据库连接问题的持续时间也不会超过30分钟。</p>

<p>通常，应用会在调用Socket.read()时由于网络问题被阻塞住，而很少在调用Socket.write()时进入waiting状态，这取决于网络构成和错误类型。当Socket.write()被调用时，数据被写入到操作系统内核的缓冲区，控制权立即回到应用手上。因此，一旦数据被写入内核缓冲区，Socket.write()调用就必然会成功。但是，如果系统内核缓冲区由于某种网络错误而满了的话，Socket.write()也会进入waiting状态。这种情况下，操作系统会尝试重新发包，当达到重试的时间限制时，将产生系统错误。在我们公司，重新发包的超时时间被设置为15分钟。</p>

<p>至此，我已经对JDBC的内部操作做了讲解，希望能够让大家学会如何正确的配置超时时间，从而减少错误的发生。<br/>
最后，我将列出一些常见的问题。</p>

<h3 id="toc_4">FAQ</h3>

<blockquote>
<p><strong>Q1. 我已经使用Statement.setQueryTimeout()方法设置了查询超时，但在网络出错时并没有产生作用。</strong><br/>
➔ 查询超时仅在socket timeout生效的前提下才有效，它并不能用来解决外部的网络错误，要解决这种问题，必须设置JDBC的socket timeout。</p>

<p><strong>Q2.  transaction timeout，statement timeout和socket timeout和DBCP的配置有什么关系？</strong> <br/>
➔ 当通过DBCP获取数据库连接时，除了DBCP获取连接时的waitTimeout配置以外，其他配置对JDBC没有什么影响。</p>

<p><strong>Q3. 如果设置了JDBC的socket timeout，那DBCP连接池中处于IDLE状态的连接是否也会在达到超时时间后被关闭？</strong><br/>
➔ 不会。socket的设置只会在产生数据读写时生效，而不会对DBCP中的IDLE连接产生影响。当DBCP中发生新连接创建，老的IDLE连接被移除，或是连接有效性校验的时候，socket设置会对其产生一定的影响，但除非发生网络问题，否则影响很小。</p>

<p><strong>Q4. socket timeout应该设置为多少？ </strong><br/>
➔ 就像我在正文中提的那样，socket timeout必须高于statement timeout，但并没有什么推荐值。在发生网络错误的时候，socket timeout将会生效，但是再小心的配置也无法避免网络错误的发生，只是在网络错误发生后缩短服务失效的时间（如果网络恢复正常的话）。</p>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[【原创】分布式之redis复习精讲]]></title>
    <link href="http://panlw.github.io/15277832551515.html"/>
    <updated>2018-06-01T00:14:15+08:00</updated>
    <id>http://panlw.github.io/15277832551515.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="http://www.cnblogs.com/rjzheng/p/9096228.html">http://www.cnblogs.com/rjzheng/p/9096228.html</a></p>
</blockquote>

<h2 id="toc_0">引言</h2>

<h3 id="toc_1">为什么写这篇文章?</h3>

<p>博主的<a href="http://www.cnblogs.com/rjzheng/p/8994962.html">《分布式之消息队列复习精讲》</a>得到了大家的好评，内心诚惶诚恐，想着再出一篇关于复习精讲的文章。但是还是要说明一下，复习精讲的文章偏面试准备，真正在开发过程中，还是脚踏实地，一步一个脚印，不要投机取巧。<br/>
考虑到绝大部分写业务的程序员，在实际开发中使用 redis 的时候，只会 setvalue 和 getvalue 两个操作，对 redis 整体缺乏一个认知。又恰逢博主某个同事下周要去培训 redis，所以博主斗胆以 redis 为题材，对 redis 常见问题做一个总结，希望能够弥补大家的知识盲点。</p>

<h3 id="toc_2">复习要点?</h3>

<p>本文围绕以下几点进行阐述<br/>
1、为什么使用 redis<br/>
2、使用 redis 有什么缺点<br/>
3、单线程的 redis 为什么这么快<br/>
4、redis 的数据类型，以及每种数据类型的使用场景<br/>
5、redis 的过期策略以及内存淘汰机制<br/>
6、redis 和数据库双写一致性问题<br/>
7、如何应对缓存穿透和缓存雪崩问题<br/>
8、如何解决 redis 的并发竞争问题</p>

<h2 id="toc_3">正文</h2>

<h3 id="toc_4">1、为什么使用 redis</h3>

<p><strong>分析</strong>: 博主觉得在项目中使用 redis，主要是从两个角度去考虑: <strong>性能</strong>和<strong>并发</strong>。当然，redis 还具备可以做分布式锁等其他功能，但是如果只是为了分布式锁这些其他功能，完全还有其他中间件 (如 zookpeer 等) 代替，并不是非要使用 redis。因此，这个问题主要从性能和并发两个角度去答。<br/>
<strong>回答</strong>: 如下所示，分为两点<br/>
<strong>（一）性能</strong><br/>
如下图所示，我们在碰到需要执行耗时特别久，且结果不频繁变动的 SQL，就特别适合将运行结果放入缓存。这样，后面的请求就去缓存中读取，使得请求能够<strong>迅速响应</strong>。<br/>
<img src="https://images.cnblogs.com/cnblogs_com/rjzheng/1202350/o_redis1.png" alt=""/><br/>
<strong>题外话：</strong>忽然想聊一下这个<strong>迅速响应</strong>的标准。其实根据交互效果的不同，这个响应时间没有固定标准。不过曾经有人这么告诉我:&quot; 在理想状态下，我们的页面跳转需要在<strong>瞬间</strong>解决，对于页内操作则需要在<strong>刹那</strong>间解决。另外，超过<strong>一弹指</strong>的耗时操作要有进度提示，并且可以随时中止或取消，这样才能给用户最好的体验。&quot;<br/>
那么<strong>瞬间、刹那、一弹指</strong>具体是多少时间呢？<br/>
根据《摩诃僧祗律》记载</p>

<pre><code>一刹那者为一念，二十念为一瞬，二十瞬为一弹指，二十弹指为一罗预，二十罗预为一须臾，一日一夜有三十须臾。
</code></pre>

<p>那么，经过周密的计算，一<strong>瞬间</strong>为 0.36 秒, 一<strong>刹那</strong>有 0.018 秒. 一<strong>弹指</strong>长达 7.2 秒。<br/>
<strong>（二）并发</strong><br/>
如下图所示，在大并发的情况下，所有的请求直接访问数据库，数据库会出现连接异常。这个时候，就需要使用 redis 做一个缓冲操作，让请求先访问到 redis，而不是直接访问数据库。<br/>
<img src="https://images.cnblogs.com/cnblogs_com/rjzheng/1202350/o_redis2.png" alt=""/></p>

<h3 id="toc_5">2、使用 redis 有什么缺点</h3>

<p><strong>分析</strong>: 大家用 redis 这么久，这个问题是必须要了解的，基本上使用 redis 都会碰到一些问题，常见的也就几个。<br/>
<strong>回答</strong>: 主要是四个问题<br/>
(一) 缓存和数据库双写一致性问题<br/>
(二) 缓存雪崩问题<br/>
(三) 缓存击穿问题<br/>
(四) 缓存的并发竞争问题<br/>
这四个问题，我个人是觉得在项目中，比较常遇见的，具体解决方案，后文给出。</p>

<h3 id="toc_6">3、单线程的 redis 为什么这么快</h3>

<p><strong>分析</strong>: 这个问题其实是对 redis 内部机制的一个考察。其实根据博主的面试经验，很多人其实都不知道 redis 是单线程工作模型。所以，这个问题还是应该要复习一下的。<br/>
<strong>回答</strong>: 主要是以下三点<br/>
(一) 纯内存操作<br/>
(二) 单线程操作，避免了频繁的上下文切换<br/>
(三) 采用了非阻塞 <strong>I/O 多路复用机制</strong></p>

<p><strong>题外话：</strong>我们现在要仔细的说一说 I/O 多路复用机制，因为这个说法实在是太通俗了，通俗到一般人都不懂是什么意思。博主打一个比方：小曲在 S 城开了一家快递店，负责同城快送服务。小曲因为资金限制，雇佣了<strong>一批</strong>快递员，然后小曲发现资金不够了，只够买<strong>一辆</strong>车送快递。<br/>
<strong>经营方式一</strong><br/>
客户每送来一份快递，小曲就让一个快递员盯着，然后快递员开车去送快递。慢慢的小曲就发现了这种经营方式存在下述问题</p>

<ul>
<li>  几十个快递员基本上时间都花在了抢车上了，大部分快递员都处在闲置状态，谁抢到了车，谁就能去送快递</li>
<li>  随着快递的增多，快递员也越来越多，小曲发现快递店里越来越挤，没办法雇佣新的快递员了</li>
<li>  快递员之间的协调很花时间</li>
</ul>

<p>综合上述缺点，小曲痛定思痛，提出了下面的经营方式<br/>
<strong>经营方式二</strong><br/>
小曲只雇佣一个快递员。然后呢，客户送来的快递，小曲按<strong>送达地点</strong>标注好，然后<strong>依次</strong>放在一个地方。最后，那个快递员<strong>依次</strong>的去取快递，一次拿一个，然后开着车去送快递，送好了就回来拿下一个快递。</p>

<p><strong>对比</strong><br/>
上述两种经营方式对比，是不是明显觉得第二种，效率更高，更好呢。在上述比喻中:</p>

<ul>
<li>  每个快递员 ------------------&gt; 每个线程</li>
<li>  每个快递 --------------------&gt; 每个 socket(I/O 流)</li>
<li>  快递的送达地点 --------------&gt;socket 的不同状态</li>
<li>  客户送快递请求 --------------&gt; 来自客户端的请求</li>
<li>  小曲的经营方式 --------------&gt; 服务端运行的代码</li>
<li>  一辆车 ----------------------&gt;CPU 的核数</li>
</ul>

<p>于是我们有如下结论<br/>
1、经营方式一就是传统的并发模型，每个 I/O 流 (快递) 都有一个新的线程 (快递员) 管理。<br/>
2、经营方式二就是 I/O 多路复用。只有单个线程 (一个快递员)，通过跟踪每个 I/O 流的状态 (每个快递的送达地点)，来管理多个 I/O 流。</p>

<p>下面类比到真实的 redis 线程模型，如图所示<br/>
<img src="https://images.cnblogs.com/cnblogs_com/rjzheng/1202350/o_redis3.png" alt=""/><br/>
参照上图，简单来说，就是。我们的 redis-client 在操作的时候，会产生具有不同事件类型的 socket。在服务端，有一段 I/0 多路复用程序，将其置入队列之中。然后，文件事件分派器，依次去队列中取，转发到不同的事件处理器中。<br/>
需要说明的是，这个 I/O 多路复用机制，redis 还提供了 select、epoll、evport、kqueue 等多路复用函数库，大家可以自行去了解。</p>

<h3 id="toc_7">4、redis 的数据类型，以及每种数据类型的使用场景</h3>

<p><strong>分析</strong>：是不是觉得这个问题很基础，其实我也这么觉得。然而根据面试经验发现，至少百分八十的人答不上这个问题。建议，在项目中用到后，再类比记忆，体会更深，不要硬记。基本上，一个合格的程序员，五种类型都会用到。<br/>
<strong>回答</strong>：一共五种<br/>
(一)String<br/>
这个其实没啥好说的，最常规的 set/get 操作，value 可以是 String 也可以是数字。一般做<strong>一些复杂的计数功能的缓存。</strong><br/>
(二)hash<br/>
这里 value 存放的是结构化的对象，比较方便的就是操作其中的某个字段。博主在做<strong>单点登录</strong>的时候，就是用这种数据结构存储用户信息，以 cookieId 作为 key，设置 30 分钟为缓存过期时间，能很好的模拟出类似 session 的效果。<br/>
(三)list<br/>
使用 List 的数据结构，可以<strong>做简单的消息队列的功能</strong>。另外还有一个就是，可以利用 lrange 命令，<strong>做基于 redis 的分页功能</strong>，性能极佳，用户体验好。<br/>
(四)set<br/>
因为 set 堆放的是一堆不重复值的集合。所以可以做<strong>全局去重的功能</strong>。为什么不用 JVM 自带的 Set 进行去重？因为我们的系统一般都是集群部署，使用 JVM 自带的 Set，比较麻烦，难道为了一个做一个全局去重，再起一个公共服务，太麻烦了。<br/>
另外，就是利用交集、并集、差集等操作，可以<strong>计算共同喜好，全部的喜好，自己独有的喜好等功能</strong>。<br/>
(五)sorted set<br/>
sorted set 多了一个权重参数 score, 集合中的元素能够按 score 进行排列。可以做<strong>排行榜应用，取 TOP N 操作</strong>。另外，参照另一篇<a href="https://www.cnblogs.com/rjzheng/p/8972725.html">《分布式之延时任务方案解析》</a>，该文指出了 sorted set 可以用来做<strong>延时任务</strong>。最后一个应用就是可以做<strong>范围查找</strong>。</p>

<h3 id="toc_8">5、redis 的过期策略以及内存淘汰机制</h3>

<p><strong>分析</strong>: 这个问题其实相当重要，到底 redis 有没用到家，这个问题就可以看出来。比如你 redis 只能存 5G 数据，可是你写了 10G，那会删 5G 的数据。怎么删的，这个问题思考过么？还有，你的数据已经设置了过期时间，但是时间到了，内存占用率还是比较高，有思考过原因么?<br/>
<strong>回答</strong>:<br/>
redis 采用的是定期删除 + 惰性删除策略。<br/>
<strong>为什么不用定时删除策略?</strong><br/>
定时删除, 用一个定时器来负责监视 key, 过期则自动删除。虽然内存及时释放，但是十分消耗 CPU 资源。在大并发请求下，CPU 要将时间应用在处理请求，而不是删除 key, 因此没有采用这一策略.<br/>
<strong>定期删除 + 惰性删除是如何工作的呢?</strong><br/>
定期删除，redis 默认每个 100ms 检查，是否有过期的 key, 有过期 key 则删除。需要说明的是，redis 不是每个 100ms 将所有的 key 检查一次，而是随机抽取进行检查 (如果每隔 100ms, 全部 key 进行检查，redis 岂不是卡死)。因此，如果只采用定期删除策略，会导致很多 key 到时间没有删除。<br/>
于是，惰性删除派上用场。也就是说在你获取某个 key 的时候，redis 会检查一下，这个 key 如果设置了过期时间那么是否过期了？如果过期了此时就会删除。<br/>
<strong>采用定期删除 + 惰性删除就没其他问题了么?</strong><br/>
不是的，如果定期删除没删除 key。然后你也没即时去请求 key，也就是说惰性删除也没生效。这样，redis 的内存会越来越高。那么就应该采用<strong>内存淘汰机制</strong>。<br/>
在 redis.conf 中有一行配置</p>

<pre><code># maxmemory-policy volatile-lru
</code></pre>

<p>该配置就是配内存淘汰策略的 (什么，你没配过？好好反省一下自己)<br/>
1）noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。<strong>应该没人用吧。</strong><br/>
2）allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key。<strong>推荐使用，目前项目在用这种。</strong><br/>
3）allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个 key。<strong>应该也没人用吧，你不删最少使用 Key, 去随机删。</strong><br/>
4）volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key。<strong>这种情况一般是把 redis 既当缓存，又做持久化存储的时候才用。不推荐</strong><br/>
5）volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 key。<strong>依然不推荐</strong><br/>
6）volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除。<strong>不推荐</strong><br/>
ps：如果没有设置 expire 的 key, 不满足先决条件 (prerequisites); 那么 volatile-lru, volatile-random 和 volatile-ttl 策略的行为, 和 noeviction(不删除) 基本上一致。</p>

<h3 id="toc_9">6、redis 和数据库双写一致性问题</h3>

<p><strong>分析</strong>: 一致性问题是分布式常见问题，还可以再分为最终一致性和强一致性。数据库和缓存双写，就必然会存在不一致的问题。答这个问题，先明白一个前提。就是<strong>如果对数据有强一致性要求，不能放缓存。</strong>我们所做的一切，只能保证最终一致性。另外，我们所做的方案其实从根本上来说，只能说<strong>降低不一致发生的概率</strong>，无法完全避免。因此，有强一致性要求的数据，不能放缓存。<br/>
<strong>回答</strong>:<a href="https://www.cnblogs.com/rjzheng/p/9041659.html">《分布式之数据库和缓存双写一致性方案解析》</a>给出了详细的分析，在这里简单的说一说。首先，采取正确更新策略，先更新数据库，再删缓存。其次，因为可能存在删除缓存失败的问题，提供一个补偿措施即可，例如利用消息队列。</p>

<h3 id="toc_10">7、如何应对缓存穿透和缓存雪崩问题</h3>

<p><strong>分析</strong>: 这两个问题，说句实在话，一般中小型传统软件企业，很难碰到这个问题。如果有大并发的项目，流量有几百万左右。这两个问题一定要深刻考虑。<br/>
<strong>回答</strong>: 如下所示<br/>
<strong>缓存穿透</strong>，即黑客故意去请求缓存中不存在的数据，导致所有的请求都怼到数据库上，从而数据库连接异常。<br/>
<strong>解决方案</strong>:<br/>
(一) 利用互斥锁，缓存失效的时候，先去获得锁，得到锁了，再去请求数据库。没得到锁，则休眠一段时间重试<br/>
(二) 采用异步更新策略，无论 key 是否取到值，都直接返回。value 值中维护一个缓存失效时间，缓存如果过期，异步起一个线程去读数据库，更新缓存。需要做<strong>缓存预热</strong> (项目启动前，先加载缓存) 操作。<br/>
(三) 提供一个能迅速判断请求是否有效的拦截机制，比如，利用布隆过滤器，内部维护一系列合法有效的 key。迅速判断出，请求所携带的 Key 是否合法有效。如果不合法，则直接返回。<br/>
<strong>缓存雪崩</strong>，即缓存同一时间大面积的失效，这个时候又来了一波请求，结果请求都怼到数据库上，从而导致数据库连接异常。<br/>
<strong>解决方案</strong>:<br/>
(一) 给缓存的失效时间，加上一个随机值，避免集体失效。<br/>
(二) 使用互斥锁，但是该方案吞吐量明显下降了。<br/>
(三) 双缓存。我们有两个缓存，缓存 A 和缓存 B。缓存 A 的失效时间为 20 分钟，缓存 B 不设失效时间。自己做缓存预热操作。然后细分以下几个小点</p>

<ul>
<li>  I 从缓存 A 读数据库，有则直接返回</li>
<li>  II A 没有数据，直接从 B 读数据，直接返回，并且异步启动一个更新线程。</li>
<li>  III 更新线程同时更新缓存 A 和缓存 B。</li>
</ul>

<h3 id="toc_11">8、如何解决 redis 的并发竞争 key 问题</h3>

<p><strong>分析</strong>: 这个问题大致就是，同时有多个子系统去 set 一个 key。这个时候要注意什么呢？大家思考过么。需要说明一下，博主提前百度了一下，发现答案基本都是推荐用 redis 事务机制。博主<strong>不推荐使用 redis 的事务机制。</strong>因为我们的生产环境，基本都是 redis 集群环境，做了数据分片操作。你一个事务中有涉及到多个 key 操作的时候，这多个 key 不一定都存储在同一个 redis-server 上。因此，<strong>redis 的事务机制，十分鸡肋。</strong><br/>
<strong>回答:</strong> 如下所示<br/>
(1) 如果对这个 key 操作，<strong>不要求顺序</strong><br/>
这种情况下，准备一个分布式锁，大家去抢锁，抢到锁就做 set 操作即可，比较简单。<br/>
(2) 如果对这个 key 操作，<strong>要求顺序</strong><br/>
假设有一个 key1, 系统 A 需要将 key1 设置为 valueA, 系统 B 需要将 key1 设置为 valueB, 系统 C 需要将 key1 设置为 valueC.<br/>
期望按照 key1 的 value 值按照 valueA--&gt;valueB--&gt;valueC 的顺序变化。这种时候我们在数据写入数据库的时候，需要保存一个时间戳。假设时间戳如下</p>

<pre><code>系统A key 1 {valueA  3:00}
系统B key 1 {valueB  3:05}
系统C key 1 {valueC  3:10}
</code></pre>

<p>那么，假设这会系统 B 先抢到锁，将 key1 设置为 {valueB 3:05}。接下来系统 A 抢到锁，发现自己的 valueA 的时间戳早于缓存中的时间戳，那就不做 set 操作了。以此类推。</p>

<p>其他方法，比如利用队列，将 set 方法变成串行访问也可以。总之，灵活变通。</p>

<h2 id="toc_12">总结</h2>

<p>本文对 redis 的常见问题做了一个总结。大部分是博主自己在工作中遇到，以及以前面试别人的时候，爱问的一些问题。另外，<strong>不推荐大家临时抱佛脚</strong>，真正碰到一些有经验的工程师，其实几下就能把你问懵。最后，希望大家有所收获吧。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[【RPC 专栏】深入理解 RPC 之动态代理篇]]></title>
    <link href="http://panlw.github.io/15277824520436.html"/>
    <updated>2018-06-01T00:00:52+08:00</updated>
    <id>http://panlw.github.io/15277824520436.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://www.cnkirito.moe/rpc-dynamic-proxy/">https://www.cnkirito.moe/rpc-dynamic-proxy/</a></p>
</blockquote>

<p>提到 JAVA 中的动态代理，大多数人都不会对 JDK 动态代理感到陌生，Proxy，InvocationHandler 等类都是 J2SE 中的基础概念。动态代理发生在服务调用方/客户端，RPC 框架需要解决的一个问题是：像调用本地接口一样调用远程的接口。于是如何组装数据报文，经过网络传输发送至服务提供方，屏蔽远程接口调用的细节，便是动态代理需要做的工作了。RPC 框架中的代理层往往是单独的一层，以方便替换代理方式（如 motan 代理层位于<code>com.weibo.api.motan.proxy</code> ，dubbo代理层位于 <code>com.alibaba.dubbo.common.bytecode</code> ）。</p>

<p>实现动态代理的方案有下列几种：</p>

<ul>
<li>  jdk 动态代理</li>
<li>  cglib 动态代理</li>
<li>  javassist 动态代理</li>
<li>  ASM 字节码</li>
<li>  javassist 字节码</li>
</ul>

<p>其中 cglib 底层实现依赖于 ASM，javassist 自成一派。由于 ASM 和 javassist 需要程序员直接操作字节码，导致使用门槛相对较高，但实际上他们的应用是非常广泛的，如 Hibernate 底层使用了 javassist（默认）和 cglib，Spring 使用了 cglib 和 jdk 动态代理。</p>

<p>RPC 框架无论选择何种代理技术，所需要完成的任务其实是固定的，不外乎‘整理报文’，‘确认网络位置’，‘序列化’,’网络传输’，‘反序列化’，’返回结果’…</p>

<h2 id="toc_0"><a href="#%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B%E7%9A%84%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0" title="技术选型的影响因素"></a>技术选型的影响因素</h2>

<p>框架中使用何种动态代理技术，影响因素也不少。</p>

<h3 id="toc_1"><a href="#%E6%80%A7%E8%83%BD" title="性能"></a>性能</h3>

<p>从早期 dubbo 的作者梁飞的博客 <a href="http://javatar.iteye.com/blog/814426">http://javatar.iteye.com/blog/814426</a> 中可以得知 dubbo 选择使用 javassist 作为动态代理方案主要考虑的因素是<strong>性能</strong>。</p>

<p>从其博客的测试结果来看 javassist &gt; cglib &gt; jdk 。但实际上他的测试过程稍微有点瑕疵：在 cglib 和 jdk 代理对象调用时，走的是反射调用，而在 javassist 生成的代理对象调用时，走的是直接调用（可以先阅读下梁飞大大的博客）。这意味着 cglib 和 jdk 慢的原因并不是由动态代理产生的，而是由反射调用产生的（顺带一提，很多人认为 jdk 动态代理的原理是反射，其实它的底层也是使用的字节码技术）。而最终我的测试结果，结论如下： javassist ≈ cglib &gt; jdk 。javassist 和 cglib 的效率基本持平 ，而他们两者的执行效率基本可以达到 jdk 动态代理的2倍（这取决于测试的机器以及 jdk 的版本，jdk1.8 相较于 jdk1.6 动态代理技术有了质的提升，所以并不是传闻中的那样：cglib 比 jdk 快 10倍）。文末会给出我的测试代码。</p>

<h3 id="toc_2"><a href="#%E4%BE%9D%E8%B5%96" title="依赖"></a>依赖</h3>

<blockquote>
<p>motan默认的实现是jdk动态代理，代理方案支持SPI扩展，可以自行扩展其他实现方式。</p>

<p>使用jdk做为默认，主要是减少core包依赖，性能不是唯一考虑因素。另外使用字节码方式javaassist性能比较优秀，动态代理模式下jdk性能也不会差多少。</p>

<p>– <strong>rayzhang0603</strong>(motan贡献者)</p>
</blockquote>

<p>motan 选择使用 jdk 动态代理，原因主要有两个：减少 motan-core 的依赖，方便。至于扩展性，dubbo 并没有预留出动态代理的扩展接口，而是写死了 bytecode ，这点上 motan 做的较好。</p>

<h3 id="toc_3"><a href="#%E6%98%93%E7%94%A8%E6%80%A7" title="易用性"></a>易用性</h3>

<p>从 dubbo 和 motan 的源码中便可以直观的看出两者的差距了，dubbo 为了使用 javassist 技术花费不少的精力，而 motan 使用 jdk 动态代理只用了一个类。dubbo 的设计者为了追求极致的性能而做出的工作是值得肯定的，motan 也预留了扩展机制，两者各有千秋。</p>

<h2 id="toc_4"><a href="#%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97" title="动态代理入门指南"></a>动态代理入门指南</h2>

<p>为了方便对比几种动态代理技术，先准备一个统一接口。</p>

<pre><code class="language-java">public interface BookApi {
    void sell();
}
</code></pre>

<h3 id="toc_5"><a href="#JDK%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86" title="JDK动态代理"></a>JDK动态代理</h3>

<pre><code class="language-java">private static BookApi createJdkDynamicProxy(final BookApi delegate) {
        BookApi jdkProxy = (BookApi) Proxy.newProxyInstance(ClassLoader.getSystemClassLoader(),
                new Class[]{BookApi.class}, new JdkHandler(delegate));
        return jdkProxy;
}

private static class JdkHandler implements InvocationHandler {

        final Object delegate;

        JdkHandler(Object delegate) {
            this.delegate = delegate;
        }

        @Override
        public Object invoke(Object object, Method method, Object[] objects)
                throws Throwable {
            //添加代理逻辑&lt;1&gt;
            if(method.getName().equals(&quot;sell&quot;)){
                System.out.print(&quot;&quot;);
            }
            return null;
//            return method.invoke(delegate, objects);
        }
</code></pre>

<p><1> 在真正的 RPC 调用中 ，需要填充‘整理报文’，‘确认网络位置’，‘序列化’,’网络传输’，‘反序列化’，’返回结果’等逻辑。</p>

<h3 id="toc_6"><a href="#Cglib%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86" title="Cglib动态代理"></a>Cglib动态代理</h3>

<pre><code class="language-java">private static BookApi createCglibDynamicProxy(final BookApi delegate) throws Exception {
        Enhancer enhancer = new Enhancer();
        enhancer.setCallback(new CglibInterceptor(delegate));
        enhancer.setInterfaces(new Class[]{BookApi.class});
        BookApi cglibProxy = (BookApi) enhancer.create();
        return cglibProxy;
    }

    private static class CglibInterceptor implements MethodInterceptor {

        final Object delegate;

        CglibInterceptor(Object delegate) {
            this.delegate = delegate;
        }

        @Override
        public Object intercept(Object object, Method method, Object[] objects,
                                MethodProxy methodProxy) throws Throwable {
            //添加代理逻辑
            if(method.getName().equals(&quot;sell&quot;)) {
                System.out.print(&quot;&quot;);
            }
            return null;
//            return methodProxy.invoke(delegate, objects);
        }
    }
</code></pre>

<p>和 JDK 动态代理的操作步骤没有太大的区别，只不过是替换了 cglib 的API而已。</p>

<p>需要引入 cglib 依赖：</p>

<pre><code class="language-xml">&lt;dependency&gt;
    &lt;groupId&gt;cglib&lt;/groupId&gt;
    &lt;artifactId&gt;cglib&lt;/artifactId&gt;
    &lt;version&gt;3.2.5&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<h3 id="toc_7"><a href="#Javassist%E5%AD%97%E8%8A%82%E7%A0%81" title="Javassist字节码"></a>Javassist字节码</h3>

<p>到了 javassist，稍微有点不同了。因为它是通过直接操作字节码来生成代理对象。</p>

<pre><code class="language-java">private static BookApi createJavassistBytecodeDynamicProxy() throws Exception {
    ClassPool mPool = new ClassPool(true);
    CtClass mCtc = mPool.makeClass(BookApi.class.getName() + &quot;JavaassistProxy&quot;);
    mCtc.addInterface(mPool.get(BookApi.class.getName()));
    mCtc.addConstructor(CtNewConstructor.defaultConstructor(mCtc));
    mCtc.addMethod(CtNewMethod.make(
            &quot;public void sell() { System.out.print(\&quot;\&quot;) ; }&quot;, mCtc));
    Class&lt;?&gt; pc = mCtc.toClass();
    BookApi bytecodeProxy = (BookApi) pc.newInstance();
    return bytecodeProxy;
}
</code></pre>

<p>需要引入 javassist 依赖：</p>

<pre><code class="language-xml">&lt;dependency&gt;
    &lt;groupId&gt;org.javassist&lt;/groupId&gt;
    &lt;artifactId&gt;javassist&lt;/artifactId&gt;
    &lt;version&gt;3.21.0-GA&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<h2 id="toc_8"><a href="#%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E6%B5%8B%E8%AF%95" title="动态代理测试"></a>动态代理测试</h2>

<p>测试环境：window i5 8g jdk1.8 cglib3.2.5 javassist3.21.0-GA</p>

<p>动态代理其实分成了两步：代理对象的创建，代理对象的调用。坊间流传的动态代理性能对比主要指的是后者；前者一般不被大家考虑，如果远程Refer的对象是单例的，其只会被创建一次，而如果是原型模式，多例对象的创建其实也是性能损耗的一个考虑因素（只不过远没有调用占比大）。</p>

<blockquote>
<p>Create JDK Proxy: 21 ms</p>

<p>Create CGLIB Proxy: 342 ms</p>

<p>Create Javassist Bytecode Proxy: 419 ms</p>
</blockquote>

<p>可能出乎大家的意料，JDK 创建动态代理的速度比后两者要快10倍左右。</p>

<p>下面是调用速度的测试：</p>

<blockquote>
<p>case 1:</p>

<p>JDK Proxy invoke cost 1912 ms</p>

<p>CGLIB Proxy invoke cost 1015 ms</p>

<p>JavassistBytecode Proxy invoke cost 1280 ms</p>

<p>case 2:</p>

<p>JDK Proxy invoke cost 1747 ms</p>

<p>CGLIB Proxy invoke cost 1234 ms</p>

<p>JavassistBytecode Proxy invoke cost 1175 ms</p>

<p>case 3:</p>

<p>JDK Proxy invoke cost 2616 ms</p>

<p>CGLIB Proxy invoke cost 1373 ms</p>

<p>JavassistBytecode Proxy invoke cost 1335 ms</p>
</blockquote>

<p>Jdk 的执行速度一定会慢于 Cglib 和 Javassist，但最慢也就2倍，并没有达到数量级的差距；Cglib 和 Javassist不相上下，差距不大（测试中偶尔发现Cglib实行速度会比平时慢10倍，不清楚是什么原因）</p>

<p>所以出于易用性和性能，私以为使用 Cglib 是一个很好的选择（性能和 Javassist 持平，易用性和 Jdk 持平）。</p>

<h2 id="toc_9"><a href="#%E5%8F%8D%E5%B0%84%E8%B0%83%E7%94%A8" title="反射调用"></a>反射调用</h2>

<p>既然提到了动态代理和 cglib ，顺带提一下反射调用如何加速的问题。RPC 框架中在 Provider 服务端需要根据客户端传递来的 className + method + param 来找到容器中的实际方法执行反射调用。除了反射调用外，还可以使用 Cglib 来加速。</p>

<h3 id="toc_10"><a href="#JDK%E5%8F%8D%E5%B0%84%E8%B0%83%E7%94%A8" title="JDK反射调用"></a>JDK反射调用</h3>

<pre><code class="language-java">Method method = serviceClass.getMethod(methodName, new Class[]{});
method.invoke(delegate, new Object[]{});
</code></pre>

<h3 id="toc_11"><a href="#Cglib%E8%B0%83%E7%94%A8" title="Cglib调用"></a>Cglib调用</h3>

<pre><code class="language-java">FastClass serviceFastClass = FastClass.create(serviceClass);
FastMethod serviceFastMethod = serviceFastClass.getMethod(methodName, new Class[]{});
serviceFastMethod.invoke(delegate, new Object[]{});
</code></pre>

<p>但实测效果发现 Cglib 并不一定比 JDK 反射执行速度快，还会跟具体的方法实现有关(大雾)。</p>

<h2 id="toc_12"><a href="#%E6%B5%8B%E8%AF%95%E4%BB%A3%E7%A0%81" title="测试代码"></a>测试代码</h2>

<pre><code class="language-java">public class Main {

    public static void main(String[] args) throws Exception {

        BookApi delegate = new BookApiImpl();
        long time = System.currentTimeMillis();
        BookApi jdkProxy = createJdkDynamicProxy(delegate);
        time = System.currentTimeMillis() - time;
        System.out.println(&quot;Create JDK Proxy: &quot; + time + &quot; ms&quot;);

        time = System.currentTimeMillis();
        BookApi cglibProxy = createCglibDynamicProxy(delegate);
        time = System.currentTimeMillis() - time;
        System.out.println(&quot;Create CGLIB Proxy: &quot; + time + &quot; ms&quot;);

        time = System.currentTimeMillis();
        BookApi javassistBytecodeProxy = createJavassistBytecodeDynamicProxy();
        time = System.currentTimeMillis() - time;
        System.out.println(&quot;Create JavassistBytecode Proxy: &quot; + time + &quot; ms&quot;);

        for (int i = 0; i &lt; 10; i++) {
            jdkProxy.sell();//warm
        }
        long start = System.currentTimeMillis();
        for (int i = 0; i &lt; 10000000; i++) {
            jdkProxy.sell();
        }
        System.out.println(&quot;JDK Proxy invoke cost &quot; + (System.currentTimeMillis() - start) + &quot; ms&quot;);

        for (int i = 0; i &lt; 10; i++) {
            cglibProxy.sell();//warm
        }
        start = System.currentTimeMillis();
        for (int i = 0; i &lt; 10000000; i++) {
            cglibProxy.sell();
        }
        System.out.println(&quot;CGLIB Proxy invoke cost &quot; + (System.currentTimeMillis() - start) + &quot; ms&quot;);

        for (int i = 0; i &lt; 10; i++) {
            javassistBytecodeProxy.sell();//warm
        }
        start = System.currentTimeMillis();
        for (int i = 0; i &lt; 10000000; i++) {
            javassistBytecodeProxy.sell();
        }
        System.out.println(&quot;JavassistBytecode Proxy invoke cost &quot; + (System.currentTimeMillis() - start) + &quot; ms&quot;);

        Class&lt;?&gt; serviceClass = delegate.getClass();
        String methodName = &quot;sell&quot;;
        for (int i = 0; i &lt; 10; i++) {
            cglibProxy.sell();//warm
        }
        // 执行反射调用
        for (int i = 0; i &lt; 10; i++) {//warm
            Method method = serviceClass.getMethod(methodName, new Class[]{});
            method.invoke(delegate, new Object[]{});
        }
        start = System.currentTimeMillis();
        for (int i = 0; i &lt; 10000000; i++) {
            Method method = serviceClass.getMethod(methodName, new Class[]{});
            method.invoke(delegate, new Object[]{});
        }
        System.out.println(&quot;反射 invoke cost &quot; + (System.currentTimeMillis() - start) + &quot; ms&quot;);

        // 使用 CGLib 执行反射调用
        for (int i = 0; i &lt; 10; i++) {//warm
            FastClass serviceFastClass = FastClass.create(serviceClass);
            FastMethod serviceFastMethod = serviceFastClass.getMethod(methodName, new Class[]{});
            serviceFastMethod.invoke(delegate, new Object[]{});
        }
        start = System.currentTimeMillis();
        for (int i = 0; i &lt; 10000000; i++) {
            FastClass serviceFastClass = FastClass.create(serviceClass);
            FastMethod serviceFastMethod = serviceFastClass.getMethod(methodName, new Class[]{});
            serviceFastMethod.invoke(delegate, new Object[]{});
        }
        System.out.println(&quot;CGLIB invoke cost &quot; + (System.currentTimeMillis() - start) + &quot; ms&quot;);

    }

    private static BookApi createJdkDynamicProxy(final BookApi delegate) {
        BookApi jdkProxy = (BookApi) Proxy.newProxyInstance(ClassLoader.getSystemClassLoader(),
                new Class[]{BookApi.class}, new JdkHandler(delegate));
        return jdkProxy;
    }

    private static class JdkHandler implements InvocationHandler {

        final Object delegate;

        JdkHandler(Object delegate) {
            this.delegate = delegate;
        }

        @Override
        public Object invoke(Object object, Method method, Object[] objects)
                throws Throwable {
            //添加代理逻辑
            if(method.getName().equals(&quot;sell&quot;)){
                System.out.print(&quot;&quot;);
            }
            return null;
//            return method.invoke(delegate, objects);
        }
    }

    private static BookApi createCglibDynamicProxy(final BookApi delegate) throws Exception {
        Enhancer enhancer = new Enhancer();
        enhancer.setCallback(new CglibInterceptor(delegate));
        enhancer.setInterfaces(new Class[]{BookApi.class});
        BookApi cglibProxy = (BookApi) enhancer.create();
        return cglibProxy;
    }

    private static class CglibInterceptor implements MethodInterceptor {

        final Object delegate;

        CglibInterceptor(Object delegate) {
            this.delegate = delegate;
        }

        @Override
        public Object intercept(Object object, Method method, Object[] objects,
                                MethodProxy methodProxy) throws Throwable {
            //添加代理逻辑
            if(method.getName().equals(&quot;sell&quot;)) {
                System.out.print(&quot;&quot;);
            }
            return null;
//            return methodProxy.invoke(delegate, objects);
        }
    }

    private static BookApi createJavassistBytecodeDynamicProxy() throws Exception {
        ClassPool mPool = new ClassPool(true);
        CtClass mCtc = mPool.makeClass(BookApi.class.getName() + &quot;JavaassistProxy&quot;);
        mCtc.addInterface(mPool.get(BookApi.class.getName()));
        mCtc.addConstructor(CtNewConstructor.defaultConstructor(mCtc));
        mCtc.addMethod(CtNewMethod.make(
                &quot;public void sell() { System.out.print(\&quot;\&quot;) ; }&quot;, mCtc));
        Class&lt;?&gt; pc = mCtc.toClass();
        BookApi bytecodeProxy = (BookApi) pc.newInstance();
        return bytecodeProxy;
    }

}
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Aspect-Oriented Programming in Spring Boot Part 2: Spring JDK Proxies vs CGLIB vs AspectJ]]></title>
    <link href="http://panlw.github.io/15277821532847.html"/>
    <updated>2018-05-31T23:55:53+08:00</updated>
    <id>http://panlw.github.io/15277821532847.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>Justin Wilson May 9, 2016<br/>
<a href="https://www.credera.com/blog/technology-insights/open-source-technology-insights/aspect-oriented-programming-in-spring-boot-part-2-spring-jdk-proxies-vs-cglib-vs-aspectj/">https://www.credera.com/blog/technology-insights/open-source-technology-insights/aspect-oriented-programming-in-spring-boot-part-2-spring-jdk-proxies-vs-cglib-vs-aspectj/</a></p>
</blockquote>

<p>This is the second entry in a three-part series on aspect-oriented programming (AOP) with Spring Boot. This entry covers the differences between using AOP with Spring JDK proxies, CGLIB proxes, and AspectJ load-time weaving (LTW). The <a href="https://www.credera.com/blog/technology-insights/open-source-technology-insights/aspect-oriented-programming-in-spring-boot-part-1-making-your-own-hystrix-aspect/">first entry</a> covered how to write your own aspect, and the <a href="https://www.credera.com/blog/technology-insights/open-source-technology-insights/aspect-oriented-programming-in-spring-boot-part-3-setting-up-aspectj-load-time-weaving/">last entry</a> will contain detailed AspectJ LTW setup information.</p>

<p><strong>How Can You Call Code Without Writing Its Invocation?</strong></p>

<p>The Spring framework supports two main methods of AOP—Spring proxies and AspectJ. Both methods allow code snippets called aspects to be injected or “woven” around existing code by targeting specific join points. However, even though both AOP styles can use the same code for their aspects, the way in which they weave those aspects into existing code is completely different.</p>

<p><strong>Spring Proxies: JDK and CGLIB Styles</strong></p>

<p>Every Spring bean (heretofore referred to as just a “bean”) can be considered a managed component. Whenever you declare a bean in XML or use @Component, @Service, or @Repository on a class targeted by Spring’s annotation component scanning (which is enabled by default for Spring Boot), that class is instantiated and managed as a singleton bean by the Spring framework. (It’s technically possible to use different scoping mechanisms to avoid singletons, but those won’t be covered here; see <a href="https://www.credera.com/blog/technology-insights/java/dependency-injection-part-3-spring/">my blog series on dependency injection</a> for more information.) Spring can inject references to that bean into other beans that request it using the @Autowired annotation (or @Resource or @Inject annotations) or beans that use “ref” in their XML definition. However, Spring doesn’t actually provide a literal reference to the original bean when it’s injected—it wraps the bean in a proxy class to give Spring a chance to weave in AOP code if needed.</p>

<p>There are two ways Spring can make proxies:</p>

<p>1. By default, Spring will try to use <a href="http://www.ibm.com/developerworks/java/library/j-jtp08305/index.html">JDK dynamic proxy libraries</a> to create a new instance of the injected bean’s interface which will act as a delegate to that bean. This behavior is demonstrated by the <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-proxy/src/test/java/org/jdw/blog/ProxyHystrixAspectTest.java#L36">unit tests</a> that use injected beans with interfaces in the <a href="https://github.com/jwilsoncredera/spring-aop-blog/tree/master/spring-aop-proxy">spring-aop-proxy sample project</a>.</p>

<p><a href="https://www.credera.com/wp-content/uploads/2016/04/Picture2-4.png"><img src="https://www.credera.com/wp-content/uploads/2016/04/xPicture2-4.png.pagespeed.ic.dhHNYptsoi.webp" alt="Picture2"/></a></p>

<p>2. If CGLIB is available on the classpath (which comes by default with Spring Boot) and there is no interface to implement a proxy with, CGLIB will be used to make a new object that extends the target bean’s class and acts as a delegate to that original bean. This behavior is also demonstrated by a <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-proxy/src/test/java/org/jdw/blog/ProxyHystrixAspectTest.java#L65">unit test</a> that uses an injected bean without an interface in the <a href="https://github.com/jwilsoncredera/spring-aop-blog/tree/master/spring-aop-proxy">spring-aop-proxy sample project</a>.</p>

<p><a href="https://www.credera.com/wp-content/uploads/2016/04/Picture3-3.png"><img src="https://www.credera.com/wp-content/uploads/2016/04/xPicture3-3.png.pagespeed.ic.W2zdwkeosT.webp" alt="Picture3"/></a></p>

<p>By default, Spring Boot will use JDK proxies if the bean has an interface and CGLIB proxies if the bean does not. If you try to inject a bean using its concrete class and that bean has an interface, Spring will fail to find the bean with a NoSuchBeanDefinitionException because it will fail to create a JDK proxy using the concrete class. In the first example above, that would be using this:</p>

<p>| </p>

<pre><code class="language-java">@Autowired
private ImplForInterface bean;
</code></pre>

<p>Instead of this:</p>

<pre><code class="language-java">@Autowired
private Interface bean;
</code></pre>

<p>To get around this limitation and be able to inject concrete classes that have interfaces, you have to tell Spring to always use CGLIB for all beans. This will disable Spring’s use of JDK proxies entirely, making Spring always extend concrete classes even if an interface is injected. To enable this in Spring Boot, just add the following Java property, as seen in the <a href="https://github.com/jwilsoncredera/spring-aop-blog/tree/master/spring-aop-proxy-cglib">spring-aop-proxy-cglib</a> sample project’s <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-proxy-cglib/src/main/resources/application.properties">application.properties</a> file:</p>

<p>spring.aop.proxy-target-class=true</p>

<p><strong>Spring Proxies: Annotations on Interfaces</strong></p>

<p>Consider an aspect that’s used to weave code around annotated methods (such as the <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-common/src/main/java/org/jdw/blog/common/aspect/HystrixAspect.java">HystrixAspect</a> that targets <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-common/src/main/java/org/jdw/blog/common/annotation/HystrixWrapper.java">@HystrixWrapper</a> annotated methods from part one in this series).</p>

<ul>
<li>  When a Spring JDK proxy is used, the join point annotation should be present on both the interface’s method and the concrete class’s method for the aspect to trigger correctly.

<ul>
<li>  The <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-proxy/src/test/java/org/jdw/blog/ProxyHystrixAspectTest.java#L72">second to last test in the spring-aop-proxy test class</a> proves that both the interface and the concrete class require the join point annotation when a JDK proxy is used.</li>
</ul></li>
<li>  When a CGLIB proxy is used, the concrete implementation must have the annotation on its method for the aspect to trigger.

<ul>
<li>  The <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-proxy-cglib/src/test/java/org/jdw/blog/ProxyCglibHystrixAspectTest.java#L96">second to last test in the spring-aop-proxy-cglib test class</a> proves that the interface’s join point annotations are ignored when CGLIB is used.</li>
</ul></li>
</ul>

<p>The <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-proxy/src/test/java/org/jdw/blog/ProxyHystrixAspectTest.java#L48">JDK proxy unit test that expects an <strong>InaccessablePointcutAnnotationException</strong></a> is of particular interest. It demonstrates that if you’re using a JDK proxy and the join point annotation is present on the concrete implementation of a bean and not on its interface, the aspect will still trigger but it won’t be able to find the annotation that was used to trigger it. This prevents us from finding the Hystrix command group key that should come from the wrapped method’s annotation, as shown in the bold parts of the example below. (<a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-common/src/main/java/org/jdw/blog/common/annotation/HystrixWrapper.java">This</a> is the GitHub link for the @HystrixWrapper annotation, and <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-common/src/main/java/org/jdw/blog/common/aspect/HystrixAspect.java">this</a> is the link for the HystrixAspect.)</p>

<pre><code class="language-java">@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.METHOD)
public @interface HystrixWrapper {
  public String commandGroupKey();
}
@Aspect
@Component
public class HystrixAspect {

  @Around(&quot;within(org.jdw.blog..*) &amp;amp;&amp;amp; &quot;  &quot;@annotation(org.jdw.blog.common.annotation.HystrixWrapper)&quot;)
  public Object around(final ProceedingJoinPoint joinPoint) {

    Method method = ((MethodSignature) joinPoint.getSignature()).getMethod();
    HystrixWrapper annotation = method.getAnnotation(HystrixWrapper.class);
    if (annotation == null) {      // Can occur when not using CCGLIB-style &#39;subclass&#39; proxies,
      // when using an interface that has a concrete class
      // that has the annotation.
      throw new InaccessablePointcutAnnotationException();    }

    // Hystrix invocation code goes here, see GitHub for details. 
    // [https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-common/src/main/java/org/jdw/blog/common/aspect/HystrixAdvice.java](https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-common/src/main/java/org/jdw/blog/common/aspect/HystrixAdvice.java)
  }
}
</code></pre>

<p><strong>I recommend always using CGLIB to avoid this edge case.</strong> Doing so makes all Spring proxy behavior consistent since JDK proxies will never be used, it allows the injection of concrete implementations of beans which can be useful for unit tests, and it negates the need to duplicate join point annotations on interfaces and concrete classes (which could easily get out of sync). As a reminder, to always use CGLIB, just set the “spring.aop.proxy-target-class” property to true.</p>

<p>Before using CGLIB, ensure your codebase always uses pre-existing AOP annotations (such as @Transactional) on concrete classes instead of only on interfaces. Interface-only AOP annotations will be ignored when CGLIB is enabled. Changing when @Transactional aspects are triggered could lead to items not being saved to the database, or poor performance due to transactional boundary shifting.</p>

<p><strong>Spring Proxies: Annotations in Concrete Classes</strong></p>

<p>Both JDK- and CGLIB-style Spring proxies have one glaring weakness—AOP can only be invoked by calling a method on an injected bean. Consider a bean with these two methods (all of the classes in <a href="https://github.com/jwilsoncredera/spring-aop-blog/tree/master/spring-aop-common/src/main/java/org/jdw/blog/common/executable">this package</a> qualify):</p>

<pre><code class="language-java">@HystrixWrapper(commandGroupKey = &quot;blog&quot;)
public long hystrixWrappedGetCurrentThreadId() {
  return getCurrentThreadId();
}

public long nestedHystrixWrappedGetCurrentThreadId() {
  return hystrixWrappedGetCurrentThreadId();
}
</code></pre>

<p>The second method cannot trigger the HystrixAspect when calling the first method because it invokes the first method directly without going through a Spring proxy—the bean is just calling one of its own methods. Spring doesn’t have any opportunities to inject the aspect code. This is demonstrated by every other test in both the <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-proxy/src/test/java/org/jdw/blog/ProxyHystrixAspectTest.java#L42">spring-aop-proxy test class</a> and <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-proxy-cglib/src/test/java/org/jdw/blog/ProxyCglibHystrixAspectTest.java#L41">spring-aop-proxy-cglib test class</a> (all the tests that start with “testNestedHystrixWrappedMethod”).</p>

<p>However, when AspectJ is used instead of Spring proxies, the HystrixAspect <u>does</u> trigger when a bean calls its own join point annotated method. This is shown by every other test in the <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-aspectj-ltw/src/test/java/org/jdw/blog/AspectJHystrixAspectTest.java#L41">spring-aop-aspectj-ltw test class</a> (which also begin with “testNestedHystrixWrappedMethod”)—every scenario that fails to invoke the Hystrix aspect for Spring proxies succeeds for AspectJ (except for <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-aspectj-ltw/src/test/java/org/jdw/blog/AspectJHystrixAspectTest.java#L101">one specific scenario</a> which will be covered later). How can this be?</p>

<p><strong>AspectJ</strong></p>

<p>AspectJ extends the Java compiler to weave aspect code directly into join points. It doesn’t require Spring to run. Instead, you need to use <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-aspectj-ltw/README.md">JVM arguments</a> to enable a compiler extension along with an <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-aspectj-ltw/src/main/resources/META-INF/aop.xml">aop.xml file</a> and <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-aspectj-ltw/src/main/java/org/jdw/blog/config/AspectJConfig.java">another configuration file</a> which will be covered in part three of this series.</p>

<p>This has two important ramifications. First, if you’re using AspectJ, you don’t need to inject Spring beans to invoke AOP functionality. Technically you don’t even need Spring beans for AOP at all when AspectJ is enabled—you could manually create a new object and invoke AOP code on it right away if it had join points your aspects could target.</p>

<p>Second, if an object calls its own methods it can trigger AOP code, unlike AOP using Spring proxies. Here’s a visual example of how this AspectJ behavior would look if the @HystrixAspect used @Before instead of @Around to inject code in front of its join point targets instead of completely wrapping them (because it’s easier to depict):</p>

<p><a href="https://www.credera.com/wp-content/uploads/2016/04/Picture5-2.png"><img src="https://www.credera.com/wp-content/uploads/2016/04/xPicture5-2.png.pagespeed.ic.PXtdJ6_MY_.webp" alt="Picture5"/></a></p>

<p>The fact that beans can trigger their own AOP code means you need to be very careful when enabling AspectJ on existing Spring projects, because this can change when database transactions are started by @Transactional annotations. For example, if you had a bean with two methods and one didn’t have an @Transactional annotation and it called another one that did, then that invocation would start a database transaction with AspectJ enabled but not with default Spring proxy-based AOP.</p>

<pre><code class="language-java">public void notAnnotated() {
  annotated();
}

@Transactional
public void annotated() {
  // With AspectJ, a direct call from notAnnotated will enable the transaction.
  // Without it, this method will only get a transaction if it was called from a Spring proxy.
}
</code></pre>

<p>Luckily, there is a way around this issue—AspectJ and Spring proxies can co-exist. The <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-aspectj-ltw/src/main/resources/META-INF/aop.xml">aop.xml file</a> lets you decide which aspects will be managed by AspectJ and which packages are included or excluded from AspectJ code weaving. If you exclude classes that use @Transactional from AspectJ weaving, their behavior will default to the same Spring proxy behavior they were using before. Stay tuned for part three of this series, which covers how to configure an aop.xml file.</p>

<p><strong>What About That Unit Test Where AspectJ Didn’t Work?</strong></p>

<p>As noted earlier, every other test in the <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-aspectj-ltw/src/test/java/org/jdw/blog/AspectJHystrixAspectTest.java#L41">spring-aop-aspectj unit test</a> (which each begin with “testNestedHystrixWrappedMethod”) demonstrates how beans can trigger the HystrixAspect when calling their own @HystrixWrapper join point annotated methods without going through Spring beans… except for the <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-aspectj-ltw/src/test/java/org/jdw/blog/AspectJHystrixAspectTest.java#L102">last test</a>.</p>

<p>That last test invokes the “nestedGetCurrentThreadId” method on a Spring CGLIB proxy of a class similar to the example below (<a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-common/src/main/java/org/jdw/blog/common/executable/NonAnnotatedImplForInterfaceWithAnnotation.java#L22">GitHub link for the original class</a>). The class’s interface annotates “getCurrentThreadId” with @HystrixWrapper, but the concrete class itself does not. Thus, when “getCurrentThreadId” is invoked by “nestedGetCurrentThreadId” within the CGLIB proxy, AspectJ looks at the concrete class instead of the interface and can’t find the @HystrixWrapper join point. Therefore, the HystrixAspect is not woven.</p>

<pre><code class="language-java">@Override
public long getCurrentThreadId () {
  return Thread.currentThread().getId();
}

@Override
public long nestedGetCurrentThreadId () {
  return getCurrentThreadId();
}
</code></pre>

<p>The takeaway from this scenario is that you should always put join point annotations on concrete classes when using AspectJ, just like you should when using Spring CGLIB proxies for AOP.</p>

<p><strong>Next Steps</strong></p>

<p>Stay tuned for the third and final entry in this blog series, which will cover how to configure AspectJ for Spring Boot. It contains a critical piece of JVM configuration information that isn’t widely found in other tutorials (including Spring’s own documentation), so be sure to catch the next entry by following @CrederaOpen on Twitter or connecting with us on LinkedIn.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[定时任务框架Quartz详解-基础篇]]></title>
    <link href="http://panlw.github.io/15277817499101.html"/>
    <updated>2018-05-31T23:49:09+08:00</updated>
    <id>http://panlw.github.io/15277817499101.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原创： 蜗牛  Java架构师之路  前天<br/>
<a href="https://mp.weixin.qq.com/s?__biz=MzI3NjU2ODA5Mg==&amp;mid=2247484030&amp;idx=1&amp;sn=13559ded2625e103a9722b7223927683&amp;chksm=eb72c30ddc054a1b07b1983fc9ad1ba4d9ec362e38b86fd9fd6a49f73a5ee1472b198d507fc9&amp;mpshare=1&amp;scene=23&amp;srcid=0529GJVxZA61CVVHbErVxkwQ%23rd">原文地址</a></p>
</blockquote>

<h3 id="toc_0">概述</h3>

<p>Quartz 是 OpenSymphony 开源组织的一个开源项目，定时任务框架，纯 Java 语言实现，最新版本为 2.3.0。<br/>
Quartz 中用到的设计模式：</p>

<ul>
<li>  Builder 模式</li>
<li>  Factory 模式</li>
<li>  组件模式</li>
<li>  链式模式</li>
</ul>

<h3 id="toc_1">Quartz 组成部分</h3>

<ul>
<li>  调度器：scheduler</li>
<li>  任务：JobDetail</li>
<li>  触发器：Trigger, 包括 SimpleTrigger 和 CronTrigger</li>
</ul>

<h3 id="toc_2">第一个 Quartz 程序</h3>

<p>实现每隔 1 秒打印一个 Hello World</p>

<h4 id="toc_3">1. 创建 Maven 项目，添加依赖：</h4>

<pre><code class="language-xml">&lt;!-- https://mvnrepository.com/artifact/org.quartz-scheduler/quartz --&gt;
&lt;dependency&gt;
  &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt;
  &lt;artifactId&gt;quartz&lt;/artifactId&gt;
  &lt;version&gt;2.3.0&lt;/version&gt;
&lt;/dependency&gt;  
</code></pre>

<h4 id="toc_4">2. 创建 HelloWorldJob 类</h4>

<pre><code class="language-java">package quartz;
import org.quartz.Job;
import org.quartz.JobExecutionContext;
import org.quartz.JobExecutionException;
import java.text.SimpleDateFormat;
import java.util.Date;
/**
 * created by Java-Road
 * created in 2018/5/26
 */
public class HelloWorldJob implements Job {
    @Override
    public void execute(JobExecutionContext jobExecutionContext) throws JobExecutionException {
        String strTime = new SimpleDateFormat(&quot;HH-mm-ss&quot;).format(new Date());
        System.out.println( strTime + &quot;:Hello World！&quot;);
    }
}
</code></pre>

<h4 id="toc_5">3. 创建 MyScheduler 类</h4>

<pre><code class="language-java">package quartz;
import org.quartz.*;
import org.quartz.impl.StdSchedulerFactory;
/**
 * created by Java-Road
 * created in 2018/5/26
 */
public class MyScheduler {
    public static void main(String[] args) throws SchedulerException {
        //创建调度器Schedule
        SchedulerFactory schedulerFactory = new StdSchedulerFactory();
        Scheduler scheduler = schedulerFactory.getScheduler();
        //创建JobDetail实例，并与HelloWordlJob类绑定
        JobDetail jobDetail = JobBuilder.newJob(HelloWorldJob.class).withIdentity(&quot;job1&quot;, &quot;jobGroup1&quot;)
                .build();
        //创建触发器Trigger实例(立即执行，每隔1S执行一次)
        Trigger trigger = TriggerBuilder.newTrigger()
                .withIdentity(&quot;trigger1&quot;, &quot;triggerGroup1&quot;)
                .startNow()
                .withSchedule(SimpleScheduleBuilder.simpleSchedule().withIntervalInSeconds(1).repeatForever())
                .build();
        //开始执行
        scheduler.scheduleJob(jobDetail, trigger);
        scheduler.start();
    }
}
</code></pre>

<p></section></p>

<h3 id="toc_6">Quartz 中几个重要的对象</h3>

<h2 id="toc_7">1.Job 和 JobDetail</h2>

<p>Job 是 Quartz 中的一个接口，接口下只有 execute 方法，在这个方法中编写业务逻辑。</p>

<p>该接口的源码：</p>

<pre><code class="language-java">package org.quartz;  
public interface Job {  
    void execute(JobExecutionContext var1) throws JobExecutionException;  
}  
</code></pre>

<p>每次调度执行 Job 时，调用 execute 方法前会创建一个新的 Job 实例，执行完后，关联的 Job 对象实例会被释放，随后 jvm 执行 GC。</p>

<p>JobDetail 是用来绑定 Job，为 Job 实例提供了许多属性，以及 JobDataMap 成员变量属性。调度器 scheduler 通过 JobDetail 对象来添加 Job 实例。</p>

<p>属性：</p>

<ul>
<li>  name</li>
<li>  group</li>
<li>  jobClass</li>
<li>  jobDataMap</li>
</ul>

<h2 id="toc_8">2.JobExecutionContext</h2>

<p>当调度器 Scheduler 调用一个 Job 时，就会将 JobExecutionContext 传递给 Job 的 execute() 方法，Job 能通过 JobExecutionContext 对象访问到 Quartz 运行时的环境以及 Job 本身的详细数据信息。</p>

<p>代码演示：</p>

<pre><code class="language-java">public class HelloWorldJob implements Job {
    @Override
    public void execute(JobExecutionContext jobExecutionContext) throws JobExecutionException {
        String strTime = new SimpleDateFormat(&quot;HH-mm-ss&quot;).format(new Date());
        System.out.println( strTime + &quot;:Hello World！&quot;);
        System.out.println(&quot;JobDetail&#39;name:&quot; + jobExecutionContext.getJobDetail().getKey().getName());
        System.out.println(&quot;JobDetail&#39;group:&quot; + jobExecutionContext.getJobDetail().getKey().getGroup());
        System.out.println(&quot;JobDetail&#39;class:&quot; + jobExecutionContext.getJobDetail().getClass());
    }
}
</code></pre>

<h2 id="toc_9">3.JobDataMap</h2>

<p>任务调度时可以通过 JobExecutionContext 获取 JobDataMap，可以装在任何可序列化的数据对象，JobDataMap 实现了 JDK 的 Map 接口，可以以 Key-Value 的形式存储数据。</p>

<p>实战：JobDetail 和 Trigger 传递数据，HelloWorldJob 类 execute 三种方式获取数据。</p>

<pre><code class="language-java">//创建JobDetail实例，并与HelloWordlJob类绑定
JobDetail jobDetail = JobBuilder.newJob(HelloWorldJob.class).withIdentity(&quot;job1&quot;, &quot;jobGroup1&quot;)
        .usingJobData(&quot;key1&quot;,&quot;this is jobDetail&quot;)
        .build();
//创建触发器Trigger实例(立即执行，每隔1S执行一次)
Trigger trigger = TriggerBuilder.newTrigger()
        .withIdentity(&quot;trigger1&quot;, &quot;triggerGroup1&quot;)
        .usingJobData(&quot;key2&quot;, &quot;this is trigger&quot;)
        .startNow()
        .withSchedule(SimpleScheduleBuilder.simpleSchedule().withIntervalInSeconds(1).repeatForever())
        .build();
</code></pre>

<p>HelloWorldJob 获取数据：</p>

<pre><code class="language-java">public class HelloWorldJob implements Job {
    private String key1;
    private String key2;
    public String getKey1() {
        return key1;
    }
    public void setKey1(String key1) {
        this.key1 = key1;
    }
    public String getKey2() {
        return key2;
    }
    public void setKey2(String key2) {
        this.key2 = key2;
    }
    @Override
    public void execute(JobExecutionContext jobExecutionContext) throws JobExecutionException {
        String strTime = new SimpleDateFormat(&quot;HH-mm-ss&quot;).format(new Date());
        System.out.println( strTime + &quot;:Hello World！&quot;);
        //获取DataMap数据方法一
        System.out.println(&quot;JobDetail JobDataMap:&quot; + jobExecutionContext.getJobDetail().getJobDataMap().get(&quot;key1&quot;));
        System.out.println(&quot;Trigger JobDataMap:&quot; + jobExecutionContext.getTrigger().getJobDataMap().get(&quot;key2&quot;));
        //获取DataMap数据方法二
        System.out.println(&quot;JobDataMap:&quot; + jobExecutionContext.getMergedJobDataMap().get(&quot;key1&quot;));
        System.out.println(&quot;JobDataMap:&quot; + jobExecutionContext.getMergedJobDataMap().get(&quot;key2&quot;));
        //获取数据方法三
        System.out.println(&quot;通过成员变量获取&quot; + key1);
        System.out.println(&quot;通过成员变量获取&quot; + key2);
    }
}
</code></pre>

<h2 id="toc_10">4.Trigger</h2>

<p>Trigger 是 Quartz 中的触发器，任务执行时会通知调度器 Scheduler 何时触发，几个重要的属性。</p>

<ol>
<li> Jobkey：表示 job 实例的标识</li>
<li> StartTime：表示触发器首次被触发的时间 (Java.util.Date)。</li>
<li> EndTime：表示触发器结束触发的时间 (Java.util.Date)</li>
</ol>

<p>实战：实现 5S 后执行，10S 后结束，期间每隔 1S 执行一次定时任务</p>

<p>代码演示：</p>

<p>MyScheduler 类</p>

<pre><code class="language-java">public class MyScheduler {
    public static void main(String[] args) throws SchedulerException {
        //创建调度器Schedule
        SchedulerFactory schedulerFactory = new StdSchedulerFactory();
        Scheduler scheduler = schedulerFactory.getScheduler();
        //创建JobDetail实例，并与HelloWordlJob类绑定
        JobDetail jobDetail = JobBuilder.newJob(HelloWorldJob.class).withIdentity(&quot;job1&quot;, &quot;jobGroup1&quot;)
                .build();
        //创建触发器Trigger实例(5S后执行，10S后结束)
            //开始时间(5S后)
        Date date1 = new Date();
        date1.setTime(date1.getTime() + 5000);
            //结束时间(10S后)
        Date date2 = new Date();
        date2.setTime(date2.getTime() + 10000);
        Trigger trigger = TriggerBuilder.newTrigger()
                .withIdentity(&quot;trigger1&quot;, &quot;triggerGroup1&quot;)
                .startAt(date1)
                .endAt(date2)
                .withSchedule(SimpleScheduleBuilder.simpleSchedule().withIntervalInSeconds(1).repeatForever())
                .build();
        //开始执行
        scheduler.scheduleJob(jobDetail, trigger);
        scheduler.start();
    }
}
</code></pre>

<h2 id="toc_11">5.SimpleTrigger</h2>

<p>SimpleTrigger 可以实现在一个指定时间段内执行一次作业任务或一个时间段内多次执行作业任务。</p>

<p>实战：5S 后开始执行，间隔时间为 1S，第一次执行后连续执行 3 次</p>

<pre><code class="language-java">package quartz2;
import org.quartz.*;
import org.quartz.impl.StdSchedulerFactory;
import java.util.Date;
/**
 * created by Java-Road
 * created in 2018/5/27
 */
public class MyScheduler2 {
    public static void main(String[] args) throws SchedulerException {
        //创建调度器Schedule
        SchedulerFactory schedulerFactory = new StdSchedulerFactory();
        Scheduler scheduler = schedulerFactory.getScheduler();
        //创建JobDetail实例，并与HelloWordlJob类绑定
        JobDetail jobDetail = JobBuilder.newJob(HelloWorldJob.class).withIdentity(&quot;job1&quot;, &quot;jobGroup1&quot;)
                .build();
        //创建触发器Trigger实例(5S后执行,一直执行)
        //开始时间(5S后)
        Date date1 = new Date();
        date1.setTime(date1.getTime() + 5000);
        SimpleTrigger trigger = (SimpleTrigger) TriggerBuilder.newTrigger()
                .withIdentity(&quot;trigger1&quot;, &quot;triggerGroup1&quot;)
                .startAt(date1)
                .withSchedule(SimpleScheduleBuilder.simpleSchedule().withIntervalInSeconds(1)
                            .withRepeatCount(3))
                .build();
        //开始执行
        scheduler.scheduleJob(jobDetail, trigger);
        scheduler.start();
    }
}
</code></pre>

<h2 id="toc_12">6.CronTrigger</h2>

<p>CronTrigger 功能非常强大，是基于日历的作业调度，而 SimpleTrigger 是精准指定间隔，所以相比 SimpleTrigger，CroTrigger 更加常用。CroTrigger 是基于 Cron 表达式的，先了解下 Cron 表达式：</p>

<p>由 7 个子表达式组成字符串的，格式如下：</p>

<p>[秒] [分] [小时] [日] [月] [周] [年]</p>

<p>Cron 表达式的语法就不多说了，因为我也记不住只能度娘，给大家提供个在线生成 Cron 表达式的工具：<a href="http://cron.qqe2.com/">http://cron.qqe2.com/</a> ，方便实用。</p>

<p>实战：实现每周一到周五上午 10:30 执行定时任务</p>

<pre><code class="language-java">package quartz2;
import org.quartz.*;
import org.quartz.impl.StdSchedulerFactory;
import java.util.Date;
/**
 * created by Java-Road
 * created in 2018/5/27
 */
public class MyScheduler3 {
    public static void main(String[] args) throws SchedulerException {
        //创建调度器Schedule
        SchedulerFactory schedulerFactory = new StdSchedulerFactory();
        Scheduler scheduler = schedulerFactory.getScheduler();
        //创建JobDetail实例，并与HelloWordlJob类绑定
        JobDetail jobDetail = JobBuilder.newJob(HelloWorldJob.class).withIdentity(&quot;job1&quot;, &quot;jobGroup1&quot;)
                .build();
        //创建触发器CronTrigger实例(每周一到周五10:30执行任务)
        CronTrigger trigger = (CronTrigger) TriggerBuilder.newTrigger()
                .withIdentity(&quot;trigger1&quot;, &quot;triggerGroup1&quot;)
                .startNow()
                .withSchedule(CronScheduleBuilder.cronSchedule(&quot;* 30 10 ? * 1/5 *&quot;))
                .build();
        //开始执行
        scheduler.scheduleJob(jobDetail, trigger);
        scheduler.start();
    }
}
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[有货移动端DevOps-自建APM系统]]></title>
    <link href="http://panlw.github.io/15277814705334.html"/>
    <updated>2018-05-31T23:44:30+08:00</updated>
    <id>http://panlw.github.io/15277814705334.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原创：曹镏  高效开发运维 2018/05/28</p>

<p><a href="https://mp.weixin.qq.com/s?__biz=MzIzNjUxMzk2NQ==&amp;mid=2247489376&amp;idx=1&amp;sn=2c06cd726d03317ab3b2134e393d101e&amp;chksm=e8d7e8a2dfa061b4b8f4b95a710e7538becf501951ca451c63473e219d9e610c230549ceb709&amp;mpshare=1&amp;scene=23&amp;srcid=0529xDVbCCeY0HgyVSeq5kjF%23rd">原文地址</a></p>
</blockquote>

<pre><code>本文概述了有货 App 团队的同学，在资源有限的情况下，自己动手，借助多种数据工具，通过简洁的架构，为有货 App 开发一套多维实时的监控系统，从而更好地完成自己的工作，也给其他团队的小伙伴在自建 APM 的道路上提供一个参考，为移动端的 DevOps 打开一些思路。
</code></pre>

<p>得益于智能手机的发展，当下我们对用户的体验追求已经到了几乎极致的程度。主流的监控 OneAPM，New Relic… 由于面向广大用户，强调数据共性，很难通过 UDID 或者 UID 来定位一些比较难以复现或局部的问题。同时由于数据保密性等原因，对事件上报的信息量也是非常的有限。随着用户规模的增长，特殊问题，局部问题覆盖的用户越来越来，自建监控就一个必然的选择。</p>

<p>需求是迫切的，资源是有限的。为了尽快的上线监控系统。有货 iOS 和 Android 的小伙伴们本着 “流自己的汗，吃自己的饭，自己的事情自己干” 的原则，不强调客观条件，自己撸出了一套 App 的 APM 监控系统。</p>

<h2 id="toc_0">设计</h2>

<p>我们把问题处理的流程大致梳理为 “持续监控”、“发现问题” 、“定位问题”、“修复问题”。这往复的四个步骤，其中“监控”，“发现” 和“定位”都依赖于系统的采集和上报。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/FE4VibF0SjfMusg7SyM0ljwABGInyksDablCiaxacwcMhpe7mshHNYgG6nx8TaOGTPnjoPibnP1wcibNY1QOSvibbKg/640?wx_fmt=png" alt=""/></p>

<p>通过对需求的分析，我们把数据以事件的形式上报，上报后数据根据场景分为两类：</p>

<ol>
<li> 指标类数据</li>
<li> 日志类数据</li>
</ol>

<h3 id="toc_1">1. 指标类数据</h3>

<p>指标类数据主要是对用户上报的信息根据我们需要的维度做聚合，便于我们掌握 app 的整体情况，比如用户整体的网络错误，网络延时。从整体的角度观察我们 app 的健康程度。 这个过程概括为 “发现问题”。</p>

<h3 id="toc_2">2. 日志类数据</h3>

<p>日志类数据则记录完整的上报数据，用于我们对问题的诊断。 这个过程概括为 “定位问题”。<br/>
围绕这两类数据的采集和分析，我们启动了第一阶段的项目。</p>

<h2 id="toc_3">第一阶段</h2>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/FE4VibF0SjfMusg7SyM0ljwABGInyksDaGjaAY4ibHZrcU93rkaS4Mrb4DSFRibVMMfXia9e5Pbyfcru4WHIBLaACw/640?wx_fmt=png" alt=""/></p>

<p>App 将事件数据上报至 OpenResty，在 OpenResty 中对请求进行简单的处理和封装，拆分成指标和日志，然后将处理完成的数据分别写入 InfluxDB 和 MongoDB 中。再通过 Grafana 实现数据可视化。通过一个 Web 页面来查询存储在 MongoDB 中的日志。</p>

<p>App 上报的数据转换为有意义的指标, 这个过程是靠 “聚合” 来实现的。这套系统中 “聚合” 则是借助 InfluxDB 完成的。</p>

<h3 id="toc_4">InfluxDB</h3>

<p>在 InfluxDB 中，我们先将所有数据写入不同类型的事实表中，然后通过多个 Continuous Query 对数据进行聚合处理，把大量的明细数据聚合成一定时间周期的聚合数据（主要的周期为 30 秒或 1 分钟），再通过 Retention Policy 来控制不同类型表的数据有效期，达到资源最高效利用的目的。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/FE4VibF0SjfMusg7SyM0ljwABGInyksDaPyFpDrdMAVBjicTsq0VNibcEaDz40Cia8A6RgHOB9hNpSMpGafRfl1LZA/640?wx_fmt=png" alt=""/></p>

<p>数据可视化部分则用 Grafana 来支撑的，我们的 Main Dashboard 通过 20 张的图表来展示整体的健康状况，同时在这 20 张图表背后，还有数个更加详细的图表支撑微观方面数据。</p>

<p>比如有张图表绘制的是当前全网用户设备发生的网络错误率。 通过这个指标和 24 小时前的对比，我们能够直观了解整体的网络健康状况。当该指标超过某个阈值或相比昨天同时段对比有明显上升，则触发告警。实现 “发现问题”。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/FE4VibF0SjfMusg7SyM0ljwABGInyksDaoHzy1BDC7ichtoHqVhdAB7UFB2CgYrGYSiadsNGM8VruT7cwzic8CePoQ/640?wx_fmt=png" alt=""/></p>

<p>当出现告警后，通过网络相关的 dashboard 查看是由哪个接口发生的，什么类型的错误。再根据错误类型去日志平台查询日志。完成 “定位问题”。</p>

<h3 id="toc_5">MongoDB</h3>

<p>对于日志数据我们直接将上报的 json 内容逐条拆分后写入 Mongodb。直接通过 web 页面从 Mongodb 查询对应时间的 App 日志。</p>

<p>这套系统的优势体现在架构非常的轻盈，成本很低。只需要简单的 OpenResty 的开发以及对 InfluxDB 做一些连续查询。在日志量不大的情况下，完全可以满足基本的移动端性能监控以及异常监测的需求。</p>

<p>在运行了一段时间后，我们有了新的需求。</p>

<p>中国有 23 个省，5 个自治区，4 个直辖市，以及香港、澳门 2 个特别行政区。电信、联通、移动三大运营商。网络制式从 2G、3G、4G 到 WiFi，还有各种各样的移动设备。这些条件构成了数量难以想象的场景。当仅仅在个别场景下发生问题，或是部分场景下发生问题的时候。我们 “发现问题” 的即时性会变差，“定位问题”所用的时间也越发的变长。</p>

<p>为了缩短 “定位问题” 的时间。提高发现问题和解决问题的效率。多维分析能力是我们所渴望的，如果在原有的系统上做多维分析，存在如下问题：</p>

<ol>
<li><p>InfluxDB 缺乏合理免费的集群方案，我们在建立了自己的 APM 以后，客户端同学对数据和监测的需求突飞猛进，数据量每天以几何级数增长，再对其进行多维分析，单机的 InfluxDB 很难满足性能要求。</p></li>
<li><p>MongoDB 在数据量很大的场景下，直接查询速度很慢。由于我们日志数据的多样性，在对多个维度建索引后，整体索引效率很低。</p></li>
</ol>

<p><section class="" style="font-family: Avenir, -apple-system-font, 微软雅黑, sans-serif;line-height: 1.1;color: rgb(63, 63, 63);font-size: 20px;text-align: center;white-space: normal;background-color: rgb(255, 255, 255);">第二阶段</section></p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/FE4VibF0SjfMusg7SyM0ljwABGInyksDaFpHbicibZsT8FMpAf9ibF97iadUDnfFBjvvWcW3CU2gcGP4oTmYk6qVVlw/640?wx_fmt=png" alt=""/></p>

<p>为了满足大量（这里暂时不成为海量）数据的多维实时查询的需求，我们对架构中数据库和数据摄入过程进行了升级，InfluxDB 变更为 Druid， MongoDB 变更为 Greenplum， 数据也不再由 OpenResty 直接插入，而是通过了 Kafka 和 log 文件把写入过程解耦，减少数据写入次数，提升处理数据的量级。</p>

<h3 id="toc_6">Druid</h3>

<p>Druid 是在海量时序数据上面提供实时分析查询的开源 OLAP 数据存储。</p>

<p>Druid 本身对于 join 这样的操作很不友好，所以我们没有考虑用一些模型（星型或者雪花型），而是采用大的事实表，其中包含了一次事实关联的所有维度，看着很臃肿，但是查询的性能让人满意。在随时变更维度的情况下，Druid 实时查询依然维持在秒级。这不得不让人刮目相看。</p>

<p>同时丰富的查询组件和聚合函数，让复杂的查询得以简单的实现。所以在查询性能、便捷性、数据量等方面的综合考量下，我们选择了 Druid 作为 InfluxDB 的升级方案。</p>

<p>不同于 InfluxDB，我们不再通过连续查询来做数据聚合，Druid 自身在摄入的过程中可以对指定的 Dimension 进行 roll-up 操作。极大的压缩了数据量。我们的数据经过 roll-up，每分钟从 25 万条压缩到了 8 万多条，效果比较显著。不过压缩数据是以牺牲原始数据为代价的，预聚合仅保留指定的 Dimension 及 Aggregations 结果，所以请妥善设置 Dimension。 Dimension 会发生 SQL 中 Group By 的效果，参数维度多势必降低聚合效果。尤其需要的注意 Dimension 的选择，如果一不小心选择了高基字段作为 Dimension，那么压缩效率会低的惊人。我们曾经因为数据处理的失误，将一个 Dimension 的 Value 数从 2000 变成了 200W, 直接导致当天的 segments 占用的空间比以往大 22 倍。通过摄入周期的调整，还可以做到更大的压缩比，这个取决 queryGranularity 字段，它决定了压缩后的数据最小时间粒度，目前我们使用的是分钟。</p>

<p>在使用 Grafana 接入 Druid 的时候还是遇到一些小坑，由于 Grafana 官方集成的 Druid 插件实在是功能太有限，比如对 Time Range 的限制，我们不得不在源码的基础上做了部分调整。</p>

<p><section class="" style="font-family: Avenir, -apple-system-font, 微软雅黑, sans-serif;line-height: 1.1;color: rgb(63, 63, 63);font-size: 16px;text-align: center;white-space: normal;background-color: rgb(255, 255, 255);">Kafka</section></p>

<p>Druid 两种数据摄入源:</p>

<ol>
<li> 流式数据</li>
<li> 静态文件数据</li>
</ol>

<p>这两种数据源又都有两种方式摄入，分别为 pull 方式和 push 方式。我们采用了 real time index（push 的一种）方式，启动实时 task 来从 kafka 中摄取数据。</p>

<h3 id="toc_7">Greenplum</h3>

<p>Greenplum 相比我们原先使用的 MongoDB, 支持列式存储, 并行查询效率更高。在不建索引的情况下，50 列千万级别的数据在 1，2 秒能查完，完全满足我们对日志查询的需求。</p>

<p>起初我们使用 Http 的方式批量插入 Greenplum，但是当写入速度超过 6 千条每秒时，查询速度直线下降。</p>

<p>明显写入不是 Greenplum 的强项，为了解决这个问题，我们是把日志以文件的形式写入 NFS 中, 由 Greenplum 定时摄入。</p>

<p>同时为了保证性能和简化写入，还对 OpenResty 的写入进行了优化，利用 nginx 的 subrequest 写 access.log 的功能来实现日志写入，并且整个过程封装到协程中执行。调整后每分钟能够轻松写入 50 万条。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/FE4VibF0SjfMusg7SyM0ljwABGInyksDaHan3wopRd9Jujiay2MExXXoq7zpGiaRwhmHJjdh0NwWnAclbFzqB1VJg/640?wx_fmt=png" alt=""/></p>

<h2 id="toc_8">总结</h2>

<p>这套系统对个性问题，共性问题的快速甄别起到了关键性的作用，也极大的提升了定位问题的速度。在针对某些维度或特征做针对性的优化的时候，效果也更容易监控。</p>

<p>通过不同类型事件的采集，我们还实现了页面响应时间（多维）、http 请求分阶段响应时间（多维）、页面卡顿分析、页面加载性能监控等多方位的监控系统，为我们的优化工作指明了方向。对于 iOS 或者 Android 开发的同学，这样一套系统既能满足需求，也易于开发，相比传统 ELK 日志分析在实时性和简易性都提升很多。</p>

<p>如果技术团队的资源比较紧张，移动端的同学自己动手实践下 DevOps 也是不错的选择。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[电商平台备战促销季的运维秘诀——高可用服务层]]></title>
    <link href="http://panlw.github.io/15274398936380.html"/>
    <updated>2018-05-28T00:51:33+08:00</updated>
    <id>http://panlw.github.io/15274398936380.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>曹林华 纯洁的微笑 2018/5/26</p>

<p><a href="https://mp.weixin.qq.com/s/5vVXBXkd-Ilh7zk5G6Wxcg">原文地址</a></p>

<p>高可用设计是互联网系统架构的基础之一，以天猫双十二交易数据为例，支付宝峰值支付次数超过 8 万笔。大家设想一下，如果这个时候系统出现不可用的情况，那后果将不可想象。<br/>
而解决这个问题的根本就是服务层的高可用。</p>
</blockquote>

<h2 id="toc_0">什么是服务层</h2>

<p>众所周知，服务层主要用来处理网站业务逻辑的，是大型业务网站的核心。比如下面三个业务系统就是典型的服务层，提供基础服务功能的聚合</p>

<ul>
<li><p>用户中心：主要负责用户注册、登录、获取用户用户信息功能</p></li>
<li><p>交易中心：主要包括正向订单生成、逆向订单、查询、金额计算等功能</p></li>
<li><p>支付中心：主要包括订单支付、收银台、对账等功能</p></li>
</ul>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlialgfhicgwy6oEeNHT1HKDxoPpnY0JKoKLZsfKd0Z7zxcfiaSzf0DdSqA/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<h2 id="toc_1">整体架构</h2>

<p>业务发展初期主要以业务为导向，一般采用 「ALL IN ONE」的架构方式来开发产品，这个阶段用一句话概括就是 「糙猛快」。当发展起来之后就会遇到下面这些问题</p>

<ul>
<li><p>文件大：一个代码文件出现超过 2000 行以上</p></li>
<li><p>耦合性严重：不相关业务都直接堆积在 Serivce 层中</p></li>
<li><p>维护代价高：人员离职后，根本没有人了解里面的业务逻辑</p></li>
<li><p>牵一发动全身：改动少量业务逻辑，需要重新把所有依赖包打包并发布</p></li>
</ul>

<p>遇到这些问题，主要还是通过「拆」来解决</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlR3f0gpEglmktfDdT7JZsPNts8y6PwIDvpoVzYzXjriaLYicTz9sib2IYQ/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<p>具体拆的方式，主要根据业务领域划分单元，进行垂直拆分。拆分开来的好处很明显，主要有以下这些：</p>

<ul>
<li><p>每个业务一个独立的业务模块</p></li>
<li><p>业务间完全解耦</p></li>
<li><p>业务间互不影响</p></li>
<li><p>业务模块独立</p></li>
<li><p>单独开发、上线、运维</p></li>
<li><p>效率高</p></li>
</ul>

<h2 id="toc_2">无状态设计</h2>

<p>对于业务逻辑服务层，一般会设计成无状态化的服务，无状态化也就是服务模块只处理业务逻辑，而无需关心业务请求的上下文信息。所以无状态化的服务器之间是相互平等且独立的。</p>

<p>只有服务变为无状态的时候，故障转移才会变的很轻松。通常故障转移就是在某一个应用服务器不能服务用户请求的时候，通过负责均衡的方式，转移用户请求到其他应用服务器上来进行业务逻辑处理</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlrVLkBKpJZja8eqMrcYwM3Mh3TVhgibEniaQickmJw3W7IiccBmMFm5sZAw/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<h2 id="toc_3">超时设置</h2>

<p>一般网站服务都会有主调服务和被调服务之分。超时设置就是主调服务在调用被调服务的时候，设置一个超时等待时间 Timeout。主调服务发现超时后，就进入超时处理流程。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlBhI38T15JZ8gV0fNRfWcjqFibk2VjNPedQAwdy5DOETk1TV43qic5aVA/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<ol>
<li><p>主调服务 A 调用被调服务 B 时，设置超时等待时间为 3 秒，可能由于 B 服务宕机、网络情况不好或程序 BUG 之类，导致 B 服务不能及时响应 A 服务的调用。</p></li>
<li><p>此时 A 服务在等待 3 秒后，将触发超时逻辑而不再关心 B 服务的回复情况。</p></li>
<li><p>A 服务的超时逻辑可以依据情况而定，比如可以采取重试，对另一个对等的 B 服务去请求，或直接放弃结束这个请求调用。</p></li>
</ol>

<p>超时设置的好处在于当某个服务不可用时，不至于整个系统发生雪崩反应。</p>

<h2 id="toc_4">异步调用</h2>

<p>一般请求调用分为同步与异步两种。同步请求就像打电话，需要实时响应，而异步请求就像发送邮件一样，不需要马上回复。</p>

<p>这两种调用各有优劣，主要看面对哪种业务场景。比如在面对并发性能要求比较高的场景，异步调用就比同步调用有比较大的优势，这就好比一个人不能同时打多个电话，但是可以发送很多邮件。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlrMaqR6L2RGN4ZNKoX60pk0ckuAypBfThcu227jxF2rbN8XENfgbLTg/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<blockquote>
<p>那我们什么时候该采用异步调用？</p>
</blockquote>

<p>其实主要看业务场景，如果业务允许延迟处理，那就采用异步的方式处理</p>

<blockquote>
<p>那我们该怎么实现异步调用呢？</p>
</blockquote>

<p>通常采用队列的方式来实现业务上的延迟处理，比如像订单中心调用配送中心，这种场景下面，业务是能接受延迟处理的。</p>

<p>那消息队列主要有哪些功能呢？</p>

<ul>
<li><p>异步处理 - 增加吞吐量</p></li>
<li><p>削峰填谷 - 提高系统稳定性</p></li>
<li><p>系统解耦 - 业务边界隔离</p></li>
<li><p>数据同步 - 最终一致性保证</p></li>
</ul>

<p>那到底有多少种队列呢？其实主要看处理业务的范围大小</p>

<ul>
<li><p>应用内部 - 采用线程池，比如 Java ThreadPool 中 BlockingQueue 来做任务级别的缓冲与处理</p></li>
<li><p>应用外部 - 比如 RabbitMQ 、ActiveMQ 就是做应用级别的队列，方便进行业务边界隔离与提高吞吐量</p></li>
</ul>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlvaSzVuYpLsyDzfC4AJF2X5Ww7L1iaaQceaRwoSjwhL05oJdibvVMQIrw/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<p>同时，技术上来讲，消息队列一般分为两种模型：Pull VS Push</p>

<ul>
<li><p>Pull 模型：消费者主动请求消息队列，获取队列中的消息。</p></li>
<li><p>Push 模型：消息队列主动推送消息到消费者</p></li>
</ul>

<p>其中 Pull 模式可以控制消费速度，不必担心自己处理不了消息，只需要维护队列中偏移量 Offset。所以对于消费量有限并且推送到队列的生产者不均匀的情况下，采用 Pull 模式比较合适。</p>

<p>Push 比较适合实时性要求比较高的情况，只要生产者消息发送到消息队列中，队列就会主动 Push 消息到消费者，不过这种模式对消费者的能力要求就提高很多，如果出现队列给消费者推送一些不能处理的消息，消费者出现 Exception 情况下，就会再次入队列，造成消费堵塞的情况。</p>

<p>不过互联网业界比较成熟的队列主要以采用 Pull 模式为主，像 Kafka、RabbitMQ（两种方式都支持）、RocketMQ 等</p>

<h2 id="toc_5">幂等</h2>

<blockquote>
<p>什么是幂等设计呢？</p>
</blockquote>

<p>其实很简单，就是一次请求和多个请求的作用是一样的。用数学上的术语，即是 f(x) = f(f(x))。</p>

<p>那我们为什么要做幂等性的设计呢？主要是因为现在的系统都是采用分布式的方式设计系统，在分布式系统中调用一般分为 3 个状态：成功、失败、超时。</p>

<p>如果调用是成功或者失败都不要紧，因为状态是明确和清晰，但是如果出现超时的情况，就不知道请求是成功还是失败的。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdla3ufL6yQt2W78GGKeq8M06eN6dmOWia6bSn1BwHwtdrw0gZf33g66wA/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<p>如果出现这种情况，我们该怎么办呢？一般采取重试的操作，重新请求对应接口。如果请求接口是 Get 操作的话，那到还好，因为请求多次的效果是一样的。但是如果是 Post 、Put 操作的话，就会造成数据不一致，甚至数据覆盖等问题。</p>

<p>举个例子：在支付收银台页面进行支付的时候，因为网络超时的问题导致支付失败，这个时候我们都会再进行一次支付操作，但是当支付成功后，发现你的账户余额被减了 2 次，这个时候心里肯定很不爽，心里都要开始骂娘了…</p>

<p>造成这个问题的关键是：网络超时后，不知道支付是什么状态？成功还是失败呢？所以说幂等性设计是必须的，尤其在电商、金融、银行等对数据要求比较高的行业中。</p>

<p>一般在这种场景下我们该怎么解决呢？</p>

<ol>
<li><p>请求方一般会生产一个唯一性 ID 标识，这个标识可以具有业务一样，比如订单号或者支付流水号，在发起请求时候带上唯一性 ID。</p></li>
<li><p>接收者在收到请求后，第一步通过获取唯一性 ID 来查询接收端是否有对应的记录，如果有的话，就直接将上次请求的结果返回，如果没有的话，就进行操作，并在操作完成后记录到对应的表里</p></li>
</ol>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlCPmk4LiaX77Zhuuv3dgZDc6PR9elSx8CEGQGRfXaeuoWPMlScROZGfA/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<h2 id="toc_6">服务降级</h2>

<p>服务降级主要解决资源不足和访问量过大的问题，比如电商平台在双十一、618 等高峰时候采用部分服务不提供访问，减少对系统的影响。</p>

<p>那降级的方式有哪些呢？</p>

<ul>
<li><p>延迟服务：比如春晚，微信发红包就出现抢到红包，但是账号余额并没有增加，要过几天才能加上去。其实这是微信内部采用延迟服务的方式来保证服务的稳定，通过队列实现记录流水账单</p></li>
<li><p>功能降级：停止不重要的功能是非常有用的方式，把相对不重要的功能暂停掉，让系统释放更多的资源。比如关闭相关文章的推荐、用户的评论功能等等，等高峰过去之后，在把服务恢复回来。</p></li>
<li><p>降低数据一致性：在大促的时候，我们发现页面上不显示真实库存的数据，只显示到底有还是没有库存这两种状态。</p></li>
</ul>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdl8ZZdFwaAib5B4yibFvXffcW1I3libeD2icxf1AaNmfYuyT85pTke7wBU3Q/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<p>刚刚说了降级的方式，那我们操作降级的时候有哪些注意点呢？</p>

<ol>
<li><p>清晰定义降级级别： 比如出现吞吐量超过 X，单位时间内响应时间超过 Y 秒、失败次数超过 Z 次等，这些阈值需要在准备的时候，通过压测的方式来确定。</p></li>
<li><p>梳理业务级别：降级之前，首先需要确定哪些业务是必须有，哪些业务是可以有的，哪些业务是可有可无的。</p></li>
<li><p>降级开关：可以通过接入配置中心（比如携程 Apollo、百度 Disconf ）的方式直接后台降级。但是如果公司没有配置中心的话，可以封装一个 API 接口来切分，不过该 API 接口要做成幂等的方式，同时需要做一些简单的签名，来保证其一定的安全性。</p></li>
</ol>

<h2 id="toc_7">总结</h2>

<p>总结一下今天分享的主要内容</p>

<ul>
<li><p>整体架构：根据业务属性进行垂直拆分，减少项目依赖，单独开发、上线、运维</p></li>
<li><p>无状态设计：应用服务中不能保存用户状态数据，如果有状态就会出现难以扩容、单点等问题</p></li>
<li><p>超时设置：当某个服务不可用时，不至于整个系统发生连锁反应</p></li>
<li><p>异步调用：同步调用改成异步调用，解决远程调用故障或调用超时对系统的影响</p></li>
<li><p>服务降级：牺牲非核心业务，保证核心业务的高可用</p></li>
</ul>

<p>所有好的架构设计首要的原则并不是追求先进，而是合理性，要与公司的业务规模和发展趋势相匹配，任何一个公司，哪怕是现在看来规模非常大的公司，比如 BAT 之类，在一开始，其系统架构也应简单和清晰的。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[面试-线程池的成长之路]]></title>
    <link href="http://panlw.github.io/15274396845490.html"/>
    <updated>2018-05-28T00:48:04+08:00</updated>
    <id>http://panlw.github.io/15274396845490.html</id>
    <content type="html"><![CDATA[
<p>尹吉欢 投稿 纯洁的微笑  3天前</p>

<blockquote>
<p><a href="https://mp.weixin.qq.com/s/5dexEENTqJWXN_17c6Lz6A">原文地址</a></p>
</blockquote>

<h1 id="toc_0"><strong>背景</strong></h1>

<p>相信大家在面试过程中遇到面试官问线程的很多，线程过后就是线程池了。从易到难，都是这么个过程，还有就是确实很多人在工作中接触线程池比较少，最多的也就是创建一个然后往里面提交线程，对于一些经验很丰富的面试官来说，一下就可以问出很多线程池相关的问题，与其被问的晕头转向，还不如好好学习。此时不努力更待何时。</p>

<h1 id="toc_1">什么是线程池？</h1>

<p>线程池是一种多线程处理形式，处理过程中将任务提交到线程池，任务的执行交由线程池来管理。</p>

<p>如果每个请求都创建一个线程去处理，那么服务器的资源很快就会被耗尽，使用线程池可以减少创建和销毁线程的次数，每个工作线程都可以被重复利用，可执行多个任务。</p>

<p>如果用生活中的列子来说明，我们可以把线程池当做一个客服团队，如果同时有 1000 个人打电话进行咨询，按照正常的逻辑那就是需要 1000 个客服接听电话，服务客户。现实往往需要考虑到很多层面的东西，比如：资源够不够，招这么多人需要费用比较多。正常的做法就是招 100 个人成立一个客服中心，当有电话进来后分配没有接听的客服进行服务，如果超出了 100 个人同时咨询的话，提示客户等待，稍后处理，等有客服空出来就可以继续服务下一个客户，这样才能达到一个资源的合理利用，实现效益的最大化。</p>

<h1 id="toc_2">Java 中的线程池种类</h1>

<p><strong>1. newSingleThreadExecutor</strong></p>

<p>创建方式：</p>

<pre><code>ExecutorService pool = Executors.newSingleThreadExecutor();
</code></pre>

<p>一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。</p>

<p>使用方式：</p>

<pre><code>import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class ThreadPool {    public static void main(String[] args) {        ExecutorService pool = Executors.newSingleThreadExecutor();        for (int i = 0; i &lt; 10; i++) {            pool.execute(() -&gt; {                System.out.println(Thread.currentThread().getName() + &quot;\t开始发车啦....&quot;);            });        }    }}
</code></pre>

<p>输出结果如下：</p>

<pre><code>pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....
</code></pre>

<p>从输出的结果我们可以看出，一直只有一个线程在运行。</p>

<p>2.<strong>newFixedThreadPool</strong></p>

<p>创建方式：</p>

<pre><code>ExecutorService pool = Executors.newFixedThreadPool(10);
</code></pre>

<p>创建固定大小的线程池。每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。</p>

<p>使用方式：</p>

<pre><code>import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class ThreadPool {    public static void main(String[] args) {        ExecutorService pool = Executors.newFixedThreadPool(10);        for (int i = 0; i &lt; 10; i++) {            pool.execute(() -&gt; {                System.out.println(Thread.currentThread().getName() + &quot;\t开始发车啦....&quot;);            });        }    }}
</code></pre>

<p>输出结果如下：</p>

<pre><code>pool-1-thread-1    开始发车啦....pool-1-thread-4    开始发车啦....pool-1-thread-3    开始发车啦....pool-1-thread-2    开始发车啦....pool-1-thread-6    开始发车啦....pool-1-thread-7    开始发车啦....pool-1-thread-5    开始发车啦....pool-1-thread-8    开始发车啦....pool-1-thread-9    开始发车啦....pool-1-thread-10 开始发车啦....
</code></pre>

<p><strong>3. newCachedThreadPool</strong></p>

<p>创建方式：</p>

<pre><code>ExecutorService pool = Executors.newCachedThreadPool();
</code></pre>

<p>创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲的线程，当任务数增加时，此线程池又添加新线程来处理任务。</p>

<p>使用方式如上 2 所示。</p>

<p>4.<strong>newScheduledThreadPool</strong></p>

<p>创建方式：</p>

<pre><code>ScheduledExecutorService pool = Executors.newScheduledThreadPool(10);
</code></pre>

<p>此线程池支持定时以及周期性执行任务的需求。</p>

<p>使用方式：</p>

<pre><code>import java.util.concurrent.Executors;import java.util.concurrent.ScheduledExecutorService;import java.util.concurrent.TimeUnit;public class ThreadPool {    public static void main(String[] args) {        ScheduledExecutorService pool = Executors.newScheduledThreadPool(10);        for (int i = 0; i &lt; 10; i++) {            pool.schedule(() -&gt; {                System.out.println(Thread.currentThread().getName() + &quot;\t开始发车啦....&quot;);            }, 10, TimeUnit.SECONDS);        }    }}
</code></pre>

<p>上面演示的是延迟 10 秒执行任务, 如果想要执行周期性的任务可以用下面的方式，每秒执行一次</p>

<pre><code>//pool.scheduleWithFixedDelay也可以pool.scheduleAtFixedRate(() -&gt; {                System.out.println(Thread.currentThread().getName() + &quot;\t开始发车啦....&quot;);}, 1, 1, TimeUnit.SECONDS);
</code></pre>

<p>5.<strong>newWorkStealingPool</strong></p>

<p>newWorkStealingPool 是 jdk1.8 才有的，会根据所需的并行层次来动态创建和关闭线程，通过使用多个队列减少竞争，底层用的 ForkJoinPool 来实现的。ForkJoinPool 的优势在于，可以充分利用多 cpu，多核 cpu 的优势，把一个任务拆分成多个 “小任务”，把多个“小任务” 放到多个处理器核心上并行执行；当多个 “小任务” 执行完成之后，再将这些执行结果合并起来即可。</p>

<h1 id="toc_3">说说线程池的拒绝策略</h1>

<p>当请求任务不断的过来，而系统此时又处理不过来的时候，我们需要采取的策略是拒绝服务。RejectedExecutionHandler 接口提供了拒绝任务处理的自定义方法的机会。在 ThreadPoolExecutor 中已经包含四种处理策略。</p>

<ul>
<li>  AbortPolicy 策略：该策略会直接抛出异常，阻止系统正常工作。</li>
</ul>

<pre><code>public static class AbortPolicy implements RejectedExecutionHandler {    public AbortPolicy() { }    public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {        throw new RejectedExecutionException(&quot;Task &quot; + r.toString() +                                                 &quot; rejected from &quot; +                                                 e.toString());    }}
</code></pre>

<ul>
<li>  CallerRunsPolicy 策略：只要线程池未关闭，该策略直接在调用者线程中，运行当前的被丢弃的任务。</li>
</ul>

<pre><code>public static class CallerRunsPolicy implements RejectedExecutionHandler {    public CallerRunsPolicy() { }    public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {        if (!e.isShutdown()) {                r.run();        }    }}
</code></pre>

<ul>
<li>  DiscardOleddestPolicy 策略： 该策略将丢弃最老的一个请求，也就是即将被执行的任务，并尝试再次提交当前任务。</li>
</ul>

<pre><code>public static class DiscardOldestPolicy implements RejectedExecutionHandler {    public DiscardOldestPolicy() { }    public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {        if (!e.isShutdown()) {            e.getQueue().poll();            e.execute(r);        }    }}
</code></pre>

<ul>
<li>  DiscardPolicy 策略：该策略默默的丢弃无法处理的任务，不予任何处理。</li>
</ul>

<pre><code>public static class DiscardPolicy implements RejectedExecutionHandler {    public DiscardPolicy() { }    public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {    }}
</code></pre>

<p>除了 JDK 默认为什么提供的四种拒绝策略，我们可以根据自己的业务需求去自定义拒绝策略，自定义的方式很简单，直接实现 RejectedExecutionHandler 接口即可</p>

<p>比如 Spring integration 中就有一个自定义的拒绝策略 CallerBlocksPolicy，将任务插入到队列中，直到队列中有空闲并插入成功的时候，否则将根据最大等待时间一直阻塞，直到超时。</p>

<pre><code>package org.springframework.integration.util;import java.util.concurrent.BlockingQueue;import java.util.concurrent.RejectedExecutionException;import java.util.concurrent.RejectedExecutionHandler;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;import org.apache.commons.logging.Log;import org.apache.commons.logging.LogFactory;public class CallerBlocksPolicy implements RejectedExecutionHandler {    private static final Log logger = LogFactory.getLog(CallerBlocksPolicy.class);    private final long maxWait;    /**     * @param maxWait The maximum time to wait for a queue slot to be     * available, in milliseconds.     */    public CallerBlocksPolicy(long maxWait) {        this.maxWait = maxWait;    }    @Override    public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) {        if (!executor.isShutdown()) {            try {                BlockingQueue&lt;Runnable&gt; queue = executor.getQueue();                if (logger.isDebugEnabled()) {                    logger.debug(&quot;Attempting to queue task execution for &quot; + this.maxWait + &quot; milliseconds&quot;);                }                if (!queue.offer(r, this.maxWait, TimeUnit.MILLISECONDS)) {                    throw new RejectedExecutionException(&quot;Max wait time expired to queue task&quot;);                }                if (logger.isDebugEnabled()) {                    logger.debug(&quot;Task execution queued&quot;);                }            }            catch (InterruptedException e) {                Thread.currentThread().interrupt();                throw new RejectedExecutionException(&quot;Interrupted&quot;, e);            }        }        else {            throw new RejectedExecutionException(&quot;Executor has been shut down&quot;);        }    }}
</code></pre>

<p>定义好之后如何使用呢？光定义没用的呀，一定要用到线程池中呀，可以通过下面的方式自定义线程池，指定拒绝策略。</p>

<pre><code>BlockingQueue&lt;Runnable&gt; workQueue = new ArrayBlockingQueue&lt;&gt;(100);ThreadPoolExecutor executor = new ThreadPoolExecutor(    10, 100, 10, TimeUnit.SECONDS, workQueue, new CallerBlocksPolicy());
</code></pre>

<h1 id="toc_4">execute 和 submit 的区别？</h1>

<p>在前面的讲解中，我们执行任务是用的 execute 方法，除了 execute 方法，还有一个 submit 方法也可以执行我们提交的任务。</p>

<p>这两个方法有什么区别呢？分别适用于在什么场景下呢？我们来做一个简单的分析。</p>

<p>execute 适用于不需要关注返回值的场景，只需要将线程丢到线程池中去执行就可以了</p>

<pre><code>public class ThreadPool {    public static void main(String[] args) {        ExecutorService pool = Executors.newFixedThreadPool(10);        pool.execute(() -&gt; {            System.out.println(Thread.currentThread().getName() + &quot;\t开始发车啦....&quot;);        });    }}
</code></pre>

<p>submit 方法适用于需要关注返回值的场景，submit 方法的定义如下：</p>

<pre><code>public interface ExecutorService extends Executor {　　...　　&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task);　　&lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result);　　Future&lt;?&gt; submit(Runnable task);　　...}
</code></pre>

<p>其子类 AbstractExecutorService 实现了 submit 方法, 可以看到无论参数是 Callable 还是 Runnable，最终都会被封装成 RunnableFuture，然后再调用 execute 执行。</p>

<pre><code>    /**     * @throws RejectedExecutionException {@inheritDoc}     * @throws NullPointerException       {@inheritDoc}     */    public Future&lt;?&gt; submit(Runnable task) {        if (task == null) throw new NullPointerException();        RunnableFuture&lt;Void&gt; ftask = newTaskFor(task, null);        execute(ftask);        return ftask;    }    /**     * @throws RejectedExecutionException {@inheritDoc}     * @throws NullPointerException       {@inheritDoc}     */    public &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result) {        if (task == null) throw new NullPointerException();        RunnableFuture&lt;T&gt; ftask = newTaskFor(task, result);        execute(ftask);        return ftask;    }    /**     * @throws RejectedExecutionException {@inheritDoc}     * @throws NullPointerException       {@inheritDoc}     */    public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) {        if (task == null) throw new NullPointerException();        RunnableFuture&lt;T&gt; ftask = newTaskFor(task);        execute(ftask);        return ftask;    }
</code></pre>

<p>下面我们来看看这三个方法分别如何去使用：</p>

<p><strong>submit(Callable<t style="max-width: 100%;box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: break-word !important;"> task);</t></strong></p>

<pre><code>public class ThreadPool {    public static void main(String[] args) throws Exception {        ExecutorService pool = Executors.newFixedThreadPool(10);        Future&lt;String&gt; future = pool.submit(new Callable&lt;String&gt;() {            @Override            public String call() throws Exception {                return &quot;Hello&quot;;            }        });        String result = future.get();        System.out.println(result);    }}
</code></pre>

<p><strong>submit(Runnable task, T result);</strong></p>

<pre><code>public class ThreadPool {    public static void main(String[] args) throws Exception {        ExecutorService pool = Executors.newFixedThreadPool(10);        Data data = new Data();        Future&lt;Data&gt; future = pool.submit(new MyRunnable(data), data);        String result = future.get().getName();        System.out.println(result);    }}class Data {    String name;    public String getName() {        return name;    }    public void setName(String name) {        this.name = name;    }}class MyRunnable implements Runnable {    private Data data;    public MyRunnable(Data data) {        this.data = data;    }    @Override    public void run() {        data.setName(&quot;yinjihuan&quot;);    }}
</code></pre>

<p><strong>Future submit(Runnable task);</strong><br/>
直接 submit 一个 Runnable 是拿不到返回值的，返回值就是 null.</p>

<h1 id="toc_5">五种线程池的使用场景</h1>

<ul>
<li><p>newSingleThreadExecutor：一个单线程的线程池，可以用于需要保证顺序执行的场景，并且只有一个线程在执行。</p></li>
<li><p>newFixedThreadPool：一个固定大小的线程池，可以用于已知并发压力的情况下，对线程数做限制。</p></li>
<li><p>newCachedThreadPool：一个可以无限扩大的线程池，比较适合处理执行时间比较小的任务。</p></li>
<li><p>newScheduledThreadPool：可以延时启动，定时启动的线程池，适用于需要多个后台线程执行周期任务的场景。</p></li>
<li><p>newWorkStealingPool：一个拥有多个任务队列的线程池，可以减少连接数，创建当前可用 cpu 数量的线程来并行执行。</p></li>
</ul>

<h1 id="toc_6">线程池的关闭</h1>

<p>关闭线程池可以调用 shutdownNow 和 shutdown 两个方法来实现</p>

<p><strong>shutdownNow：对正在执行的任务全部发出 interrupt()，停止执行，对还未开始执行的任务全部取消，并且返回还没开始的任务列表</strong></p>

<pre><code>public class ThreadPool {    public static void main(String[] args) throws Exception {        ExecutorService pool = Executors.newFixedThreadPool(1);        for (int i = 0; i &lt; 5; i++) {            System.err.println(i);            pool.execute(() -&gt; {                try {                    Thread.sleep(30000);                    System.out.println(&quot;--&quot;);                } catch (Exception e) {                    e.printStackTrace();                }            });        }        Thread.sleep(1000);        List&lt;Runnable&gt; runs = pool.shutdownNow();    }}
</code></pre>

<p>上面的代码模拟了立即取消的场景，往线程池里添加 5 个线程任务，然后 sleep 一段时间，线程池只有一个线程，如果此时调用 shutdownNow 后应该需要中断一个正在执行的任务和返回 4 个还未执行的任务，控制台输出下面的内容：</p>

<pre><code>01234[fs.ThreadPool$$Lambda$1/990368553@682a0b20, fs.ThreadPool$$Lambda$1/990368553@682a0b20, fs.ThreadPool$$Lambda$1/990368553@682a0b20, fs.ThreadPool$$Lambda$1/990368553@682a0b20]java.lang.InterruptedException: sleep interrupted    at java.lang.Thread.sleep(Native Method)    at fs.ThreadPool.lambda$0(ThreadPool.java:15)    at fs.ThreadPool$$Lambda$1/990368553.run(Unknown Source)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    at java.lang.Thread.run(Thread.java:745)
</code></pre>

<p><strong>shutdown：当我们调用 shutdown 后，线程池将不再接受新的任务，但也不会去强制终止已经提交或者正在执行中的任务</strong></p>

<pre><code>public class ThreadPool {    public static void main(String[] args) throws Exception {        ExecutorService pool = Executors.newFixedThreadPool(1);        for (int i = 0; i &lt; 5; i++) {            System.err.println(i);            pool.execute(() -&gt; {                try {                    Thread.sleep(30000);                    System.out.println(&quot;--&quot;);                } catch (Exception e) {                    e.printStackTrace();                }            });        }        Thread.sleep(1000);        pool.shutdown();        pool.execute(() -&gt; {            try {                Thread.sleep(30000);                System.out.println(&quot;--&quot;);            } catch (Exception e) {                e.printStackTrace();            }        });    }}
</code></pre>

<p>上面的代码模拟了正在运行的状态，然后调用 shutdown，接着再往里面添加任务，肯定是拒绝添加的，请看输出结果：</p>

<pre><code>01234Exception in thread &quot;main&quot; java.util.concurrent.RejectedExecutionException: Task fs.ThreadPool$$Lambda$2/1747585824@3d075dc0 rejected from java.util.concurrent.ThreadPoolExecutor@214c265e[Shutting down, pool size = 1, active threads = 1, queued tasks = 4, completed tasks = 0]    at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)    at fs.ThreadPool.main(ThreadPool.java:24)
</code></pre>

<p>还有一些业务场景下需要知道线程池中的任务是否全部执行完成，当我们关闭线程池之后，可以用 isTerminated 来判断所有的线程是否执行完成，千万不要用 isShutdown，isShutdown 只是返回你是否调用过 shutdown 的结果。</p>

<pre><code>public class ThreadPool {    public static void main(String[] args) throws Exception {        ExecutorService pool = Executors.newFixedThreadPool(1);        for (int i = 0; i &lt; 5; i++) {            System.err.println(i);            pool.execute(() -&gt; {                try {                    Thread.sleep(3000);                    System.out.println(&quot;--&quot;);                } catch (Exception e) {                    e.printStackTrace();                }            });        }        Thread.sleep(1000);        pool.shutdown();        while(true){              if(pool.isTerminated()){                  System.out.println(&quot;所有的子线程都结束了！&quot;);                  break;              }              Thread.sleep(1000);            }      }}
</code></pre>

<h1 id="toc_7">自定义线程池</h1>

<p>在实际的使用过程中，大部分我们都是用 Executors 去创建线程池直接使用，如果有一些其他的需求，比如指定线程池的拒绝策略，阻塞队列的类型，线程名称的前缀等等，我们可以采用自定义线程池的方式来解决。</p>

<p>如果只是简单的想要改变线程名称的前缀的话可以自定义 ThreadFactory 来实现，在 Executors.new… 中有一个 ThreadFactory 的参数，如果没有指定则用的是 DefaultThreadFactory。</p>

<p>自定义线程池核心在于创建一个 ThreadPoolExecutor 对象，指定参数</p>

<p>下面我们看下 ThreadPoolExecutor 构造函数的定义：</p>

<pre><code>public ThreadPoolExecutor(int corePoolSize,                              int maximumPoolSize,                              long keepAliveTime,                              TimeUnit unit,                              BlockingQueue&lt;Runnable&gt; workQueue,                              ThreadFactory threadFactory,                              RejectedExecutionHandler handler) ;
</code></pre>

<ul>
<li><p>corePoolSize<br/>
线程池大小，决定着新提交的任务是新开线程去执行还是放到任务队列中，也是线程池的最最核心的参数。一般线程池开始时是没有线程的，只有当任务来了并且线程数量小于 corePoolSize 才会创建线程。</p></li>
<li><p>maximumPoolSize<br/>
最大线程数，线程池能创建的最大线程数量。</p></li>
<li><p>keepAliveTime<br/>
在线程数量超过 corePoolSize 后，多余空闲线程的最大存活时间。</p></li>
<li><p>unit<br/>
时间单位</p></li>
<li><p>workQueue<br/>
存放来不及处理的任务的队列，是一个 BlockingQueue。</p></li>
<li><p>threadFactory<br/>
生产线程的工厂类，可以定义线程名，优先级等。</p></li>
<li><p>handler<br/>
拒绝策略，当任务来不及处理的时候，如何处理, 前面有讲解。</p></li>
</ul>

<p>了解上面的参数信息后我们就可以定义自己的线程池了，我这边用 ArrayBlockingQueue 替换了 LinkedBlockingQueue，指定了队列的大小，当任务超出队列大小之后使用 CallerRunsPolicy 拒绝策略处理。</p>

<p>这样做的好处是严格控制了队列的大小，不会出现一直往里面添加任务的情况，有的时候任务处理的比较慢，任务数量过多会占用大量内存，导致内存溢出。</p>

<p>当然你也可以在提交到线程池的入口进行控制，比如用 CountDownLatch, Semaphore 等。</p>

<pre><code>/** * 自定义线程池&lt;br&gt; * 默认的newFixedThreadPool里的LinkedBlockingQueue是一个无边界队列，如果不断的往里加任务，最终会导致内存的不可控&lt;br&gt; * 增加了有边界的队列，使用了CallerRunsPolicy拒绝策略 * @author yinjihuan * */public class FangjiaThreadPoolExecutor {    private static ExecutorService executorService = newFixedThreadPool(50);    private static ExecutorService newFixedThreadPool(int nThreads) {        return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS,                new ArrayBlockingQueue&lt;Runnable&gt;(10000), new DefaultThreadFactory(), new CallerRunsPolicy());    }    public static void execute(Runnable command) {        executorService.execute(command);    }    public static void shutdown() {        executorService.shutdown();    }    static class DefaultThreadFactory implements ThreadFactory {        private static final AtomicInteger poolNumber = new AtomicInteger(1);        private final ThreadGroup group;        private final AtomicInteger threadNumber = new AtomicInteger(1);        private final String namePrefix;        DefaultThreadFactory() {            SecurityManager s = System.getSecurityManager();            group = (s != null) ? s.getThreadGroup() :                                  Thread.currentThread().getThreadGroup();            namePrefix = &quot;FSH-pool-&quot; +                          poolNumber.getAndIncrement() +                         &quot;-thread-&quot;;        }        public Thread newThread(Runnable r) {            Thread t = new Thread(group, r,                                  namePrefix + threadNumber.getAndIncrement(),                                  0);            if (t.isDaemon())                t.setDaemon(false);            if (t.getPriority() != Thread.NORM_PRIORITY)                t.setPriority(Thread.NORM_PRIORITY);            return t;        }    }}
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[不理解Zookeeper一致性原理，谈何异地多活改造]]></title>
    <link href="http://panlw.github.io/15274395979385.html"/>
    <updated>2018-05-28T00:46:37+08:00</updated>
    <id>http://panlw.github.io/15274395979385.html</id>
    <content type="html"><![CDATA[
<p>原创： 陈东明 DBAplus社群 <em>1周前</em></p>

<p><strong>作者介绍</strong><br/>
*<strong><em>陈东明</em></strong>*，饿了么北京技术中心架构组负责人，负责饿了么的产品线架构设计及基础架构研发工作。曾任百度架构师，负责百度即时通讯产品的架构设计。具有丰富的大规模系统构建和基础架构的研发经验，善于复杂业务需求下的大并发、分布式系统设计和持续优化。<br/>
在2017年饿了么做异地多活建设之时，我的团队承担了Zookeeper的异地多活改造。在此期间，我听到了关于Zookeeper一致性的两种不同说法：</p>

<ul>
<li>一种说法是Zookeeper是最终一致性，由于多副本，以及保证大多数成功的Zab协议，当一个客户端进程写入一个新值，另一个客户端进程不能保证马上就会读到，但能保证最终会读到这个值；</li>
<li>另一种说法是Zookeeper的Zab协议类似于Paxos协议，并且提供了强一致性。</li>
</ul>

<p>每当听到这两种说法，我都想纠正一下——不对，Zookeeper是顺序一致性（sequential consistency）。但解释起来比较复杂，需要一篇长文来说明，于是就有了本文，下面就和大家一起讨论下我的看法。</p>

<p>*<strong><em>什么是sequetial consistency</em></strong>*</p>

<p>*<strong><em>从Zookeeper的文档中我们可以看到，里面明确写出它的一致性是sequential consistency。</em></strong>*（详细参见Zookeeper官方文档[1]）</p>

<p>那么，什么是sequential consistency呢？</p>

<p>sequential consistency是Lamport在1979年首次提出的。（详细参见他的这篇论文：<em>How to make a multiprocessor computer that correctly executes multiprocess programs</em>）</p>

<p>论文中定义，当满足下面这个条件时就是sequential consistency：</p>

<p>“<br/>
<em>the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.</em></p>

<p>这段英文定义很晦涩（这是Lamport大神的一贯风格，严谨但晦涩，Paxos协议也是如此），我第一次看到时的感觉是：“这是什么鬼？”为啥每个英文单词我都认识，但就是不知道他在说什么？第一次看到这句话和我有同感的小伙伴留个言吧~</p>

<p>后文我再把这段英文定义翻译成中文，现在我们先看看这篇论文的标题和定义中出现的一个关键词，来说明一下sequential consistency的应用范围。</p>

<p>论文的标题和定义中包含<strong>“multiprocessor</strong>”****这个词，multiprocessor是多核处理器的意思。从这个关键字来看，sequential consistency是用来定义多核处理器和跑在多核处理器上的程序的一个特性。Lamport这篇论文的标题可以翻译成：“如何让具有多核处理器的计算机正确执行多进程程序”，也就是说如果一个多核处理器具有sequential consistency的特性，这个多核处理器就可以正确运行，后面我会解释这个正确运行是什么意思（也就是本文后面讲到的sequential consistency的作用）。</p>

<p>从这个标题我们还可以看出，sequential consistency<strong>应该是个并发编程（concurrent programming）领域的概念</strong>。但我们现在常常在分布式系统领域讨论sequential consistency，比如本文主要讨论的Zookeeper（Zookeeper很明显是一个分布式系统）的一致性。实际上，多核处理器上运行的多个程序，其实也是一种分布式系统（Lamport在他的这篇 <em>Time, Clocks, and the Ordering of Events in a Distributed System</em> 分布式系统的开山之作中也阐述了这个观点）。所以虽然sequential consistency最早在并发编程中提出，但是它可以<strong>应用在分布式系统中</strong>，比如本文讨论的Zookeeper这种分布式存储系统。另外一个比较重要的Linearizability（线性一致性），也是在并发编程中最早提出的，目前也被广泛应用在分布式系统领域中。</p>

<p>接下来我们尝试翻译一下上文那段晦涩的定义。做这段翻译让我找到了上学时做阅读理解的感觉，我先不直接翻译，因为就算我把它翻译成中文，估计很多人还是会有“为啥每个中文字我都懂，还是不知道在说什么”的感觉。</p>

<p>首先，我来解释一下个别的词。首先，<strong>“any execution”</strong>是什么意思？你有多个程序（program）在多核处理器上运行，例如你有两个程序，第一个程序叫P1，它的代码如下： <br/>
P1_write(x);<br/>
P1_read(y);</p>

<p>第二个程序叫P2，代码如下：<br/>
P2_write(u);<br/>
P2_read(v);</p>

<p>从理论上来讲，两个程序运行在两个独立处理器的核上，有多少种执行的可能？我列举其中几种来举例说明。</p>

<p>第一种：<br/>
P1---write(x)--------read(y)--------<br/>
P2-----------write(u)-------read(v)-</p>

<p>第二种：</p>

<p>P1----------write(x)-read(y)--------<br/>
P2--write(u)----------------read(v)-</p>

<p>第三种：<br/>
P1---read(y)----------write(x)------<br/>
P2-----------write(u)---------read(v)-</p>

<p>我们有24种可能的执行顺序，也就是这四个操作任意的排列组合，也就是4!=24。类似第一种和第二种这样的可能性很好理解。为什么会出现像第三种这样的可能的执行呢？那是因为就算在同一个程序中，由于处理会有多级的缓存，以及处理器中coherence的存在，虽然你的程序中是先write后read，在内存中真正生效的顺序，也有可能是先read后write。</p>

<p>其实还会出现类似下面这样的执行，两个操作在两个处理器上同时执行。<br/>
P1--write(x)-read(y)--------<br/>
P2--write(u)--------read(v)-</p>

<p>如果加上同时运行的这种情况，那就有更多种可能性。我的算数不太好，这里就不继续算下去了，因为到底有多少个不重要，重要的是要知道有很多种可能性。那么定义中的“any execution”，就是指任意一种可能的执行，在定义中也可以理解为所有的这些可能的执行。</p>

<p>接着我们再来解释一个词——<strong>“sequential order”</strong>。什么叫sequential order？我们来翻一下英语词典（感觉更像在做阅读理解了）。<br/>
“<br/>
<em>sequential：连续的；相继的；有顺序的</em><br/>
<em>order：命令；顺序；规则；[贸易] 定单</em></p>

<p>sequential order——有顺序的顺序，这又是什么鬼？</p>

<p>其实sequential是有一个接一个的意思，在处理器的这种上下文中，sequential就是指操作（operartion）一个接一个地执行，也就是顺序执行，并且没有重叠。order是指经过一定的调整，让某样东西按照一定的规则变得有序。比如，在算法中的排序算法就是ordering，就是让数组这个东西按照从大到小或从小到大的规则变得有序。<strong>那么sequential order就是指让操作（operation）按照一个接一个这样的规则排列，并且没有重叠。</strong></p>

<p>再说回上文的例子，如果把四个操作，按一个接一个的规则排列，这时就可以得到4！的排列组合个可能的排列（order），同样的，有多少个不重要。</p>

<p>比如：<br/>
P1_write(x);P1_read(y);P2_write(u);P2_read(v);<br/>
P1_read(y);P1_write(x);P2_write(u);P2:read(v);<br/>
P2_write(u);P2_read(v);P1_read(y);P1:write(x);</p>

<p>我这里只列举其中三个，其他的大家可以自己排一下。</p>

<p>重点来了，其实sequential order就是让这四个操作按照一个接一个的顺序执行，并且没有重叠。注意这个排列不是真实的执行，真实的执行是any execution，这里说的是逻辑上的假设，也就是为什么定义有一个as if。</p>

<p>做了这么多的铺垫，下面我们开始翻译定义中的第一句话：</p>

<p>“<br/>
*任意一种可能的执行效果，与所有的处理器上的某一种操作按照顺序排列执行的效果是一样的。 *</p>

<p>注意，some在这里是指“某一种”的意思，不是一些，因为order是单数（真的是在做阅读理解）。</p>

<p>*<strong><em>这句话的意思就是说，一个处理器要满足这个条件，就只能允许满足这个条件的那些可能的执行存在，其他不满足的可能的执行都不会出现。</em></strong>*</p>

<p>从第一句话中我们可以看出，一种多核处理器要想满足sequential consistency，那么多个程序在多个核运行效果“等同”于在一个核上顺序执行所有操作的效果是差不多的。如果这样的话，其实多核的威力基本就消失了。所以无论是从Lamport写这篇论文的1979年，还是现在，没有任何一个现实的多核处理器实现了sequential consistency。</p>

<p>那么，为什么Lamport大神提出这样一个不现实的概念呢？（要注意Lamport写这篇论文时，并没有把它引申到分布式系统领域，就是针对多核处理器、并发编程领域提出的）我们稍后再论述。</p>

<p>这里还要注意的一点是，在我的翻译里用了“效果”一词，但实际上英文原文中用的是“result（结果）”一词。那效果和结果有什么区别吗？我们解释一下什么叫执行结果。</p>

<p>不管是任何真实的执行，还是某种经过顺序排序后的假设执行，程序会产生一定的结果，比如print出来的结果（result）。实际上定义中说的是结果一样。如果定义中用“效果”的话，那这个定义就只是一个定性的定义，如果用“结果”的话，那这个定义就是一个定量的定义。定量的，也就是说可以通过数学证明的。从这点我们可以看出，大神就是不一样，任何理论都是可以通过数学证明为正确的。本文后面还会提到证明的事情，我们这里再卖个关子。</p>

<p>到这里，关于定义的第一句，更准确的翻译就是： </p>

<p>“<br/>
*任意一种可能的执行的结果，与某一种所有的处理器上的操作按照顺序排列执行的结果是一样的。 *</p>

<p>这里我们还要注意一点，结果一样就意味着，如果有人真的要实现一种sequential consistency的多核处理器的话，因为要保证结果一样，所以他是有一定的空间来优化，而不会完全是一个核顺序执行的效果。但是估计这种优化也是非常有限的。</p>

<p>好了，终于把最难的第一句话解释完了，大家可以松口气，第二句就非常简单了。我们还是先解释第二句种出现的一个词——<strong>“sequence”</strong>。刚刚解释过的sequential order是顺序排序（就是按一个接一个排序），其实这是一个动作，动作会产生结果，它的结果产生了一个操作（operation）的队列。第二句中出现的sequence就是指这个操作（operation）的队列。</p>

<p>好，那第二句的翻译就是：</p>

<p>“<br/>
*并且每个独立的处理器的操作，都会按照程序指定的顺序出现在操作队列中。 *</p>

<p>也就是说如果程序里是先write(x);后read(y);，那么只有满足这个顺序的操作队列是符合条件的。这样，我们刚刚说的很多可能的执行就少了很多，这里我也就不计算少了多少，还是那句话，数量不重要，反正是有，而且变少了。</p>

<p>那么第二句话意味着什么？意味着如果一个多核处理器实现了sequential consistency，那这种多核处理器基本上就告别自（缓）行（存）车了。这里我还要继续卖关子，连缓存这种最有效提高处理器性能的优化都没了，大神为什么要提出这个概念？</p>

<p>好了，到这里我们可以把两句翻译合起来，完整看一下：</p>

<p>“<br/>
*任意一种可能的执行的结果，与某一种所有的处理器上的操作按照顺序排列执行的结果是一样的，并且每个独立的处理器的操作，都会按照程序指定的顺序出现在操作队列中。 *</p>

<p>从这个定义中，我们可以看出，此概念的核心就是sequential order，这也就是为什么Lamport老爷子把这种一致性模型称之为sequential consistency。可以说这个命名是非常贴切的，不知道这种贴切对于以英语为母语的人来说是不是更好理解一些，应该不会出现“顺序的顺序是什么鬼”这种情况。如果你看完本文，也觉得sequential很贴切的话，那就说明我讲清楚了。</p>

<p>接下来我们举个具体的例子，再来说明一下： <br/>
execution A<br/>
P0 writex=1-------------------------------<br/>
P1 -------write x=2----------------------<br/>
P2 -----------------read x<mark>1--read x</mark>2<br/>
P3 -----------------read x<mark>1--read x</mark>2</p>

<p>sequetial order: P0_write x=1,P3_read x<mark>1,P4_read x</mark>1,P1_write x=2,P3_read x<mark>2,P4_read x</mark>2</p>

<p>execution B<br/>
P0 write=1-------------------------------<br/>
P1 -------write x=2----------------------<br/>
P2 -----------------read x<mark>2--read x</mark>1<br/>
P3 -----------------read x<mark>2--read x</mark>1</p>

<p>sequetial order: P1_write x=2,P3_read x<mark>2,P4_read x</mark>2,P0_write x=1,P3_read x<mark>1,P4_read x</mark>1</p>

<p>execution C<br/>
P0 write=1-------------------------------<br/>
P1 -------write x=2----------------------<br/>
P2 -----------------read x<mark>1--read x</mark>2<br/>
P3 -----------------read x<mark>2--read x</mark>1</p>

<p>sequetial order: 你找不出一个符合定义中2个条件的一种order。</p>

<p>所以说如果一个多核处理器只允许execution A和B出现，不允许C出现，那么这个多核处理器就是sequetial consistency的。如果它允许C出现，那它就不是sequetial consistency。</p>

<p>到这里，我们已经完整地解释完什么是sequetial consistency。但是，细心的朋友可能会问，如果你的program是多线程的程序怎么办？那我们再把定义中最后的一个细节解释一下：program这个词。</p>

<p>*<strong><em>program</em></strong>*是指可以直接运行在处理器上的指令序列。这个并不是program的严格定义，但是我要指出的是这个program是在操作系统都没有的远古时代就存在的概念，在上文的定义中prgram就是指那个时代的program。</p>

<p>这个program里没有进程、线程的概念，这些概念都是在有了操作系统之后才出现的。因为没有操作系统，也没有内存空间的概念。不像我们现在所说的程序（program），不同的程序有自己独立的内存地址空间。我们这里，内存（memory）对于不同的program来说是shared。另外，需要注意的是program可以用来说明各种程序，不管你是操作系统内核，还是应用程序，都适用。</p>

<p>*<strong><em>sequential consistency</em></strong>*<br/>
*<strong><em>是分布式领域的概念</em></strong>*</p>

<p>刚刚我们说了，sequential consistency虽然是针对并发编程领域提出的，但实际上它是分布式领域的概念，特别是分布式存储系统。在 <em>Distributed system: Principles and Paradigms</em> （作者Andrew S.Tanenbaum, Maarten Van Steen），作者稍微修改了一下Lamport的定义，让这个定义更贴近分布式领域中的概念，我们来看一下作者是怎么改的： </p>

<p>“<br/>
*The result of any execution is the same as if the (read and write) operations by all processes on the data store were executed in some sequential order and the operations of-each individual process appear in this sequence in the order specified by its program. *</p>

<p>作者把processor换成了process，并且加了on the data store这个限定，在Lamport的定义里没有这个限定，其实默认指的是memory（内存）。process就是指进程，以Zookeeper为例，就是指访问Zookeeper的应用进程。program也不是那么底层概念，也是基于操作系统的应用程序了。</p>

<p>*<strong><em>sequential consistency的作用</em></strong>*</p>

<p>好了，下面该揭晓我上面卖的两个关子了。在Lamport的论文中，给出了一个小例子，如下： <br/>
process 1<br/>
 a := 1;<br/>
 if b = 0 then critical section:<br/>
 a := 0<br/>
 else ... fi</p>

<p>process 2<br/>
 b := 1;<br/>
 if a = 0 then critical section:<br/>
 b := 0<br/>
 else ... fi</p>

<p>Lamport在论文中说，如果一种多核处理满足sequential consistency的条件，<strong>那么最多只有一个程序能够进入critical section</strong>。在论文中，Lamport老爷子并没有解释为什么最多只有一个程序能够进入critical section。而是把这个证明留给了论文的读者，就像我们常见的教科书中的课后习题一样。</p>

<p>Lamport老爷子应该是认为这个证明太简单了，不需要花费笔墨来证明它。sequential consistency这篇论文只有不到两页A4纸，是我见过的最短的论文。这是Lamport一贯的做事风格，他的Paxos论文中，有很多细节都是一笔带过的，给读者留下无尽的遐想（瞎想）。</p>

<p>假设现在我们已经证明这个是正确的（虽然我也没去证明，论文给出了两个参考文献用于证明），那这个例子说明了什么呢？</p>

<p>大家也许注意到了，这个例子没有用到任何锁，但它实现了critical section，critical section是一种多线程synchronization 机制。如果多核处理器是sequential consistency的，那么你写的并发程序“天然就是正确的”。</p>

<p>但是处理器的设计者为了追求性能，将保证程序正确的任务丢给程序开发者。只在硬件级别提供了一些fence、cas等指令，基于这些指令操作内核和语言基础库实现了各种synchronization机制，用来保证操作系统的正确性和应用程序的正确性。程序员必须小心谨慎地使用线程和这些synchronization机制，否则就会出各种意想不到的问题。如果你没有debug一个多线程bug连续加班两天，那说明你是大神。</p>

<p>这些指令都是具有更高一致性级别，也就是linearizability，（关于linearizability可以参见我的另外一篇文章《线性一致性（Linearizability）是并发控制的基础》[2]），虽然一致性级别高，但只是个别指令的，处理器整体只是实现了比sequential consistency低很多的一致性级别。所以实现难度大大降低了。</p>

<p>虽然Lamport老爷子的sequential consistency概念在concurrent programming领域中还没有实际意义，但却给我们指出了程序员的天堂在哪里。在程序员的天堂里，没有多（车）线（来）程（车）编（往）程，只要写程序就行。你面试的时候不会再有人问你多线程编程，不会再问你各种锁。</p>

<p>在分布式领域中，sequential consistency更实际一些。Zookeeper就实现了sequential consistency。同理，这应该也是可以证明的，但目前还没发现有Zookeeper社区有任何论文来证明这个。如果你已经明白上面解释的定义，就可以想清楚Zookeeper是sequential consistency。欢迎大家一起来探讨。</p>

<p>*<strong><em>为何Zookeeper要实现</em></strong>*<br/>
*<strong><em>sequential consistency</em></strong>*</p>

<p>实际上，Zookeeper的一致性更复杂一些，Zookeeper的读操作是sequential consistency的，Zookeeper的写操作是linearizability的。关于这个说法，Zookeeper的官方文档中没有写出来，但在社区的邮件组有详细的讨论（邮件组的讨论参见[3]）。</p>

<p>另外，在 <em>Modular Composition of Coordination Services</em> 这篇关于Zookeeper的论文中也有提到这个观点（这篇论文不是Zookeeper的主流论文，但全面分析了Zookeeper的特性，以及Zookeeper跨机房方案，饿了么的Zookeeper异地多活改造也参考了这篇论文中的一些观点），我们可以这么理解Zookeeper，从整体（read操作+write操作）上来说是sequential consistency，写操作实现了linearizability。</p>

<p>通过简单的推理，我们可以得出Lamport论文中的小例子，在Zookeeper中也是成立的。我们可以这样实现分布式锁。但Zookeeper官方推荐的分布式实现方法并没有采用这个方式，而是利用了Zookeeper的linearizability特性实现了分布式锁（关于Zookeeper官方是如何实现分布式锁的，请参见我这篇文章《Zookeeper实现分布式锁和选主》[4]）。</p>

<p>为什么Zookeeper要实现sequential consistency？Zookeeper最核心的功能是用来做coordination service，也就是用来做分布式锁服务，在分布式的环境下，Zookeeper本身怎么做到“天然正确”？没有其他的synchronization机制保证Zookeeper是正确的，所以只要Zookeeper实现了sequential consistency，那它自身就可以保证正确性，从而对外提供锁服务。 </p>

<p>*<strong><em>参考文章：</em></strong>*<br/>
[1]<a href="http://zookeeper.apache.org/doc/r3.4.9/zookeeperProgrammers.html#ch_zkGuarantees">http://zookeeper.apache.org/doc/r3.4.9/zookeeperProgrammers.html#ch_zkGuarantees</a><br/>
[2]<a href="https://blog.csdn.net/cadem/article/details/79932574">https://blog.csdn.net/cadem/article/details/79932574</a><br/>
[3]<a href="http://comments.gmane.org/gmane.comp.java.hadoop.zookeeper.user/5221">http://comments.gmane.org/gmane.comp.java.hadoop.zookeeper.user/5221</a><br/>
[4]<a href="https://blog.csdn.net/cadem/article/details/56289825">https://blog.csdn.net/cadem/article/details/56289825</a></p>

<p><strong>近期热文</strong><br/>
<a href="http://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650767616&amp;idx=1&amp;sn=5ef24be557292923c5ce8c0daa24f2cf&amp;chksm=f3f93495c48ebd83f9a0a70c67abcb5785362606ccdde8751fccd2ba1cf62b43913f2d5026df&amp;scene=21#wechat_redirect">_深入浅出分布式缓存的通用方法_</a><br/>
<a href="http://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650767604&amp;idx=1&amp;sn=c3e41c1f9f645b1db91e90d87cbd6953&amp;chksm=f3f93561c48ebc77bf04cf44c22710d487f16a3e43bb75e4c2d0fe380a80606a3e97f5948b79&amp;scene=21#wechat_redirect">_一文详解消息队列的常见功能场景与使用精髓_</a><br/>
<a href="http://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650767596&amp;idx=1&amp;sn=4c438f4bd2e65d0b9cd92150380d8b18&amp;chksm=f3f93579c48ebc6f7a306c84bdaa9e2b038ed82ccb37440eafb2f5ff99aa1db06bf97f4e71a5&amp;scene=21#wechat_redirect">_MySQL上云后引发的雪崩_</a><br/>
<a href="http://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650767592&amp;idx=1&amp;sn=d2c79676981c63d70d7275486c88abdf&amp;chksm=f3f9357dc48ebc6b50f01ee6c4329df0ea1c4b0d416969911f52805dc3b44ec408f75d250b37&amp;scene=21#wechat_redirect">_方法论与技术栈双管齐下的运维可用性能力建设_</a><br/>
<a href="http://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650767557&amp;idx=1&amp;sn=9a1935096cbe2fbd658dc28c8f52a402&amp;chksm=f3f93550c48ebc462c16e23d3e6941b28fd71ce50c790d8e39188e20f57f4a34b3e01ef22409&amp;scene=21#wechat_redirect">_中小型企业大数据体系建设的核心技术选型_</a></p>

<p><strong>近期活动</strong><br/>
<a href="http://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650767615&amp;idx=1&amp;sn=243dd45953cad2716f389bf25f99a01d&amp;chksm=f3f9356ac48ebc7c4ed927bfa23def9a14f927d08e4e255c026c9587de83a69062cfc4b057ad&amp;scene=21#wechat_redirect">_2018 DAMS中国数据资产管理峰会_</a><br/>
微信扫一扫<br/>
关注该公众号</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[消息队列之 RabbitMQ]]></title>
    <link href="http://panlw.github.io/15274392082175.html"/>
    <updated>2018-05-28T00:40:08+08:00</updated>
    <id>http://panlw.github.io/15274392082175.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://juejin.im/post/5a67f7836fb9a01cb74e8931">原文地址</a></p>
</blockquote>

<p>关于消息队列，从前年开始断断续续看了些资料，想写很久了，但一直没腾出空，近来分别碰到几个朋友聊这块的技术选型，是时候把这块的知识整理记录一下了。</p>

<p>市面上的消息队列产品有很多，比如老牌的 ActiveMQ、RabbitMQ ，目前我看最火的 Kafka ，还有 ZeroMQ ，去年底阿里巴巴捐赠给 Apache 的 RocketMQ ，连 redis 这样的 NoSQL 数据库也支持 MQ 功能。总之这块知名的产品就有十几种，就我自己的使用经验和兴趣只打算谈谈 RabbitMQ、Kafka 和 ActiveMQ ，本文先讲 RabbitMQ ，在此之前先看下消息队列的相关概念。</p>

<h1 id="toc_0">什么叫消息队列</h1>

<p>消息（Message）是指在应用间传送的数据。消息可以非常简单，比如只包含文本字符串，也可以更复杂，可能包含嵌入对象。</p>

<p>消息队列（Message Queue）是一种应用间的通信方式，消息发送后可以立即返回，由消息系统来确保消息的可靠传递。消息发布者只管把消息发布到 MQ 中而不用管谁来取，消息使用者只管从 MQ 中取消息而不管是谁发布的。这样发布者和使用者都不用知道对方的存在。</p>

<h1 id="toc_1">为何用消息队列</h1>

<p>从上面的描述中可以看出消息队列是一种应用间的异步协作机制，那什么时候需要使用 MQ 呢？</p>

<p>以常见的订单系统为例，用户点击【下单】按钮之后的业务逻辑可能包括：扣减库存、生成相应单据、发红包、发短信通知。在业务发展初期这些逻辑可能放在一起同步执行，随着业务的发展订单量增长，需要提升系统服务的性能，这时可以将一些不需要立即生效的操作拆分出来异步执行，比如发放红包、发短信通知等。这种场景下就可以用 MQ ，在下单的主流程（比如扣减库存、生成相应单据）完成之后发送一条消息到 MQ 让主流程快速完结，而由另外的单独线程拉取 MQ 的消息（或者由 MQ 推送消息），当发现 MQ 中有发红包或发短信之类的消息时，执行相应的业务逻辑。</p>

<p>以上是用于业务解耦的情况，其它常见场景包括最终一致性、广播、错峰流控等等。</p>

<h1 id="toc_2">RabbitMQ 特点</h1>

<p>RabbitMQ 是一个由 Erlang 语言开发的 AMQP 的开源实现。</p>

<p>AMQP ：Advanced Message Queue，高级消息队列协议。它是应用层协议的一个开放标准，为面向消息的中间件设计，基于此协议的客户端与消息中间件可传递消息，并不受产品、开发语言等条件的限制。</p>

<p>RabbitMQ 最初起源于金融系统，用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。具体特点包括：</p>

<ol>
<li><p>可靠性（Reliability） RabbitMQ 使用一些机制来保证可靠性，如持久化、传输确认、发布确认。</p></li>
<li><p>灵活的路由（Flexible Routing） 在消息进入队列之前，通过 Exchange 来路由消息的。对于典型的路由功能，RabbitMQ 已经提供了一些内置的 Exchange 来实现。针对更复杂的路由功能，可以将多个 Exchange 绑定在一起，也通过插件机制实现自己的 Exchange 。</p></li>
<li><p>消息集群（Clustering） 多个 RabbitMQ 服务器可以组成一个集群，形成一个逻辑 Broker 。</p></li>
<li><p>高可用（Highly Available Queues） 队列可以在集群中的机器上进行镜像，使得在部分节点出问题的情况下队列仍然可用。</p></li>
<li><p>多种协议（Multi-protocol） RabbitMQ 支持多种消息队列协议，比如 STOMP、MQTT 等等。</p></li>
<li><p>多语言客户端（Many Clients） RabbitMQ 几乎支持所有常用语言，比如 Java、.NET、Ruby 等等。</p></li>
<li><p>管理界面（Management UI） RabbitMQ 提供了一个易用的用户界面，使得用户可以监控和管理消息 Broker 的许多方面。</p></li>
<li><p>跟踪机制（Tracing） 如果消息异常，RabbitMQ 提供了消息跟踪机制，使用者可以找出发生了什么。</p></li>
<li><p>插件机制（Plugin System） RabbitMQ 提供了许多插件，来从多方面进行扩展，也可以编写自己的插件。</p></li>
</ol>

<h1 id="toc_3">RabbitMQ 中的概念</h1>

<h3 id="toc_4">消息模型</h3>

<p>所有 MQ 产品从模型抽象上来说都是一样的过程： 消费者（consumer）订阅某个队列。生产者（producer）创建消息，然后发布到队列（queue）中，最后将消息发送到监听的消费者。</p>

<p><img src="https://user-gold-cdn.xitu.io/2018/1/24/161260568dd200d6?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/></p>

<h3 id="toc_5">RabbitMQ 基本概念</h3>

<p>上面只是最简单抽象的描述，具体到 RabbitMQ 则有更详细的概念需要解释。上面介绍过 RabbitMQ 是 AMQP 协议的一个开源实现，所以其内部实际上也是 AMQP 中的基本概念：</p>

<p><img src="https://user-gold-cdn.xitu.io/2018/1/24/161260568dd66584?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/></p>

<ol>
<li> Message 消息，消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括 routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。</li>
<li> Publisher 消息的生产者，也是一个向交换器发布消息的客户端应用程序。</li>
<li> Exchange 交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。</li>
<li> Binding 绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。</li>
<li> Queue 消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。</li>
<li> Connection 网络连接，比如一个 TCP 连接。</li>
<li> Channel 信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的 TCP 连接内地虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。</li>
<li> Consumer 消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。</li>
<li> Virtual Host 虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。</li>
<li> Broker 表示消息队列服务器实体。</li>
</ol>

<h4 id="toc_6">AMQP 中的消息路由</h4>

<p>AMQP 中消息的路由过程和 Java 开发者熟悉的 JMS 存在一些差别，AMQP 中增加了 Exchange 和 Binding 的角色。生产者把消息发布到 Exchange 上，消息最终到达队列并被消费者接收，而 Binding 决定交换器的消息应该发送到那个队列。</p>

<p><img src="https://user-gold-cdn.xitu.io/2018/1/24/161260568e434217?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/></p>

<h4 id="toc_7">Exchange 类型</h4>

<p>Exchange 分发消息时根据类型的不同分发策略有区别，目前共四种类型：direct、fanout、topic、headers 。headers 匹配 AMQP 消息的 header 而不是路由键，此外 headers 交换器和 direct 交换器完全一致，但性能差很多，目前几乎用不到了，所以直接看另外三种类型：</p>

<ol>
<li> direct <img src="https://user-gold-cdn.xitu.io/2018/1/24/161260568dc498b6?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/> 消息中的路由键（routing key）如果和 Binding 中的 binding key 一致， 交换器就将消息发到对应的队列中。路由键与队列名完全匹配，如果一个队列绑定到交换机要求路由键为 “dog”，则只转发 routing key 标记为“dog” 的消息，不会转发 “dog.puppy”，也不会转发“dog.guard” 等等。它是完全匹配、单播的模式。</li>
<li> fanout <img src="https://user-gold-cdn.xitu.io/2018/1/24/161260568fe5ce35?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/> 每个发到 fanout 类型交换器的消息都会分到所有绑定的队列上去。fanout 交换器不处理路由键，只是简单的将队列绑定到交换器上，每个发送到交换器的消息都会被转发到与该交换器绑定的所有队列上。很像子网广播，每台子网内的主机都获得了一份复制的消息。fanout 类型转发消息是最快的。</li>
<li> topic <img src="https://user-gold-cdn.xitu.io/2018/1/24/161260569051565f?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/> topic 交换器通过模式匹配分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。它将路由键和绑定键的字符串切分成单词，这些单词之间用点隔开。它同样也会识别两个通配符：符号 “#” 和符号“<em>”。# 匹配 0 个或多个单词，</em> 匹配不多不少一个单词。</li>
</ol>

<h1 id="toc_8">RabbitMQ 安装</h1>

<p>一般来说安装 RabbitMQ 之前要安装 Erlang ，可以去 <a href="https://link.juejin.im?target=http%3A%2F%2Fwww.erlang.org%2Fdownloads">Erlang 官网</a>下载。接着去 <a href="https://link.juejin.im?target=https%3A%2F%2Fwww.rabbitmq.com%2Fdownload.html">RabbitMQ 官网</a>下载安装包，之后解压缩即可。根据操作系统不同官网提供了相应的安装说明：<a href="https://link.juejin.im?target=http%3A%2F%2Fwww.rabbitmq.com%2Finstall-windows.html">Windows</a>、<a href="https://link.juejin.im?target=http%3A%2F%2Fwww.rabbitmq.com%2Finstall-debian.html">Debian / Ubuntu</a>、<a href="https://link.juejin.im?target=http%3A%2F%2Fwww.rabbitmq.com%2Finstall-rpm.html">RPM-based Linux</a>、<a href="https://link.juejin.im?target=http%3A%2F%2Fwww.rabbitmq.com%2Finstall-standalone-mac.html">Mac</a></p>

<p>如果是 Mac 用户，个人推荐使用 HomeBrew 来安装，安装前要先更新 brew：</p>

<pre><code>brew update

</code></pre>

<p>接着安装 rabbitmq 服务器：</p>

<pre><code>brew install rabbitmq

</code></pre>

<p>这样 RabbitMQ 就安装好了，安装过程中会自动其所依赖的 Erlang 。</p>

<h1 id="toc_9">RabbitMQ 运行和管理</h1>

<ol>
<li> 启动 启动很简单，找到安装后的 RabbitMQ 所在目录下的 sbin 目录，可以看到该目录下有 6 个以 rabbitmq 开头的可执行文件，直接执行 rabbitmq-server 即可，下面将 RabbitMQ 的安装位置以 . 代替，启动命令就是：</li>
</ol>

<pre><code>./sbin/rabbitmq-server

</code></pre>

<p>启动正常的话会看到一些启动过程信息和最后的 completed with 7 plugins，这也说明启动的时候默认加载了 7 个插件。</p>

<p><img src="https://user-gold-cdn.xitu.io/2018/1/24/16126056ba03d9f0?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/></p>

<ol>
<li> 后台启动 如果想让 RabbitMQ 以守护程序的方式在后台运行，可以在启动的时候加上 -detached 参数：</li>
</ol>

<pre><code>./sbin/rabbitmq-server -detached

</code></pre>

<ol>
<li> 查询服务器状态 sbin 目录下有个特别重要的文件叫 rabbitmqctl ，它提供了 RabbitMQ 管理需要的几乎一站式解决方案，绝大部分的运维命令它都可以提供。 查询 RabbitMQ 服务器的状态信息可以用参数 status ：</li>
</ol>

<pre><code>./sbin/rabbitmqctl status

</code></pre>

<p>该命令将输出服务器的很多信息，比如 RabbitMQ 和 Erlang 的版本、OS 名称、内存等等</p>

<ol>
<li> 关闭 RabbitMQ 节点 我们知道 RabbitMQ 是用 Erlang 语言写的，在 Erlang 中有两个概念：节点和应用程序。节点就是 Erlang 虚拟机的每个实例，而多个 Erlang 应用程序可以运行在同一个节点之上。节点之间可以进行本地通信（不管他们是不是运行在同一台服务器之上）。比如一个运行在节点 A 上的应用程序可以调用节点 B 上应用程序的方法，就好像调用本地函数一样。如果应用程序由于某些原因奔溃，Erlang 节点会自动尝试重启应用程序。 如果要关闭整个 RabbitMQ 节点可以用参数 stop ：</li>
</ol>

<pre><code>./sbin/rabbitmqctl stop

</code></pre>

<p>它会和本地节点通信并指示其干净的关闭，也可以指定关闭不同的节点，包括远程节点，只需要传入参数 -n ：</p>

<pre><code>./sbin/rabbitmqctl -n rabbit@server.example.com stop 

</code></pre>

<p>-n node 默认 node 名称是 rabbit@server ，如果你的主机名是 server.example.com ，那么 node 名称就是 <a href="mailto:rabbit@server.example.com">rabbit@server.example.com</a> 。</p>

<ol>
<li> 关闭 RabbitMQ 应用程序 如果只想关闭应用程序，同时保持 Erlang 节点运行则可以用 stop_app：</li>
</ol>

<pre><code>./sbin/rabbitmqctl stop_app

</code></pre>

<p>这个命令在后面要讲的集群模式中将会很有用。</p>

<ol>
<li> 启动 RabbitMQ 应用程序</li>
</ol>

<pre><code>./sbin/rabbitmqctl start_app

</code></pre>

<ol>
<li> 重置 RabbitMQ 节点</li>
</ol>

<pre><code>./sbin/rabbitmqctl reset

</code></pre>

<p>该命令将清除所有的队列。</p>

<ol>
<li> 查看已声明的队列</li>
</ol>

<pre><code>./sbin/rabbitmqctl list_queues

</code></pre>

<ol>
<li> 查看交换器</li>
</ol>

<pre><code>./sbin/rabbitmqctl list_exchanges

</code></pre>

<p>该命令还可以附加参数，比如列出交换器的名称、类型、是否持久化、是否自动删除：</p>

<pre><code>./sbin/rabbitmqctl list_exchanges name type durable auto_delete

</code></pre>

<ol>
<li> 查看绑定</li>
</ol>

<pre><code>./sbin/rabbitmqctl list_bindings

</code></pre>

<h1 id="toc_10">Java 客户端访问</h1>

<p>RabbitMQ 支持多种语言访问，以 Java 为例看下一般使用 RabbitMQ 的步骤。</p>

<ol>
<li> maven 工程的 pom 文件中添加依赖</li>
</ol>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt;
    &lt;artifactId&gt;amqp-client&lt;/artifactId&gt;
    &lt;version&gt;4.1.0&lt;/version&gt;
&lt;/dependency&gt;

</code></pre>

<ol>
<li> 消息生产者</li>
</ol>

<pre><code>package org.study.rabbitmq;
import com.rabbitmq.client.Channel;
import com.rabbitmq.client.Connection;
import com.rabbitmq.client.ConnectionFactory;
import java.io.IOException;
import java.util.concurrent.TimeoutException;
public class Producer {

    public static void main(String[] args) throws IOException, TimeoutException {
        //创建连接工厂
        ConnectionFactory factory = new ConnectionFactory();
        factory.setUsername(&quot;guest&quot;);
        factory.setPassword(&quot;guest&quot;);
        //设置 RabbitMQ 地址
        factory.setHost(&quot;localhost&quot;);
        //建立到代理服务器到连接
        Connection conn = factory.newConnection();
        //获得信道
        Channel channel = conn.createChannel();
        //声明交换器
        String exchangeName = &quot;hello-exchange&quot;;
        channel.exchangeDeclare(exchangeName, &quot;direct&quot;, true);

        String routingKey = &quot;hola&quot;;
        //发布消息
        byte[] messageBodyBytes = &quot;quit&quot;.getBytes();
        channel.basicPublish(exchangeName, routingKey, null, messageBodyBytes);

        channel.close();
        conn.close();
    }
}

</code></pre>

<ol>
<li> 消息消费者</li>
</ol>

<pre><code>package org.study.rabbitmq;
import com.rabbitmq.client.*;
import java.io.IOException;
import java.util.concurrent.TimeoutException;
public class Consumer {

    public static void main(String[] args) throws IOException, TimeoutException {
        ConnectionFactory factory = new ConnectionFactory();
        factory.setUsername(&quot;guest&quot;);
        factory.setPassword(&quot;guest&quot;);
        factory.setHost(&quot;localhost&quot;);
        //建立到代理服务器到连接
        Connection conn = factory.newConnection();
        //获得信道
        final Channel channel = conn.createChannel();
        //声明交换器
        String exchangeName = &quot;hello-exchange&quot;;
        channel.exchangeDeclare(exchangeName, &quot;direct&quot;, true);
        //声明队列
        String queueName = channel.queueDeclare().getQueue();
        String routingKey = &quot;hola&quot;;
        //绑定队列，通过键 hola 将队列和交换器绑定起来
        channel.queueBind(queueName, exchangeName, routingKey);

        while(true) {
            //消费消息
            boolean autoAck = false;
            String consumerTag = &quot;&quot;;
            channel.basicConsume(queueName, autoAck, consumerTag, new DefaultConsumer(channel) {
                @Override
                public void handleDelivery(String consumerTag,
                                           Envelope envelope,
                                           AMQP.BasicProperties properties,
                                           byte[] body) throws IOException {
                    String routingKey = envelope.getRoutingKey();
                    String contentType = properties.getContentType();
                    System.out.println(&quot;消费的路由键：&quot; + routingKey);
                    System.out.println(&quot;消费的内容类型：&quot; + contentType);
                    long deliveryTag = envelope.getDeliveryTag();
                    //确认消息
                    channel.basicAck(deliveryTag, false);
                    System.out.println(&quot;消费的消息体内容：&quot;);
                    String bodyStr = new String(body, &quot;UTF-8&quot;);
                    System.out.println(bodyStr);

                }
            });
        }
    }
}

</code></pre>

<ol>
<li> 启动 RabbitMQ 服务器</li>
</ol>

<pre><code>./sbin/rabbitmq-server

</code></pre>

<ol>
<li> 运行 Consumer 先运行 Consumer ，这样当生产者发送消息的时候能在消费者后端看到消息记录。</li>
<li> 运行 Producer 接着运行 Producer , 发布一条消息，在 Consumer 的控制台能看到接收的消息： <img src="https://user-gold-cdn.xitu.io/2018/1/24/16126056b9f84c7d?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/></li>
</ol>

<h1 id="toc_11">RabbitMQ 集群</h1>

<p>RabbitMQ 最优秀的功能之一就是内建集群，这个功能设计的目的是允许消费者和生产者在节点崩溃的情况下继续运行，以及通过添加更多的节点来线性扩展消息通信吞吐量。RabbitMQ 内部利用 Erlang 提供的分布式通信框架 OTP 来满足上述需求，使客户端在失去一个 RabbitMQ 节点连接的情况下，还是能够重新连接到集群中的任何其他节点继续生产、消费消息。</p>

<h3 id="toc_12">RabbitMQ 集群中的一些概念</h3>

<p>RabbitMQ 会始终记录以下四种类型的内部元数据：</p>

<ol>
<li> 队列元数据 包括队列名称和它们的属性，比如是否可持久化，是否自动删除</li>
<li> 交换器元数据 交换器名称、类型、属性</li>
<li> 绑定元数据 内部是一张表格记录如何将消息路由到队列</li>
<li> vhost 元数据 为 vhost 内部的队列、交换器、绑定提供命名空间和安全属性</li>
</ol>

<p>在单一节点中，RabbitMQ 会将所有这些信息存储在内存中，同时将标记为可持久化的队列、交换器、绑定存储到硬盘上。存到硬盘上可以确保队列和交换器在节点重启后能够重建。而在集群模式下同样也提供两种选择：存到硬盘上（独立节点的默认设置），存在内存中。</p>

<p>如果在集群中创建队列，集群只会在单个节点而不是所有节点上创建完整的队列信息（元数据、状态、内容）。结果是只有队列的所有者节点知道有关队列的所有信息，因此当集群节点崩溃时，该节点的队列和绑定就消失了，并且任何匹配该队列的绑定的新消息也丢失了。还好 RabbitMQ 2.6.0 之后提供了镜像队列以避免集群节点故障导致的队列内容不可用。</p>

<p>RabbitMQ 集群中可以共享 user、vhost、exchange 等，所有的数据和状态都是必须在所有节点上复制的，例外就是上面所说的消息队列。RabbitMQ 节点可以动态的加入到集群中。</p>

<p>当在集群中声明队列、交换器、绑定的时候，这些操作会直到所有集群节点都成功提交元数据变更后才返回。集群中有内存节点和磁盘节点两种类型，内存节点虽然不写入磁盘，但是它的执行比磁盘节点要好。内存节点可以提供出色的性能，磁盘节点能保障配置信息在节点重启后仍然可用，那集群中如何平衡这两者呢？</p>

<p>RabbitMQ 只要求集群中至少有一个磁盘节点，所有其他节点可以是内存节点，当节点加入或离开集群时，它们必须要将该变更通知到至少一个磁盘节点。如果只有一个磁盘节点，刚好又是该节点崩溃了，那么集群可以继续路由消息，但不能创建队列、创建交换器、创建绑定、添加用户、更改权限、添加或删除集群节点。换句话说集群中的唯一磁盘节点崩溃的话，集群仍然可以运行，但直到该节点恢复，否则无法更改任何东西。</p>

<h3 id="toc_13">RabbitMQ 集群配置和启动</h3>

<p>如果是在一台机器上同时启动多个 RabbitMQ 节点来组建集群的话，只用上面介绍的方式启动第二、第三个节点将会因为节点名称和端口冲突导致启动失败。所以在每次调用 rabbitmq-server 命令前，设置环境变量 RABBITMQ_NODENAME 和 RABBITMQ_NODE_PORT 来明确指定唯一的节点名称和端口。下面的例子端口号从 5672 开始，每个新启动的节点都加 1，节点也分别命名为 test_rabbit_1、test_rabbit_2、test_rabbit_3。</p>

<p>启动第 1 个节点：</p>

<pre><code>RABBITMQ_NODENAME=test_rabbit_1 RABBITMQ_NODE_PORT=5672 ./sbin/rabbitmq-server -detached

</code></pre>

<p>启动第 2 个节点：</p>

<pre><code>RABBITMQ_NODENAME=test_rabbit_2 RABBITMQ_NODE_PORT=5673 ./sbin/rabbitmq-server -detached

</code></pre>

<p>启动第 2 个节点前建议将 RabbitMQ 默认激活的插件关掉，否则会存在使用了某个插件的端口号冲突，导致节点启动不成功。</p>

<p>现在第 2 个节点和第 1 个节点都是独立节点，它们并不知道其他节点的存在。集群中除第一个节点外后加入的节点需要获取集群中的元数据，所以要先停止 Erlang 节点上运行的 RabbitMQ 应用程序，并重置该节点元数据，再加入并且获取集群的元数据，最后重新启动 RabbitMQ 应用程序。</p>

<p>停止第 2 个节点的应用程序：</p>

<pre><code>./sbin/rabbitmqctl -n test_rabbit_2 stop_app

</code></pre>

<p>重置第 2 个节点元数据：</p>

<pre><code>./sbin/rabbitmqctl -n test_rabbit_2 reset

</code></pre>

<p>第 2 节点加入第 1 个节点组成的集群：</p>

<pre><code>./sbin/rabbitmqctl -n test_rabbit_2 join_cluster test_rabbit_1@localhost

</code></pre>

<p>启动第 2 个节点的应用程序</p>

<pre><code>./sbin/rabbitmqctl -n test_rabbit_2 start_app

</code></pre>

<p>第 3 个节点的配置过程和第 2 个节点类似：</p>

<pre><code>RABBITMQ_NODENAME=test_rabbit_3 RABBITMQ_NODE_PORT=5674 ./sbin/rabbitmq-server -detached

./sbin/rabbitmqctl -n test_rabbit_3 stop_app

./sbin/rabbitmqctl -n test_rabbit_3 reset

./sbin/rabbitmqctl -n test_rabbit_3 join_cluster test_rabbit_1@localhost

./sbin/rabbitmqctl -n test_rabbit_3 start_app

</code></pre>

<h3 id="toc_14">RabbitMQ 集群运维</h3>

<p>停止某个指定的节点，比如停止第 2 个节点：</p>

<pre><code>RABBITMQ_NODENAME=test_rabbit_2 ./sbin/rabbitmqctl stop

</code></pre>

<p>查看节点 3 的集群状态：</p>

<pre><code>./sbin/rabbitmqctl -n test_rabbit_3 cluster_status

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RPC 原理及 RPC 实例分析 - God is a Coder..]]></title>
    <link href="http://panlw.github.io/15274391142573.html"/>
    <updated>2018-05-28T00:38:34+08:00</updated>
    <id>http://panlw.github.io/15274391142573.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://my.oschina.net/hosee/blog/711632">原文地址</a></p>
</blockquote>

<p>在学校期间大家都写过不少程序，比如写个 hello world 服务类，然后本地调用下，如下所示。这些程序的特点是服务消费方和服务提供方是本地调用关系。</p>

<pre><code>public class Test {
     public static void main(String[] args) {
         HelloWorldService helloWorldService = new HelloWorldServiceImpl();
         helloWorldService.sayHello(&quot;test&quot;);
     }
}
</code></pre>

<p>而一旦踏入公司尤其是大型互联网公司就会发现，公司的系统都由成千上万大大小小的服务组成，各服务部署在不同的机器上，由不同的团队负责。</p>

<p>这时就会遇到两个问题：</p>

<ol>
<li> 要搭建一个新服务，免不了需要依赖他人的服务，而现在他人的服务都在远端，怎么调用？</li>
<li> 其它团队要使用我们的新服务，我们的服务该怎么发布以便他人调用？下文将对这两个问题展开探讨。</li>
</ol>

<h2 id="toc_0">1.  如何调用他人的远程服务？</h2>

<p>由于各服务部署在不同机器，服务间的调用免不了网络通信过程，服务消费方每调用一个服务都要写一坨网络通信相关的代码，不仅复杂而且极易出错。</p>

<p>如果有一种方式能让我们像调用本地服务一样调用远程服务，而让调用者对网络通信这些细节透明，那么将大大提高生产力，比如服务消费方在执行 helloWorldService.sayHello(&quot;test&quot;) 时，实质上调用的是远端的服务。这种方式其实就是 RPC（Remote Procedure Call Protocol），在各大互联网公司中被广泛使用，如阿里巴巴的 hsf、dubbo（开源）、Facebook 的 thrift（开源）、Google grpc（开源）、Twitter 的 finagle（开源）等。</p>

<p>要让网络通信细节对使用者透明，我们需要对通信细节进行封装，我们先看下一个 RPC 调用的流程涉及到哪些通信细节：</p>

<p><img src="http://static.oschina.net/uploads/space/2016/0714/102634_AAIe_2243330.png" alt=""/></p>

<ol>
<li> 服务消费方（client）调用以本地调用方式调用服务；</li>
<li> client stub 接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体；</li>
<li> client stub 找到服务地址，并将消息发送到服务端；</li>
<li> server stub 收到消息后进行解码；</li>
<li> server stub 根据解码结果调用本地的服务；</li>
<li> 本地服务执行并将结果返回给 server stub；</li>
<li> server stub 将返回结果打包成消息并发送至消费方；</li>
<li> client stub 接收到消息，并进行解码；</li>
<li> 服务消费方得到最终结果。</li>
</ol>

<p>RPC 的目标就是要 2~8 这些步骤都封装起来，让用户对这些细节透明。</p>

<h3 id="toc_1">1.1 怎么做到透明化远程服务调用？</h3>

<p>怎么封装通信细节才能让用户像以本地调用方式调用远程服务呢？对 java 来说就是使用代理！java 代理有两种方式：</p>

<ol>
<li> jdk 动态代理</li>
<li> 字节码生成</li>
</ol>

<p>尽管字节码生成方式实现的代理更为强大和高效，但代码维护不易，大部分公司实现 RPC 框架时还是选择动态代理方式。</p>

<p>下面简单介绍下动态代理怎么实现我们的需求。我们需要实现 RPCProxyClient 代理类，代理类的 invoke 方法中封装了与远端服务通信的细节，消费方首先从 RPCProxyClient 获得服务提供方的接口，当执行 helloWorldService.sayHello(&quot;test&quot;) 方法时就会调用 invoke 方法。</p>

<pre><code>public class RPCProxyClient implements java.lang.reflect.InvocationHandler{
    private Object obj;

    public RPCProxyClient(Object obj){
        this.obj=obj;
    }

    /**
     * 得到被代理对象;
     */
    public static Object getProxy(Object obj){
        return java.lang.reflect.Proxy.newProxyInstance(obj.getClass().getClassLoader(),
                obj.getClass().getInterfaces(), new RPCProxyClient(obj));
    }

    /**
     * 调用此方法执行
     */
    public Object invoke(Object proxy, Method method, Object[] args)
            throws Throwable {
        //结果参数;
        Object result = new Object();
        // ...执行通信相关逻辑
        // ...
        return result;
    }
}
</code></pre>

<pre><code>public class Test {
     public static void main(String[] args) {
         HelloWorldService helloWorldService = (HelloWorldService)RPCProxyClient.getProxy(HelloWorldService.class);
         helloWorldService.sayHello(&quot;test&quot;);
     }
 }
</code></pre>

<h3 id="toc_2">1.2  怎么对消息进行编码和解码？</h3>

<h4 id="toc_3">1.2.1 确定消息数据结构</h4>

<p>　　上节讲了 invoke 里需要封装通信细节（通信细节再后面几章详细探讨），而通信的第一步就是要确定客户端和服务端相互通信的消息结构。客户端的请求消息结构一般需要包括以下内容：</p>

<p>1）接口名称</p>

<p>　　在我们的例子里接口名是 “HelloWorldService”，如果不传，服务端就不知道调用哪个接口了；</p>

<p>2）方法名</p>

<p>　　一个接口内可能有很多方法，如果不传方法名服务端也就不知道调用哪个方法；</p>

<p>3）参数类型 &amp; 参数值</p>

<p>　　参数类型有很多，比如有 bool、int、long、double、string、map、list，甚至如 struct（class）；以及相应的参数值；</p>

<p>4）超时时间</p>

<p>5）requestID，标识唯一请求 id，在下面一节会详细描述 requestID 的用处。</p>

<p>　　同理服务端返回的消息结构一般包括以下内容。</p>

<p>1）返回值</p>

<p>2）状态 code</p>

<p>3）requestID </p>

<h4 id="toc_4">1.2.2 序列化</h4>

<p>一旦确定了消息的数据结构后，下一步就是要考虑序列化与反序列化了。</p>

<p>什么是序列化？序列化就是将数据结构或对象转换成二进制串的过程，也就是编码的过程。</p>

<p>什么是反序列化？将在序列化过程中所生成的二进制串转换成数据结构或者对象的过程。</p>

<p>为什么需要序列化？转换为二进制串后才好进行网络传输嘛！</p>

<p>为什么需要反序列化？将二进制转换为对象才好进行后续处理！</p>

<p>现如今序列化的方案越来越多，每种序列化方案都有优点和缺点，它们在设计之初有自己独特的应用场景，那到底选择哪种呢？从 RPC 的角度上看，主要看三点：</p>

<ol>
<li> 通用性，比如是否能支持 Map 等复杂的数据结构；</li>
<li> 性能，包括时间复杂度和空间复杂度，由于 RPC 框架将会被公司几乎所有服务使用，如果序列化上能节约一点时间，对整个公司的收益都将非常可观，同理如果序列化上能节约一点内存，网络带宽也能省下不少；</li>
<li> 可扩展性，对互联网公司而言，业务变化飞快，如果序列化协议具有良好的可扩展性，支持自动增加新的业务字段，而不影响老的服务，这将大大提供系统的灵活度。</li>
</ol>

<p>目前互联网公司广泛使用 Protobuf、Thrift、Avro 等成熟的序列化解决方案来搭建 RPC 框架，这些都是久经考验的解决方案。</p>

<h3 id="toc_5">1.3  通信</h3>

<p>消息数据结构被序列化为二进制串后，下一步就要进行网络通信了。目前有两种常用 IO 通信模型：1）BIO；2）<a href="http://my.oschina.net/hosee/blog/615269">NIO</a>。一般 RPC 框架需要支持这两种 IO 模型。</p>

<p>如何实现 RPC 的 IO 通信框架呢？</p>

<ol>
<li> 使用 java nio 方式自研，这种方式较为复杂，而且很有可能出现隐藏 bug，但也见过一些互联网公司使用这种方式；</li>
<li> 基于 mina，mina 在早几年比较火热，不过这些年版本更新缓慢；</li>
<li> 基于 netty，现在很多 RPC 框架都直接基于 netty 这一 IO 通信框架，省力又省心，比如阿里巴巴的 HSF、dubbo，Twitter 的 finagle 等。</li>
</ol>

<h3 id="toc_6">1.4  <strong>消息里为什么要有 requestID？</strong></h3>

<p>如果使用 netty 的话，一般会用 channel.writeAndFlush()方法来发送消息二进制串，这个方法调用后对于整个远程调用 (从发出请求到接收到结果) 来说是一个异步的，即对于当前线程来说，将请求发送出来后，线程就可以往后执行了，至于服务端的结果，是服务端处理完成后，再以消息的形式发送给客户端的。于是这里出现以下两个问题：</p>

<ol>
<li> 怎么让当前线程 “暂停”，等结果回来后，再向后执行？</li>
<li> 如果有多个线程同时进行远程方法调用，这时建立在 client server 之间的 socket 连接上会有很多双方发送的消息传递，前后顺序也可能是随机的，server 处理完结果后，将结果消息发送给 client，client 收到很多消息，怎么知道哪个消息结果是原先哪个线程调用的？</li>
</ol>

<p>如下图所示，线程 A 和线程 B 同时向 client socket 发送请求 requestA 和 requestB，socket 先后将 requestB 和 requestA 发送至 server，而 server 可能将 responseA 先返回，尽管 requestA 请求到达时间更晚。我们需要一种机制保证 responseA 丢给 ThreadA，responseB 丢给 ThreadB。</p>

<p><img src="http://static.oschina.net/uploads/space/2016/0714/104316_FAgB_2243330.png" alt=""/></p>

<p>怎么解决呢？</p>

<ol>
<li> client 线程每次通过 socket 调用一次远程接口前，生成一个唯一的 ID，即 requestID（requestID 必需保证在一个 Socket 连接里面是唯一的），一般常常使用 AtomicLong 从 0 开始累计数字生成唯一 ID；</li>
<li> 将处理结果的回调对象 callback，存放到全局 ConcurrentHashMap 里面 put(requestID, callback)；</li>
<li> 当线程调用 channel.writeAndFlush() 发送消息后，紧接着执行 callback 的 get() 方法试图获取远程返回的结果。在 get() 内部，则使用 synchronized 获取回调对象 callback 的锁，再先检测是否已经获取到结果，如果没有，然后调用 callback 的 wait() 方法，释放 callback 上的锁，让当前线程处于等待状态。</li>
<li> 服务端接收到请求并处理后，将 response 结果（此结果中包含了前面的 requestID）发送给客户端，客户端 socket 连接上专门监听消息的线程收到消息，分析结果，取到 requestID，再从前面的 ConcurrentHashMap 里面 get(requestID)，从而找到 callback 对象，再用 synchronized 获取 callback 上的锁，将方法调用结果设置到 callback 对象里，再调用 callback.notifyAll() 唤醒前面处于等待状态的线程。</li>
</ol>

<pre><code>public Object get() {
        synchronized (this) { // 旋锁
            while (!isDone) { // 是否有结果了
                wait(); //没结果是释放锁，让当前线程处于等待状态
            }
        }
}
</code></pre>

<pre><code>private void setDone(Response res) {
        this.res = res;
        isDone = true;
        synchronized (this) { //获取锁，因为前面wait()已经释放了callback的锁了
            notifyAll(); // 唤醒处于等待的线程
        }
    }
</code></pre>

<h2 id="toc_7">2 如何发布自己的服务？</h2>

<p>如何让别人使用我们的服务呢？有同学说很简单嘛，告诉使用者服务的 IP 以及端口就可以了啊。确实是这样，这里问题的关键在于是自动告知还是人肉告知。</p>

<p>人肉告知的方式：如果你发现你的服务一台机器不够，要再添加一台，这个时候就要告诉调用者我现在有两个 ip 了，你们要轮询调用来实现负载均衡；调用者咬咬牙改了，结果某天一台机器挂了，调用者发现服务有一半不可用，他又只能手动修改代码来删除挂掉那台机器的 ip。现实生产环境当然不会使用人肉方式。</p>

<p>有没有一种方法能实现自动告知，即机器的增添、剔除对调用方透明，调用者不再需要写死服务提供方地址？当然可以，现如今 zookeeper 被广泛用于实现服务自动注册与发现功能！</p>

<p>简单来讲，zookeeper 可以充当一个<code>服务注册表</code>（Service Registry），让多个<code>服务提供者</code>形成一个集群，让<code>服务消费者</code>通过服务注册表获取具体的服务访问地址（ip + 端口）去访问具体的服务提供者。如下图所示：</p>

<p><img src="http://static.oschina.net/uploads/space/2016/0714/105148_gSi2_2243330.png" alt=""/></p>

<p>具体来说，zookeeper 就是个分布式文件系统，每当一个服务提供者部署后都要将自己的服务注册到 zookeeper 的某一路径上: /{service}/{version}/{ip:port}, 比如我们的 HelloWorldService 部署到两台机器，那么 zookeeper 上就会创建两条目录：分别为 / HelloWorldService/1.0.0/100.19.20.01:16888  /HelloWorldService/1.0.0/100.19.20.02:16888。</p>

<p>zookeeper 提供了 “心跳检测” 功能，它会定时向各个服务提供者发送一个请求（实际上建立的是一个 Socket 长连接），如果长期没有响应，服务中心就认为该服务提供者已经“挂了”，并将其剔除，比如 100.19.20.02 这台机器如果宕机了，那么 zookeeper 上的路径就会只剩 / HelloWorldService/1.0.0/100.19.20.01:16888。</p>

<p>服务消费者会去监听相应路径（/HelloWorldService/1.0.0），一旦路径上的数据有任务变化（增加或减少），zookeeper 都会通知服务消费方服务提供者地址列表已经发生改变，从而进行更新。</p>

<p>更为重要的是 zookeeper 与生俱来的容错容灾能力（比如 leader 选举），可以确保服务注册表的高可用性。</p>

<h2 id="toc_8">3.Hadoop 中 <strong>RPC</strong> 实例分析</h2>

<p>ipc.RPC 类中有一些内部类，为了大家对 RPC 类有个初步的印象，就先罗列几个我们感兴趣的分析一下吧：</p>

<p><strong>Invocation</strong> ：用于封装方法名和参数，作为数据传输层。<br/>
<strong>ClientCache</strong> ：用于存储 client 对象，用 socket factory 作为 hash key, 存储结构为 hashMap <SocketFactory, Client>。<br/>
<strong>Invoker</strong> ：是动态代理中的调用实现类，继承了 InvocationHandler.<br/>
<strong>Server</strong> ：是 ipc.Server 的实现类。</p>

<pre><code>    public Object invoke(Object proxy, Method method, Object[] args)
      throws Throwable {
      •••
      ObjectWritable value = (ObjectWritable)
        client.call(new Invocation(method, args), remoteId);
      •••
      return value.get();
    }

</code></pre>

<p>如果你发现这个 invoke() 方法实现的有些奇怪的话，那你就对了。一般我们看到的<a href="http://my.oschina.net/hosee/blog/656945">动态代理</a>的 invoke() 方法中总会有 method.invoke(ac, arg);  这句代码。而上面代码中却没有，这是为什么呢？其实使用 method.invoke(ac, arg); 是在本地 JVM 中调用；而在 hadoop 中，是将数据发送给服务端，服务端将处理的结果再返回给客户端，所以这里的 invoke() 方法必然需要进行网络通信。而网络通信就是下面的这段代码实现的：</p>

<pre><code>ObjectWritable value = (ObjectWritable)
client.call(new Invocation(method, args), remoteId);

</code></pre>

<p>Invocation 类在这里封装了方法名和参数。其实这里网络通信只是调用了 Client 类的 call() 方法。那我们接下来分析一下 ipc.Client 源码吧。和第一章一样，同样是 3 个问题</p>

<ol>
<li> 客户端和服务端的连接是怎样建立的？</li>
<li> 客户端是怎样给服务端发送数据的？</li>
<li> 客户端是怎样获取服务端的返回数据的？</li>
</ol>

<h3 id="toc_9">3.1 客户端和服务端的连接是怎样建立的？</h3>

<pre><code>public Writable call(Writable param, ConnectionId remoteId)  
                       throws InterruptedException, IOException {
    Call call = new Call(param);       //将传入的数据封装成call对象
    Connection connection = getConnection(remoteId, call);   //获得一个连接
    connection.sendParam(call);     // 向服务端发送call对象
    boolean interrupted = false;
    synchronized (call) {
      while (!call.done) {
        try {
          call.wait(); // 等待结果的返回，在Call类的callComplete()方法里有notify()方法用于唤醒线程
        } catch (InterruptedException ie) {
          // 因中断异常而终止，设置标志interrupted为true
          interrupted = true;
        }
      }
      if (interrupted) {
        Thread.currentThread().interrupt();
      }

      if (call.error != null) {
        if (call.error instanceof RemoteException) {
          call.error.fillInStackTrace();
          throw call.error;
        } else { // 本地异常
          throw wrapException(remoteId.getAddress(), call.error);
        }
      } else {
        return call.value; //返回结果数据
      }
    }
  }

</code></pre>

<p>具体代码的作用我已做了注释，所以这里不再赘述。但到目前为止，你依然不知道 RPC 机制底层的网络连接是怎么建立的。分析代码后，我们会发现和网络通信有关的代码只会是下面的两句了：</p>

<pre><code>  Connection connection = getConnection(remoteId, call);   //获得一个连接
  connection.sendParam(call);      // 向服务端发送call对象

</code></pre>

<p>先看看是怎么获得一个到服务端的连接吧，下面贴出 ipc.Client 类中的 getConnection() 方法。</p>

<pre><code>private Connection getConnection(ConnectionId remoteId,
                                   Call call)
                                   throws IOException, InterruptedException {
    if (!running.get()) {
      // 如果client关闭了
      throw new IOException(&quot;The client is stopped&quot;);
    }
    Connection connection;
//如果connections连接池中有对应的连接对象，就不需重新创建了；如果没有就需重新创建一个连接对象。
//但请注意，该//连接对象只是存储了remoteId的信息，其实还并没有和服务端建立连接。
    do {
      synchronized (connections) {
        connection = connections.get(remoteId);
        if (connection == null) {
          connection = new Connection(remoteId);
          connections.put(remoteId, connection);
        }
      }
    } while (!connection.addCall(call)); //将call对象放入对应连接中的calls池，就不贴出源码了
   //这句代码才是真正的完成了和服务端建立连接哦~
    connection.setupIOstreams();
    return connection;
  }

</code></pre>

<p>下面贴出 Client.Connection 类中的 setupIOstreams() 方法：</p>

<pre><code>  private synchronized void setupIOstreams() throws InterruptedException {
   •••
      try {
       •••
        while (true) {
          setupConnection();  //建立连接
          InputStream inStream = NetUtils.getInputStream(socket);     //获得输入流
          OutputStream outStream = NetUtils.getOutputStream(socket);  //获得输出流
          writeRpcHeader(outStream);
          •••
          this.in = new DataInputStream(new BufferedInputStream
              (new PingInputStream(inStream)));   //将输入流装饰成DataInputStream
          this.out = new DataOutputStream
          (new BufferedOutputStream(outStream));   //将输出流装饰成DataOutputStream
          writeHeader();
          // 跟新活动时间
          touch();
          //当连接建立时，启动接受线程等待服务端传回数据，注意：Connection继承了Tread
          start();
          return;
        }
      } catch (IOException e) {
        markClosed(e);
        close();
      }
    }

</code></pre>

<p>再有一步我们就知道客户端的连接是怎么建立的啦，下面贴出 Client.Connection 类中的 setupConnection() 方法：</p>

<pre><code>  private synchronized void setupConnection() throws IOException {
      short ioFailures = 0;
      short timeoutFailures = 0;
      while (true) {
        try {
          this.socket = socketFactory.createSocket(); //终于看到创建socket的方法了
          this.socket.setTcpNoDelay(tcpNoDelay);
         •••
          // 设置连接超时为20s
          NetUtils.connect(this.socket, remoteId.getAddress(), 20000);
          this.socket.setSoTimeout(pingInterval);
          return;
        } catch (SocketTimeoutException toe) {
          /* 设置最多连接重试为45次。
           * 总共有20s*45 = 15 分钟的重试时间。
           */
          handleConnectionFailure(timeoutFailures++, 45, toe);
        } catch (IOException ie) {
          handleConnectionFailure(ioFailures++, maxRetries, ie);
        }
      }
    }

</code></pre>

<p>终于，我们知道了客户端的连接是怎样建立的了，其实就是创建一个普通的 socket 进行通信。</p>

<h3 id="toc_10">3.2 客户端是怎样给服务端发送数据的？ </h3>

<p>下面贴出 Client.Connection 类的 sendParam() 方法吧：</p>

<pre><code>public void sendParam(Call call) {
      if (shouldCloseConnection.get()) {
        return;
      }
      DataOutputBuffer d=null;
      try {
        synchronized (this.out) {
          if (LOG.isDebugEnabled())
            LOG.debug(getName() + &quot; sending #&quot; + call.id);
          //创建一个缓冲区
          d = new DataOutputBuffer();
          d.writeInt(call.id);
          call.param.write(d);
          byte[] data = d.getData();
          int dataLength = d.getLength();
          out.writeInt(dataLength);        //首先写出数据的长度
          out.write(data, 0, dataLength); //向服务端写数据
          out.flush();
        }
      } catch(IOException e) {
        markClosed(e);
      } finally {
        IOUtils.closeStream(d);
      }
    }  

</code></pre>

<h3 id="toc_11">3.3 客户端是怎样获取服务端的返回数据的？ </h3>

<p>下面贴出 Client.Connection 类和 Client.Call 类中的相关方法：</p>

<pre><code>方法一：  
  public void run() {
      •••
      while (waitForWork()) {
        receiveResponse();  //具体的处理方法
      }
      close();
     •••
}

方法二：
private void receiveResponse() {
      if (shouldCloseConnection.get()) {
        return;
      }
      touch();
      try {
        int id = in.readInt();                    // 阻塞读取id
        if (LOG.isDebugEnabled())
          LOG.debug(getName() + &quot; got value #&quot; + id);
          Call call = calls.get(id);    //在calls池中找到发送时的那个对象
        int state = in.readInt();     // 阻塞读取call对象的状态
        if (state == Status.SUCCESS.state) {
          Writable value = ReflectionUtils.newInstance(valueClass, conf);
          value.readFields(in);           // 读取数据
        //将读取到的值赋给call对象，同时唤醒Client等待线程，贴出setValue()代码方法三
          call.setValue(value);              
          calls.remove(id);               //删除已处理的call    
        } else if (state == Status.ERROR.state) {
        •••
        } else if (state == Status.FATAL.state) {
        •••
        }
      } catch (IOException e) {
        markClosed(e);
      }
}

方法三：
public synchronized void setValue(Writable value) {
      this.value = value;
      callComplete();   //具体实现
}
protected synchronized void callComplete() {
      this.done = true;
      notify();         // 唤醒client等待线程
    }

</code></pre>

<p>完成的功能主要是：启动一个处理线程，读取从服务端传来的 call 对象，将 call 对象读取完毕后，唤醒 client 处理线程。就这么简单，客户端就获取了服务端返回的数据了哦~。客户端的源码分析就到这里了哦，下面我们来分析 Server 端的源码吧。</p>

<h3 id="toc_12">3.4 ipc.Server 源码分析</h3>

<p>为了让大家对 ipc.Server 有个初步的了解，我们先分析一下它的几个内部类吧：</p>

<p><strong>Call</strong> ：用于存储客户端发来的请求<br/>
<strong>Listener</strong> ： 监听类，用于监听客户端发来的请求，同时 Listener 内部还有一个静态类，Listener.Reader，当监听器监听到用户请求，便让 Reader 读取用户请求。<br/>
<strong>Responder</strong> ：响应 RPC 请求类，请求处理完毕，由 Responder 发送给请求客户端。<br/>
<strong>Connection</strong> ：连接类，真正的客户端请求读取逻辑在这个类中。<br/>
<strong>Handler</strong> ：请求处理类，会循环阻塞读取 callQueue 中的 call 对象，并对其进行操作。</p>

<pre><code>private void initialize(Configuration conf) throws IOException {
   •••
    // 创建 rpc server
    InetSocketAddress dnSocketAddr = getServiceRpcServerAddress(conf);
    if (dnSocketAddr != null) {
      int serviceHandlerCount =
        conf.getInt(DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_KEY,
                    DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_DEFAULT);
      //获得serviceRpcServer
      this.serviceRpcServer = RPC.getServer(this, dnSocketAddr.getHostName(), 
          dnSocketAddr.getPort(), serviceHandlerCount,
          false, conf, namesystem.getDelegationTokenSecretManager());
      this.serviceRPCAddress = this.serviceRpcServer.getListenerAddress();
      setRpcServiceServerAddress(conf);
}
//获得server
    this.server = RPC.getServer(this, socAddr.getHostName(),
        socAddr.getPort(), handlerCount, false, conf, namesystem
        .getDelegationTokenSecretManager());

   •••
    this.server.start();  //启动 RPC server   Clients只允许连接该server
    if (serviceRpcServer != null) {
      serviceRpcServer.start();  //启动 RPC serviceRpcServer 为HDFS服务的server
    }
    startTrashEmptier(conf);
  }

</code></pre>

<p>查看 Namenode 初始化源码得知：RPC 的 server 对象是通过 ipc.RPC 类的 getServer() 方法获得的。下面咱们去看看 ipc.RPC 类中的 getServer() 源码吧：</p>

<pre><code>public static Server getServer(final Object instance, final String bindAddress, final int port,
                                 final int numHandlers,
                                 final boolean verbose, Configuration conf,
                                 SecretManager&lt;? extends TokenIdentifier&gt; secretManager) 
    throws IOException {
    return new Server(instance, conf, bindAddress, port, numHandlers, verbose, secretManager);
  }

</code></pre>

<p>这时我们发现 getServer()是一个创建 Server 对象的工厂方法，但创建的却是 RPC.Server 类的对象。哈哈，现在你明白了我前面说的 “RPC.Server 是 ipc.Server 的实现类” 了吧。不过 RPC.Server 的构造函数还是调用了 ipc.Server 类的构造函数的，因篇幅所限，就不贴出相关源码了。</p>

<p>初始化 Server 后，Server 端就运行起来了，看看 ipc.Server 的 start() 源码吧：</p>

<pre><code>  /** 启动服务 */
  public synchronized void start() {
    responder.start();  //启动responder
    listener.start();   //启动listener
    handlers = new Handler[handlerCount];

    for (int i = 0; i &lt; handlerCount; i++) {
      handlers[i] = new Handler(i);
      handlers[i].start();   //逐个启动Handler
    }
  }

</code></pre>

<p>分析过 ipc.Client 源码后，我们知道 Client 端的底层通信直接采用了阻塞式 IO 编程，当时我们曾做出猜测：Server 端是不是也采用了阻塞式 IO。现在我们仔细地分析一下吧，如果 Server 端也采用阻塞式 IO，当连接进来的 Client 端很多时，势必会影响 Server 端的性能。hadoop 的实现者们考虑到了这点，所以他们采用了 java  NIO 来实现 Server 端，那 Server 端采用 java NIO 是怎么建立连接的呢？分析源码得知，Server 端采用 Listener 监听客户端的连接，下面先分析一下 Listener 的构造函数吧：</p>

<pre><code>    public Listener() throws IOException {
      address = new InetSocketAddress(bindAddress, port);
      // 创建ServerSocketChannel,并设置成非阻塞式
      acceptChannel = ServerSocketChannel.open();
      acceptChannel.configureBlocking(false);

      // 将server socket绑定到本地端口
      bind(acceptChannel.socket(), address, backlogLength);
      port = acceptChannel.socket().getLocalPort(); 
      // 获得一个selector
      selector= Selector.open();
      readers = new Reader[readThreads];
      readPool = Executors.newFixedThreadPool(readThreads);
      //启动多个reader线程，为了防止请求多时服务端响应延时的问题
      for (int i = 0; i &lt; readThreads; i++) {       
        Selector readSelector = Selector.open();
        Reader reader = new Reader(readSelector);
        readers[i] = reader;
        readPool.execute(reader);
      }
      // 注册连接事件
      acceptChannel.register(selector, SelectionKey.OP_ACCEPT);
      this.setName(&quot;IPC Server listener on &quot; + port);
      this.setDaemon(true);
    }

</code></pre>

<p>在启动 Listener 线程时，服务端会一直等待客户端的连接，下面贴出 Server.Listener 类的 run() 方法：</p>

<pre><code>  public void run() {
     •••
      while (running) {
        SelectionKey key = null;
        try {
          selector.select();
          Iterator&lt;SelectionKey&gt; iter = selector.selectedKeys().iterator();
          while (iter.hasNext()) {
            key = iter.next();
            iter.remove();
            try {
              if (key.isValid()) {
                if (key.isAcceptable())
                  doAccept(key);     //具体的连接方法
              }
            } catch (IOException e) {
            }
            key = null;
          }
        } catch (OutOfMemoryError e) {
       •••         
    }

</code></pre>

<p>下面贴出 Server.Listener 类中 doAccept() 方法中的关键源码吧：</p>

<pre><code>    void doAccept(SelectionKey key) throws IOException,  OutOfMemoryError {
      Connection c = null;
      ServerSocketChannel server = (ServerSocketChannel) key.channel();
      SocketChannel channel;
      while ((channel = server.accept()) != null) { //建立连接
        channel.configureBlocking(false);
        channel.socket().setTcpNoDelay(tcpNoDelay);
        Reader reader = getReader();  //从readers池中获得一个reader
        try {
          reader.startAdd(); // 激活readSelector，设置adding为true
          SelectionKey readKey = reader.registerChannel(channel);//将读事件设置成兴趣事件
          c = new Connection(readKey, channel, System.currentTimeMillis());//创建一个连接对象
          readKey.attach(c);   //将connection对象注入readKey
          synchronized (connectionList) {
            connectionList.add(numConnections, c);
            numConnections++;
          }
        ••• 
        } finally {
//设置adding为false，采用notify()唤醒一个reader,其实代码十三中启动的每个reader都使
//用了wait()方法等待。因篇幅有限，就不贴出源码了。
          reader.finishAdd();
        }
      }
    }

</code></pre>

<p>当 reader 被唤醒，reader 接着执行 doRead() 方法。</p>

<p>下面贴出 Server.Listener.Reader 类中的 doRead() 方法和 Server.Connection 类中的 readAndProcess() 方法源码：</p>

<pre><code>方法一：   
 void doRead(SelectionKey key) throws InterruptedException {
      int count = 0;
      Connection c = (Connection)key.attachment();  //获得connection对象
      if (c == null) {
        return;  
      }
      c.setLastContact(System.currentTimeMillis());
      try {
        count = c.readAndProcess();    // 接受并处理请求  
      } catch (InterruptedException ieo) {
       •••
      }
     •••    
}

方法二：
public int readAndProcess() throws IOException, InterruptedException {
      while (true) {
        •••
        if (!rpcHeaderRead) {
          if (rpcHeaderBuffer == null) {
            rpcHeaderBuffer = ByteBuffer.allocate(2);
          }
         //读取请求头
          count = channelRead(channel, rpcHeaderBuffer);
          if (count &lt; 0 || rpcHeaderBuffer.remaining() &gt; 0) {
            return count;
          }
        // 读取请求版本号  
          int version = rpcHeaderBuffer.get(0);
          byte[] method = new byte[] {rpcHeaderBuffer.get(1)};
        •••  

          data = ByteBuffer.allocate(dataLength);
        }
        // 读取请求  
        count = channelRead(channel, data);

        if (data.remaining() == 0) {
         •••
          if (useSasl) {
         •••
          } else {
            processOneRpc(data.array());//处理请求
          }
        •••
          }
        } 
        return count;
      }
    }

</code></pre>

<p>下面贴出 Server.Connection 类中的 processOneRpc() 方法和 processData() 方法的源码。</p>

<pre><code>方法一：   
 private void processOneRpc(byte[] buf) throws IOException,
        InterruptedException {
      if (headerRead) {
        processData(buf);
      } else {
        processHeader(buf);
        headerRead = true;
        if (!authorizeConnection()) {
          throw new AccessControlException(&quot;Connection from &quot; + this
              + &quot; for protocol &quot; + header.getProtocol()
              + &quot; is unauthorized for user &quot; + user);
        }
      }
}
方法二：
    private void processData(byte[] buf) throws  IOException, InterruptedException {
      DataInputStream dis =
        new DataInputStream(new ByteArrayInputStream(buf));
      int id = dis.readInt();      // 尝试读取id
      Writable param = ReflectionUtils.newInstance(paramClass, conf);//读取参数
      param.readFields(dis);        

      Call call = new Call(id, param, this);  //封装成call
      callQueue.put(call);   // 将call存入callQueue
      incRpcCount();  // 增加rpc请求的计数
    }

</code></pre>

<h2 id="toc_13">4. RPC 与 web service</h2>

<p>RPC：</p>

<p><img src="http://static.oschina.net/uploads/space/2016/0714/114103_HQGm_2243330.png" alt=""/></p>

<p>Web service<img src="http://static.oschina.net/uploads/space/2016/0714/114022_sKwT_2243330.png" alt=""/></p>

<p>web service 接口就是 RPC 中的 stub 组件，规定了 server 能够提供的服务（web service），这在 server 和 client 上是一致的，但是也是跨语言跨平台的。同时，由于 web service 规范中的 WSDL 文件的存在，现在各平台的 web service 框架，都可以基于 WSDL 文件，自动生成 web service 接口 。</p>

<p>其实两者差不多，只是传输的协议不同。</p>

<h2 id="toc_14">Reference：</h2>

<p>1. <a href="http://www.cnblogs.com/LBSer/p/4853234.html">http://www.cnblogs.com/LBSer/p/4853234.html</a><br/>
2. <a href="http://weixiaolu.iteye.com/blog/1504898">http://weixiaolu.iteye.com/blog/1504898</a><br/>
3. <a href="http://kyfxbl.iteye.com/blog/1745550">http://kyfxbl.iteye.com/blog/1745550</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kubernetes API 使用文档]]></title>
    <link href="http://panlw.github.io/15274389377310.html"/>
    <updated>2018-05-28T00:35:37+08:00</updated>
    <id>http://panlw.github.io/15274389377310.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/">原文地址</a></p>
</blockquote>

<h2 id="toc_0">API 概述</h2>

<h4 id="toc_1">※ 资源分类</h4>

<p>这是对 Kubernetes API 及其主要功能提供的基本资源类型的高级概述。</p>

<ul>
<li>  <strong>Workloads</strong>： 用于在集群中管理和运行容器；</li>
<li>  <strong>Discovery &amp; LB</strong>：用于将 Workloads “缝合” 到一个可从外部访问的负载均衡的服务中；</li>
<li>  <strong>Config &amp; Storage</strong>：用于将初始化数据注入到应用程序中，并保留容器外部的数据；</li>
<li>  <strong>Cluster</strong>：用于定义如何配置集群，这些通常仅由集群运营商使用；</li>
<li>  <strong>Metadata</strong>：用于配置群集内其他资源的行为，例如用于扩展 Workloads 的 HorizontalPodAutoscaler</li>
</ul>

<h4 id="toc_2">※ 资源对象</h4>

<p>资源对象通常包含 3 个组件：</p>

<ul>
<li>  <strong>ResourceSpec</strong>：这由用户定义并描述系统所期望的状态，在创建或更新对象时填写；</li>
<li>  <strong>ResourceStatus</strong>：这由服务器填写并报告系统的当前状态，只有 Kubernetes 组件才能填写此内容；</li>
<li>  <strong>Resource ObjectMeta</strong>：这是关于资源的元数据，例如 name / type / api version / annotations / labels，包含可能由后端用户和系统更新的字段（如 annotations）</li>
</ul>

<h4 id="toc_3">※ 资源操作</h4>

<p>大部分资源提供如下操作：</p>

<p><strong>▶ Create</strong></p>

<p>Create 操作会在存储后端创建资源。资源创建后，系统将应用所期望的状态。</p>

<p><strong>▶ Update</strong></p>

<p>更新有 2 种形式：<strong>Replace</strong> 和 <strong>Patch</strong></p>

<p><strong>Replace</strong>：替换资源对象将通过提供的 spec 替换现有的 spec 来更新资源。对于读写操作，这是安全的，因为如果资源在读和写之间被修改，则会发生乐观锁定失败。注意：ResourceStatus 将被系统忽略，并且不会被更新，要更新状态，必须调用特定的状态更新操作。</p>

<p>注意：替换资源对象可能不会立即传播到下游对象。例如，替换 ConfigMap 或 Secret 资源不会导致所有 Pod 看到更改，除非 Pod 在带外重新启动。</p>

<p><strong>Patch</strong>：Patch 会更改特定字段，如何合并更改是每个字段定义的。列表可以被替换或合并，合并列表不会保留排序。</p>

<p><strong>Patch 操作永远不会导致乐观锁定失败，并且最后一次写入会获胜。</strong>如果在更新之前未读取完整状态，或者乐观锁定失败不可取，则建议使用 patch 操作。修补复杂 types、arrays 和 maps 时，如何应用修补程序是以每个字段为基础定义的，并可以替换字段的当前值，也可以将内容合并到当前值中。</p>

<p><strong>▶ Read</strong></p>

<p>读取有 3 种方式：<strong>Get</strong> 、 <strong>List</strong> 和 <strong>Watch</strong></p>

<p><strong>Get</strong>： 按名称检索特定的资源对象；</p>

<p><strong>List</strong>：检索命名空间内特定类型的所有资源对象，并且结果可以限制为与选择器查询结果匹配的资源；</p>

<p><strong>List All Namespaces</strong>：像 List 一样，但是跨所有命名空间检索资源；</p>

<p><strong>Watch</strong>：Watch 将在对象更新时汇出结果，类似于回调，watch 用于响应资源的更改。</p>

<p><strong>▶ Delete</strong></p>

<p>Delete 将删除资源。根据特定的资源，子对象可能会或可能不会被服务器当做垃圾收集，详情请参阅特定资源对象的注释。</p>

<p><strong>▶ 额外操作</strong></p>

<p>资源可以定义特定于该资源类型的附加操作。</p>

<p><strong>Rollback</strong>：将 PodTemplate 回滚到以前的版本，仅适用于某些资源类型；</p>

<p><strong>Read / Write Scale</strong>：读取或更新给定资源的副本数量，仅适用于某些资源类型；</p>

<p><strong>Read / Write Status</strong>：读取或更新资源对象的状态，状态只能通过这些更新操作进行更改。</p>

<h2 id="toc_4">WORKLOADS</h2>

<p>Workloads 资源负责管理和运行集群上的容器。容器（Containers）由控制器（Controllers）通过 Pod 创建，Pods 运行容器并提供环境依赖，如注入到容器中的共享或永久存储卷、配置或加密数据。</p>

<p>最常见的控制器是：</p>

<ul>
<li>  Deployments 无状态持久应用（如 http servers）</li>
<li>  StatefulSets 有状态持久应用（如 database）</li>
<li>  Jobs 运行至完成的应用 （如 batch jobs）</li>
</ul>

<h2 id="toc_5">DISCOVERY &amp; LOAD BALANCING</h2>

<p>Discovery and Load Balancing 负责将 Workloads “缝合” 到一个可从外部访问的负载均衡的服务中。默认情况下，Workloads 只能在群集内访问，它们必须通过 LoadBalancer 或 NodePort Service 暴露到外部。对于开发，可以使用 <code>kubectl proxy</code>命令通过代理 api 主机访问内部可访问的 Workloads 。</p>

<p>常用的资源类型：</p>

<ul>
<li>  Services 提供跨多个 Workload 副本的负载均衡的单个 IP 端点。</li>
<li>  Ingress 提供路由到一个或多个服务的 https(s) 端点</li>
</ul>

<h2 id="toc_6">CONFIG &amp; STORAGE</h2>

<p>Config and Storage 资源负责将数据注入到应用程序中，并保留容器外部的数据。</p>

<p>常用的资源类型：</p>

<ul>
<li>  ConfigMaps 通过环境变量、命令行参数或文件提供注入应用的 K-V 键值对</li>
<li>  Secrets 通过文件提供注入应用的二进制数据</li>
<li>  Volumes 提供容器外部的文件系统。 可能在同一个 Pod 中跨 Container 容器共享，并且其寿命持续超出 Container 或 Pod。</li>
</ul>

<h2 id="toc_7">METADATA</h2>

<p>Metadata resources 负责集群内其他资源的行为。</p>

<p>常用的资源类型：</p>

<ul>
<li>  HorizontalPodAutoscaler 自动缩放 workloads 的副本数量以响应加载</li>
<li>  PodDisruptionBudget 在执行维护时，可以配置给定 workloads 中的多少副本可能同时不可用</li>
<li>  ThirdPartyResource 使用自己的类型扩展 Kubernetes API</li>
<li>  Event 群集中资源生命周期事件的通知</li>
</ul>

<h2 id="toc_8">API 调用方式</h2>

<p><strong>▶ kubectl</strong></p>

<p>示例：创建 Deployment</p>

<pre><code>1.  `$ echo &#39;apiVersion: apps/v1`

2.  `kind: Deployment`

3.  `metadata:`

4.  `name: deployment-example`

5.  `spec:`

6.  `replicas: 3`

7.  `revisionHistoryLimit: 10`

8.  `template:`

9.  `metadata:`

10.  `labels:`

11.  `app: nginx`

12.  `spec:`

13.  `containers:`

14.  `- name: nginx`

15.  `image: nginx:1.10`

16.  `ports:`

17.  `- containerPort: 80`

18.  `&#39; | kubectl create -f -`

</code></pre>

<p>yaml 文件编写的规则拆解如下图：</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/wbiax4xEAl5xuUnZt7eHIfEe51sP9P3DjdKKVzb6BmPcZAQzsLM5ZM6SsD46djF4siaLHADeaQnrT6qOJDGSCkibw/640?wx_fmt=png" alt=""/></p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/wbiax4xEAl5xuUnZt7eHIfEe51sP9P3DjrdW9EOgJy7BickNuoCKaX7R5t1TZHqDF8RN8qSCRzse8nRDO23IA66w/640?wx_fmt=png" alt=""/></p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/wbiax4xEAl5xuUnZt7eHIfEe51sP9P3DjEr3QHuRt71DVQHpNZHibDhFnDibRvT6ghUdMvCOLqSuUIZYO7YXWZyLg/640?wx_fmt=png" alt=""/></p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/wbiax4xEAl5xuUnZt7eHIfEe51sP9P3Djl2MJd16ycvpxFCy7ibcQmaqMVo0ndFic3xlVqvyzwFtl4VTw6y3WuRdg/640?wx_fmt=png" alt=""/></p>

<p>其他资源的创建或其他操作，都可以按照这种方式来操作。</p>

<p><strong>▶ curl</strong></p>

<p>需要使用 <code>kubectl proxy</code></p>

<pre data-initialized="true" data-gclp-id="6" style="box-sizing: border-box;margin-top: 0px;margin-bottom: 0px;padding: 8px 0px 6px;background-color: rgb(241, 239, 238);border-radius: 0px;overflow-y: auto;color: rgb(80, 97, 109);font-size: 10px;line-height: 12px;">

1.  `$ kubectl proxy`

2.  `$ curl -X POST -H 'Content-Type: application/yaml' --data '`

3.  `apiVersion: apps/v1beta1`

4.  `kind: Deployment`

5.  `metadata:`

6.  `name: deployment-example`

7.  `spec:`

8.  `replicas: 3`

9.  `revisionHistoryLimit: 10`

10.  `template:`

11.  `metadata:`

12.  `labels:`

13.  `app: nginx`

14.  `spec:`

15.  `containers:`

16.  `- name: nginx`

17.  `image: nginx:1.10`

18.  `ports:`

19.  `- containerPort: 80`

20.  `' http://127.0.0.1:8001/apis/apps/v1/namespaces/default/deployments` 

</pre>

<p><section class="" powered-by="xiumi.us" style="white-space: normal;box-sizing: border-box;"></p>

<p><section class="" style="box-sizing: border-box;"></p>

<p><section class="" style="font-size: 14px;box-sizing: border-box;"></p>

<p>推荐：<a href="http://mp.weixin.qq.com/s?__biz=MzU0MDEwMjgwNA==&amp;mid=2247484509&amp;idx=1&amp;sn=e8b7f12fb3660d15379e2087c2e0e5c6&amp;chksm=fb3f1da6cc4894b0421d62ebecd16c9de11a3abf919d2c506643d3b864d0357ffc4eec1bfd29&amp;scene=21#wechat_redirect">译：基于注解的控制器：Spring Web/WebFlux 和 测试</a></p>

<p>上一篇：<a href="http://mp.weixin.qq.com/s?__biz=MzU0MDEwMjgwNA==&amp;mid=2247484512&amp;idx=1&amp;sn=4130871a4e6360b4f1b8c1b4dac4b101&amp;chksm=fb3f1d9bcc48948d5430ce9e1cef224c40421241d3a925a891bb1cf2c1753d90db040529848a&amp;scene=21#wechat_redirect">Spring-5-webflux 和阻塞与非阻塞 JDBC</a></p>

<p></section></p>

<p></section></p>

<p></section></p>

<p><section class="" powered-by="xiumi.us" style="white-space: normal;box-sizing: border-box;"></p>

<p><section class="" style="box-sizing: border-box;"></p>

<p><section class="" style="display: inline-block;vertical-align: top;width: 279px;box-sizing: border-box;"></p>

<p><section class="" powered-by="xiumi.us" style="box-sizing: border-box;"></p>

<p><section class="" style="box-sizing: border-box;"></p>

<p><section class="" style="text-align: center;color: rgb(160, 160, 160);font-size: 14px;box-sizing: border-box;"></p>

<p>最好的赞赏</p>

<p>就是你的关注</p>

<p></section></p>

<p></section></p>

<p></section></p>

<p></section></p>

<p><section class="" style="display: inline-block;vertical-align: top;width: 279px;box-sizing: border-box;"></p>

<p><section class="" powered-by="xiumi.us" style="box-sizing: border-box;"></p>

<p><section class="" style="margin-top: 10px;margin-bottom: 10px;text-align: center;box-sizing: border-box;"></p>

<p><section class="" style="max-width: 100%;vertical-align: middle;display: inline-block;box-sizing: border-box;overflow: hidden !important;"><img src="https://mmbiz.qpic.cn/mmbiz_jpg/wbiax4xEAl5zQkzvFqgk7DUAem1u05eybPdhEythAoe3O0FxUHy0tmzgytKI7tJaiaDsEWib43ZFSYmEROK4MNNAQ/640?wx_fmt=jpeg" alt=""/></section></p>

<p></section></p>

<p></section></p>

<p></section></p>

<p></section></p>

<p></section></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[基于 Docker 搭建 MySQL 主从复制 - 秋田君]]></title>
    <link href="http://panlw.github.io/15274384746652.html"/>
    <updated>2018-05-28T00:27:54+08:00</updated>
    <id>http://panlw.github.io/15274384746652.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://my.oschina.net/u/3773384/blog/1810111">原文地址</a></p>
</blockquote>

<pre><code>本篇博文相对简单，因为是初次使用 Docker，MySQL 的主从复制之前也在 Centos 环境下搭建过，但是也忘的也差不多了，因此本次尝试在 Docker 中搭建。根据网上教程走还是踩了一些坑，不过所幸最终搭建成功，因此记录下来，避免以后踩了重复的坑。
</code></pre>

<h2 id="toc_0">搭建环境</h2>

<p>Centos 7.2 64 位</p>

<p>MySQL 5.7.13</p>

<p>Docker 1.13.1</p>

<p>接下来，我们将会在一台服务器上安装 docker，并使用 docker 运行三个 MySQL 容器，分别为一主两从。</p>

<h2 id="toc_1">安装 docker</h2>

<p>执行命令</p>

<pre><code>[root@VM_0_17_centos ~]# yum install docker
</code></pre>

<p>如果有提示，一路 y 下去</p>

<p>安装成功启动 Docker 后，查看版本</p>

<pre><code>[root@VM_0_17_centos ~]# docker version
Client:
 Version:         1.13.1
 API version:     1.26
 Package version: &lt;unknown&gt;
 Go version:      go1.8.3
 Git commit:      774336d/1.13.1
 Built:           Wed Mar  7 17:06:16 2018
 OS/Arch:         linux/amd64

Server:
 Version:         1.13.1
 API version:     1.26 (minimum version 1.12)
 Package version: &lt;unknown&gt;
 Go version:      go1.8.3
 Git commit:      774336d/1.13.1
 Built:           Wed Mar  7 17:06:16 2018
 OS/Arch:         linux/amd64
 Experimental:    false
</code></pre>

<p>出现版本信息，则安装成功</p>

<h2 id="toc_2">启动 Docker</h2>

<p>启动 Docker 并设置为开机自启动</p>

<pre><code>[root@VM_0_17_centos ~]# systemctl  start docker.service
[root@VM_0_17_centos ~]# systemctl  enable docker.service
</code></pre>

<h2 id="toc_3">安装 MySQL</h2>

<p>使用 Docker 拉取 MySQL 镜像</p>

<pre><code>[root@VM_0_17_centos ~]# docker pull mysql:5.7.13
</code></pre>

<h2 id="toc_4">运行主容器</h2>

<pre><code>[root@VM_0_17_centos ~]# docker run --name master -p 3306:3306 -e MYSQL_ROOT_PASSWORD=root -d mysql:5.7.13

</code></pre>

<p>--name 为容器指定名称，这里是 master</p>

<p>-p 将容器的指定端口映射到主机的指定端口，这里是将容器的 3306 端口映射到主机的 3306 端口</p>

<p>-e 设置环境变量，这里是指定 root 账号的密码为 root</p>

<p>-d 后台运行容器，并返回容器 ID</p>

<p>mysql:5.7.13 指定运行的 mysql 版本</p>

<h2 id="toc_5">检验是否启动成功</h2>

<p>docker ps -a 显示所有的容器，包括未运行的</p>

<pre><code>[root@VM_0_17_centos ~]# docker ps -a
ee86c19336f8        mysql:5.7.13        &quot;docker-entrypoint...&quot;   About an hour ago   Up About an hour    0.0.0.0:3306-&gt;3306/tcp   master

</code></pre>

<p>注意，是 UP 状态，表示正在运行中</p>

<p>开放 3306 端口</p>

<pre><code>[root@VM_0_17_centos ~]# firewall-cmd --zone=public --add-port=3306/tcp --permanent
[root@VM_0_17_centos ~]# firewall-cmd --reload

</code></pre>

<p>--permanent 永久开启，避免下次开机需要再次手动开启端口</p>

<p>使用 Navicat 连接测试</p>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/114916_0h3I_3773384.png" alt=""/></p>

<p>MySQL 主容器已经启动成功</p>

<h2 id="toc_6">创建主容器的复制账号</h2>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/120249_0ZQx_3773384.png" alt=""/></p>

<p>使用 Navicat 友好的图像化界面执行 SQL</p>

<pre><code>GRANT REPLICATION SLAVE ON *.* to &#39;backup&#39;@&#39;%&#39; identified by &#39;backup&#39;;
show grants for &#39;backup&#39;@&#39;%&#39;;
</code></pre>

<p>出现如下信息表示授权成功</p>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/121018_Vtxn_3773384.png" alt=""/></p>

<h2 id="toc_7">修改 MySQL 配置环境</h2>

<p>创建配置文件目录</p>

<p>目录结构如下</p>

<p>/usr/local/mysql/master</p>

<p>/usr/local/mysql/slave1</p>

<p>/usr/local/mysql/slave2</p>

<p>拷贝一份 MySQL 配置文件</p>

<pre><code>[root@VM_0_17_centos local]# docker cp master:/etc/mysql/my.cnf /usr/local/mysql/master/my.cnf

</code></pre>

<p>进到 master 目录下，已存在拷贝的 my.cnf</p>

<pre><code>[root@VM_0_17_centos master]# ll
total 4
-rw-r--r-- 1 root root 1801 May 10 10:27 my.cnf

</code></pre>

<p>修改 my.cnf，在 [mysqld] 节点最后加上后保存</p>

<pre><code>log-bin=mysql-bin
server-id=1
</code></pre>

<p>log-bin=mysql-bin 使用 binary logging，mysql-bin 是 log 文件名的前缀</p>

<p>server-id=1 唯一服务器 ID，非 0 整数，不能和其他服务器的 server-id 重复</p>

<p>将修改后的文件覆盖 Docker 中 MySQL 中的配置文件</p>

<pre><code>[root@VM_0_17_centos master]# docker cp /usr/local/mysql/master/my.cnf master:/etc/mysql/my.cnf

</code></pre>

<p>重启 mysql 的 docker , 让配置生效</p>

<pre><code>[root@VM_0_17_centos master]# docker restart master
</code></pre>

<p>启动后，重新测试连接，连接成功表示主容器配置成功</p>

<h2 id="toc_8">运行 MySQL 从容器</h2>

<p>首先运行从容器</p>

<pre><code>[root@VM_0_17_centos ~]# docker run --name slave1 -p 3307:3306 -e MYSQL_ROOT_PASSWORD=root -d mysql:5.7.13

</code></pre>

<p>与主容器相似，拷贝配置文件至 slave1 目录修改后覆盖回 Docker 中</p>

<pre><code>log-bin=mysql-bin
server-id=2
</code></pre>

<p>别忘记，重启 slave1 容器，使配置生效</p>

<h2 id="toc_9">配置主从复制</h2>

<p>使用 Navicat 连接 slave1 后新建查询，执行以下 SQL</p>

<pre><code>CHANGE MASTER TO 
MASTER_HOST=&#39;ip&#39;,
MASTER_PORT=3306,
MASTER_USER=&#39;backup&#39;,
MASTER_PASSWORD=&#39;backup&#39;;

START SLAVE;
</code></pre>

<p>MASTER_HOST 填 Navicat 连接配置中的 ip 应该就可以</p>

<p>MASTER_PORT 主容器的端口</p>

<p>MASTER_USER 同步账号的用户名</p>

<p>MASTER_PASSWORD 同步账号的密码</p>

<h2 id="toc_10">检查是否配置成功</h2>

<pre><code>show slave status;
</code></pre>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/123902_gnvI_3773384.png" alt=""/></p>

<p>Slave_IO_State 如果是 Waiting for master to send event，那么就成功一半了，如果是 Connecting to master，基本就是配置失败了，建议重新检查下配置，具体失败的原因可以查看日志追踪</p>

<pre><code>[root@VM_0_17_centos master]# docker logs slave -f
</code></pre>

<p>我遇到的是 MASTER_USER 和 MASTER_PASSWORD 是否手打输错了，贴出错误日志</p>

<pre><code>2018-05-10T02:57:00.688887Z 11 [ERROR] Slave I/O for channel &#39;&#39;: error connecting to master &#39;bakcup@ip:3306&#39; - retry-time: 60  retries: 2, Error_code: 1045
2018-05-10T02:58:00.690476Z 11 [ERROR] Slave I/O for channel &#39;&#39;: error connecting to master &#39;bakcup@ip:3306&#39; - retry-time: 60  retries: 3, Error_code: 1045
</code></pre>

<p>注意看日志中的 bakcup，解决方法如下</p>

<pre><code>STOP SLAVE;

CHANGE MASTER TO 
MASTER_HOST=&#39;连接Navicat的ip&#39;,
MASTER_PORT=正确的端口,
MASTER_USER=&#39;正确的用户名&#39;,
MASTER_PASSWORD=&#39;正确的密码&#39;;

START SLAVE;
</code></pre>

<p>接着上文，我们说成功一半，并没有说成功了，那么另一半在于 Slave_IO_Running 与 Slave_SQL_Running</p>

<p>如果都是 Yes，那么恭喜你，可以测试主从复制的效果了，如果有一个不是 Yes，一半是重启从容器后，事务回滚引起的，那么给出解决方法如下</p>

<pre><code>stop slave ;
set GLOBAL SQL_SLAVE_SKIP_COUNTER=1;
start slave ;
</code></pre>

<p>执行后，再次观察三个关键字段应该就都没问题了</p>

<p>至此，一主一从已经搭建完成，再添加从实例的方式与上文一致，这里就不在赘述了。</p>

<h2 id="toc_11">测试主从复制</h2>

<p>首先，在主实例中创建一个测试数据库</p>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/130207_7Ty1_3773384.png" alt=""/></p>

<p>打开（刷新）从实例，可见 test 库已存在</p>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/130133_ITNO_3773384.png" alt=""/></p>

<p>在 test 库中创建一个表 t_test，添加一个 id 测试字段</p>

<p>向表中添加几个数据</p>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/130357_IEZg_3773384.png" alt=""/></p>

<p>刷新从库，可见 t_test 表及其中 1、2、3、4 数据已存在</p>

<p>至此，一个具备主从复制的一主两从的 MySQL 就已搭建完成。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Never touch your local /etc/hosts file in OS X again]]></title>
    <link href="http://panlw.github.io/15274377762013.html"/>
    <updated>2018-05-28T00:16:16+08:00</updated>
    <id>http://panlw.github.io/15274377762013.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://gist.github.com/ogrrd/5831371">原文地址</a></p>

<p>To setup your computer to work with *.dev domains, e.g. project.dev, awesome.dev and so on, without having to add to your hosts file each time.</p>
</blockquote>

<h2 id="toc_0">Requirements</h2>

<ul>
<li><a href="http://mxcl.github.io/homebrew/">Homebrew</a></li>
<li>Mountain Lion</li>
</ul>

<h2 id="toc_1">Install</h2>

<pre><code>brew install dnsmasq
</code></pre>

<h2 id="toc_2">Setup</h2>

<h3 id="toc_3">Create config directory</h3>

<pre><code>mkdir -pv $(brew --prefix)/etc/
</code></pre>

<h3 id="toc_4">Setup *.dev</h3>

<pre><code>echo &#39;address=/.dev/127.0.0.1&#39; &gt; $(brew --prefix)/etc/dnsmasq.conf
</code></pre>

<p>You should probably add <code>strict-order</code> to <code>dnsmasq.conf</code> to keep nameserver order of <code>resolv.conf</code> (<a href="https://gist.github.com/drye/5387341">see here</a>).</p>

<h2 id="toc_5">Autostart</h2>

<h3 id="toc_6">Work after reboot</h3>

<pre><code>sudo cp -v $(brew --prefix dnsmasq)/homebrew.mxcl.dnsmasq.plist /Library/LaunchDaemons
</code></pre>

<h3 id="toc_7">Get it going right now</h3>

<pre><code>sudo launchctl load -w /Library/LaunchDaemons/homebrew.mxcl.dnsmasq.plist
</code></pre>

<h2 id="toc_8">Add to resolvers</h2>

<h3 id="toc_9">Create resolver directory</h3>

<pre><code>sudo mkdir -v /etc/resolver
</code></pre>

<h3 id="toc_10">Add your nameserver to resolvers</h3>

<pre><code>sudo bash -c &#39;echo &quot;nameserver 127.0.0.1&quot; &gt; /etc/resolver/dev&#39;
</code></pre>

<h2 id="toc_11">Add local DNS to search order in System Preferences</h2>

<p>System Preferences &gt; Network &gt; Wi-Fi (or whatever you use) &gt; Advanced... &gt; DNS &gt; add 127.0.0.1 to top of the list.</p>

<h2 id="toc_12">Finished</h2>

<p>That&#39;s it! You can run scutil --dns to show all of your current resolvers, and you should see that all requests for a domain ending in .dev will go to the DNS server at 127.0.0.1</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install NGINX with PHP7-FPM on Mac OS X with Homebrew]]></title>
    <link href="http://panlw.github.io/15274361469556.html"/>
    <updated>2018-05-27T23:49:06+08:00</updated>
    <id>http://panlw.github.io/15274361469556.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://gist.github.com/dtomasi/ab76d14338db82ec24a1fc137caff75b">原文地址</a></p>
</blockquote>

<h2 id="toc_0">Install Commandline Tools</h2>

<p><code>xcode-select --install</code></p>

<h2 id="toc_1">Install Homebrew</h2>

<p><code>ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</code></p>

<h4 id="toc_2">Check Installation</h4>

<p><code>brew doctor</code></p>

<h4 id="toc_3">Install brew services</h4>

<p><code>brew tap homebrew/services</code></p>

<h4 id="toc_4">Install bash completion (Optional)</h4>

<p><code>brew install bash-completion</code></p>

<h4 id="toc_5">Update Brew and Packages if allready installed</h4>

<p><code>brew update &amp;&amp; brew upgrade</code></p>

<h4 id="toc_6">Setup Environment</h4>

<p><code>sudo nano ~/.bash_profile</code></p>

<p>Add following lines</p>

<pre><code>  ##
  # Homebrew
  ##
  export PATH=&quot;/usr/local/bin:$PATH&quot;
  export PATH=&quot;/usr/local/sbin:$PATH&quot;
  
  ##
  # Homebrew bash completion
  ##
  if [ -f $(brew --prefix)/etc/bash_completion ]; then
    source $(brew --prefix)/etc/bash_completion
  fi
</code></pre>

<h2 id="toc_7">DNSMasq</h2>

<p>DNSMasq is used to resolve all domains that end with .dev to 127.0.0.1. So you don´t need to touch hosts-File anymore.</p>

<h4 id="toc_8">Install</h4>

<pre><code>brew install dnsmasq
</code></pre>

<h4 id="toc_9">Configure</h4>

<pre><code>curl -L https://gist.githubusercontent.com/dtomasi/ab76d14338db82ec24a1fc137caff75b/raw/550c84393c4c1eef8a3e68bb720df561b5d3f175/dnsmasq.conf -o /usr/local/etc/dnsmasq.conf

sudo curl -L https://gist.githubusercontent.com/dtomasi/ab76d14338db82ec24a1fc137caff75b/raw/550c84393c4c1eef8a3e68bb720df561b5d3f175/dev -o /etc/resolver/dev
</code></pre>

<h4 id="toc_10">Start, Stop and Restart</h4>

<pre><code># Start
sudo brew services start dnsmasq

# Stop
sudo brew services stop dnsmasq

# Restart
sudo brew services restart dnsmasq
</code></pre>

<h4 id="toc_11">Test</h4>

<pre><code>dig testing.a.domain.that.should.point.to.localhost.dev @127.0.0.1
</code></pre>

<h2 id="toc_12">PHP-FPM</h2>

<h4 id="toc_13">Install php70</h4>

<pre><code>  brew tap homebrew/dupes &amp;&amp; \
  brew tap homebrew/php &amp;&amp; \
  brew install --without-apache --with-fpm --with-mysql php70
</code></pre>

<h4 id="toc_14">Configure</h4>

<p><code>sudo nano /usr/local/etc/php/7.0/php-fpm.d/www.conf</code></p>

<pre><code>  user = YOUR_USERNAME
  group = YOUR_GROUP || staff
</code></pre>

<h4 id="toc_15">Testing</h4>

<p>start php-fpm</p>

<p><code>sudo brew services start php70</code></p>

<p>show running processes</p>

<p><code>lsof -Pni4 | grep LISTEN | grep php</code></p>

<h2 id="toc_16">NGINX</h2>

<h4 id="toc_17">Install NGINX</h4>

<pre><code>brew tap homebrew/nginx &amp;&amp; \
brew install nginx
</code></pre>

<h4 id="toc_18">Test Installation</h4>

<pre><code>  ## Start Nginx
  sudo brew services start nginx
  
  ## Check if Nginx is running on default port
  curl -IL http://127.0.0.1:8080
</code></pre>

<p>Output should look like this</p>

<pre><code>HTTP/1.1 200 OK
Server: nginx/1.10.0
Date: Sat, 07 May 2016 07:36:32 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 26 Apr 2016 13:31:24 GMT
Connection: keep-alive
ETag: &quot;571f6dac-264&quot;
Accept-Ranges: bytes
</code></pre>

<h4 id="toc_19">Stop Nginx</h4>

<p><code>sudo brew services stop nginx</code></p>

<h4 id="toc_20">Configure</h4>

<p>Create missing directories</p>

<pre><code>  mkdir -p /usr/local/etc/nginx/sites-available &amp;&amp; \
  mkdir -p /usr/local/etc/nginx/sites-enabled &amp;&amp; \
  mkdir -p /usr/local/etc/nginx/conf.d &amp;&amp; \
  mkdir -p /usr/local/etc/nginx/ssl
</code></pre>

<p>Configure nginx.conf</p>

<pre><code># Remove default
rm /usr/local/etc/nginx/nginx.conf
# Copy mine
curl -L https://gist.githubusercontent.com/dtomasi/ab76d14338db82ec24a1fc137caff75b/raw/c7c99476e6d8bd5b23e814c5593861adb9b54765/nginx.conf -o /usr/local/etc/nginx/nginx.conf
</code></pre>

<p>Start and Test Nginx</p>

<pre><code>  ## Start Nginx
  sudo brew services start nginx
  
  ## Check if Nginx is running on default port
  curl -IL http://localhost

  ## Output should look like this
  HTTP/1.1 200 OK
  Server: nginx/1.10.0
  Date: Sat, 07 May 2016 08:35:57 GMT
  Content-Type: text/html
  Content-Length: 612
  Last-Modified: Tue, 26 Apr 2016 13:31:24 GMT
  Connection: keep-alive
  ETag: &quot;571f6dac-264&quot;
  Accept-Ranges: bytes
</code></pre>

<h2 id="toc_21">Setup SSL</h2>

<p>Create a folder for our SSL certificates and private keys:</p>

<p><code>mkdir -p /usr/local/etc/nginx/ssl</code></p>

<p>Generate 4096 bit RSA keys and the self-sign the certificates in one command:</p>

<p><code>openssl req -new -newkey rsa:4096 -days 365 -nodes -x509 -subj &quot;/C=US/ST=State/L=Town/O=Office/CN=localhost&quot; -keyout /usr/local/etc/nginx/ssl/localhost.key -out /usr/local/etc/nginx/ssl/localhost.crt</code></p>

<h2 id="toc_22">Setup example virtual hosts</h2>

<p>These are working presets. But you need to edit Document-Root</p>

<pre><code>curl -L https://gist.githubusercontent.com/dtomasi/ab76d14338db82ec24a1fc137caff75b/raw/c7c99476e6d8bd5b23e814c5593861adb9b54765/default -o /usr/local/etc/nginx/sites-available/default &amp;&amp; \
curl -L https://gist.githubusercontent.com/dtomasi/ab76d14338db82ec24a1fc137caff75b/raw/c7c99476e6d8bd5b23e814c5593861adb9b54765/default-ssl -o /usr/local/etc/nginx/sites-available/default-ssl
</code></pre>

<p>Activate Virtual Hosts</p>

<pre><code>ln -sfv /usr/local/etc/nginx/sites-available/default /usr/local/etc/nginx/sites-enabled/default
ln -sfv /usr/local/etc/nginx/sites-available/default-ssl /usr/local/etc/nginx/sites-enabled/default-ssl
</code></pre>

<p>Create info.php for testing</p>

<p><code>echo &quot;&lt;?php phpinfo();&quot; &gt; /path/to/your/document/root</code></p>

<p>Test</p>

<pre><code>sudo brew services restart nginx

curl -IL http://localhost/info.php

# Output should look like this
HTTP/1.1 200 OK
Server: nginx/1.10.0
Date: Sat, 07 May 2016 08:40:36 GMT
Content-Type: text/html; charset=UTF-8
Connection: keep-alive
X-Powered-By: PHP/7.0.6
</code></pre>

]]></content>
  </entry>
  
</feed>
