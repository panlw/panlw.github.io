<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Junkman]]></title>
  <link href="http://panlw.github.io/atom.xml" rel="self"/>
  <link href="http://panlw.github.io/"/>
  <updated>2018-09-04T09:37:14+08:00</updated>
  <id>http://panlw.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[重构到更深层的模型]]></title>
    <link href="http://panlw.github.io/15389311738065.html"/>
    <updated>2018-10-08T00:52:53+08:00</updated>
    <id>http://panlw.github.io/15389311738065.html</id>
    <content type="html"><![CDATA[
<pre><code>作者 Paul Rayner，译者 刘嘉洋 发布于 2018年9月19日
</code></pre>

<blockquote>
<p>原文地址 <a href="http://www.infoq.com/cn/articles/refactoring-deeper-model">http://www.infoq.com/cn/articles/refactoring-deeper-model</a></p>

<h2 id="toc_0">本文要点</h2>

<ul>
<li>  重构有三个层次：代码层次微重构，模式重构，以及更深层的模型重构。</li>
<li>  无论是对系统还是你的理解来说，做许多小的变更可以形成复杂的大的变更。</li>
<li>  这里提出的案例是 Nexia Home Automation 的摄像机整合案例。重构之后，开发人员可以更方便地了解领域模型，以及系统中的 Java 和 Ruby 代码。</li>
<li>  重构加强了好的 DDD 实践，比如说强边界的上下文，以及跨边界的显式转换。</li>
<li>  使用功能开关和阶段性发布提供了一个推迟做出决策的选择，可以让你在掌握了足够的信息之后再做出明智的选择。</li>
</ul>
</blockquote>

<p>本文改编自 <a href="http://exploreddd.com/">Explore DDD</a> 2017 的一个<a href="https://www.youtube.com/watch?v=vO86vdkv0wM">演说</a>。</p>

<p>在化学的世界中，你可以提取不同的物质，每个物质本身都处在稳定的状态，然后你可以将它们组合起来，它们互相发生反应，成为比反应物加起来还要好的物质。相同的，在软件行业，也会有不同的重构反应物，每个都具有不同的工作量、频率和能力。当它们与领域驱动的探索发现过程催化剂相互碰撞的时候，这些重构反应物就会产生代码的 “化学” 反应，将代码转换为丰富的领域模型。</p>

<p>本文讲述了 Nexia Home Intelligence 中一个持续很久的摄像机支持系统的重构故事。Nexia 是一个大规模 Ruby on Rails 应用程序，需要支持使用成千上万个摄像机的客户集群需求。</p>

<p>我将从三个层次介绍重构。Martin Fowler 的《重构》一书中谈到了微重构，就是在代码级别不断进行小的变更，以实现增量提升。好的开发人员会花时间去记忆并养成如何使用重构工具的习惯，所以这些微重构就成为了第二天性。</p>

<p>Joshua Kerievsky 在《重构与模式》一书中谈到了更高阶的模型，比如说策略模式。他在书中还定义了各种 “坏味道”，比如霰弹式修改，进行一个小的变更也需要很多额外的其他变更。这让我很放心，因为我的设计没有必要一开始就完全正确，我随时可以开始开发，当碰到一种“坏味道” 的时候，我就有了可以重构的工具，但也只在必要的时候进行重构。</p>

<p>我将介绍第三层次的重构，即重构到更深层次的模型，Eric Evans 已经在《领域驱动设计》一书中为我们介绍过这一层次。当我第一次阅读这本书的时候，第三部分吸引了我的注意力。在第三部分中，他谈到了一个项目，在项目中模型并不适用，他们就提出了一个新的重构方式模型，它彻底改变了项目。</p>

<p>看一下这三个层次，如果你可以在你的模型中引入新的概念，这就是一次有用的重构。你需要精通于微重构，充分使用模式来重构到更深层次的模型中去。</p>

<h2 id="toc_1">有关 Nexia Home Automation</h2>

<p>Nexia Home Automation 系统是 Ruby 语言编写的，可以帮助你完成各种家庭自动化工作，比如了解窗户是打开还是关上的，需要系统与运动传感器集成，并连接摄像机。Dan Sharp 和我负责摄像机系统，这也是我将介绍的内容。</p>

<p>家庭自动化不像银行业或保险业等其他领域，它是一个高科技领域，需要处理硬件和固件。这就意味着客户并不了解很多技术的问题，你不能直接问他们固件相关的问题。</p>

<p>我们的目标是不断研究新功能，同时提升对新的摄像机的支持。当新的摄像机面世后，通常需要几周或几个月的时间，通过大量霰弹式修改，才能添加 Nexia 对它的支持。我们希望能大大缩短这个时间。</p>

<p>如果你要完成向 Nexia 添加摄像机的过程设置，你会注意到它使用的一些术语。比如说，并不是添加摄像机，而是需要注册一个新的摄像机，之后进行激活步骤。注册步骤是想让 Nexia 知道相机的存在，需要在连接到 Nexia 之前完成。</p>

<h2 id="toc_2">架构处理</h2>

<p>安装在客户家庭的几千台摄像机和许多相机管理器组件通信。相机管理器是由 Java 编写的，通信是通过 HTTP 和 SSL 实现的。当消息从摄像机进入管理器之后，我们将这些消息放到 Redis 作业队列中。这些消息被 Portal Workers 从队列中去除，在后台中运行。Portal Workers 是由 Ruby 编写的。Nexia 需要回复摄像机，所以我们在 RabbitMQ 消息总线上将这些消息排队，这些消息会通过相机管理器处理。图 1 展示了这个架构很高层次的一个视图。</p>

<p><a href="https://s3.amazonaws.com/infoq.content.live.0/articles/refactoring-deeper-model/zh/resources/4711-1536938911172.jpg"><img src="https://res.infoq.com/articles/refactoring-deeper-model/zh/resources/4711-1536938911172.jpg" alt=""/></a><br/>
<small>图 1：Nexia 架构</small></p>

<p>这个应用程序本身是 Rails 应用程序，代码库的部分内容如图 2 所示。如果你不熟悉 Rails 开发，models 文件夹通常不是控制器所在的地方，所以不要认为它是富领域模型。我特别展示了一些我提到的 workers，以及和自动化相关的 camera 文件。比如在日落时执行一些工作，或是在指定时间调暗灯光，都是 Nexia 的自动化例子。</p>

<p><a href="https://s3.amazonaws.com/infoq.content.live.0/articles/refactoring-deeper-model/zh/resources/3312-1536938910575.jpg"><img src="https://res.infoq.com/articles/refactoring-deeper-model/zh/resources/3312-1536938910575.jpg" alt=""/></a><br/>
<small>图 2：Rails 应用程序架构</small></p>

<h2 id="toc_3">三个主要挑战</h2>

<p>我们遇到的第一个挑战是代码很难推断。Java 相机管理器过度架构，它们使用了有许多抽象的元架构，让它可以和任何与 Nexia 连接的任何类型摄像机一起工作。实际上大多数摄像机都非常类似，比如说都使用 SSL 和 HTTP，我们不需要额外的抽象层。</p>

<p>举一个系统性问题的例子，图 3 展示的是<code>handleRequest()</code>方法的一部分。任何 DDD 从业者都会对这段代码的语言表达产生疑问。91 行引入了 Zombie 一词，什么是 Zombie？93 行提到了 “如果没有进行授权（<code>isAuthorized</code>）”，但是 94 行的注释提到的行的注释提到的认证（<code>authenticated</code>）和它并不是同一个东西。更糟糕的是，98 行将一个变量声明为<code>auth</code>，这既能代表前者，也能代表后者。虽然这仅仅是个小例子，但是这段代码也能代表我们相机管理器代码库中遇到的一些问题了。</p>

<p><a href="https://s3.amazonaws.com/infoq.content.live.0/articles/refactoring-deeper-model/zh/resources/2513-1536938909817.jpg"><img src="https://res.infoq.com/articles/refactoring-deeper-model/zh/resources/2513-1536938909817.jpg" alt=""/></a><br/>
<small>图 3：相机管理器<code>handleRequest()</code>代码示例</small></p>

<p>在 Ruby 端，网站工作人员对于摄像机的支持随着时间推移而增长。由于大多数工作都是由不同的开发人员（主要是外包）按照需要完成的，所以过多地倾向于职责实现了，而未进行有目的地建模。</p>

<p>Ruby 代码的优点是十分简洁，可以在几行内表达很多内容。但是，<code>CameraWorker</code>情况不同，它负责验证并关闭摄像机的链接。先声明一下，由于不能本文中展示超过 130 多行代码，因此图 4 中展示了部分代码。在多个地方，worker 需要对摄像机对象进行状态修改，而不是声明所需的行为。我们还碰到了一些不太好的命名，比如 89 行的<code>start_motion</code>调用，看上去像是开始行动的命令，但其实并不是。</p>

<p><a href="https://s3.amazonaws.com/infoq.content.live.0/articles/refactoring-deeper-model/zh/resources/2314-1536938910847.jpg"><img src="https://res.infoq.com/articles/refactoring-deeper-model/zh/resources/2314-1536938910847.jpg" alt=""/></a><br/>
<small>图 4：<code>CameraWorker</code>代码示例</small></p>

<p>和那段 Java 代码类似，这只是其中的一个小片段，但它也可以代表系统性问题了。这些都造成了代码很难推算。</p>

<p>遇到的第二个挑战是相机管理器与设备管理器过于耦合了。想要理解这个问题，就得先了解一些架构的历史了。相机管理器（CM）是从通用设备管理器（DM）发展而来的，后者可以管理各种类型的设备。这就导致需要和其他 Nexia 的部分共享内核。这成为了一个重大的部署问题，这意味着基本上我们就不能升级 Java 了。最终，我们认识到这种耦合是没有必要的。尽管摄像机是个设备，但是它和其他设备没有很多类似之处，比如门锁等等。</p>

<p>第三个挑战真正涉及到了 DDD，领域知识出现在错误的位置。大多数领域逻辑是在 Java 相机管理器代码之中，这就代表着增加新的功能会很复杂、耗时、容易发生错误以及很难测试。同时，修改代码代表着要对所有东西进行霰弹式修改。</p>

<h2 id="toc_4">DDD 关注点</h2>

<p>我列出所有问题，不仅仅是吐槽糟糕的代码，而是要明确它们并不是不可克服的挑战。此外，DDD 提供了可以在很大程度上改善这种情况的技术。首先回顾一下 DDD 的四大关注点。</p>

<p>首先，我们希望在代码中演进并表达深层的领域模型。第二，我们希望将代码重构为通用的语言，内容一致易于理解，代码意图清晰。第三，我们要清楚地描述模型和模块的边界和职责。很难在没有明确边界的情况下实现高内聚和松耦合。最后，我们需要严格遵守模型边界（即有界的上下文），跨边界时进行显式转换。</p>

<h2 id="toc_5">从何开始？</h2>

<p>当你遇到这样的代码的时候你会怎么做？现在有一些选择，我知道现在一些人已经试过某些选择了，但并没有成功。一个选择是 “清除积水”，尝试删除所有旧的代码，重新开始。人们可能需要空出几个礼拜进行重大重构，直到“修复” 的时候再解脱出来。第二种选择是<a href="https://en.wikipedia.org/wiki/Somebody_else%27s_problem">将问题甩给别人</a>，自己不做任何处理。</p>

<p>我喜欢选择进行试验，看看你能做什么。我喜欢 <a href="https://hbr.org/2015/10/how-1-performance-improvements-led-to-olympic-gold">2012 年夏季奥运会英国自行车队获得金牌的故事</a>。这个团队进行了许多试验，找了很多方法，从小的地方开始改进，并做了许多改变。英国自行车队负责人 Sir Dave Brailsford 说：“我觉得我们应该从小处进行改变，通过小收益的累积，采取不断提升的哲学思想。抛弃完美，关注事情的进展，尝试各种改进。” 对于敏捷软件开发人员来说，持续改进的思想并不陌生。</p>

<h2 id="toc_6">一小步</h2>

<p>在我们的项目里，我们尝试了各种不同的事情，但大多数都没有用。在 2014 年 3 月，我们尝试了 “一小步”，我们意识到摄像机的概念实际上需要完成两个不同的任务。它充当了物理设备，也叫实体。但同时它还需要作为命令处理程序，提供向物理设备发送命令和查询的接口。</p>

<p>首先看一下 Ruby 代码，我们发现这两个任务都在摄像机对象中。它是设备的子类，有许多问题。由于没有进一步的子类，所有的逻辑都在巨大的 “上帝” 对象摄像机中。</p>

<p>我们首先决定添加新的领域服务，而不是改变摄像机对象，这遵循了开放和修改的原则。这个新的<code>Camera::CommandService</code>存放所有的摄像机指令和查询。由于我们将它作为扩展来写，我们可以用好的测试驱动开发和结对编程实践来实现它，在不破坏其他东西的情况下创造更高质量的设计工作。我们有一个很好的测试组件，它覆盖了 controllers、 workers 和 collaborators，我们可以更放心地进行更新。这一小步实现了虽然小，但是不可忽视的改进。</p>

<h2 id="toc_7">寻找接缝</h2>

<p>Martin Fowler 在其《修改代码的艺术》一书中聊到了寻找代码的接缝，也就是你可以加入新东西。我们召开了事件风暴会，了解设备注册 Nexia 的不同方法，这有助于可视化这些工作流中的相似之处。通过查看 Java 代码，我们发现相机管理器组件太过 “智能”，它仅需要管理摄像机会话就可以了，但却做了太多其他事情。我们希望让相机管理器成为通用的 http 代理，由于所有指令都是 HTTP 调用，并整合 Ruby 的所有摄像机逻辑。</p>

<p>我们在接缝相机管理器加入了新的通用<code>send_url()</code>指令。我们将 http 代理模型运用在连接管理、验证、摄像机到入口消息传递和日志记录上（来帮助故障排除以及未来计划）。在 Ruby 端，我们可以在前一年的进展上，使用<code>Camera:CommandService</code>向摄像机发送任何指令。</p>

<h2 id="toc_8">迁移领域逻辑</h2>

<p>在推行了一小步并发现新的接缝一年之后，我们可以将摄像机的领域逻辑从 Java 端迁移到 Ruby 端。我们将<code>Camera::CommandService</code>指令（例如 Pan-Tilt）迁移到使用通用的相机管理器接口。这个方法最棒的地方是不需要修改 Java 代码。作为 Ruby 端的内部重构，我们可以只使用一个指令进行测试，并迭代它直到可以运行。</p>

<p>我聊到这个故事的时候，通常会被问一个问题：“你怎么验证这些重构？” 我指出，我们正在继续交付应用程序的功能，这是我们随时可以进行的工作。同时，这些小的步骤也获得了一些进展。由于我们为摄像机提供通用的 URLs，可以从 Ruby 发送指令，因此我们可以在所有安装的相机上批量升级固件。此外，我们可以在 Ruby 端简单、快速地进行变更，这代表着我们可以进行试验，发现其他的边界改进。在这之前，需要 Java 和 Ruby 合作才能完成变更。</p>

<p>我想再次强调小进展的重要性。非技术利益相关者并不关心你是否重构代码。我相信一般来说，他们相信你是专业人士，会竭尽最大努力写可维护的代码。这代表着你必须建立信任和信誉，可以通过实现小的进展来完成这一点。</p>

<p>我们还发现，重构可以帮助清理代码。在<code>Camera::CameraWorker</code>中就有三个验证。首先，在重新连接的时候验证已存在的摄像机。其次，处理新的摄像机的创建和验证。第三，去掉 “僵尸” 摄像机，就是已经连接但没有验证的摄像机。通过重构到更深层的模型，代码可以更容易地推断，正如图 5 中的 8-14 行所示。</p>

<p><a href="https://s3.amazonaws.com/infoq.content.live.0/articles/refactoring-deeper-model/zh/resources/2315-1536938909299.jpg"><img src="https://res.infoq.com/articles/refactoring-deeper-model/zh/resources/2315-1536938909299.jpg" alt=""/></a><br/>
<small>图 5：验证的三个不同方向</small></p>

<p>在我们引入了更多的领域逻辑之后，Ruby 代码占据了 Nexia 普遍用的语言的比重更高了。我们不需要给摄像机对象进行许多修改，并设置许多属性，我们发现工厂模式更加适合。普遍使用的语言包括心跳的概念，对于这个系统来说就是摄像机连接到 Nexia，就像它们或者一样。之后我们创造了名为<code>update_from_heartbeat</code>和<code>create_from_heartbeat</code>的工厂方法，来分别处理现有的摄像机和新的摄像机。</p>

<p>Java 端也得益于重构。较之前在图 3 中部分展示的<code>handleRequest()</code>方法，变成了图 6 中的 5 行代码。对一些提取的方法进行重构，功能变得更加容易理解。</p>

<p><a href="https://s3.amazonaws.com/infoq.content.live.0/articles/refactoring-deeper-model/zh/resources/2116-1536938910279.jpg"><img src="https://res.infoq.com/articles/refactoring-deeper-model/zh/resources/2116-1536938910279.jpg" alt=""/></a><br/>
<small>图 6：新 Java 代码示例（与图 3 相比较）</small></p>

<p>摄像机类很大，因此你经常会跑到这段代码里。小贴士，处理这种情况并不需要通过代码重构使它更加清晰。当代码杂乱不堪时，简单地重新排列一下代码会很有效，虽然它只是个简单的设计技巧。看一看模式，把类似的方法放在一起，这将缓解你在处理庞大代码域时的认知负担。</p>

<h2 id="toc_9">重构到更深层次</h2>

<p>处理一个庞大、混乱的代码库就像雾中漫步，你不能看到周围的一切，你可能看到的只是一棵树，或是一座山。当你做了一些小的变更之后（如重组织代码，提取方法），这些小的收益会累积，雾开始消散。实现微重构和 Fowler 和 Kerievsky 提到的模式能产生累积的效果，因此可以对模型有更深层次的了解。</p>

<p>比如说，在 Ruby 端，我们发现我们正在向摄像机发送指令。所以我们按照 Kerievsky 的建议，使用命令模式，使之大大简化了。我们为指令设置了基类，以及标准的<code>execute()</code>方法。之后我们创建了 camera/command 文件夹，开始写每个指令，实现这个基类。此外，我们还引入了功能开关，帮助旧代码继续执行，直到相应的指令已经转换。我强烈推荐使用功能开关来帮助你安全地进行重构。</p>

<p>我推荐的另一个方法是阶段性发布。我们希望避免每台摄像机突然断开连接的情况，大多数现有的客户群的产品都有共同的目标，就是不要影响到所有客户。在第一个月我们仅仅部署到 Nexia IP 地址，让 QA、开发人员和支持人员在部署到客户之前先尝试新系统。第二个月我们加大部署，部署到一部分客户，但只使用一个生产服务器。直到第三个月我们才会部署到所有的摄像机、所有的客户和所有的生产服务器上。这比我职业生涯中参与的其他生产环境发布都要顺利。</p>

<p>功能开关和阶段性发布的另一个好处是它们提供了有价值的选择。我推荐<a href="https://www.amazon.com/Commitment-Novel-about-Managing-Project/dp/9462410038">《Commitment: Novel about Managing Project Risk》</a>一书，介绍了实际选择权的概念。通常，当某人在会议中说 “我们需要作出决定” 的时候，就会有两个选择，一个是根据有限的知识作出选择，要么不做选择。实际选择权指出还有第三个选项，在我们更好地理解之前，战略性地推迟决策。功能开关和阶段性发布都可以帮助你推迟做出决定，直到你可以做出更好的决定为止，这非常关键。</p>

<h2 id="toc_10">回顾</h2>

<p>回看一开始的时候，我们已经获得了很大的成就。之前，添加新的摄像机需要几周甚至几个月。现在，我们可以在几小时内添加新的摄像机。我们不再需要在 Java 和 Ruby 中都作出变更，并保持代码的同步，我们只需要在 Ruby 中进行修改。尽管旧代码在一些方面不一致，但新的代码更加内聚，也很容易推断，因为上下文很清晰。我们移除了<strong>相机管理器</strong>和<strong>设备管理器</strong>之间粗糙的依赖，所以我们可以更新 Java 了。</p>

<p>根据这些经验，有一些通用的重构技巧。不要只选择一个变更的实现方法，比如说命名新的东西，尝试至少三种语言和 / 或模型选项。在你的日常工作中，同样要注意一些小的收益。我们往往太过于高估大变更的效果，却低估了小的累积的变化的力量。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[IAM of Google Cloud]]></title>
    <link href="http://panlw.github.io/15389278377243.html"/>
    <updated>2018-10-07T23:57:17+08:00</updated>
    <id>http://panlw.github.io/15389278377243.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">User Profile</h2>

<ul>
<li>Communication

<ul>
<li>Product notifications</li>
<li>Updates &amp; offers</li>
</ul></li>
<li>Language &amp; region

<ul>
<li>Language</li>
<li>Date format</li>
<li>Time format</li>
<li>Number format</li>
</ul></li>
<li>Personalization

<ul>
<li>Allow google to track your activity</li>
</ul></li>
</ul>

<h2 id="toc_1">Authorization</h2>

<ul>
<li>User &lt;-1:n-&gt; Roles

<ul>
<li>List view by Members or Roles (Role/Member)</li>
</ul></li>
<li>Role &lt;-1:n-&gt; Permissions

<ul>
<li>Create custom role and add permissions (COULD filtered by existed roles)</li>
</ul></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RSocket, a New Application Network Protocol for Reactive Applications, Announced at SpringOne]]></title>
    <link href="http://panlw.github.io/15389224951575.html"/>
    <updated>2018-10-07T22:28:15+08:00</updated>
    <id>http://panlw.github.io/15389224951575.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://www.infoq.com/news/2018/10/rsocket-facebook">https://www.infoq.com/news/2018/10/rsocket-facebook</a></p>
</blockquote>

<p>Announced at the <a href="https://springoneplatform.io/2018/speakers/ben-hale">SpringOne Platform</a> conference in Washington DC, <a href="http://rsocket.io">RSocket</a> is a new layer 7, language-agnostic application network protocol. It is a bi-directional, multiplexed, message-based, binary protocol based on Reactive Streams back pressure. It has been developed by engineers from Facebook, Netifi and Pivotal amongst others, with implementations available in <a href="https://github.com/rsocket/rsocket-java">Java</a>, <a href="https://github.com/rsocket/rsocket-js">JavaScript</a>, <a href="https://github.com/rsocket/rsocket-cpp">C++</a>, and <a href="https://github.com/rsocket/rsocket-kotlin">Kotlin</a>.</p>

<p>The protocol is specifically designed to work well with Reactive-style applications, which are fundamentally non-blocking and often (but not always) paired with asynchronous behaviour. The use of Reactive back pressure, the idea that a publisher cannot send data to a subscriber until that subscriber has indicated that it is ready, is a key differentiator from &quot;async&quot;.</p>

<p>&quot;I personally believe,&quot; Cloud Foundry Java Experience Lead <a href="https://springoneplatform.io/2018/speakers/ben-hale">Ben Hale</a> said, &quot;Reactive programming is the next frontier in Java for high efficiency applications.&quot; There are, Hale said, two major roadblocks to Reactive programing - data access, and networking. RSocket is intended to address the latter problem, whilst <a href="https://www.infoq.com/news/2018/10/springone-r2dbc">R2DBC</a> is intended to address the former.</p>

<p>In a microservice-style application HTTP is widely used as the communication protocol. Setting out the reasoning behind developing the new protocol, Pivotal&#39;s <a href="http://projectreactor.io">Project Reactor</a> lead <a href="https://www.linkedin.com/in/smaldini/">Stephane Maldini</a> pointed out that HTTP was designed for a very different world.</p>

<blockquote>
<p>We have iPhones and Android phones, we listen for notifications so we don&#39;t necessarily request something and get a reply, we get multiple replies without necessarily interacting with the device. We also use Smart watches when we exercise which interact with a back-end server giving us statistics. We have Smart assistants interacting with a back-end server. And all these interaction models are part of what we could call a connected experience. HTTP wasn&#39;t really designed for this.</p>
</blockquote>

<p>One significant issue with HTTP, Maldini argued, is that it puts all the onus on the client to handle different kinds of errors with retry logic, timeouts, circuit breakers and so on. Applications built using a Reactive architecture can make efficiency gains and scale well, but, Maldini argued, &quot;Reactive support stops at the application boundary.&quot;</p>

<p>One way that RSocket differs from HTTP is that it defines four interaction models:</p>

<ol>
<li> Fire-and-Forget: an optimization of request/response that is useful when a response is not needed, such as for non-critical event logging.</li>
<li> Request/Response: when you send one request and receive one response, exactly like HTTP. Even here though, the protocol has advantages over HTTP in that it is asynchronous and multiplexed.</li>
<li> Request/Stream: analogous to Request/Response returning a collection, the collection is streamed back instead of querying until complete, so for example send a bank account number, respond with a real-time stream of account transactions.</li>
<li> Channel: a bi-directional stream of messages allowing for arbitrary interaction models.</li>
</ol>

<p>Being message-based means that the protocol can support multiplexing on a single connection. In addition, like TCP, it is truly bi-directional, such that once a client initiates a connection to a server both parties in the connection become equivalent to one another - in essence, the server can request data from the client.</p>

<p>RSocket also supports flow-control on a per-message basis. During the keynote, Facebook engineer <a href="https://www.linkedin.com/in/stevegury/">Steve Gury</a> stated that:</p>

<blockquote>
<p>When you send a message you also specify how many responses you are able to satisfy, and the server must satisfy that constraint, but when I&#39;ve finished processing those responses I can ask for more. RSocket also works across the chain, so if you link multiple RSocket connections the flow control will work end to end.</p>
</blockquote>

<p><img src="media/15389224951575/15389225986478.jpg" alt=""/></p>

<p>In essence, as Hale stated in a follow-on session later in the day, the problem that RSocket solves is cross-process back pressure, that is, back pressure over a network.</p>

<blockquote>
<p>I can guarantee that I never call for more data than I can handle inside of a process, but what happens when I have to make a call to another Microservice in my service mesh. How do I guarantee that it doesn&#39;t materialise a whole bunch of data and that it doesn&#39;t attempt to send me all of that data.</p>
</blockquote>

<p>RSocket is transport-agnostic supporting TCP, <a href="https://en.wikipedia.org/wiki/WebSocket">WebSocket</a> and <a href="https://github.com/real-logic/aeron">Aeron UDP</a>, and supporting mixed transport protocols with no semantic loss - both the back pressure and flow control will continue to work.<br/>
<img src="media/15389224951575/15389226069708.jpg" alt=""/></p>

<p>It also supports connection resumption. When you establish an RSocket connection you can specify the ID of the previous connection, and if the server still has the stream in memory you can resume the consumption of your stream.</p>

<p>During his aforementioned talk, Hale gave considerably more detail about how the protocol works. As noted above, it is a message-driven binary protocol. &quot;We have this idea that given a network connection, a requester-responder interaction is broken down into a discrete set of frames,&quot; Hales said. &quot;Each one of these frames encapsulates a message of some kind.&quot; The framing is binary, rather than human-readable like JSON or XML, giving significant efficiencies for machine-to-machine communication. As with all messaging protocols, payloads are just streams of bytes so can be anything you want, including XML or JSON.</p>

<p>At Facebook, RSocket is used is for a service called LiveServer, which is responsible for responding to a live query which can be thought of as a GraphQL subscription. The server responds with the data but also a stream of future updates.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[如何形象地描述RxJava中的背压和流控机制？]]></title>
    <link href="http://panlw.github.io/15389220103546.html"/>
    <updated>2018-10-07T22:20:10+08:00</updated>
    <id>http://panlw.github.io/15389220103546.html</id>
    <content type="html"><![CDATA[
<pre><code>2016-12-15
</code></pre>

<blockquote>
<p>原文地址 <a href="http://zhangtielei.com/posts/blog-rxjava-backpressure.html">http://zhangtielei.com/posts/blog-rxjava-backpressure.html</a></p>
</blockquote>

<hr/>

<p>之前我在知乎上受邀回答过一个关于 RxJava 背压（Backpressure）机制的问题，今天我把它整理出来，希望对更多的人能有帮助。</p>

<p>RxJava 的官方文档中对于背压（Backpressure）机制比较系统的描述是下面这个：</p>

<blockquote>
<p><a href="https://github.com/ReactiveX/RxJava/wiki/Backpressure">https://github.com/ReactiveX/RxJava/wiki/Backpressure</a></p>
</blockquote>

<p>但本文的题目既然是要 “形象地” 描述各个机制，自然会力求表达简洁，让人一看就懂。所以，下面我会尽量抛开一些抽象的描述，主要采用打比方的方式来阐明我对于这些机制的理解。</p>

<p>首先，从大的方面说，上面这篇文档的题目，虽然叫 “Backpressure”（背压），但却是在讲述一个更大的话题——“Flow Control”（流控）。Backpressure 只是 Flow Control 的其中一个方案。</p>

<p>在 RxJava 中，可以通过对 Observable 连续调用多个 Operator 组成一个调用链，其中数据从上游向下游传递。当上游发送数据的速度大于下游处理数据的速度时，就需要进行 Flow Control 了。</p>

<p>这就像小学做的那道数学题：一个水池，有一个进水管和一个出水管。如果进水管水流更大，过一段时间水池就会满（溢出）。这就是没有 Flow Control 导致的结果。</p>

<p>Flow Control 有哪些思路呢？大概是有四种:</p>

<ul>
<li>  (1) 背压（Backpressure）。</li>
<li>  (2) 节流（Throttling）。</li>
<li>  (3) 打包处理。</li>
<li>  (4) 调用栈阻塞（Callstack blocking）。</li>
</ul>

<p>下面分别详细介绍。</p>

<p>注意：目前 RxJava 的 1.x 和 2.x 两个版本序列同时并存，2.x 相对于 1.x 在接口上有很大变动，其中也包括 Backpressure 的部分。但是，这里要讨论的 Flow Control 机制中的相关概念，却都是适用的。</p>

<h3 id="toc_0">Flow Control 的几种思路</h3>

<h4 id="toc_1">背压（Backpressure）</h4>

<p>Backpressure，也称为 Reactive Pull，就是下游需要多少（具体是通过下游的 request 请求指定需要多少），上游就发送多少。这有点类似于 TCP 里的流量控制，接收方根据自己的接收窗口的情况来控制接收速率，并通过反向的 ACK 包来控制发送方的发送速率。</p>

<p>这种方案只对于所谓的 cold Observable 有效。cold Observable 指的是那些允许降低速率的发送源，比如两台机器传一个文件，速率可大可小，即使降低到每秒几个字节，只要时间足够长，还是能够完成的。相反的例子是音视频直播，数据速率低于某个值整个功能就没法用了（这种就属于 hot Observable 了）。</p>

<h4 id="toc_2">节流（Throttling）</h4>

<p>节流（Throttling），说白了就是丢弃。消费不过来，就处理其中一部分，剩下的丢弃。还是举音视频直播的例子，在下游处理不过来的时候，就需要丢弃数据包。</p>

<p>而至于处理哪些和丢弃哪些数据，就有不同的策略。主要有三种策略：</p>

<ul>
<li>  sample (也叫 throttleLast)</li>
<li>  throttleFirst</li>
<li>  debounce (也叫 throttleWithTimeout)</li>
</ul>

<p>从细的方面分别解释一下。</p>

<p>sample，采样。类比一下音频采样，8kHz 的音频就是每 125 微秒采一个值。sample 可以配置成，比如每 100 毫秒采样一个值，但 100 毫秒内上游可能过来很多值，选哪个值呢，就是选最后那个值。所以它也叫 throttleLast。</p>

<p><a href="/assets/photos_rxjava/backpressure/bp.sample.png"><img src="http://zhangtielei.com/assets/photos_rxjava/backpressure/bp.sample.png" alt=""/></a></p>

<p>throttleFirst 跟 sample 类似，比如还是每 100 毫秒采样一个值，但选这 100 毫秒内的第一个值。在 Android 开发中有时候可以把 throttleFirst 用作点击事件的防抖动处理，就是因为它可以在指定的一段时间内处理第一个点击事件（即采样第一个值），但丢弃后面的点击事件。</p>

<p><a href="/assets/photos_rxjava/backpressure/bp.throttleFirst.png"><img src="http://zhangtielei.com/assets/photos_rxjava/backpressure/bp.throttleFirst.png" alt=""/></a></p>

<p>debounce，也叫 throttleWithTimeout，名字里就包含一个例子。比如，一个网络程序维护一个 TCP 连接，不停地收发数据，但中间没数据可以收发的时候，就有间歇。这段间歇的时间，可以称为 idle time。当 idle time 超过一个预设值的时候，就算超时了（time out），这个时候可能就需要把连接断开了。实际上一些做 server 端的网络程序就是这么工作的。每收发一个数据包之后，启动一个计时器，等待一个 idle time。如果计时器到时之前，又有收发数据包的行为，那么计时器重置，等待一个新的 idle time；而如果计时器时间到了，就超时了（time out），这个连接就可以关闭了。debounce 的行为，跟这个非常类似，可以用它来找到那些连续的收发事件之后的 idle time 超时事件。换句话说，debounce 可以把连续发生的事件之间的较大的间歇找出来。</p>

<p><a href="/assets/photos_rxjava/backpressure/bp.debounce.png"><img src="http://zhangtielei.com/assets/photos_rxjava/backpressure/bp.debounce.png" alt=""/></a></p>

<h4 id="toc_3">打包处理</h4>

<p>打包就是把上游来的小包裹打成大包裹，分发到下游。这样下游需要处理的包裹的个数就减少了。RxJava 中提供了两类这样的机制：buffer 和 window。</p>

<p><a href="/assets/photos_rxjava/backpressure/bp.buffer2.png"><img src="http://zhangtielei.com/assets/photos_rxjava/backpressure/bp.buffer2.png" alt=""/></a></p>

<p><a href="/assets/photos_rxjava/backpressure/bp.window1.png"><img src="http://zhangtielei.com/assets/photos_rxjava/backpressure/bp.window1.png" alt=""/></a></p>

<p>buffer 和 window 的功能基本一样，只是输出格式不太一样：buffer 打包后的包裹用一个 List 表示，而 window 打包后的包裹又是一个 Observable。</p>

<h4 id="toc_4">调用栈阻塞（Callstack blocking）</h4>

<p>这是一种特殊情况，阻塞住整个调用栈（Callstack blocking）。之所以说这是一种特殊情况，是因为这种方式只适用于整个调用链都在一个线程上同步执行的情况，这要求中间的各个 operator 都不能启动新的线程。在平常使用中这种应该是比较少见的，因为我们经常使用 subscribeOn 或 observeOn 来切换执行线程，而且有些复杂的 operator 本身也会在内部启动新的线程来处理。另外，如果真的出现了完全同步的调用链，前面的另外三种 Flow Control 思路仍然可能是适用的，只不过这种阻塞的方式更简单，不需要额外的支持。</p>

<p>这里举个例子把调用栈阻塞和前面的 Backpressure 比较一下。“调用栈阻塞”相当于很多车行驶在盘山公路上，而公路只有一条车道。那么排在最前面的第一辆车就挡住了整条路，后面的车也只能排在后面。而 “Backpressure” 相当于银行办业务时的窗口叫号，窗口主动叫某个号过去（相当于请求），那个人才过去办理。</p>

<h3 id="toc_5">如何让 Observable 支持 Backpressure？</h3>

<p>在 RxJava 1.x 中，有些 Observable 是支持 Backpressure 的，而有些不支持。但不支持 Backpressure 的 Observable 可以通过一些 operator 来转化成支持 Backpressure 的 Observable。这些 operator 包括：</p>

<ul>
<li>  onBackpressureBuffer</li>
<li>  onBackpressureDrop</li>
<li>  onBackpressureLatest</li>
<li>  onBackpressureBlock（已过期）</li>
</ul>

<p>它们转化成的 Observable 分别具有不同的 Backpressure 策略。</p>

<p>而在 RxJava 2.x 中，Observable 不再支持 Backpressure，而是改用 Flowable 来专门支持 Backpressure。上面提到的四种 operator 的前三种分别对应 Flowable 的三种 Backpressure 策略：</p>

<ul>
<li>  BackpressureStrategy.BUFFER</li>
<li>  BackpressureStrategy.DROP</li>
<li>  BackpressureStrategy.LATEST</li>
</ul>

<p>onBackpressureBuffer 是不丢弃数据的处理方式。把上游收到的全部缓存下来，等下游来请求再发给下游。相当于一个水库。但上游太快，水库（buffer）就会溢出。</p>

<p><a href="/assets/photos_rxjava/backpressure/bp.obp.buffer.png"><img src="http://zhangtielei.com/assets/photos_rxjava/backpressure/bp.obp.buffer.png" alt=""/></a></p>

<p>onBackpressureDrop 和 onBackpressureLatest 比较类似，都会丢弃数据。这两种策略相当于一种令牌机制（或者配额机制），下游通过 request 请求产生令牌（配额）给上游，上游接到多少令牌，就给下游发送多少数据。当令牌数消耗到 0 的时候，上游开始丢弃数据。但这两种策略在令牌数为 0 的时候有一点微妙的区别：onBackpressureDrop 直接丢弃数据，不缓存任何数据；而 onBackpressureLatest 则缓存最新的一条数据，这样当上游接到新令牌的时候，它就先把缓存的上一条 “最新” 数据发送给下游。可以结合下面两幅图来理解。</p>

<p><a href="/assets/photos_rxjava/backpressure/bp.obp.drop.png"><img src="http://zhangtielei.com/assets/photos_rxjava/backpressure/bp.obp.drop.png" alt=""/></a></p>

<p><a href="/assets/photos_rxjava/backpressure/bp.obp.latest.png"><img src="http://zhangtielei.com/assets/photos_rxjava/backpressure/bp.obp.latest.png" alt=""/></a></p>

<p>onBackpressureBlock 是看下游有没有需求，有需求就发给下游，下游没有需求，不丢弃，但试图堵住上游的入口（能不能真堵得住还得看上游的情况了），自己并不缓存。这种策略已经废弃不用。</p>

<hr/>

<p>本文重点在于以宏观的角度来描述和对比 RxJava 中的 Flow Control 机制和 Backpressure 的各种机制，很多细节没有涉及。比如，buffer 和 window 除了能把一段时间内收到的数据打包，还能把固定数量的数据进行打包。再比如，onBackpressureDrop 和 onBackpressureLatest 在一次收到下游多条数据的请求时分别会如何表现，本文没有详细说明。大家可以查阅相应的 API Reference 来获得答案，也欢迎留言与我一起讨论。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Modeling Uncertainty with Reactive DDD]]></title>
    <link href="http://panlw.github.io/15389214670417.html"/>
    <updated>2018-10-07T22:11:07+08:00</updated>
    <id>http://panlw.github.io/15389214670417.html</id>
    <content type="html"><![CDATA[
<pre><code>Posted by Vaughn Vernon, reviewed by Thomas Betts on Sep 29, 2018
</code></pre>

<blockquote>
<p>原文地址 <a href="https://www.infoq.com/articles/modeling-uncertainty-reactive-ddd">https://www.infoq.com/articles/modeling-uncertainty-reactive-ddd</a></p>
</blockquote>

<h2 id="toc_0">Key Takeaways</h2>

<ul>
<li>  Reactive, distributed applications must decide how to handle uncertainty regarding the delivery of messages, including multiple delivery and out-of-order delivery.</li>
<li>  Domain-Driven Design can aid with managing uncertainty through the use of good modeling.</li>
<li>  Rules for handling uncertainty must be defined in business logic that domain experts have agreed to, not buried in a technical implementation.</li>
<li>  The implementation of a message de-duplicator or re-sequencer may initially sound straightforward, but gets unwieldy when operating at real-world scale.</li>
<li>  When the rules have been defined using ubiquitous language, the implementation can be very simple and still robust.</li>
</ul>

<p>Domain-Driven Design is the way I think software should be developed. It&#39;s been a bit of an uphill climb from Evan&#39;s original publication of <a href="https://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-Software/dp/0321125215">Domain-Driven Design, Tackling Complexity in the Heart of Software</a>, and we&#39;ve come a long way since. Now there are actually conferences on Domain-Driven Design, and I see a lot of growing interest, including business people getting involved, which is really key.</p>

<p>Reactive is a big thing these days, and I&#39;ll explain later why it&#39;s gaining a lot of traction. What I think is really interesting is that the way DDD was used or implemented, say back in 2003, is quite different from the way that we use DDD today. If you&#39;ve read my red book, <a href="https://vaughnvernon.co/?page_id=168">Implementing Domain-Driven Design</a>, you&#39;re probably familiar with the fact that the bounded contexts that I model in the book are separate processes, with separate deployments. Whereas, in Evan&#39;s blue book, bounded contexts were separated logically, but sometimes deployed in the same deployment unit, perhaps in a web server or an application server. In our modern day use of DDD, I’m seeing more people adopting DDD because it aligns with having separate deployments, such as in microservices.</p>

<p>One thing to keep clear is that the essence of Domain-Driven Design is really still what it always was -- It&#39;s modeling a ubiquitous language in a bounded context. So, what is a bounded context? Basically, the idea behind bounded context is to put a clear delineation between one model and another model. This delineation and boundary that&#39;s put around a domain model, makes the model that is inside the boundary very explicit with very clear meaning as to the concepts, the elements of the model, and the way that the team, including domain experts, think about the model.</p>

<p>You&#39;ll find a ubiquitous language that is spoken by the team and that is modeled in software by the team. In scenarios and discussions where somebody says, for example,&quot;product,&quot; they know in that context exactly what product means. In another context, product can have a different meaning, one that was defined by another team. The product may share identities across bounded contexts, but, generally speaking, the product in another context has at least a slightly different meaning, and possibly even a vastly different meaning.</p>

<p>We&#39;re making an effort with DDD to recognize that there is no practical way to have a canonical, enterprise data model where every single element in the model is representative of how every team in the enterprise would want to use it. It just doesn&#39;t happen. There&#39;s always some difference, and many times there are many differences that make it very painful for one team to try to use the model that another team has created. That&#39;s why we&#39;re focused on the bounded context with a ubiquitous language.</p>

<p>Once you understand that there is one, very definite definition for an entity, in one team situation, with one ubiquitous language, then you realize that there are other models, developed by other teams. Perhaps even the same team developing this model could be responsible for other models. You have a situation where you have multiple bounded contexts because, naturally, we cannot define every meaning in a single enterprise, or within a single system, for every single concept that we&#39;re going to use.</p>

<p>Given that we have multiple contexts and multiple languages, we have to collaborate and integrate between them. To do that, we use a technique called context mapping or a tool called a context map.</p>

<p><img src="media/15389214670417/15389216211566.jpg" alt="" style="width:600px;"/><br/>
<small><strong>Figure 1 - Context Mapping</strong></small></p>

<p>In this simple diagram, the lines between the bounded contexts are contextual mappings, and could appropriately be called a translation. If one of the bounded contexts speaks one language, and a connected context speaks a different language, what do you need to have between languages so that you can understand one model to another? A translation. Typically, we will try to translate between models to keep each separate model pure and clean.</p>

<p>When creating your context map, don&#39;t confuse yourself or place limits on what the line means. While it can cover the technical integration, style or technique, it&#39;s very important to define the team relationship between contexts. Whether I&#39;m using RPC or REST isn&#39;t important. Who I am integrating with is more important than how I am integrating.</p>

<p>It&#39;s very important to define the team relationship between contexts. Who I am integrating with is more important than how I am integrating.</p>

<p>There are various context mapping tools for different types of relationships, including partnership, customer-supplier, or conformist relationships. In a partnership relationship, one team will know a lot about the model on another team. A customer-supplier relationship features an anti-corruption layer between two very separate models, one upstream and one downstream. We will anti-corrupt the upstream model as it&#39;s being consumed by the downstream model. If the downstream model needs to send something back to the upstream, then it will translate it back to the upstream model so that data can be consistently and reliably exchanged, with clear meaning.</p>

<p>The strategic design that I&#39;ve described so far is really the essence of, and therefore the most important part, of Domain-Driven Design.</p>

<p>In some cases, we will decide to model a particular ubiquitous language in a very careful, even fine-grained way. If you think of strategic design as painting with broad brush strokes, then think of tactical design as using a fine brush to fill in all the details.</p>

<h2 id="toc_1">General Guidance on DDD Modeling</h2>

<p>Based on my observations of conference presentations that mention DDD, and my time working with teams, I&#39;ve been able to identify a few little tips to help with modeling. These aren&#39;t meant to call out any specific actions as wrong. Rather, I hope to provide some guidance to nudge you in the right direction.</p>

<p>One thing that we have to remember about DDD when we&#39;re modeling, especially tactically, is we need help from domain experts -- It shouldn&#39;t just be programmers. We have to limit our use of their time, because the people who play the role of a domain expert on a team will be very busy with other matters relating to the business. Therefore, we have to make the experience very rewarding for them.</p>

<p>Another thing we want to do is avoid the anemic domain model. Whenever you see a presentation about a domain model that includes annotations that will automatically create the getters and setters, <code>Equals(), GetHashCode()</code>, etc., think seriously about running away from that. If our domain models were only about data, then that might be a perfect solution. However, there are some questions we need to ask. How does the data come about? How do we actually get data into our domain model? Is it being expressed according to the mental model of the business and any domain expert who&#39;s working with us? Getters and setters do not give you an explicit indication of what the model means -- it&#39;s only moving data around. If you&#39;re thinking in tactical DDD, then you have to think getters and setters are ultimately the enemy, because what you really want to model are behaviors that express the way that the business considers how work should get done.</p>

<p>When modeling, be explicit. For example, say you saw the business identity of an entity or an aggregate being called a UUID. There&#39;s nothing wrong with using a UUID as a business identifier, but why not wrap that in an ID type that is strongly typed? Consider that another bounded context that is not using Java may not understand what a UUID is. You will most likely have to generate a UUID <code>ToString()</code>, and then hold that string in another type, or translate that string from the type when you&#39;re sending out events between bounded contexts.</p>

<p>Instead of using <code>BigDecimal</code> directly, why not think about a value object called Money. If you&#39;ve used <code>BigDecimal</code>, you know that identifying the rounding factor is a common difficulty. If we let <code>BigDecimal</code> slip in all over our model, then how do we round some amount of money? The solution is to use a Money type that standardizes on what the business says should be the rounding specification.</p>

<p>One other little tip is to not worry about what persistence mechanism is used, or what messaging mechanism is used. Use what meets your specific service level agreements. Be reasonable about the throughput and performance you need, and don&#39;t complicate things. DDD is not really talking about technology so much as it is in need of using technology.</p>

<h2 id="toc_2">Reactive Systems</h2>

<p>I have been seeing, at least in my world, a trend towards reactive systems. Not just reactive within a microservice, but building entire systems that are reactive. In DDD, reactive behavior is also happening within the bounded context. Being reactive isn&#39;t entirely new, and Eric Evans was far ahead of the industry when he introduced eventing. Using domain events means we have to react to events that happened in the past, and bring our system into harmony.</p>

<p>If you were to visualize all the connections at different layers of a system, you&#39;ll see patterns that repeat themselves. Whether you&#39;re looking at the entire internet, or all the applications at an enterprise level, or individual actors or asynchronous components within a microservice, every layer has a lot of connections and associated complexity. That gives us a lot of very interesting problems to solve. I want to emphasize that we shouldn&#39;t solve those problems with technology, but model them. If we are developing microservices in the cloud as a means to form a business, then distributed computing is part of the business that we&#39;re working in. Not that distributed computing makes our business (in some cases, it does), but we are definitely solving problems with distributed computing. Therefore, make the distributed computing aspects explicit by modeling them.</p>

<p>I need to take a minute to address what some developers use as an argument against asynchrony, parallelism, concurrency, or any technique that either gives the impression of, or actually facilitates, multiple actions happening at once. Donald Knuth is often quoted as saying, &quot;Premature optimization is the root of all evil.&quot; But that&#39;s just the end of his expression. He really said,&quot;We should forget about small efficiencies… premature optimization is the root of all evil.&quot; Said another way, if we have big bottlenecks in our system, we should address those. We can address those with reactive.</p>

<p>Donald Knuth also said something else very interesting: &quot;People who are more than casually interested in computers should have at least some idea of what the underlying hardware is like. Otherwise, the programs they write will be pretty weird.&quot; He&#39;s simply saying we need to take advantage of the hardware we have today by how we write our software.</p>

<p>If we go back to 1973 and look at the way processors were being manufactured, there were very few transistors, and the clock speed was below 1MHz. Moore&#39;s Law said we&#39;d see the doubling of transistors and processor speed every couple of years. And yet, when we reached 2011, the clock speeds started to fall off. Today, what used to take a year and a half or two years to double clock speed, is now taking about ten years, if not longer. The number of transistors is continuing to increase, but the clock speeds aren&#39;t. Today, what we have are cores. Instead of being faster, we have more cores. So what do we do with all these cores?</p>

<p>If you&#39;ve been keeping up with <a href="https://spring.io/">Spring</a> and the <a href="https://projectreactor.io/">Reactor project</a>, which uses reactive streams, this is essentially what we&#39;re able to do now. We have a publisher, and a publisher is publishing something, let&#39;s call them domain events, the little orange boxes in Figure 2. These events are being delivered to each of the subscribers on the stream.</p>

<p><img src="media/15389214670417/15389216359123.jpg" alt=""/><br/>
<small><strong>Figure 2 - Reactive</strong></small></p>

<p>Notice the lilac boxes on the stream that have question marks on them. That is actually a policy. And that policy is between the stream and the subscriber. For example, a subscriber may have a limit on how many events or messages it can handle, and the policy specifies the limit for any given subscriber. What&#39;s important is that separate threads are being used to run the publisher, the stream, and all three of the subscribers. If this is an intelligently implemented, large, complex component, then the threads are not being blocked at any point in time. Because if the threads are being blocked, then some other piece of the puzzle is starving for a thread. We have to make sure that the implementation underneath is also making good use of threads. This will become more important as we dive deeper into modeling uncertainty.</p>

<p>Within a microservice, we are reactive. But, when we look inside, there are all kinds of components that could be running concurrently, or in parallel. When an event is published inside one of those microservices, it&#39;s ultimately being published outside the bounded context to some sort of topic, possibly using Kafka. To keep it simple, let&#39;s say there&#39;s just one topic. All the other microservices in our reactive system are consuming the events published on the topic, and they&#39;re reactively doing something inside their microservice.</p>

<p><img src="media/15389214670417/15389216450437.jpg" alt=""/><br/>
<small><strong>Figure 3 - Reactive Systems</strong></small></p>

<p>Ultimately, this is where we want to be. When everything is happening asynchronously everywhere, what happens? That brings us to uncertainty.</p>

<h2 id="toc_3">Welcome Uncertainty</h2>

<p>In an ideal situation, when we publish a series of events, we want those events to be delivered sequentially, and exactly once. Each subscriber will receive Event 1, followed by Event 2, followed by Event 3, and each event appears once and only once. Programmers have been taught to jealously guard this scenario because it makes us feel certain about what we are doing. And yet, in distributed computing, it just doesn&#39;t happen.</p>

<p>With microservices and reactive comes uncertainty, starting with uncertainty about what order events might be delivered in, and if an event has been received more than once, or not at all. Even if you&#39;re using a messaging system like Kafka, if you think you&#39;re going to consume them in sequential order, you&#39;re fooling yourself. If there is any possibility of any message being out of order, you have to plan for all of them being out of order.</p>

<p>If there is any possibility of any message being out of order, you have to plan for all of them being out of order.</p>

<p>I found a succinct definition for uncertainty: The <strong>state</strong> of being uncertain. Uncertainty being a state means we can deal with it, because we have ways of reasoning about the state of a system. Ultimately, we want to be in a state where we&#39;re certain and can feel comfortable, even if that only lasts a millisecond. Uncertainty creeps up and it makes things unpredictable, unreliable, and risky. Uncertainty is uncomfortable.</p>

<h3 id="toc_4">Addiction</h3>

<p>Most developers learned the &quot;right way&quot; to develop software that leads down a path of addiction. When you have two components that need to communicate with each other, it&#39;s helpful to refer to one as the client and the other as the server. That doesn&#39;t mean it has to be a remote client. The client is going to invoke a method, or call a function, on the server. While that invocation occurs, the client is just sitting there, waiting to receive a response from the server, or possibly an exception will be thrown. In any case, the client is certain that it will get control again. That certainty of execution flow is one type of addiction we have to deal with.</p>

<p>The second type of common addiction is an addiction to the ordering of things, with no duplicates. This is taught to us in school at a young age, when we learn to count, in order. It makes us feel good when we know the order in which things happened.</p>

<p>Another addiction is the locking database. If I have three nodes in my data source, when I write to one of those nodes, I believe I have a firm lock on the database. Which means that when I get back a success response, I believe that the data is persisted on all three nodes. But then you start using Cassandra, which doesn&#39;t lock all the nodes, how do you query for a value that hasn&#39;t propagated across the cluster, yet?</p>

<p>All of these things create an uncomfortable feeling in us, and we have to learn to deal with that. And yet, it&#39;s okay to feel uncomfortable.</p>

<h2 id="toc_5">Defense Mechanisms</h2>

<p>Because we&#39;re addicted to certainty, blocking, and synchronization, developers tend to deal with the uncertainty by building a fortress. If you&#39;re creating a microservice that is modeled as a bounded context, you design it so that everything within that context is blocking, synchronized, and non-duplicate. That&#39;s where we build our fortress, so all the uncertainty exists outside our context, and we get to develop with certainty.</p>

<p>Starting at the infrastructure layer, we create a de-duplicator and a re-sequencer. These both come from Enterprise Integration Patterns.</p>

<h3 id="toc_6">De-Duplicator</h3>

<p>If events 1, 2, 1, and 3 come in, when they pass through the de-duplicator, we&#39;re certain that we only have events 1, 2, and 3. That doesn&#39;t seem too difficult a problem to solve, but think about the implementation. We&#39;ll probably have some caching enabled, but we can&#39;t store infinite events in memory. This means having some database table to store the event ID, to allow checking every incoming event to know if it&#39;s been seen before.</p>

<p>If an event hasn&#39;t been seen before, pass it along. If it has been seen it before, then it can be ignored, right? Well, why has it been seen before? Is it because we didn&#39;t acknowledge receiving it? If so, should we acknowledge that we&#39;ve received it again? How long do we keep those IDs around to say we&#39;ve seen the event? Is a week enough? How about a month?</p>

<p>Talk about uncertainty. Trying to solve this by throwing technology at it can be very difficult.</p>

<h3 id="toc_7">Re-Sequencer</h3>

<p>For an example of re-sequencing, imagine we see event 3, then event 1, then event 4. For whatever reason, event 2 just hangs out for a really long time. We first have to find a way to re-order events 1, 3, and 4. If event 2 hasn&#39;t arrived, have we effectively shut down our system? We may allow event 1 through, but we could have some rule that says to not process event 1 until event 2 arrives. Once event 2 arrives, then we can safely let all four events into the application.</p>

<p>With any implementation you choose, all of these things are hard. There is uncertainty.</p>

<p>The worst part is we haven&#39;t solved any business problems, yet. We&#39;re just solving technology problems. But, if we acknowledge that distributed computing is part of our business now, then we can solve these problems in the model.</p>

<p>We want to say, &quot;Stop everything. Okay, I&#39;m ready now.&quot;But there is no&quot;now.&quot;If you think your business model is consistent, it may only be consistent for a nanosecond, and then it&#39;s inconsistent again.</p>

<p>A good example of this is a Java call to <code>LocalTime.now()</code>, which is simply not true. As soon as you call into it, it&#39;s no longer now. When you get a response back, it&#39;s no longer now.</p>

<h2 id="toc_8">Uncertainty of Distributed Systems</h2>

<p>This all brings us back to the fact that distributed systems are all about uncertainty. And distributed systems are here to stay. If you don&#39;t like it, change careers. Maybe open a restaurant. However, you&#39;ll still be dealing with distributed systems, you just won&#39;t be writing them.</p>

<p>Modeling uncertainty matters. It matters because multiple cores are here to stay. It matters because the cloud is here to stay.</p>

<p>Microservices matter, because it&#39;s the way everyone is going. Most people approach me about learning Domain-Driven Design because they see it as a great way to implement microservices. I agree with that. Also, people and companies want out of the monolith. They&#39;re stuck in the mud, and it takes months to get a release out.</p>

<p>Latency matters. When you have a network as part of your distributed system, latency matters.</p>

<p>IoT matters. Lots of little, cheap devices, all talking over the network.</p>

<h2 id="toc_9">DDD and Modeling</h2>

<p>What I refer to as Good Design, Bad Design, and Effective Design.</p>

<p>You can completely design software well, and yet miss what the business needs. Take SOLID, for example. You can design 100 classes to be SOLID, and completely miss what the business wants. Alan Kay, the inventor of OO and Smalltalk, said the really important thing about objects was the messages sent between them.</p>

<p>Ma, a Japanese word that means &quot;the space between.&quot; In this case, the space between the objects. The objects need to be designed well enough so they are able to play their single, responsible role. But it&#39;s not just about the insides of the objects. We need to care about the names of the objects and the messages that we send between them. That is where the expressiveness of the ubiquitous language comes in. If you are able to capture that in your model, that is what makes your model effective, not just good, because you will be meeting the needs of the business. That is what Domain-Driven Design is about.</p>

<p>We want to develop reactive systems. We&#39;re going to deal with the fact that uncertainty is a state. We&#39;re going to start solving some problems.</p>

<p>In his paper, <a href="https://queue.acm.org/detail.cfm?id=3025012">Life Beyond Distributed Transactions, an Apostate&#39;s Opinion</a>, Pat Helland wrote, &quot;In a system that cannot count on distributed transactions, the management of uncertainty must be implemented in the business logic.&quot; This realization came after a long history of implementing distributed transactions. However, while working at Amazon, he determined that the scale they needed could not be achieved by using distributed transactions.</p>

<p>In the paper, Helland talks about activities. An activity between two partner entities occurs when one entity has a state change and raises an event describing that change, and the second entity eventually, hopefully, it receives that message. But when will it receive the message? What if it is never received? There are a lot of &quot;what ifs&quot; and Pat Helland says these &quot;what if&quot; should be handled by activities.</p>

<p>Between any two partners, each partner has to manage the activity that it has seen from its partner to itself. Figure 4 is how I interpret what Pat Helland meant by these.</p>

<p><img src="media/15389214670417/15389216582705.jpg" alt=""/><br/>
<small><strong>Figure 4 - Activity</strong></small></p>

<p>Every partner has a <code>PartnerActivities</code> object that represents the activities of one related entity we receive activities from.  When we see some activity directed toward us, we can record that. Then, at some future time, I can ask, &quot;have I seen this partner activity?&quot; This handles both the case where I&#39;m dependent on that activity having occurred, or to check if I&#39;m seeing it again.</p>

<p>This is fairly straightforward, but gets more complicated in a long-lived entity. Even Pat Helland says this can grow enormously. Any given long-lived entity, potentially with a lot of partners, can result in collecting huge entities, or what DDD would call an Aggregate, just to try and track the activities. Furthermore, this isn&#39;t explicit -- it doesn&#39;t say a thing about the business itself.</p>

<p>I think we should go further than a <code>PartnerActivity</code>. The technique I&#39;m proposing is to put this in the heart of the software, in the domain model. We eliminate those elements in the infrastructure layer (the de-duplicator and re-sequencer) and we let everything through as it happens. I believe this makes the system less complex.</p>

<p>In an explicit model, we listen to events and send commands that correspond to business actions. Our domain aggregates each contain the business logic for how to handle those commands, including how to respond to the uncertainty. The communication of events and commands is handled by a Process Manager, as depicted in Figure 5. While the diagram may look complex, it&#39;s actually fairly straightforward to implement, and follows reactive patterns.</p>

<p><img src="media/15389214670417/15389216664581.jpg" alt=""/><br/>
<small><strong>Figure 5 - Process Manager</strong></small></p>

<p>Each domain entity is responsible for tracking its state, based on the commands it receives. By following good DDD practices, the state can be safely tracked based on these commands, and using event sourcing to persist the state change events.</p>

<p>Each entity is also responsible for knowing how to handle any potential uncertainty, according to decisions made by domain experts. For example, if a duplicate event is received, the aggregate will know that it has already seen it, and can decide how to respond. Figure 6 shows one way to handle this. When a <code>deniedForPricing()</code> command is received, we can check the current progress, and if we already have seen a <code>PricingDenied</code> event then we won&#39;t emit a new domain event, and instead respond indicating that the denial was already seen.</p>

<p><img src="media/15389214670417/15389216730497.jpg" alt=""/><br/>
<small><strong>Figure 6 - Handling Duplicate message</strong></small></p>

<p>This example is almost underwhelming, but that&#39;s intentional. It shows that treating uncertainty as part of our domain means we can use DDD practices to make it just another aspect of our business logic. And that really is the point of DDD; to help manage complexity, such as uncertainty, in the heart of our software.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PolicyServer]]></title>
    <link href="http://panlw.github.io/15389187307729.html"/>
    <updated>2018-10-07T21:25:30+08:00</updated>
    <id>http://panlw.github.io/15389187307729.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://solliance.net/products/policyserver">https://solliance.net/products/policyserver</a></p>
</blockquote>

<p>PolicyServer is an authorization solution for modern applications. It supports the necessary patterns to achieve the separation of authentication and authorization - including a management application, management APIs and a runtime engine that deals with advanced scenarios around policy design, policy hierarchy, and integration with identity.<br/>
<img src="media/15389187307729/15389187593403.jpg" alt="" style="width:500px;"/></p>

<h2 id="toc_0">IDENTITY != PERMISSIONS</h2>

<p><img src="media/15389187307729/15389187731668.jpg" alt="" style="width:358px;"/></p>

<p>Authorization is hard - and authorization is all too often conflated with authentication and identity. These concerns should be clearly separated. Managing &quot;Identity and Access Management&quot; using a single product or solution leads to problems as your software becomes more complex.</p>

<p>Identity has it&#39;s own complexities including integration workflows with different protocols and external identity providers. In addition, while proof of authentication from an identity system produces identity claims, possibly including role claims from that system - these &quot;identity roles&quot; are not typically meaningful to applications in a solution - and thus are not a good fit for authorization.</p>

<h2 id="toc_1">IDENTITY + PERMISSIONS == AUTHORIZATION</h2>

<p>Identity is universal. Permissions are application specific. This is why the identity system should not define permissions. Instead, identity should be one of the inputs to an authorization system. The combination of identity and an application-specific policy produces the actual permissions for the application.</p>

<p>While there are many ways to model authorization, the concept of roles and permissions are the most prevelant. Despite the simplicity of these concepts, modeling authorization is a design-intensive activity - not to be taken lightly - that requires tools to simplify modeling and the execution of application policy.</p>

<h2 id="toc_2">A FLEXIBLE POLICY MODELING SOLUTION</h2>

<p>PolicyServer supports simple role-based policies, granular permission-based policies and complex policy hierarchies.</p>

<ul>
<li><p>Role-Based Authorization<br/>
<img src="media/15389187307729/15389188554351.png" alt="" style="width:87px;"/></p></li>
<li><p>Permission-Based Authorization<br/>
<img src="media/15389187307729/15389188622382.png" alt="" style="width:87px;"/></p></li>
<li><p>Policy Hierarchies<br/>
<img src="media/15389187307729/15389188700368.png" alt="" style="width:87px;"/></p></li>
</ul>

<h2 id="toc_3">POLICYSERVER FEATURES</h2>

<h3 id="toc_4">POLICY MANAGEMENT</h3>

<p><img src="media/15389187307729/15389189087208.jpg" alt="" style="width:87px;"/></p>

<p>We provide you with the management tools to design your policies, from simple to complex. </p>

<p>These tools include:<br/>
* Management UI to help you define and manage policies<br/>
* Policy visualizer<br/>
* Integration with identity users and roles for transformation<br/>
* Policy evaluation tester<br/>
* Management APIs to support DevOps workflows</p>

<h3 id="toc_5">CLIENT LIBRARIES</h3>

<p><img src="media/15389187307729/15389189497826.jpg" alt="" style="width:87px;"/></p>

<p>Integrate authorization to your applications with ease using our client libraries. </p>

<p>With our tools you can:<br/>
* Spend more time on policy design, less on authorization tooling<br/>
* Integrate our policy engine with your application<br/>
* Use your technology platform&#39;s authorization mechanism to enforce policy<br/>
* Connect your identity system output to policy evaluation<br/>
* Migrate legacy applications to modern authorization</p>

<h3 id="toc_6">POLICY ENGINE</h3>

<p><img src="media/15389187307729/15389189694024.jpg" alt="" style="width:87px;"/></p>

<p>Our policy engine supports complex hierarchical policy evaluation. It is distributed as a lightweight and scalable API that you can host according to your own solution topology requirements. Integration to the policy engine is made simple through our client libraries.</p>

<h3 id="toc_7">AUDIT LOGS AND COMPLIANCE</h3>

<p><img src="media/15389187307729/15389189893698.jpg" alt="" style="width:87px;"/></p>

<p>Our management tools and policy runtime are both built to support detailed audit logs for reporting and compliance. Our built-in auditing features can support connectors to push audit logs to the desired target for your hosting and data topology.</p>

<h3 id="toc_8">SERVICES</h3>

<h4 id="toc_9">POLICY DESIGN</h4>

<p>PolicyServer provides you with the necessary tools to design and execute authorization policies - allowing you to focus on the actual authorization requirements for your solution and the policy design. Still, defining policies for any solution is a design-intensive task - and we can provide you with the support you need to get there marrying our team&#39;s experience in policy authorization to your team&#39;s domain experience.</p>

<h4 id="toc_10">IDENTITY PROVIDER INTEGRATION</h4>

<p>At Solliance, our security practice specializes in identity protocols, integrations with identity providers including IdentityServer, Azure AD, Auth0, Okta, Ping Federate, OneLogin and others. We can help you design your identity workflows, including API security, and link that identity story to your PolicyServer implementation as well.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[4 Docker Command-Line Tips for BeginnersTwitterLinkedInFacebookGoogle+Hacker NewsAddthis]]></title>
    <link href="http://panlw.github.io/15389184927612.html"/>
    <updated>2018-10-07T21:21:32+08:00</updated>
    <id>http://panlw.github.io/15389184927612.html</id>
    <content type="html"><![CDATA[
<pre><code>October 4, 2018 by: Justin Kulesza
</code></pre>

<blockquote>
<p>原文地址 <a href="https://spin.atomicobject.com/2018/10/04/docker-command-line/">https://spin.atomicobject.com/2018/10/04/docker-command-line/</a></p>
</blockquote>

<p>While I often utilize <a href="https://www.docker.com/">Docker</a> as part of a hosting and deployment system, I also use it locally for all sorts of exploration, spikes, and development. There are a couple of especially useful patterns and invocations that I show to people when I am introducing them to Docker and its usage.</p>

<p>The following tips are not comprehensive, but they do cover some of the most common operations that beginning users are likely to need when first working with containers. All of the following tips work with the standard Docker CLI from version <code>18.06.0-ce</code>. I’ve organized them by sub-commands to the main <code>docker</code> executable.</p>

<h2 id="toc_0">Docker Build</h2>

<p>Docker <code>build</code> creates Docker images from <a href="https://docs.docker.com/engine/reference/builder/">Dockerfiles</a>.</p>

<p>The anatomy of a Dockerfile is beyond the scope of this post, but looking at examples from official repositories on <a href="https://hub.docker.com/">Docker Hub</a> and the Docker documentation can be helpful.</p>

<p>You can easily build a new image from a Dockerfile and tag it with:</p>

<p><code>docker build PATH -f FILE -t NAME:TAG</code></p>

<pre><code class="language-bash">docker build . -f docker/Dockefile -t my-rails-app:latest
</code></pre>

<p>The <code>-f</code> specifies the path to the actual Dockerfile, whereas the <code>PATH</code> (<code>.</code> in the example) tells Docker what to use for its context or current directory when building the image. (This is important when considering commands specifying files and paths within the Dockerfile.)</p>

<p>The <code>-t</code> will tag the image built by Docker.</p>

<p>All Docker images have an image identifier (a generated, 12-character alphanumeric string). They may also be given a name and a tag. If only a name is provided, the default tag of “latest” is used. Image names and tags help tremendously to readily (and unambiguously) reference specific images.</p>

<h2 id="toc_1">Docker Run</h2>

<p>Docker <code>run</code> is for creating containers. It is important to differentiate it from <code>exec</code>, which is used to interact with containers that are already running.</p>

<p>Sometimes, it’s useful to just start a container to poke around, and then discard it afterwards. The following will start a new container, drop into a shell, and then destroy the container after you exit:</p>

<p><code>docker run -it --rm IMAGE COMMAND</code></p>

<pre><code class="language-bash">docker run -it --rm ruby:latest bash
</code></pre>

<p>The <code>-it</code> runs Docker interactively (so you get a pseudo-TTY with STDIN). The <code>--rm</code> causes Docker to automatically remove the container when it exits.</p>

<p>The image being used to create the container is generally specified as <code>&lt;name&gt;:&lt;tag&gt;</code> such as <code>ruby:latest</code>. If the specified image is not available locally, Docker will attempt to retrieve it from Docker Hub (or any connected Docker registry).</p>

<p>If you need to find Docker images available locally, you can run: <code>docker images</code> or <code>docker image ls</code>.</p>

<p>The following will start a container with specified environment variables and forwarding ports from your local computer into the Docker container:</p>

<p><code>docker run -p HOST_PORT:CONTAINER_PORT -e ENV_VAR=VALUE IMAGE COMMAND</code></p>

<pre><code class="language-bash">docker run -p 8080:3000 -e RACK_ENV=development my-rails-app:latest rails s
</code></pre>

<p>The <code>-p</code> defines the port mapping from the host to the container, and the <code>-e</code> defines key value pairs to set in the container’s environment when it starts up. Multiple parameters can be provided:</p>

<pre><code class="language-bash">docker run -it -—rm -p 8080:3000 -p 8081:3001 -e RACK_ENV=development -e HOSTNAME=my-container my-rails-app:latest rackup
</code></pre>

<h2 id="toc_2">Docker Exec</h2>

<p>As mentioned earlier, Docker <code>exec</code> only interacts with containers that are actively running.</p>

<p>If there is an existing container that was started headless (such as by <code>docker-compose</code>), you can easily drop into a shell to check on the state of things:</p>

<p><code>docker exec -it CONTAINER COMMAND</code></p>

<pre><code class="language-bash">docker exec -it 90cdc54358fa bash
</code></pre>

<p>As in the earlier example, the <code>-it</code> allows an interactive session. The container to run <code>exec</code> can either be specified by the container ID or the more friendly name that is generated or specified when the container is created.</p>

<p>If you do not know the name of the container, you can easily find it with: <code>docker ps</code> or <code>docker container ls</code>.</p>

<h2 id="toc_3">Docker Image/Container</h2>

<p>The main resources used for local containers are images, containers, and volumes. Containers are the basic unit of execution. Images are the basis for containers, and volumes are mounted by containers.</p>

<p>Docker has sub-commands which allow you to examine and interact with these resources. While there are many dozens of sub-commands (and sub-sub commands), I find that these are the ones I use most frequently:</p>

<p><code>docker container</code> allows you to manage containers, for example:</p>

<ul>
<li>  <code>docker container ls</code> to list all running containers</li>
<li>  <code>docker container kill CONTAINER</code> to forcefully stop a running container</li>
<li>  <code>docker container rm CONTAINER</code> to remove a container</li>
<li>  <code>docker inspect CONTAINER</code> to view detailed information about a running container</li>
</ul>

<p><code>docker images</code> allows you to manage images, for example:</p>

<ul>
<li>  <code>docker image ls</code> to list all locally available images</li>
<li>  <code>docker image rm IMAGE</code> to remove an image</li>
<li>  <code>docker image prune</code> to remove dangling images (those not attached to a container, or a dependency for an image that is)</li>
<li>  <code>docker image prune -a</code> to remove all images</li>
<li>  <code>docker image tag IMAGE IMAGE</code> to tag an image from one image id or tag, to a new tag. (e.g. <code>my-app:latest</code> to <code>my-app:1.0.0</code>)</li>
</ul>

<p>I hope this post will encourage you to experiment with Docker.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Authentication and Authorization: OpenID vs OAuth2 vs SAML]]></title>
    <link href="http://panlw.github.io/15389167880127.html"/>
    <updated>2018-10-07T20:53:08+08:00</updated>
    <id>http://panlw.github.io/15389167880127.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://spin.atomicobject.com/2016/05/30/openid-oauth-saml/">https://spin.atomicobject.com/2016/05/30/openid-oauth-saml/</a></p>
</blockquote>

<p>My current project at AO has provided a lot of opportunity to learn about web security and what’s going on when you click that ubiquitous “Sign in with Google/Facebook” button. As both a computer developer and an end user, I want applications that are secure without being too difficult to use.</p>

<p>Looking for an option to fit both our application and our customer’s security policies, we investigated <a href="http://openid.net">OpenID</a>, <a href="http://oauth.net/2">OAuth2</a>, and <a href="https://wiki.oasis-open.org/security/FrontPage">SAML</a>.</p>

<h2 id="toc_0">Authorization &amp; Authentication Basics</h2>

<p>Our project, a single-page application, will be a public-facing website. We want to restrict access to registered users only. Furthermore, we want to tailor each user’s experience, and the amount and type of data that they can view, to their individual roles and access levels.</p>

<p>In other words, we want to be able to <u>authenticate</u> and <u>authorize</u> each user. Authentication means verifying that someone is indeed who they claim to be. Authorization means deciding which resources a certain user should be able to access, and what they should be allowed to do with those resources. Oftentimes, as in our case, an application will require a little bit of both.</p>

<p>With sites like Facebook or Google, a user can log in to one application with a set of credentials. This same set of credentials can then be used to log in to related websites or applications (like websites that ask you, “Sign up with Facebook or Google account?”).</p>

<p>Likewise, a business may have an internal-facing employee portal with links to intranet sites regarding timesheets, health insurance, or company news. Rather than requiring an employee to log in at each website, a better solution would be to have the employee log in at a portal, and have that portal automatically authenticate the user with the other intranet sites. This idea, called single sign-on (SSO), allows a user to enter one username and password in order to access multiple applications.</p>

<p>The benefits are pretty nice for the user. The use of linked identities means they have to manage only one username and password for the related websites. The user experience is better for them, as they can avoid multiple logins. A user’s (single set of) credentials will be stored in one database, rather than multiple credentials stored across multiple databases (with, let’s be honest, likely repeated passwords). This also means that developers of the various applications don’t have to store passwords. Instead, they can accept proof of identity or authorization from a trusted source.</p>

<p>There are multiple solutions for implementing SSO. The three most common web security protocols (at the time of this writing) are OpenID, OAuth, and SAML. Implementations and libraries exist in multiple languages already, and going with a standardized protocol allows better interoperability than a custom solution.</p>

<h2 id="toc_1">OpenID</h2>

<p>OpenID is an open standard for authentication, promoted by the non-profit OpenID Foundation. As of March 2016, there are over a billion OpenID-enabled accounts on the internet, and organizations such as Google, WordPress, Yahoo, and PayPal use OpenId to authenticate users.</p>

<p>A <u>user</u> must obtain an OpenID account through an OpenID <u>identity provider</u> (for example, Google). The user will then use that account to sign into any website (the <u>relying party</u>) that accepts OpenID authentication (think YouTube or another site that accepts a Google account as a login). The OpenID standard provides a framework for the communication that must take place between the identity provider and the relying party.</p>

<p>This exchange can be compared to a border crossing.  Imagine that Alice is a Canadian citizen who wants to visit the US. At the border, the US asks for proof of identity (her passport). Because the US government trusts the Canadian government to accurately provide identification for its citizens, the US accepts Alice’s passport as reliable proof of her identity, and thus, lets her enter the US. In this example, Alice is the end user, the US is the relying party, and Canada is the identity provider.</p>

<p>This exchange works because Alice can provide proof of identity to the US that originates from an entity that the US trusts. Similarly, the relying party (or website that a user is trying to log in to) must trust the OpenID identity provider that will verify the user’s identity.</p>

<p>On a website, the exchange looks like this:<br/>
<img src="media/15389167880127/15389172254087.png" alt="" style="width:295px;"/></p>

<p>Let’s return to Alice, who wants to log in to her MyBlogger account (the relying party). She navigates to the login screen, where she is offered a “Sign in with Google” option. She clicks that, and MyBlogger initiates association with Google and requests and receives an association handle. MyBlogger then forwards Alice to the Google login page. She enters her credentials, and Google validates them. She is then redirected back to MyBlogger, along with a token stating that Google believes she is who she claims to be (Alice). MyBlogger trusts this token and creates a session for her.</p>

<p><u>Notes:</u></p>

<ol>
<li> <u>OpenID is technically a URL that a user owns (e.g. alice2016.openid.com), so some websites offer the option to manually enter an OpenID.</u></li>
<li> <u>The latest version of OpenID is OpenID Connect, which combines OpenID authentication and OAuth2 authorization</u></li>
<li> <img src="media/15389167880127/15389172519177.png" alt=""/>
<u>Facebook previously used OpenID but has since moved to Facebook Connect.</u></li>
</ol>

<h2 id="toc_2">OAuth2</h2>

<p><img src="media/15389167880127/15389197462631.png" alt=""/></p>

<blockquote>
<p><a href="https://www.mutuallyhuman.com/blog/2013/05/09/choosing-an-sso-strategy-saml-vs-oauth2/">https://www.mutuallyhuman.com/blog/2013/05/09/choosing-an-sso-strategy-saml-vs-oauth2/</a></p>
</blockquote>

<p>By contrast, OAuth2 is an open standard for authorization. Confusingly, OAuth2 is also the basis for OpenID Connect, which provides OpenID (authentication) on top of OAuth2 (authorization) for a more complete security solution. OpenID Connect (OIDC) was created in early 2014. This primer will instead focus on OAuth2 by itself, not as a part of OIDC.</p>

<p>OAuth2 provides secure delegated access, meaning that an application, called a <u>client</u>, can take actions or access resources on a resource server on the behalf of a <u>user</u>, without the user sharing their credentials with the application. OAuth2 does this by allowing tokens to be issued by an identity provider to these third-party applications, with the approval of the user. The client then uses the token to access the resource server on behalf of the user.</p>

<p>Yet Twitter’s <a href="https://dev.twitter.com/oauth/overview/faq">OAuth guide</a> says that OAuth2 is an authentication standard. So what gives? As it turns out, authorization can be used as a form of pseudo-authentication.</p>

<p>An authorization use case of OAuth2 might be as follows: Alice is leaving town and she wants her friend Bob to house-sit. Alice gives Bob the house key, and he now has access to enter the house. The key gives him authorization to enter the house, as authorization relates to which resources a user should have access to, and what they can do with those resources. In this metaphor, the homeowner is the user, Bob is the client, the door lock is the identity provider, and the house is the resource server.</p>

<p>This can be twisted into a pseudo-authentication use case by assuming that the person who has the house key is the homeowner. However, as we can see with Bob house-sitting for Alice, this is not always the case.</p>

<p>Online, an OAuth2 use case might look like this:  Alice signs up for a new account at NewApp and is offered the option to see which of her friends already use NewApp so she can connect with them. There’s a button labeled “import contacts from Facebook.” Alice clicks that button, and she is redirected to Facebook to log in. Alice successfully logs in and is asked if she wants to share her Facebook friend list with NewApp. She clicks yes, and is forwarded back to NewApp along with a token. NewApp now has permission (with the token) to access Alice’s friend list, without her sharing her credentials directly with NewApp. This eliminates the risk of NewApp logging into Facebook on Alice’s behalf and doing things she wouldn’t want (posting status updates, changing her password, etc.).</p>

<h2 id="toc_3">SAML</h2>

<p><img src="media/15389167880127/15389197704524.png" alt=""/></p>

<blockquote>
<p><a href="https://www.mutuallyhuman.com/blog/2013/05/09/choosing-an-sso-strategy-saml-vs-oauth2/">https://www.mutuallyhuman.com/blog/2013/05/09/choosing-an-sso-strategy-saml-vs-oauth2/</a></p>
</blockquote>

<p>SAML is the oldest standard of the three, originally developed in 2001, with its most recent major update in 2005. SAML, pronounced “sam-el,” stands for Security Assertion Markup Language. It’s an open standard that provides both authentication and authorization.</p>

<p>Similar to the terminology of the other two standards, SAML defines a <u>principal</u>, which is the end user trying to access a resource. There is a <u>service provider</u>, which is the web server that the principal is trying to access. And there is an <u>identity provider</u>, which is the server that holds the principal’s identities and credentials.</p>

<p>The US/Canada metaphor can be used here, as well. Alice wishes to enter the US from Canada. The US, wishing to verify her identity or other information about her–perhaps whether she has a valid driver’s license that will allow her to drive in the US)–makes a request to Canada for authentication and/or authorization information regarding Alice. Canada responds by sending the requested information to the requested address, along with some proof that Canada was indeed the sender of the message. All metaphors break down eventually, but this proof might take the form of a passport, as before, or official government documents or visas (where authorization requests are involved). And, as before, the system is predicated on US trust that Canada is issuing driver’s licenses, visas, etc. properly.</p>

<p>In our example, Alice is the principal, the US is the service provider, and Canada is once again the identity provider. The request made to Canada by the US is analogous to an XML message that states what information is being requested, who is asking, and to whom the response should be returned. Canada’s response would be called an <u>assertion</u>, in SAML terms (similar to a token for OpenID or OAuth2). This assertion can contain statements about authentication, authorization, and/or attributes (specific information about a user, such as email or phone number).</p>

<p>The SAML 2.0 specification defines <u>assertions</u> (as discussed above); <u>protocols</u>, which are assertion requests and responses; <u>bindings</u>, or how these requests and responses happen between the service provider and identity provider, using standard communication methods (e.g. HTTP POST); and <u>profiles</u>, which are combinations of assertions, protocols and bindings for various use cases, like SSO.</p>

<p>An SSO use case might look like this: Alice is a manager at Acme Corp. She accesses Acme Corp’s intranet portal, where she logs in with her credentials. After logging in, she can click on a number of links that may be of interest to her (payroll, company news, Salesforce, etc.). She clicks on the Salesforce link, which contains a SAML assertion about Alice. She is forwarded to Salesforce, which receives the SAML assertion. Salesforce trusts Acme Corp, and thus trusts the assertion. Using information in the token, Alice is automatically logged in, and the appropriate data is shown to her based on attributes in the assertion.</p>

<h2 id="toc_4">Summary</h2>

<p>These three options are summarized in the table.  Our application implements <a href="https://identityserver.github.io/Documentation/">IdentityServer</a>, a .NET framework that implements both OAuth2 and OpenID Connect.</p>

<table>
<thead>
<tr>
<th>Features</th>
<th><strong>OAuth2</strong></th>
<th><strong>OpenId</strong></th>
<th><strong>SAML</strong></th>
</tr>
</thead>

<tbody>
<tr>
<td><strong>Token (or assertion) format</strong></td>
<td>JSON or SAML2</td>
<td>JSON</td>
<td>XML</td>
</tr>
<tr>
<td><strong>Authorization?</strong></td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td><strong>Authentication?</strong></td>
<td>Pseudo-authentication</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td><strong>Year created</strong></td>
<td>2005</td>
<td>2006</td>
<td>2001</td>
</tr>
<tr>
<td><strong>Current version</strong></td>
<td>OAuth2</td>
<td>OpenID Connect</td>
<td>SAML 2.0</td>
</tr>
<tr>
<td><strong>Transport</strong></td>
<td>HTTP</td>
<td>HTTP GET and HTTP POST</td>
<td>HTTP Redirect (GET) binding, SAML SOAP binding, HTTP POST binding, and others</td>
</tr>
<tr>
<td><strong>Security Risks</strong></td>
<td>Phishing OAuth 2.0 does not support signature, encryption, channel binding, or client verification.  Instead, it relies completely on TLS for confidentiality.</td>
<td>Phishing Identity providers have a log of OpenID logins, making a compromised account a bigger privacy breach</td>
<td><a href="https://www.usenix.org/system/files/conference/usenixsecurity12/sec12-final91.pdf">XML Signature Wrapping</a> to impersonate any user</td>
</tr>
<tr>
<td><strong>Best suited for</strong></td>
<td>API authorization</td>
<td>Single sign-on for consumer apps</td>
<td>Single sign-on for enterprise Note:  not well suited for mobile</td>
</tr>
</tbody>
</table>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[安全领域的重要概念]]></title>
    <link href="http://panlw.github.io/15389112375926.html"/>
    <updated>2018-10-07T19:20:37+08:00</updated>
    <id>http://panlw.github.io/15389112375926.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0"><a href="https://zh.wikipedia.org/wiki/%E5%AE%89%E5%85%A8%E6%96%AD%E8%A8%80%E6%A0%87%E8%AE%B0%E8%AF%AD%E8%A8%80">安全断言标记语言(Security Assertion Markup Language，简称SAML)</a></h2>

<h3 id="toc_1">原则</h3>

<p><img src="media/15389112375926/15389191925301.png" alt=""/></p>

<ol>
<li><p>HTTP Request to Service Provider<br/>
In step 1, the principal, via an HTTP User Agent, makes an HTTP request for a secured resource at the service provider without a security context.</p></li>
<li><p>Service Provider Determines Identity Provider<br/>
In step 2, the service provider obtains the location of an endpoint at an identity provider for the authentication request protocol that supports its preferred binding. The means by which this is accomplished is implementation-dependent. The service provider MAY use the SAML identity provider discovery profile described in Section 4.3.</p></li>
<li><p><AuthnRequest> issued by Service Provider to Identity Provider<br/>
In step 3, the service provider issues an <AuthnRequest> message to be delivered by the user<br/>
agent to the identity provider. Either the HTTP Redirect, HTTP POST, or HTTP Artifact binding<br/>
can be used to transfer the message to the identity provider through the user agent.</p></li>
<li><p>Identity Provider identifies Principal<br/>
In step 4, the principal is identified by the identity provider by some means outside the scope of this profile. This may require a new act of authentication, or it may reuse an existing authenticated session.</p></li>
<li><p>Identity Provider issues <Response> to Service Provider<br/>
In step 5, the identity provider issues a <Response> message to be delivered by the user agent to the service provider. Either the HTTP POST, or HTTP Artifact binding can be used to transfer the message to the service provider through the user agent. The message may indicate an error, or will include (at least) an authentication assertion. The HTTP Redirect binding MUST NOT be used, as the response will typically exceed the URL length permitted by most user agents.</p></li>
<li><p>Service Provider grants or denies access to Principal<br/>
In step 6, having received the response from the identity provider, the service provider can<br/>
respond to the principal&#39;s user agent with its own error, or can establish its own security context for the principal and return the requested resource.</p>

<p>Note that an identity provider can initiate this profile at step 5 and issue a <Response> message to a service provider without the preceding steps.</p></li>
</ol>

<p>SAML规范定义了三个角色：委托人（通常为一名用户）、身份提供者（IdP），服务提供者（SP）。在用SAML解决的使用案例中，委托人从服务提供者那里请求一项服务。服务提供者请求身份提供者并从那里并获得一个身份断言。服务提供者可以基于这一断言进行访问控制的判断——即决定委托人是否有权执行某些服务。</p>

<p>在将身份断言发送给服务提供者之前，身份提供者也可能向委托人要求一些信息——例如用户名和密码，以验证委托人的身份。SAML规范了三方之间的断言，尤其是断言身份消息是由身份提供者传递给服务提供者。在SAML中，一个身份提供者可能提供SAML断言给许多服务提供者。同样的，一个服务提供者可以依赖并信任许多独立的身份提供者的断言。</p>

<h3 id="toc_2">设计</h3>

<p>SAML 创建在一些现有标准之上：</p>

<ul>
<li><p>Extensible Markup Language (XML)<br/>
大多数SAML交换是以一个标准化的XML方言表示，这也是SAML的名称（Security Assertion Markup Language）的根源。; XML Schema (XSD): SAML断言和协议部分采用XML Schema。</p></li>
<li><p>XML Signature<br/>
SAML 1.1和SAML 2.0都为身份验证和消息完整性使用基于XML Signature标准的数字签名。</p></li>
<li><p>XML Encryption<br/>
SAML 2.0使用XML Encryption为加密名称标识符、加密属性和加密断言提供元素（SAML 1.1没有加密功能）。但XML加密据报有着严重的安全问题。[8][9]</p></li>
<li><p>Hypertext Transfer Protocol (HTTP)<br/>
SAML很大程度上依赖超文本传输协议作为其通信协议。</p></li>
<li><p>SOAP<br/>
SAML指定使用SOAP，尤其是SOAP 1.1。</p></li>
</ul>

<p>SAML定义了基于XML的断言、协议、绑定和配置。术语SAML核心（SAML Core）指SAML断言的一般语法和语义，以及用于请求和在系统实体间传输这些断言的协议。SAML协议指谁来传输，而不是如何传输（后者由所选择的绑定决定）。因此SAML核心只定义了“纯粹的”SAML断言，以及SAML的请求和响应元素。</p>

<p>SAML绑定决定SAML请求和响应如何映射到标准的消息或通信协议。一个重要的、同步的绑定是SAML SOAP绑定。</p>

<p>SAML配置是使用特定断言、协议和绑定组成的适用于所定义使用情况的一个具体表现形式。</p>

<h3 id="toc_3">断言</h3>

<p>一个SAML断言包含一个安全信息包：</p>

<pre><code class="language-xml">&lt;saml:Assertion ...&gt;
   ..
&lt;/saml:Assertion&gt;
</code></pre>

<p>简而言之，依赖方按下述方式解释一个断言：</p>

<pre><code>断言A是在条件C 有效的前提下由发布者R 关于主题S在时间t发布的 。
</code></pre>

<p>SAML断言通常从IdP传送到SP。断言包括一些SP用来做权限控制的声明（statements）。SAML提供如下三种语句：</p>

<ol>
<li>身份验证（Authentication）声明</li>
<li>属性（Attribute）声明</li>
<li>授权决策（Authorization decision）声明</li>
</ol>

<p>身份验证声明会向SP断言，该委托人确实被IdP在一个特定的时间通过一个特定的认证方式成功认证。关于该被认证的委托人的其他信息（也叫身份验证上下文）也可能会包含在一条身份验证声明中。</p>

<p>属性声明会断言，一个主题和一些属性关联。一个属性是一个简单的键值对。依赖方使用属性来实现访问控制。</p>

<p>授权决策声明会断言，在证据E下一个主题是否被允许在资源R上执行动作A。SAML中的授权决策声明的功能被有意的限制了，因为在更高级的使用场景中更推荐使用XACML。</p>

<h3 id="toc_4">协议</h3>

<p><img src="media/15389112375926/15389114752946.jpg" alt=""/><br/>
SAML协议响应</p>

<p>A SAML protocol describes how certain SAML elements (including assertions) are packaged within SAML request and response elements, and gives the processing rules that SAML entities must follow when producing or consuming these elements. For the most part, a SAML protocol is a simple request-response protocol.</p>

<p>The most important type of SAML protocol request is called a query. A service provider makes a query directly to an identity provider over a secure back channel. Thus query messages are typically bound to SOAP.</p>

<p>Corresponding to the three types of statements, there are three types of SAML queries:</p>

<ol>
<li>Authentication query</li>
<li>Attribute query</li>
<li>Authorization decision query</li>
</ol>

<p>Of these, the attribute query is perhaps most important (and still the object of much research[来源请求]). The result of an attribute query is a SAML response containing an assertion, which itself contains an attribute statement. See the SAML 2.0 topic for an example of attribute query/response.</p>

<h3 id="toc_5">绑定</h3>

<p>A SAML binding is a mapping of a SAML protocol message onto standard messaging formats and/or communications protocols. For example, the SAML SOAP binding specifies how a SAML message is encapsulated in a SOAP envelope, which itself is bound to an HTTP message.</p>

<p>SAML 1.1 specifies just one binding, the SAML SOAP Binding. In addition to SOAP, implicit in SAML 1.1 Web Browser SSO are the precursors of the HTTP POST Binding, the HTTP Redirect Binding, and the HTTP Artifact Binding. These are not defined explicitly, however, and are only used in conjunction with SAML 1.1 Web Browser SSO. The notion of binding is not fully developed until SAML 2.0.</p>

<p>SAML 2.0 completely separates the binding concept from the underlying profile. In fact, there is a brand new binding specification in SAML 2.0 that defines the following standalone bindings:</p>

<ol>
<li>SAML SOAP Binding (based on SOAP 1.1)</li>
<li>Reverse SOAP (PAOS) Binding</li>
<li>HTTP Redirect (GET) Binding</li>
<li>HTTP POST Binding</li>
<li>HTTP Artifact Binding</li>
<li>SAML URI Binding</li>
</ol>

<p><img src="media/15389112375926/15389114495009.jpg" alt=""/><br/>
SAML over SOAP over HTTP</p>

<h3 id="toc_6">配置</h3>

<p>A SAML profile describes in detail how SAML assertions, protocols, and bindings combine to support a defined use case. The most important SAML profile is the Web Browser SSO Profile.</p>

<p>The Web Browser SSO Profile was completely refactored for SAML 2.0. Conceptually, SAML 1.1 Browser/Artifact and Browser/POST are special cases of SAML 2.0 Web Browser SSO. The latter is considerably more flexible than its SAML 1.1 counterpart due to the new &quot;plug-and-play&quot; binding design of SAML 2.0. Unlike previous versions, SAML 2.0 browser flows begin with a request at the service provider. This provides greater flexibility, but SP-initiated flows naturally give rise to the so-called Identity Provider Discovery problem, the focus of much research today. In addition to Web Browser SSO, SAML 2.0 introduces numerous new profiles:</p>

<ul>
<li>SSO Profiles

<ul>
<li>Web Browser SSO Profile</li>
<li>Enhanced Client or Proxy (ECP) Profile</li>
<li>Identity Provider Discovery Profile</li>
<li>Single Logout Profile</li>
<li>Name Identifier Management Profile</li>
</ul></li>
<li>Artifact Resolution Profile</li>
<li>Assertion Query/Request Profile</li>
<li>Name Identifier Mapping Profile</li>
<li>SAML Attribute Profiles</li>
</ul>

<h3 id="toc_7">安全</h3>

<p>SAML规范推荐、并在某些情况下要求各种安全机制：</p>

<ul>
<li>TLS 1.0+用于传输层安全</li>
<li>XML Signature和XML Encryption用于消息层安全</li>
</ul>

<h3 id="toc_8">使用</h3>

<p>SAML的主要用途是“网页浏览器单点登录（SSO）。</p>

<p>A user wielding a user agent (usually a web browser) requests a web resource protected by a SAML service provider. The service provider, wishing to know the identity of the requesting user, issues an authentication request to a SAML identity provider through the user agent. The resulting protocol flow is depicted in the following diagram.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[授权策略（Authorization Policy）基本元素]]></title>
    <link href="http://panlw.github.io/15388979027531.html"/>
    <updated>2018-10-07T15:38:22+08:00</updated>
    <id>http://panlw.github.io/15388979027531.html</id>
    <content type="html"><![CDATA[
<pre><code>阿里云 &gt; 访问控制 RAM &gt; 授权策略管理
</code></pre>

<blockquote>
<p>原文地址 <a href="https://help.aliyun.com/document_detail/28663.html">https://help.aliyun.com/document_detail/28663.html</a></p>
</blockquote>

<p>RAM 中使用授权策略（Policy）来描述授权的具体内容，授权内容包含以下基本因素：效力（Effect）、资源（Resource）、对资源所授予的操作权限（Action）以及限制条件（Condition）。</p>

<h2 id="toc_0">效力（Effect）</h2>

<p>授权效力包括两种：允许（Allow）和拒绝（Deny）。</p>

<h2 id="toc_1">资源（Resource）</h2>

<p>资源是指被授权的具体对象。</p>

<p>比如，访问策略 “允许张三对资源 SampleBucket 执行 GetBucket 操作” 中的资源是“SampleBucket”。</p>

<h2 id="toc_2">操作权限（Action）</h2>

<p>操作方法是指对具体资源的操作。</p>

<p>比如，访问策略 “允许张三对资源 SampleBucket 执行 GetBucket 操作” 中的操作是“GetBucket”。</p>

<h2 id="toc_3">限制条件（Condition）</h2>

<p>限制条件是指授权生效的限制条件。</p>

<p>比如，访问策略 “允许张三在 2011 年 12 月 31 日之前对资源 SampleBucket 执行 GetBucket 操作” 中的限制条件是“在 2011 年 12 月 31 日之前”。</p>

<h2 id="toc_4">授权策略样例</h2>

<p>下面是一个权限策略实例，它描述的含义：允许对 OSS 的 samplebucket 进行只读操作，条件是请求者的 IP 来源为 <code>42.160.1.0</code>。</p>

<pre><code class="language-json">{
      &quot;Version&quot;: &quot;1&quot;,
      &quot;Statement&quot;:
        [{
          &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: [&quot;oss:List*&quot;, &quot;oss:Get*&quot;],
            &quot;Resource&quot;: [&quot;acs:oss:*:*:samplebucket&quot;, &quot;acs:oss:*:*:samplebucket/*&quot;],
            &quot;Condition&quot;:
             {
                &quot;IpAddress&quot;:
                 {
                    &quot;acs:SourceIp&quot;: &quot;42.160.1.0&quot;
                  }
              }
         }]
}
</code></pre>

<h2 id="toc_5">Policy 结构</h2>

<p>授权策略（Policy）结构包括 Policy 版本号及授权语句（Statement）列表。每个授权语句又包括以下元素：Effect（授权类型）、Action（操作名称列表）、Resource（操作对象列表）以及 Condition（条件限制），其中 Condition 是可选项。</p>

<p>Policy 结构简述如下：</p>

<p>图 1. Policy 结构<br/>
<img src="media/15388979027531/15389054067698.png" alt=""/></p>

<h2 id="toc_6">格式检查（JSON）</h2>

<p>RAM 仅支持 JSON 格式的描述。当创建或更新 Policy 时，RAM 会首先检查 JSON 格式的正确性。</p>

<ul>
<li>  关于 JSON 的语法标准请参考 <a href="http://tools.ietf.org/html/rfc7159">RFC 7159</a>。</li>
<li>  用户也可以使用一些在线的 JSON 格式验证器和编辑器来校验 JSON 文本的有效性。</li>
</ul>

<hr/>

<h2 id="toc_7">Policy 语法</h2>

<p>了解 Policy 中用到的字符及规则，以及 Policy 语法描述。</p>

<p><strong>字符及规则</strong></p>

<p>Policy 中所包含的 JSON 字符有：<code>{ } [ ] &quot; , :</code>；描述语法使用的特殊字符有：<code>= &lt; &gt; ( ) |</code>。</p>

<p>字符使用说明如下：</p>

<ul>
<li>  当一个元素允许多值时，使用逗号和省略号来表达，比如：<code>[ &lt;action_string&gt;, &lt;action_string&gt;, ...]</code>。在所有支持多值的语法中，使用单值也是有效的。而且两种表达方式是等效的：<code>&quot;Action&quot;: [&lt;action_string&gt;]</code> 和 <code>&quot;Action&quot;: &lt;action_string&gt;</code></li>
<li>  带有问号的元素表示这是一个可选元素，比如：<code>&lt;condition_block?&gt;</code></li>
<li>  多值之间用竖线 （|）隔开，表示取值只能选取这些值中的某一个。比如：<code>(&quot;Allow&quot; | &quot;Deny&quot;)</code></li>
<li>  使用双引号引起了的元素，表示它是文本串。比如： <code>&lt;version_block&gt; = &quot;Version&quot; : (&quot;1&quot;)</code></li>
</ul>

<p><strong>语法描述及说明</strong></p>

<p>Policy 语法描述如下：</p>

<pre><code>policy  = {
     &lt;version_block&gt;,
     &lt;statement_block&gt;
}
&lt;version_block&gt; = &quot;Version&quot; : (&quot;1&quot;)
&lt;statement_block&gt; = &quot;Statement&quot; : [ &lt;statement&gt;, &lt;statement&gt;, ... ]
&lt;statement&gt; = { 
    &lt;effect_block&gt;,
    &lt;action_block&gt;,
    &lt;resource_block&gt;,
    &lt;condition_block?&gt;
}
&lt;effect_block&gt; = &quot;Effect&quot; : (&quot;Allow&quot; | &quot;Deny&quot;)  
&lt;action_block&gt; = (&quot;Action&quot; | &quot;NotAction&quot;) : 
    (&quot;*&quot; | [&lt;action_string&gt;, &lt;action_string&gt;, ...])
&lt;resource_block&gt; = (&quot;Resource&quot; | &quot;NotResource&quot;) : 
    (&quot;*&quot; | [&lt;resource_string&gt;, &lt;resource_string&gt;, ...])
&lt;condition_block&gt; = &quot;Condition&quot; : &lt;condition_map&gt;
&lt;condition_map&gt; = {
  &lt;condition_type_string&gt; : { 
      &lt;condition_key_string&gt; : &lt;condition_value_list&gt;,
      &lt;condition_key_string&gt; : &lt;condition_value_list&gt;,
      ...
  },
  &lt;condition_type_string&gt; : {
      &lt;condition_key_string&gt; : &lt;condition_value_list&gt;,
      &lt;condition_key_string&gt; : &lt;condition_value_list&gt;,
      ...
  }, ...
}  
&lt;condition_value_list&gt; = [&lt;condition_value&gt;, &lt;condition_value&gt;, ...]
&lt;condition_value&gt; = (&quot;String&quot; | &quot;Number&quot; | &quot;Boolean&quot;)
</code></pre>

<p>语法说明如下：</p>

<ul>
<li>  <strong>版本</strong>：当前支持的 Policy 版本为 1。</li>
<li>  <strong>授权语句</strong>：一个 Policy 可以有多条授权语句。

<ul>
<li>  每条授权语句要么是 Deny，要么是 Allow。一条授权语句中，Action 是一个支持多个操作的列表，Resource 也是一个支持多个对象的列表。</li>
<li>  每条授权语句都支持独立的限制条件（Condition）。一个条件块可以支持多种条件操作类型，以及对这多种条件的逻辑组合。</li>
</ul></li>
<li>  <strong>Deny 优先</strong>： 一个用户可以被授予多个 Policy，当这些 Policy 存在多条授权语句既包含有 Allow 又包含有 Deny 时，遵循 Deny 优先（只认 Deny 不认 Allow）原则。</li>
<li><p><strong>元素取值</strong>：</p>

<ul>
<li>  当取值为数字（Number）或布尔值（Boolean）时，与字符串类似，需要用双引号引起。</li>
<li><p>当元素取值为字符串值（String）时，支持（*）和（？）模糊匹配。</p>

<ul>
<li>  (*) 代表 0 个或多个任意的英文字母。</li>
<li>  (?) 代表 1 个任意的英文字母。</li>
</ul>

<p>比如，<code>ecs:Describe*</code> 可以表示 ecs 的所有以 Describe 开头的 API 操作名称。</p></li>
</ul></li>
</ul>

<h2 id="toc_8">Policy 元素使用</h2>

<p>了解 Policy 语法中各元素的使用规则。</p>

<p><strong>Effect（授权类型）</strong></p>

<p>Effect 取值 为 Allow 或 Deny。比如，<code>&quot;Effect&quot;: &quot;Allow&quot;</code></p>

<p><strong>Action（操作名称列表）</strong></p>

<p>Action 支持多值，取值为云服务所定义的 API 操作名称，其格式定义如下：</p>

<pre><code>&lt;service-name&gt;:&lt;action-name&gt;
</code></pre>

<p><strong>格式说明</strong>：</p>

<ul>
<li>  service-name: 阿里云产品名称，如 ecs, rds, slb, oss, ots 等。</li>
<li>  action-name: service 相关的 api 操作接口名称。</li>
</ul>

<p><strong>描述样例</strong>：</p>

<pre><code>&quot;Action&quot;: [&quot;oss:ListBuckets&quot;, &quot;ecs:Describe*&quot;, &quot;rds:Describe*&quot;]
</code></pre>

<h2 id="toc_9">Resource（操作对象列表）</h2>

<p>Resource 通常指操作对象，比如 ECS 虚拟机实例，OSS 存储对象。我们使用如下格式来命名阿里云服务的资源命名。</p>

<pre><code>acs:&lt;service-name&gt;:&lt;region&gt;:&lt;account-id&gt;:&lt;relative-id&gt;
</code></pre>

<p><strong>格式说明</strong>：</p>

<ul>
<li>  <code>acs</code>: Aliyun Cloud Service 的首字母缩写，表示阿里云的公有云平台。</li>
<li>  <code>service-name</code>: 阿里云提供的 Open Service 的名字，如 ecs, oss, ots 等。</li>
<li>  <code>region</code>: 地区信息。如果不支持该项，可以使用通配符 “*” 号来代替。</li>
<li>  <code>account-id</code>: 账号 ID，比如 <code>1234567890123456</code>，也可以用 “*” 代替。</li>
<li>  <code>relative-id</code>: 与 service 相关的资源描述部分，其语义由具体 service 指定。这部分的格式描述支持类似于一个文件路径的树状结构。以 oss 为例，<code>relative-id = “mybucket/dir1/object1.jpg”</code> 表示一个 OSS 对象。</li>
</ul>

<p><strong>描述样例</strong>：</p>

<pre><code>&quot;Resource&quot;: [&quot;acs:ecs:*:*:instance/inst-001&quot;, &quot;acs:ecs:*:*:instance/inst-002&quot;, &quot;acs:oss:*:*:mybucket&quot;, &quot;acs:oss:*:*:mybucket/*&quot;]
</code></pre>

<p><strong>Condition（条件限制）</strong></p>

<p>条件块（Condition Block）由一个或多个条件子句构成。一个条件子句由条件操作类型、条件关键字和条件值组成。条件操作类型和条件关键字在下文中会有详细描述。</p>

<p><strong>条件块判断逻辑</strong></p>

<p>是否满足条件的判断原则如下图所示：</p>

<p>图 2. 是否满足条件的判断原则<br/>
<img src="media/15388979027531/15389054664258.png" alt=""/></p>

<p>具体规则如下：</p>

<ul>
<li>  一个条件关键字可以指定一个或多个值，在条件检查时，如果条件关键字的值与指定值中的某一个相等，即可判定条件满足。</li>
<li>  同一种条件操作类型的条件子句下的多个条件关键字同时满足的情况下，才能判定该条件子句满足。</li>
<li>  条件块下的所有条件子句同时满足的情况下，才能判定该条件块满足。</li>
</ul>

<p><strong>条件操作类型</strong></p>

<p>支持如下条件操作类型：字符串类型（String）、数字类型（Numeric）、日期类型（Data and time）、布尔类型（Boolean）和 IP 地址类型（IP address）。</p>

<p>每种条件操作类型分别支持如下的方法：</p>

<table>
<thead>
<tr>
<th>String</th>
<th>Numeric</th>
<th>Date and time</th>
<th>Boolean</th>
<th>IP address</th>
</tr>
</thead>

<tbody>
<tr>
<td>StringEquals</td>
<td>NumericEquals</td>
<td>DateEquals</td>
<td>Bool</td>
<td>IpAddress</td>
</tr>
<tr>
<td>StringNotEquals</td>
<td>NumericNotEquals</td>
<td>DateNotEquals</td>
<td>-</td>
<td>NotIpAddress</td>
</tr>
<tr>
<td>StringEqualsIgnoreCase</td>
<td>NumericLessThan</td>
<td>DateLessThan</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>StringNotEqualsIgnoreCase</td>
<td>NumericLessThanEquals</td>
<td>DateLessThanEquals</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>StringLike</td>
<td>NumericGreaterThan</td>
<td>DateGreaterThan</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>StringNotLike</td>
<td>NumericGreaterThanEquals</td>
<td>DateGreaterThanEquals</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>

<p><strong>条件关键字（Condition-key）</strong></p>

<p>阿里云服务保留的条件关键字命名格式为：</p>

<pre><code>acs:&lt;condition-key&gt;
</code></pre>

<p>阿里云服务保留的通用条件关键字如下：</p>

<table>
<thead>
<tr>
<th>通用条件关键字</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>acs:CurrentTime</code></td>
<td>Date and time</td>
<td>Web Server 接收到请求的时间，以 ISO 8601 格式表示，如 <code>2012-11-11T23:59:59Z</code></td>
</tr>
<tr>
<td><code>acs:SecureTransport</code></td>
<td>Boolean</td>
<td>发送请求是否使用了安全信道，如 HTTPS</td>
</tr>
<tr>
<td><code>acs:SourceIp</code></td>
<td>IP address</td>
<td>发送请求时的客户端 IP 地址</td>
</tr>
<tr>
<td><code>acs:MFAPresent</code></td>
<td>Boolean</td>
<td>用户登录时是否使用了多因素认证（二步认证）</td>
</tr>
</tbody>
</table>

<p>云产品可以定义产品级别的条件关键字，格式如下：</p>

<pre><code>&lt;service-name&gt;:&lt;condition-key&gt;
</code></pre>

<p>部分云产品定义的条件关键字如下：</p>

<table>
<thead>
<tr>
<th>产品名称</th>
<th>条件关键字</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>

<tbody>
<tr>
<td>ECS</td>
<td><code>ecs:tag/&lt;tag-key&gt;</code></td>
<td>String</td>
<td>ECS 资源的标签关键字，可由用户自定义</td>
</tr>
<tr>
<td>RDS</td>
<td><code>rds:ResourceTag/&lt;tag-key&gt;</code></td>
<td>String</td>
<td>RDS 资源的标签关键字，可由用户自定义</td>
</tr>
<tr>
<td>OSS</td>
<td><code>oss:Delimiter</code></td>
<td>String</td>
<td>OSS 对 Object 名字进行分组的分隔符</td>
</tr>
<tr>
<td></td>
<td><code>oss:Prefix</code></td>
<td>String</td>
<td>OSS Object 名称的前缀</td>
</tr>
</tbody>
</table>

<h2 id="toc_10">Policy 样例</h2>

<p>如下所示的 Policy 样例中，包含两条授权语句（Statement）：</p>

<ul>
<li>  第 1 条授权语句是允许对 region 华东 1（杭州）所有 ecs 资源有查看权限 (<code>ecs:Describe*</code>)；</li>
<li>  第 2 条授权语句是允许对 oss 的 mybucket 存储桶中的对象具有读访问权限 (<code>oss:ListObjects</code>, <code>oss:GetObject</code>)，并限制请求者的 IP 来源必须是<code>42.120.88.10</code>或<code>42.120.66.0/24</code>。</li>
</ul>

<pre><code class="language-json">{
    &quot;Version&quot;: &quot;1&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: &quot;ecs:Describe*&quot;,
            &quot;Resource&quot;: &quot;acs:ecs:cn-hangzhou:*:*&quot;
        },
        {
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: [
                &quot;oss:ListObjects&quot;,
                &quot;oss:GetObject&quot;
            ],
            &quot;Resource&quot;: [
                &quot;acs:oss:*:*:mybucket&quot;,
                &quot;acs:oss:*:*:mybucket/*&quot;
            ],
            &quot;Condition&quot;:{
                &quot;IpAddress&quot;: {
                    &quot;acs:SourceIp&quot;: [&quot;42.120.88.10&quot;, &quot;42.120.66.0/24&quot;]
                }
            }
        }
    ]
}
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to install Tomcat 8.5 on CentOS 7]]></title>
    <link href="http://panlw.github.io/15382134789865.html"/>
    <updated>2018-09-29T17:31:18+08:00</updated>
    <id>http://panlw.github.io/15382134789865.html</id>
    <content type="html"><![CDATA[
<pre><code>MARCH 31, 2018
</code></pre>

<blockquote>
<p><a href="https://linuxize.com/post/how-to-install-tomcat-8-5-on-centos-7/">https://linuxize.com/post/how-to-install-tomcat-8-5-on-centos-7/</a></p>
</blockquote>

<p>This tutorial shows you how to install Tomcat 8.5 on CentOS 7. Tomcat is an open source implementation of the Java Servlet, JavaServer Pages, Java Expression Language and Java WebSocket technologies.</p>

<h2 id="toc_0"><a href="/post/how-to-install-tomcat-8-5-on-centos-7/#prerequisites">Prerequisites</a></h2>

<p>Before starting with this tutorial, make sure you are logged into your server with a user account with sudo privileges or with the root user. It is best practice to run administrative commands as sudo user instead of root, if you don’t have a sudo user on your system you can create one by following <a href="/post/create-a-sudo-user-on-centos/">these instructions</a>.</p>

<h2 id="toc_1"><a href="/post/how-to-install-tomcat-8-5-on-centos-7/#install-openjdk">Install OpenJDK</a></h2>

<p>Tomcat 8.5 requires Java SE 7 or later. In this tutorial we will install OpenJDK, the open source implementation of the Java Platform which is the default Java development and runtime in CentOS 7.</p>

<p>The installation is simple and straight forward:</p>

<pre><code>sudo yum install java-1.8.0-openjdk-devel
</code></pre>

<p>Copy</p>

<p>If you want to install Oracle Java instead of OpenJDK please check <a href="/post/install-java-on-centos-7/">this guide</a>.</p>

<h2 id="toc_2"><a href="/post/how-to-install-tomcat-8-5-on-centos-7/#create-tomcat-system-user">Create Tomcat system user</a></h2>

<p>Running Tomcat as a root user is a security risk and is not recommended. Instead we will create a new system user and group with home directory <code>/opt/tomcat</code> that will run the Tomcat service:</p>

<pre><code>sudo useradd -m -U -d /opt/tomcat -s /bin/false tomcat
</code></pre>

<p>Copy</p>

<h2 id="toc_3"><a href="/post/how-to-install-tomcat-8-5-on-centos-7/#download-tomcat">Download Tomcat</a></h2>

<p>We will download the latest version of Tomcat 8.5.x from the <a href="https://tomcat.apache.org/download-80.cgi">Tomcat downloads page</a>. At the time of writing, the latest version is 8.5.33. Before continuing with the next step you should check the download page for any new version.</p>

<p>Change to the <code>/tmp</code> directory and use <code>wget</code> to download the zip file:</p>

<pre><code>cd /tmp
wget http://www-us.apache.org/dist/tomcat/tomcat-8/v8.5.33/bin/apache-tomcat-8.5.33.zip
</code></pre>

<p><span class="code-copy button main small" data-clipboard-text="cd /tmp<br/>
wget http://www-us.apache.org/dist/tomcat/tomcat-8/v8.5.33/bin/apache-tomcat-8.5.33.zip">Copy</p>

<p>Once the download is completed, extract the zip file and move it to the <code>/opt/tomcat</code> directory:</p>

<pre><code>unzip apache-tomcat-*.zip
sudo mkdir -p /opt/tomcat
sudo mv apache-tomcat-8.5.33 /opt/tomcat/
</code></pre>

<p><span class="code-copy button main small" data-clipboard-text="unzip apache-tomcat-*.zip<br/>
sudo mkdir -p /opt/tomcat<br/>
sudo mv apache-tomcat-8.5.33 /opt/tomcat/">Copy</p>

<p>Because Tomcat 8.5 is updated frequently to have more control over versions and updates, we will create a symbolic link <code>latest</code> which will point to the Tomcat installation directory:</p>

<pre><code>sudo ln -s /opt/tomcat/apache-tomcat-8.5.33 /opt/tomcat/latest
</code></pre>

<p>Copy</p>

<p>The tomcat user that we previously set up needs to have access to the tomcat directory, so we will change the directory ownership to user and group tomcat:</p>

<pre><code>sudo chown -R tomcat: /opt/tomcat
</code></pre>

<p>Copy</p>

<p>and we will also make the scripts inside <code>bin</code> directory executable:</p>

<pre><code>sudo chmod +x /opt/tomcat/latest/bin/*.sh
</code></pre>

<p>Copy<small class="text-grey"><small>Advertisement</small></small></p>

<h2 id="toc_4"><a href="/post/how-to-install-tomcat-8-5-on-centos-7/#create-a-systemd-unit-file">Create a systemd unit file</a></h2>

<p>To run Tomcat as a service we will create a <code>tomcat.service</code> unit file in the <code>/etc/systemd/system/</code> directory with the following contents:</p>

<p>/etc/systemd/system/tomcat.service</p>

<pre><code>[Unit]
Description=Tomcat 8.5 servlet container
After=network.target

[Service]
Type=forking

User=tomcat
Group=tomcat

Environment=&quot;JAVA_HOME=/usr/lib/jvm/jre&quot;
Environment=&quot;JAVA_OPTS=-Djava.security.egd=file:///dev/urandom&quot;

Environment=&quot;CATALINA_BASE=/opt/tomcat/latest&quot;
Environment=&quot;CATALINA_HOME=/opt/tomcat/latest&quot;
Environment=&quot;CATALINA_PID=/opt/tomcat/latest/temp/tomcat.pid&quot;
Environment=&quot;CATALINA_OPTS=-Xms512M -Xmx1024M -server -XX:+UseParallelGC&quot;

ExecStart=/opt/tomcat/latest/bin/startup.sh
ExecStop=/opt/tomcat/latest/bin/shutdown.sh

[Install]
WantedBy=multi-user.target
</code></pre>

<p>&lt;span class=&quot;code-copy button main small&quot; data-clipboard-text=&quot;[Unit]<br/>
Description=Tomcat 8.5 servlet container<br/>
After=network.target</p>

<p>[Service]<br/>
Type=forking</p>

<p>User=tomcat<br/>
Group=tomcat</p>

<p>Environment=&quot;JAVA_HOME=/usr/lib/jvm/jre&quot;<br/>
Environment=&quot;JAVA_OPTS=-Djava.security.egd=file:///dev/urandom&quot;</p>

<p>Environment=&quot;CATALINA_BASE=/opt/tomcat/latest&quot;<br/>
Environment=&quot;CATALINA_HOME=/opt/tomcat/latest&quot;<br/>
Environment=&quot;CATALINA_PID=/opt/tomcat/latest/temp/tomcat.pid&quot;<br/>
Environment=&quot;CATALINA_OPTS=-Xms512M -Xmx1024M -server -XX:+UseParallelGC&quot;</p>

<p>ExecStart=/opt/tomcat/latest/bin/startup.sh<br/>
ExecStop=/opt/tomcat/latest/bin/shutdown.sh</p>

<p>[Install]<br/>
WantedBy=multi-user.target&quot;&gt;Copy</p>

<p>Notify systemd that we created a new unit file and start the Tomcat service by executing:</p>

<pre><code>sudo systemctl daemon-reload
sudo systemctl start tomcat
</code></pre>

<p><span class="code-copy button main small" data-clipboard-text="sudo systemctl daemon-reload<br/>
sudo systemctl start tomcat">Copy</p>

<p>You can check the service status with the following command:</p>

<pre><code>sudo systemctl status tomcat
</code></pre>

<p>Copy</p>

<pre><code> tomcat.service - Tomcat 8.5 servlet container
   Loaded: loaded (/etc/systemd/system/tomcat.service; disabled; vendor preset: disabled)
   Active: active (running) since Sat 2018-03-31 16:30:48 UTC; 3s ago
  Process: 23826 ExecStart=/opt/tomcat/latest/bin/startup.sh (code=exited, status=0/SUCCESS)
 Main PID: 23833 (java)
   CGroup: /system.slice/tomcat.service
           └─23833 /usr/lib/jvm/jre/bin/java -Djava.util.logging.config.file=/opt/tomcat/latest/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djava.security.egd=fi...
</code></pre>

<p><span class="code-copy button main small" data-clipboard-text=" tomcat.service - Tomcat 8.5 servlet container<br/>
   Loaded: loaded (/etc/systemd/system/tomcat.service; disabled; vendor preset: disabled)<br/>
   Active: active (running) since Sat 2018-03-31 16:30:48 UTC; 3s ago<br/>
  Process: 23826 ExecStart=/opt/tomcat/latest/bin/startup.sh (code=exited, status=0/SUCCESS)<br/>
 Main PID: 23833 (java)<br/>
   CGroup: /system.slice/tomcat.service<br/>
           └─23833 /usr/lib/jvm/jre/bin/java -Djava.util.logging.config.file=/opt/tomcat/latest/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djava.security.egd=fi...">Copy</p>

<p>and if there are no errors you can enable the Tomcat service to be automatically started at boot time:</p>

<pre><code>sudo systemctl enable tomcat
</code></pre>

<p>Copy</p>

<h2 id="toc_5"><a href="/post/how-to-install-tomcat-8-5-on-centos-7/#adjust-the-firewall">Adjust the Firewall</a></h2>

<p>If your server is <a href="/post/how-to-setup-a-firewall-with-firewalld-on-centos-7/">protected by a firewall</a> and you want to access the tomcat interface from the outside of the local network you also need to open port <code>8080</code>.</p>

<p>Use the following commands to open the necessary port:</p>

<pre><code>sudo firewall-cmd --zone=public --permanent --add-port=8080/tcp
sudo firewall-cmd --reload
</code></pre>

<p><span class="code-copy button main small" data-clipboard-text="sudo firewall-cmd --zone=public --permanent --add-port=8080/tcp<br/>
sudo firewall-cmd --reload">Copy</p>

<p>In most cases, when running Tomcat in a production environment you will use a load balancer or reverse proxy and it’s a best practice to allow access to port 8080 only to your internal network.</p>

<h2 id="toc_6"><a href="/post/how-to-install-tomcat-8-5-on-centos-7/#configure-tomcat-web-management-interface">Configure Tomcat Web Management Interface</a></h2>

<p>At this point Tomcat is installed and we can access it with a web browser on port <code>8080</code>, but we can not access the web management interface because we have not created a user yet.</p>

<p>Tomcat users and their roles are defined in the <code>tomcat-users.xml</code> file.</p>

<p>If you open the file you will notice that it is filled with comments and examples describing how to configure the file.</p>

<pre><code>sudo vim /opt/tomcat/latest/conf/tomcat-users.xml
</code></pre>

<p>Copy</p>

<p>To add a new user who will be able to access the tomcat web interface (manager-gui and admin-gui) we need to define the user in <code>tomcat-users.xml</code> file as shown bellow. Make sure you change the username and password to something more secure:</p>

<p>/opt/tomcat/latest/conf/tomcat-users.xml</p>

<pre><code>&lt;tomcat-users&gt;
&lt;!--
    Comments
--&gt;
   &lt;role role/&gt;
   &lt;role role/&gt;
   &lt;user user/&gt;
&lt;/tomcat-users&gt;
</code></pre>

<p><span class="code-copy button main small" data-clipboard-text="<tomcat-users><br/>
&lt;!--<br/>
    Comments<br/>
--&gt;<br/>
   <role rolename=&quot;admin-gui&quot;>Copy</p>

<p>By default Tomcat web management interface is configured to allow access only from the localhost. If you want to be able to access the web interface from a remote IP or from anywhere which is not recommended because it is a security risk you can open the following files and make the following changes.</p>

<p>If you need to access the web interface from anywhere open the following files and comment or remove the lines highlighted in yellow:</p>

<p>/opt/tomcat/latest/webapps/manager/META-INF/context.xml</p>

<pre><code>&lt;Context antiResourceLocking=&quot;false&quot; privileged=&quot;true&quot; &gt;
&lt;!--
  &lt;Valve class
         allow=&quot;127\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1&quot; /&gt;
--&gt;
&lt;/Context&gt;
</code></pre>

<p><span class="code-copy button main small" data-clipboard-text="<Context antiResourceLocking=&quot;false&quot; privileged=&quot;true&quot; ><br/>
&lt;!--<br/>
  <Valve className=&quot;org.apache.catalina.valves.RemoteAddrValve&quot;<br/>
         allow=&quot;127\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1&quot; >Copy/opt/tomcat/latest/webapps/host-manager/META-INF/context.xml</p>

<pre><code>&lt;Context antiResourceLocking=&quot;false&quot; privileged=&quot;true&quot; &gt;
&lt;!--
  &lt;Valve class
         allow=&quot;127\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1&quot; /&gt;
--&gt;
&lt;/Context&gt;
</code></pre>

<p><span class="code-copy button main small" data-clipboard-text="<Context antiResourceLocking=&quot;false&quot; privileged=&quot;true&quot; ><br/>
&lt;!--<br/>
  <Valve className=&quot;org.apache.catalina.valves.RemoteAddrValve&quot;<br/>
         allow=&quot;127\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1&quot; >Copy</p>

<p>If you need to access the web interface only from a specific IP, instead of commenting the blocks add your public IP to the list. Let’s say your public IP is <code>41.41.41.41</code> and you want to allow access only from that IP:</p>

<p>/opt/tomcat/latest/webapps/manager/META-INF/context.xml</p>

<pre><code>&lt;Context antiResourceLocking=&quot;false&quot; privileged=&quot;true&quot; &gt;
  &lt;Valve class
         allow=&quot;127\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1|41.41.41.41&quot; /&gt;
&lt;/Context&gt;
</code></pre>

<p><span class="code-copy button main small" data-clipboard-text="<Context antiResourceLocking=&quot;false&quot; privileged=&quot;true&quot; ><br/>
  <Valve className=&quot;org.apache.catalina.valves.RemoteAddrValve&quot;<br/>
         allow=&quot;127\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1|41.41.41.41&quot; >Copy/opt/tomcat/latest/webapps/host-manager/META-INF/context.xml</p>

<pre><code>&lt;Context antiResourceLocking=&quot;false&quot; privileged=&quot;true&quot; &gt;
  &lt;Valve class
         allow=&quot;127\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1|41.41.41.41&quot; /&gt;
&lt;/Context&gt;
</code></pre>

<p><span class="code-copy button main small" data-clipboard-text="<Context antiResourceLocking=&quot;false&quot; privileged=&quot;true&quot; ><br/>
  <Valve className=&quot;org.apache.catalina.valves.RemoteAddrValve&quot;<br/>
         allow=&quot;127\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1|41.41.41.41&quot; >Copy</p>

<p>The list of allowed IP addresses is a list separated with vertical bar <code>|</code>. You can add single IP addresses or use a regular expressions.</p>

<p>Restart the Tomcat service for changes to take effect:</p>

<pre><code>sudo systemctl restart tomcat
</code></pre>

<p>Copy</p>

<h2 id="toc_7"><a href="/post/how-to-install-tomcat-8-5-on-centos-7/#test-the-installation">Test the Installation</a></h2>

<p>Open your browser and type: <code>http://&lt;your_domain_or_IP_address&gt;:8080</code></p>

<p>Upon successful installation, a screen similar to the following will appear:</p>

<p><img src="https://linuxize.com/post/how-to-install-tomcat-8-5-on-centos-7/tomcat-home.jpg" alt=""/></p>

<p>Tomcat web application manager dashboard is available at <code>http://&lt;your_domain_or_IP_address&gt;:8080/manager/html</code>. From here you can deploy, undeploy, start, stop and reload your applications.</p>

<p><img src="https://linuxize.com/post/how-to-install-tomcat-8-5-on-centos-7/tomcat-manager.jpg" alt=""/></p>

<p>Tomcat virtual host manager dashboard is available at <code>http://&lt;your_domain_or_IP_address&gt;:8080/host-manager/html</code>. From here you can create, delete and manage Tomcat virtual hosts.</p>

<p><img src="https://linuxize.com/post/how-to-install-tomcat-8-5-on-centos-7/tomcat-host-manager.jpg" alt=""/></p>

<h2 id="toc_8"><a href="/post/how-to-install-tomcat-8-5-on-centos-7/#conclusion">Conclusion</a></h2>

<p>You have successfully installed Tomcat 8.5 on your CentOS 7 system and learned how to access the Tomcat management interface. You can now visit the official <a href="https://tomcat.apache.org/tomcat-8.5-doc/index.html">Apache Tomcat 8 Documentation</a> and learn more about the Apache Tomcat features.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How To Back Up, Restore, and Migrate a MongoDB Database on Ubuntu 14.04]]></title>
    <link href="http://panlw.github.io/15381224274794.html"/>
    <updated>2018-09-28T16:13:47+08:00</updated>
    <id>http://panlw.github.io/15381224274794.html</id>
    <content type="html"><![CDATA[
<pre><code>April 15, 2016, Anatoliy Dimitrov
</code></pre>

<blockquote>
<p>原文地址 <a href="https://www.digitalocean.com/community/tutorials/how-to-back-up-restore-and-migrate-a-mongodb-database-on-ubuntu-14-04">https://www.digitalocean.com/community/tutorials/how-to-back-up-restore-and-migrate-a-mongodb-database-on-ubuntu-14-04</a></p>
</blockquote>

<p>MongoDB is one of the most popular NoSQL database engines. It is famous for being scalable, powerful, reliable and easy to use. In this article we&#39;ll show you how to back up, restore, and migrate your MongoDB databases.</p>

<p>Importing and exporting a database means dealing with data in a human-readable format, compatible with other software products. In contrast, the backup and restore operations create or use MongoDB-specific binary data, which preserves not only the consistency and integrity of your data but also its specific MongoDB attributes. Thus, for migration its usually preferable to use backup and restore as long as the source and target systems are compatible.</p>

<h2 id="toc_0">Prerequisites</h2>

<p>Before following this tutorial, please make sure you complete the following prerequisites:</p>

<ul>
<li>  Ubuntu 14.04 Droplet</li>
<li>  Non-root sudo user. Check out <a href="https://www.digitalocean.com/community/tutorials/initial-server-setup-with-ubuntu-14-04">Initial Server Setup with Ubuntu 14.04</a> for details.</li>
<li>  MongoDB installed and configured using the article <a href="https://www.digitalocean.com/community/tutorials/how-to-install-mongodb-on-ubuntu-14-04">How to Install MongoDB on Ubuntu 14.04</a>.</li>
<li>  Example MongoDB database imported using the instructions in <a href="https://www.digitalocean.com/community/tutorials/how-to-import-and-export-a-mongodb-database-on-ubuntu-14-04">How To Import and Export a MongoDB Database on Ubuntu 14.04</a></li>
</ul>

<p>Except otherwise noted, all of the commands that require root privileges in this tutorial should be run as a non-root user with sudo privileges.</p>

<h2 id="toc_1">Understanding the Basics</h2>

<p>Before continue further with this article some basic understanding on the matter is needed. If you have experience with popular relational database systems such as MySQL, you may find some similarities when working with MongoDB.</p>

<p>The first thing you should know is that MongoDB uses <a href="http://json.org/">json</a> and bson (binary json) formats for storing its information. Json is the human-readable format which is perfect for exporting and, eventually, importing your data. You can further manage your exported data with any tool which supports json, including a simple text editor.</p>

<p>An example json document looks like this:</p>

<p>Example of json Format</p>

<pre><code class="language-json">{&quot;address&quot;:[
    {&quot;building&quot;:&quot;1007&quot;, &quot;street&quot;:&quot;Park Ave&quot;},
    {&quot;building&quot;:&quot;1008&quot;, &quot;street&quot;:&quot;New Ave&quot;},
]}
</code></pre>

<p>Json is very convenient to work with, but it does not support all the data types available in bson. This means that there will be the so called &#39;loss of fidelity&#39; of the information if you use json. For backing up and restoring, it&#39;s better to use the binary bson.</p>

<p>Second, you don&#39;t have to worry about explicitly creating a MongoDB database. If the database you specify for import doesn&#39;t already exist, it is automatically created. Even better is the case with the collections&#39; (database tables) structure. In contrast to other database engines, in MongoDB the structure is again automatically created upon the first document (database row) insert.</p>

<p>Third, in MongoDB reading or inserting large amounts of data, such as for the tasks of this article, can be resource intensive and consume much of the CPU, memory, and disk space. This is something critical considering that MongoDB is frequently used for large databases and Big Data. The simplest solution to this problem is to run the exports and backups during the night or during non-peak hours.</p>

<p>Fourth, information consistency could be problematic if you have a busy MongoDB server where the information changes during the database export or backup process. There is no simple solution to this problem, but at the end of this article, you will see recommendations to further read about replication.</p>

<p>While you can use the <a href="https://www.digitalocean.com/community/tutorials/how-to-import-and-export-a-mongodb-database-on-ubuntu-14-04">import and export functions</a> to backup and restore your data, there are better ways to ensure the full integrity of your MongoDB databases. To backup your data you should use the command <code>mongodump</code>. For restoring, use <code>mongorestore</code>. Let&#39;s see how they work.</p>

<h2 id="toc_2">Backing Up a MongoDB Database</h2>

<p>Let&#39;s cover backing up your MongoDB database first.</p>

<p>An important argument to <code>mongodump</code> is <code>--db</code>, which specifies the name of the database which you want to back up. If you don&#39;t specify a database name, <code>mongodump</code> backups all of your databases. The second important argument is <code>--out</code> which specifies the directory in which the data will be dumped. Let&#39;s take an example with backing up the <code>newdb</code> database and storing it in the <code>/var/backups/mongobackups</code> directory. Ideally, we&#39;ll have each of our backups in a directory with the current date like <code>/var/backups/mongobackups/01-20-16</code> (20th January 2016). First, let&#39;s create that directory <code>/var/backups/mongobackups</code> with the command:</p>

<pre><code class="language-bash">sudo mkdir /var/backups/mongobackups
</code></pre>

<p>Then our backup command should look like this:</p>

<pre><code class="language-bash">sudo mongodump --db newdb --out /var/backups/mongobackups/`date +&quot;%m-%d-%y&quot;`

</code></pre>

<p>A successfully executed backup will have an output such as:</p>

<p>Output of mongodump</p>

<pre><code class="language-log">2016-01-20T10:11:57.685-0500    writing newdb.restaurants to /var/backups/mongobackups/01-20-16/newdb/restaurants.bson
2016-01-20T10:11:57.907-0500    writing newdb.restaurants metadata to /var/backups/mongobackups/01-20-16/newdb/restaurants.metadata.json
2016-01-20T10:11:57.911-0500    done dumping newdb.restaurants (25359 documents)
2016-01-20T10:11:57.911-0500    writing newdb.system.indexes to /var/backups/mongobackups/01-20-16/newdb/system.indexes.bson
</code></pre>

<p>Note that in the above directory path we have used <code>date +&quot;%m-%d-%y&quot;</code> which gets the current date automatically. This will allow us to have the backups inside the directory <code>/var/backups/01-20-16/</code>. This is especially convenient when we automate the backups.</p>

<p>At this point you have a complete backup of the <code>newdb</code> database in the directory <code>/var/backups/mongobackups/01-20-16/newdb/</code>. This backup has everything to restore the <code>newdb</code> properly and preserve its so called &quot;fidelity&quot;.</p>

<p>As a general rule, you should make regular backups, such as on a daily basis, and preferably during a time when the server is least loaded. Thus, you can set the <code>mongodump</code> command as a cron job so that it&#39;s run regularly, e.g. every day at 03:03 AM. To accomplish this open crontab, cron&#39;s editor like this:</p>

<pre><code class="language-bash">sudo crontab -e
</code></pre>

<p>Note that when you run <code>sudo crontab</code> you will be editing the cron jobs for the root user. This is recommended because if you set the crons for your user, they might not be executed properly, especially if your sudo profile requires password verification.</p>

<p>Inside the crontab prompt insert the following <code>mongodump</code> command:</p>

<p>Crontab window</p>

<pre><code class="language-cron">3 3 * * * mongodump --out /var/backups/mongobackups/`date +&quot;%m-%d-%y&quot;`
</code></pre>

<p>In the above command we are omitting the <code>--db</code> argument on purpose because typically you will want to have all of your databases backed up.</p>

<p>Depending on your MongoDB database sizes you may soon run out of disk space with too many backups. That&#39;s why it&#39;s also recommended to clean the old backups regularly or to compress them. For example, to delete all the backups older than 7 days you can use the following bash command:</p>

<pre><code class="language-bash">find /var/backups/mongobackups/ -mtime +7 -exec rm -rf {} \;
</code></pre>

<p>Similarly to the previous <code>mongodump</code> command, this one can be also added as a cron job. It should run just before you start the next backup, e.g. at 03:01 AM. For this purpose open again crontab:</p>

<pre><code class="language-bash">sudo crontab -e
</code></pre>

<p>After that insert the following line:</p>

<p>Crontab window</p>

<pre><code class="language-cron">3 1 * * * find /var/backups/mongobackups/ -mtime +7 -exec rm -rf {} \;

</code></pre>

<p>Completing all the tasks in this step will ensure a good backup solution for your MongoDB databases.</p>

<h2 id="toc_3">Restoring and Migrating a MongoDB Database</h2>

<p>By restoring your MongoDB database from a previous backup (such as one from the previous step) you will be able to have the exact copy of your MongoDB information taken at a certain time, including all the indexes and data types. This is especially useful when you want to migrate your MongoDB databases. For restoring MongoDB we&#39;ll be using the command <code>mongorestore</code> which works with the binary backup produced by <code>mongodump</code>.</p>

<p>Let&#39;s continue our examples with the <code>newdb</code> database and see how we can restore it from the previously taken backup. As arguments we&#39;ll specify first the name of the database with the <code>--db</code> argument. Then with <code>--drop</code> we&#39;ll make sure that the target database is first dropped so that the backup is restored in a clean database. As a final argument we&#39;ll specify the directory of the last backup <code>/var/backups/mongobackups/01-20-16/newdb/</code>. So the whole command will look like this (replace with the date of the backup you wish to restore):</p>

<pre><code class="language-bash">sudo mongorestore --db newdb --drop /var/backups/mongobackups/01-20-16/newdb/
</code></pre>

<p>A successful execution will show the following output:</p>

<p>Output of mongorestore</p>

<pre><code class="language-log">2016-01-20T10:44:47.876-0500    building a list of collections to restore from /var/backups/mongobackups/01-20-16/newdb/ dir
2016-01-20T10:44:47.908-0500    reading metadata file from /var/backups/mongobackups/01-20-16/newdb/restaurants.metadata.json
2016-01-20T10:44:47.909-0500    restoring newdb.restaurants from file /var/backups/mongobackups/01-20-16/newdb/restaurants.bson
2016-01-20T10:44:48.591-0500    restoring indexes for collection newdb.restaurants from metadata
2016-01-20T10:44:48.592-0500    finished restoring newdb.restaurants (25359 documents)
2016-01-20T10:44:48.592-0500    done
</code></pre>

<p>In the above case we are restoring the data on the same server where the backup has been created. If you wish to migrate the data to another server and use the same technique, you should just copy the backup directory, which is <code>/var/backups/mongobackups/01-20-16/newdb/</code> in our case, to the other server.</p>

<h2 id="toc_4">Conclusion</h2>

<p>This article has introduced you to the essentials of managing your MongoDB data in terms of backing up, restoring, and migrating databases. You can continue further reading on <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-a-scalable-mongodb-database">How To Set Up a Scalable MongoDB Database</a> in which MongoDB replication is explained.</p>

<p>Replication is not only useful for scalability, but it&#39;s also important for the current topics. Replication allows you to continue running your MongoDB service uninterrupted from a slave MongoDB server while you are restoring the master one from a failure. Part of the replication is also the <a href="https://docs.mongodb.org/manual/core/replica-set-oplog/">operations log (oplog)</a>, which records all the operations that modify your data. You can use this log, just as you would use the binary log in MySQL, to restore your data after the last backup has taken place. Recall that backups usually take place during the night, and if you decide to restore a backup in the evening you will be missing all the updates since the last backup.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[10 tips for your next Identity and Access Management (IAM) project]]></title>
    <link href="http://panlw.github.io/15377173770033.html"/>
    <updated>2018-09-23T23:42:57+08:00</updated>
    <id>http://panlw.github.io/15377173770033.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://www.linkedin.com/pulse/10-tips-your-next-identity-access-management-iam-project-schwartz/">https://www.linkedin.com/pulse/10-tips-your-next-identity-access-management-iam-project-schwartz/</a></p>
</blockquote>

<p>It’s no secret that identity and access management (IAM) requirements can be extremely diverse from one organization to the next. And because IAM is a fairly specialized skill set, the cost of finding and employing contractors can be high. As with any special purpose integrator, you want to use their services tactically to help you jump start the project.</p>

<p>Here are ten recommendations to help you get the most out of your next identity and access management project:</p>

<ol>
<li>Keep the initial scope limited. You can always define subsequent phases. I would suggest a pilot where you propose a design and build out a sample infrastructure including your IAM platform (like the <a href="http://gluu.org/overview">Gluu Server!</a>) and sample Web, mobile, and desktop applications. This would buy you some time to do a more detailed analysis of how to roll out the new access management service to all your applications.</li>
<li>Remember--Identity Management (IDM) and Access Management <em>are different</em>. IDM enables you to define a workflow for user create, update, and delete events. Once you know who the people are, access management enables you to define who can get to what, and what the available interfaces are to the applications.</li>
<li>Outsourcing IAM is great, but will it give you the flexibility and security your organization requires? Also are you ready to pay per user? If the answer to any of these questions is &quot;no,&quot; then you may need an on-premise identity solution. </li>
<li>Centralize! Consolidation saves money. It&#39;s more efficient if application developers focus on business processes than security. Also, hard coding security into applications can lead to problems when security policies change, or new technologies become available--then you have to update and re-QA the application or API. Create services centrally to identify people, and to define policies about who can access what resources. </li>
<li>Make sure your organization’s operational staff is involved from day one. You don’t want to have the integrators just hand over the keys at the end of the project.</li>
<li>Future proof your solution by making sure your federation platform supports OAuth2, including <a href="http://openid.net/connect">OpenID Connect</a> which provides API&#39;s for user identification, and <a href="http://kantarainitiative.org/confluence/display/uma/Home">UMA</a> which provides API&#39;s for central access management. Like any other API&#39;s, you need to market these new OAuth2 API&#39;s to developers at your organization. Leave time to develop a showcase application so developers can look at actual code. Make the pilot environment available for testing by application developers.</li>
<li>To drive down the cost of the application integrations, you need to provide developers with information about the federation schema, standards, and best practices. Just to give you an idea, here is a <a href="http://ox.gluu.org/tech-info">sample federation site</a>. </li>
<li>Do you want to support external identities? If your enterprise customers have their own SAML or OpenID Connect identity provider (IDP), and your organization wants to enable those users to use their home credentials, you will need to define both business and technical processes to onboard them. Social login is also a form of external identity. If you decide to trust a consumer IDP like Google or Facebook, you must also define the workflow for user enrollment and authentication.</li>
<li>Think about how you identity proof people (how does a person prove they are who they say they are!). If you issue strong credentials to someone impersonating another individual, your strong credentials mean nothing. The integrity of your identity system relies on a careful business process to issue and manage credentials. Can you in-person identity proof a person? For example, when an employee provides an I-9, someone at your company looks at the person, looks at their id documents, and validates they match. Do you validate the identity documents are valid? You can use services like <a href="http://authenticid.co/">AuthenticID</a> to validate these physical credentials. If you enroll a person online, you can ask the person to take a selfie, and to take a picture of their physical credentials, like a front and back of the drivers license.</li>
<li>Two-factor authentication, a.k.a. 2FA: One of the main advantages of a centralized authentication and authorization platform is that it enables many applications to leverage your investment in a strong authentication technology. <a href="http://duosecurity.com/">Duo Authentication</a> and <a href="http://yubico.com/">Yubikey</a> are two examples of 2FA that you can implement with ease. If you need a custom 2FA application, you may be interested in Gluu&#39;s free open source mobile software called <a href="http://ox.gluu.org/doku.php?id=oxpush:home">oxPush</a>. But no matter what technology you choose, remember a credential is only as strong as the weakest reset mechanism. If you can reset your hardware token by receiving an email, hackers will skip the token and go straight for the account reset process! Create many ways to strongly identify a person, and implement policies so that in order to reset a credential, you must provide a credential of equal or greater strength. If you lose your most secure credential, the person should have to re-identity proof. But don&#39;t wait. 2FA is something every company needs to implement right now, if only for a subset of the people in your organization.</li>
</ol>

<p>Should you have questions about this advice, please <a href="http://gluu.youcanbook.me/">schedule a meeting</a>with me to discuss further.</p>

<p><em>If you need to identity customers, partners, employees, and devices, you should check out the free open source</em> <em><a href="http://gluu.org/">Gluu Server</a>. Using a Gluu Server, you can centralize your authentication and authorization service and leverage open web standards to enable federated single sign-on (SSO) and web and API access management.</em></p>

<p>More posts from Mike Schwartz:</p>

<ul>
<li><a href="https://www.linkedin.com/pulse/authorization-authz-new-authentication-authn-mike-schwartz">Authorization (AuthZ) is the new Authentication (AuthN)</a></li>
<li><a href="https://www.linkedin.com/pulse/state-digital-identity-2016-mike-schwartz">The State of Digital Identity in 2016</a></li>
<li><a href="https://www.linkedin.com/pulse/5-reasons-you-need-openid-connect-uma-your-identity-access-schwartz">5 Reasons You Need OpenID Connect and UMA in Your Identity and Access Management (IAM) Stack</a></li>
<li><a href="https://www.linkedin.com/pulse/ideas-managing-iot-your-house-mike-schwartz">Ideas on managing IoT in your house</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kafka实践：到底该不该把不同类型的消息放在同一个主题中？]]></title>
    <link href="http://panlw.github.io/15376981994343.html"/>
    <updated>2018-09-23T18:23:19+08:00</updated>
    <id>http://panlw.github.io/15376981994343.html</id>
    <content type="html"><![CDATA[
<pre><code>作者 Martin Kleppmann，译者 无明  发布于 2018年8月22日
</code></pre>

<blockquote>
<p><a href="http://www.infoq.com/cn/articles/event-types-in-kafka-topic">http://www.infoq.com/cn/articles/event-types-in-kafka-topic</a></p>

<p>英文原文：<a href="http://martin.kleppmann.com/2018/01/18/event-types-in-kafka-topic.html">http://martin.kleppmann.com/2018/01/18/event-types-in-kafka-topic.html</a></p>
</blockquote>

<p>如果你使用了像 <a href="http://kafka.apache.org/">Kafka</a> 这样的流式处理平台，就要搞清楚一件事情：你需要用到哪些主题？特别是如果你要将一堆不同的事件作为消息发布到 Kafka，是将它们放在同一个主题中，还是将它们拆分到不同的主题中？</p>

<p>Kafka 主题最重要的一个功能是可以让消费者指定它们想要消费的消息子集。在极端情况下，将所有数据放在同一个主题中可能不是一个好主意，因为这样消费者就无法选择它们感兴趣的事件——它们需要消费所有的消息。另一种极端情况，拥有数百万个不同的主题也不是一个好主意，因为 Kafka 的每个主题都是有成本的，拥有大量主题会损害性能。</p>

<p>实际上，从性能的角度来看，分区数量才是关键因素。在 Kafka 中，每个主题至少对应一个分区，如果你有 n 个主题，至少会有 n 个分区。不久之前，Jun Rao 写了一篇<a href="https://www.confluent.io/blog/how-to-choose-the-number-of-topicspartitions-in-a-kafka-cluster/">博文</a>，解释了拥有多个分区的成本（端到端延迟、文件描述符、内存开销、发生故障后的恢复时间）。根据经验，如果你关心延迟，那么每个节点分配几百个分区就可以了。如果每个节点的分区数量超过成千上万个，就会造成较大的延迟。</p>

<p>关于性能的讨论为设计主题结构提供了一些指导：如果你发现自己有数千个主题，那么将一些细粒度、低吞吐量的主题合并到粗粒度主题中可能是个明智之举，这样可以避免分区数量蔓延。</p>

<p>然而，性能并不是我们唯一关心的问题。在我看来，更重要的是主题结构的数据完整性和数据模型。我们将在本文的其余部分讨论这些内容。</p>

<h2 id="toc_0">主题等于相同类型事件的集合？</h2>

<p>人们普遍认为应该将相同类型的事件放在同一主题中，不同的事件类型应该使用不同的主题。这种思路让我们联想到关系型数据库，其中表是相同类型记录的集合，于是我们就有了数据库表和 Kafka 主题之间的类比。</p>

<p><a href="https://docs.confluent.io/current/schema-registry/docs/index.html">Confluent Avro Schema Registry</a> 进一步强化了这种概念，因为它鼓励你对主题的所有消息使用相同的 Avro 模式（schema）。模式可以在保持兼容性的同时进行演化（例如通过添加可选字段），但所有消息都必须符合某种记录类型。稍后我会再回过头来讨论这个问题。</p>

<p>对于某些类型的流式数据，例如活动事件，要求同一主题中所有消息都符合相同的模式，这是合理的。但是，有些人把 Kafka 当成了数据库来用，例如<a href="https://docs.microsoft.com/en-us/azure/architecture/patterns/event-sourcing">事件溯源</a>，或者<a href="https://www.confluent.io/blog/build-services-backbone-events/">在微服务之间交换数据</a>。对于这种情况，我认为是否将主题定义为具有相同模式的消息集合就不那么重要了。这个时候，更重要的是主题分区中的消息必须是有序的。</p>

<p>想象一下这样的场景：你有一个实体（比如客户），这个实体可能会发生许多不同的事情，比如创建客户、客户更改地址、客户向帐户中添加新的信用卡、客户发起客服请求，客户支付账单、客户关闭帐户。</p>

<p>这些事件之间的顺序很重要。例如，我们希望其他事件必须在创建客户之后才能发生，并且在客户关闭帐户之后不能再发生其他事件。在使用 Kafka 时，你可以将它们全部放在同一个主题分区中来保持它们的顺序。在这个示例中，你可以使用客户 ID 作为分区的键，然后将所有事件放在同一个主题中。它们必须位于同一主题中，因为不同的主题对应不同的分区，而 Kafka 是不保证分区之间的顺序的。</p>

<h2 id="toc_1">顺序问题</h2>

<p>如果你为 customerCreated、customerAddressChanged 和 customerInvoicePaid 事件使用了不同的主题，那么这些主题的消费者可能就看不到这些事件之间的顺序。例如，消费者可能会看到一个不存在的客户做出的地址变更（这个客户尚未创建，因为相应的 customerCreated 事件可能发生了延迟）。</p>

<p>如果消费者暂停一段时间（比如进行维护或部署新版本），那么事件出现乱序的可能性就更高了。在消费者停止期间，事件继续发布，并且这些事件被存储在特定定的主题分区中。当消费者再次启动时，它会消费所有积压在分区中的事件。如果消费者只消费一个分区，那就没问题：积压的事件会按照它们存储的顺序依次被处理。但是，如果消费者同时消费几个主题，就会按任意顺序读取主题中数据。它可以先读取积压在一个主题上的所有数据，然后再读取另一个主题上积压的数据，或者交错地读取多个主题的数据。</p>

<p>因此，如果你将 customerCreated、customerAddressChanged 和 customerInvoicePaid 事件放在三个单独的主题中，那么消费者可能会在看到 customerCreated 事件之前先看到 customerAddressChanged 事件。因此，消费者很可能会看到一个客户的 customerAddressChanged 事件，但这个客户却未被创建。</p>

<p>你可能会想到为每条消息附加时间戳，并用它来对事件进行排序。如果你将事件导入数据仓库，再对事件进行排序，或许是没有问题的。但在流数据中只使用时间戳是不够的：在你收到一个具有特定时间戳的事件时，你不知道是否需要等待具有较早时间戳的事件，或者所有之前的事件是否已经在当前事情之前到达。依靠时钟进行同步通常会导致噩梦，有关时钟问题的更多详细信息，请参阅 “Designing Data-Intensive Applications” 的第 8 章。</p>

<h2 id="toc_2">何时拆分主题，何时合并主题？</h2>

<p>基于这个背景，我将给出一些经验之谈，帮你确定哪些数据应该放在同一主题中，以及哪些数据应该放在不同的主题中。</p>

<ol>
<li><p>首先，需要保持固定顺序的事件必须放在同一主题中（并且需要使用相同的分区键）。如果事件属于同一实体，那么事件的顺序就很重要。因此，我们可以说，同一实体的所有事件都应该保存在同一主题中。</p>

<p>如果你使用事件溯源进行数据建模，事件的排序尤为重要。聚合对象的状态是通过以特定的顺序重放事件日志而得出的。因此，即使可能存在不同的事件类型，聚合所需要的所有事件也必须在同一主题中。</p></li>
<li><p>对于不同实体的事件，它们应该保存在相同的主题中还是不同的主题中？我想说，如果一个实体依赖于另一个实体（例如一个地址属于一个客户），或者经常需要同时用到它们，那么它们也应该保存在同一主题中。另一方面，如果它们不相关，并且属于不同的团队，那么最好将它们放在不同的主题中。</p>

<p>另外，这也取决于事件的吞吐量：如果一个实体类型的事件吞吐量比其他实体要高很多，那么最好将它分成几个主题，以免让只想消费低吞吐量实体的消费者不堪重负（参见第 4 点）。不过，可以将多个具有低吞吐量的实体合并起来。</p></li>
<li><p>如果一个事件涉及多个实体该怎么办？例如，订单涉及到产品和客户，转账至少涉及到两个账户。</p>

<p>我建议在一开始将这些事件记录为单个原子消息，而不是将其分成几个属于不同主题的消息。在记录事件时，最好可以保持原封不动，即尽可能保持数据的原始形式。你可以随后使用流式处理器来拆分复合事件，但如果过早进行拆分，想要重建原始事件会难得多。如果能够为初始事件分配一个唯一 ID（例如 UUID）就更好了，之后如果你要拆分原始事件，可以带上这个 ID，从而可以追溯到每个事件的起源。</p></li>
<li><p>看看消费者需要订阅的主题数量。如果几个消费者都订阅了一组特定的主题，这表明可能需要将这些主题合并在一起。</p>

<p>如果将细粒度的主题合并成粗粒度的主题，一些消费者可能会收到他们不需要的事件，需要将其忽略。这不是什么大问题：消费消息的成本非常低，即使最终忽略了一大半的事件，总的成本可能也不会很大。只有当消费者需要忽略绝大多数消息（例如 99.9％是不需要的）时，我才建议将大容量事件流拆分成小容量事件流。</p></li>
<li><p>用作 Kafka Streams 状态存储（KTable）的变更日志主题应该与其他主题分开。在这种情况下，这些主题由 Kafka Streams 流程来管理，所以不应该包含其他类型的事件。</p>

<p>最后，如果基于上述的规则依然无法做出正确的判断，该怎么办？那么就按照类型对事件进行分组，把相同类型的事件放在同一个主题中。不过，我认为这条规则是最不重要的。</p></li>
</ol>

<h2 id="toc_3">模式管理</h2>

<p>如果你的数据是普通文本（如 JSON），而且没有使用静态的模式，那么就可以轻松地将不同类型的事件放在同一个主题中。但是，如果你使用了模式编码（如 Avro），那么在单个主题中保存多种类型的事件则需要考虑更多的事情。</p>

<p>如上所述，基于 Avro 的 Kafka Confluent Schema Registry 假设了一个前提，即每个主题都有一个模式（更确切地说，一个模式用于消息的键，一个模式用于消息的值）。你可以注册新版本的模式，注册表会检查模式是否向前和向后兼容。这样设计的一个好处是，你可以让不同的生产者和消费者同时使用不同版本的模式，并且仍然保持彼此的兼容性。</p>

<p>Confluent 的 Avro 序列化器通过 subject 名称在注册表中注册模式。默认情况下，消息键的 subject 为 <topic>-key，消息值的 subject 为 &lt; topic&gt;-value。模式注册表会检查在特定 subject 下注册的所有模式的相互兼容性。</p>

<p>最近，我为 Avro 序列化器提供了一个补丁（<a href="https://github.com/confluentinc/schema-registry/pull/680">https://github.com/confluentinc/schema-registry/pull/680</a>），让兼容性检查变得更加灵活。这个补丁添加了两个新的配置选项：key.subject.name.strategy（用于定义如何构造消息键的 subject 名称）和 value.subject.name.strategy（用于定义如何构造消息值的 subject 名称）。它们的值可以是如下几个：</p>

<ul>
<li>  io.confluent.kafka.serializers.subject.TopicNameStrategy（默认）：消息键的 subject 名称为 <topic>-key，消息值为 &lt; topic&gt;-value。这意味着主题中所有消息的模式必须相互兼容。</li>
<li>  io.confluent.kafka.serializers.subject.RecordNameStrategy：subject 名称是 Avro 记录类型的完全限定名。因此，模式注册表会检查特定记录类型的兼容性，而不管是哪个主题。这个设置允许同一主题包含不同类型的事件。</li>
<li>  io.confluent.kafka.serializers.subject.TopicRecordNameStrategy：subject 名称是 <topic>-<type>，其中 &lt; topic &gt; 是 Kafka 主题名，<type > 是 Avro 记录类型的完全限定名。这个设置允许同一主题包含不同类型的事件，并进一步对当前主题进行兼容性检查。</li>
</ul>

<p>有了这个新特性，你就可以轻松地将属于特定实体的所有不同类型的事件放在同一个主题中。现在，你可以自由选择主题的粒度，而不仅限于一个类型对应一个主题。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[识别领域事件 – ThoughtWorks 洞见]]></title>
    <link href="http://panlw.github.io/15376942836824.html"/>
    <updated>2018-09-23T17:18:03+08:00</updated>
    <id>http://panlw.github.io/15376942836824.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://insights.thoughtworks.cn/recognize-domain-event/">https://insights.thoughtworks.cn/recognize-domain-event/</a></p>
</blockquote>

<p>随着微服务架构的兴起，微服务设计与拆分的的最佳实践 DDD 已然成为大家讨论与实践的热点，整个行业都在探索如何用 DDD 建模来实现微服务设计。事件风暴作为最接地气的实践，在不同的项目中野蛮生长，不断演进，今天已经渐渐成熟。作为事件风暴的灵魂——领域事件，值得我们投入更多的精力去设计与打磨。</p>

<p>领域事件是用特定方式 (已发生的时态) 表达发生在问题域中的重要事情，是领域通用语言 (UL) 的一部分。为了方便理解这个概念，这里举一个宠物的例子：如果做为宠物主人，你的问题域是如何养好一只猫，那么是不是已经打了疫苗，给宠物饲喂食物等将成为你关注的事情，领域事件会有：疫苗已注射，猫粮已饲喂等。如果你是宠物医生，问题域是如何治好宠物的病，关注的事情是宠物的身体构成，准确的诊断宠物病情，对症下药，领域事件会有：病情已确诊，药方已开治。虽说二者关注的都是宠物，在不同的问题域下领域事件是不同的。</p>

<p><a href="https://insights.thoughtworks.cn/wp-content/uploads/2018/09/1.png"><img src="https://insights.thoughtworks.cn/wp-content/uploads/2018/09/1.png" alt=""/></a></p>

<p>DDD 的提出者和圈内的大师先后提到领域事件在领域建模中的价值，前沿实践者们已经开始应用领域事件来表达业务全景。在 DDD 建模过程中，以领域事件为线索逐步得到领域模型已经成为了主流的实践，即：事件风暴。</p>

<p>事件风暴是以更专注的方式发现与提取领域事件，并将以领域事件为中心的概念模型逐渐演化成以聚合为中心的领域模型，以快速可落地的方式实现了 DDD 建模。</p>

<p>对于高质量的事件风暴，首先要解决识别领域事件的问题，理想的情况下领域专家和研发团队一起参加事件风暴，从业务的视角去分析涉众关心的领域事件，短时间内高度可视化交流，集中思考，快速发散收敛形成所有参与者一致认可的领域事件集合。我在多个项目上实现事件风暴后，总结了一些坑和应对办法，供大家参考：</p>

<h3 id="toc_0">1. 组织没有领域专家</h3>

<p>对问题域有深刻见解的主题专家称为领域专家，在大多数组织中没有这个角色，当 DDD 建模需要领域专家支持时，组织往往找业务部门的业务人员，BA，产品经理或在这个领域有多年开发经验的 DEV 来充当。</p>

<p>这些一线业务人员和开发团队都清楚有什么功能，但往往不清楚为什么有这些功能。举个例子：如果我们的问题是打开一瓶红酒，你去调研每天都会打开酒瓶的 waiter, 给你的答案是：开瓶器。但换做领域专家的视角来看，会回归问题的本质，如果我们希望打开酒瓶，需要把瓶塞移除，移除瓶塞的方式有多种，包括推，撬与拉拽，对于拉拽可能基于吸力或螺旋拉拽，下面右图的开瓶器只不过是螺旋拉拽的一种解决方案。领域专家应该对问题域及其中的各种可行方案有更深入的理解。</p>

<p><a href="https://insights.thoughtworks.cn/wp-content/uploads/2018/09/2.png"><img src="https://insights.thoughtworks.cn/wp-content/uploads/2018/09/2.png" alt=""/></a></p>

<p>在辅导团队的过程中，为了弥补这部分视角的缺失，往往会在事件风暴之前，组织业务愿景和场景分析，与被指派的业务干系人对齐业务愿景，一起分析业务场景背后的问题域，找到问题域的本质后再展开事件风暴。</p>

<h3 id="toc_1">2. 面向复杂业务系统的事件风暴</h3>

<p>高效事件风暴的规模推荐 5-8 人，超过 8 人的事件风暴就会出现讨论时间过长，部分成员参与度不高，业务之间的相关度弱等问题。在一个以支付中台为主题的事件风暴中，对于电商商城的支付与理财产品的支付相关性就很弱，各自关心的是自己的业务，让这两组人在一起讨论，在得到同样产出的情况下，会花费双倍的时间。</p>

<p>在处理复杂问题时，一个有效又好用的方法就是分而治之，对于复杂系统的事件风暴也是同样如此。在业务干系人达到一定规模后，将业务干系人分成多组，组织多轮事件风暴，迭代演进领域模型也是一种不错的选择。</p>

<p>分组的基本原则应以业务线为线索，如果目标系统的业务干系人在同一个业务主线上，每一组人代表业务主线上的一个环节 (如下图)，这种情况按照业务结点进行分组即可。对于业务相对简单的结点，可以将其与相临结点合并组织事件风暴。</p>

<p><a href="https://insights.thoughtworks.cn/wp-content/uploads/2018/09/3.png"><img src="https://insights.thoughtworks.cn/wp-content/uploads/2018/09/3.png" alt=""/></a></p>

<p>当目标系统是多条业务线上的某几个公共结点，一般业务中台会出现这种情况，如支付中台要为不同的业务部门 (保险，商城，还信用卡等) 提供支付服务，如下图中的虚线部分。这类业务往往结点之间的边界并没有那么清楚，系统做什么与不做什么只有在梳理完整条业务线才能确认下来，这种情况按每条业务线分组展开事件风暴，然后针对多组产出结果进行统一业务概念抽象，建立系统边界内的统一事件流。</p>

<p><a href="https://insights.thoughtworks.cn/wp-content/uploads/2018/09/4.png"><img src="https://insights.thoughtworks.cn/wp-content/uploads/2018/09/4.png" alt=""/></a></p>

<h3 id="toc_2">3. 业务代表或领域专家用自己的语言表达业务</h3>

<p>事件风暴的第一个环节是让参与者头脑风暴，各自找出业务干系人关注的领域事件，对于业务干系人来讲，往往不适应把自己理解的业务按领域事件的方式表达出来，他们看到一串领域事件，也不觉得这种表达方式比传统方式直观，在这种情况下，我们就需要考虑如何引导业务共同输出领域事件。留心领域专家在表达需求过程中的一些模式：</p>

<pre><code>1\. 当…
2\. 如果发生…
3\. 当…的时候请通知我
4\. 发生…时

</code></pre>

<p>通过模式中的关键字转换成领域事件，按时间顺序排序后，基于商业模式与价值定位与领域专家讨论领域事件，以统一的语言与统一的业务视角修正并验证领域事件。高质量的领域事件定义自然是清楚的，是可以找到问题域中的某个 actor 是关注它的，通过讲述领域事件是可以体现商业价值的。</p>

<h3 id="toc_3">4. 事件风暴可能识别不出来所有领域事件</h3>

<p>通过事件风暴可以快速把整个问题域主线梳理出来，这样的产出是相当的高效和有价值，但对于正在尝试用事件风暴成果代替传统交付物的组织，往往会质疑事件风暴是否可以发现所有领域事件。</p>

<p>试考虑一个投资者，为一座摩天大楼的建造提供资金，投资者未必对建造过程的细节感兴趣，材料的选择及各种工程细节会议对于建造者来说是很重要的活动，对于投资者来讲，感兴趣的是良好的投资回报，保护投资免受风险，较为务实的投资者会设立明确的里程碑，每个里程碑通过后再做下一次注资。例如，在项目开始时，提供适量资金进行建筑设计工作。当建造事宜被批准时，再为项目提供较多的资金以进行设计工作。在设计通过评审通过后，才拔给更大量的资金，以便建造者破土动工。梳理得到事件如下：</p>

<p><a href="https://insights.thoughtworks.cn/wp-content/uploads/2018/09/5.png"><img src="https://insights.thoughtworks.cn/wp-content/uploads/2018/09/5.png" alt=""/></a></p>

<p>系统建模同理，我们不关注所有事件，仅关注对干系人解决特定问题有价值的事件，并且这个特定问题应该已经在项目初期，业务愿景梳理的过程中在组织内达成了共识，就像上述投资者关注的问题一样清楚，在业务场景梳理与事件风暴的过程中，不断还原具体过程，以确保识别出的活动或事件真正可以解决业务问题。所以在事件风暴的过程中，并不需要担心是不是找出所有领域事件，只要真正解决了业务问题就好了。</p>

<p>另外，当开始采用新的方法论时，实践过程与角度都有差别，旧有体系的交付物不适用是常有的情况，重点关注的新的方法会不会以更简洁的方式解决实际问题。在存疑的风险处，活学活用新方法的交付物能够让组织更顺利的落地，当然必要的开发过程与交付物改进也是需要的，即可以更高效的完成设计工作，也能够让团队更专注在问题上。</p>

<h3 id="toc_4">总结</h3>

<p>有人说微服务的设计与拆分是一门艺术，经验性的成份占了很大比重。当我们准备基于经验来做微服务的设计决策时，结合业务愿景，找出问题域内所有业务干系人真正关心的领域事件，展开完整的事件风暴，循序渐进的让场景变得更加具体，让经验与艺术在生动的问题域之中得到最大的发挥。</p>

<p>另一方面，有效地识别领域事件，既统一了语言，又助力在模型中体现出业务价值部分，为设计关注业务价值的领域模型打下了坚实的基础。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[给你一份 Spring Boot 知识清单]]></title>
    <link href="http://panlw.github.io/15376918786410.html"/>
    <updated>2018-09-23T16:37:58+08:00</updated>
    <id>http://panlw.github.io/15376918786410.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://www.jianshu.com/p/83693d3d0a65">https://www.jianshu.com/p/83693d3d0a65</a></p>

<ol>
<li> 预警：本文非常长，建议先 mark 后看，也许是最后一次写这么长的文章</li>
<li> 说明：前面有 4 个小节关于 Spring 的基础知识，分别是：IOC 容器、JavaConfig、事件监听、SpringFactoriesLoader 详解，它们占据了本文的大部分内容，虽然它们之间可能没有太多的联系，但这些知识对于理解 Spring Boot 的核心原理至关重要，如果你对 Spring 框架烂熟于心，完全可以跳过这 4 个小节。正是因为这个系列的文章是由这些看似不相关的知识点组成，因此取名知识清单。</li>
</ol>
</blockquote>

<p>在过去两三年的 Spring 生态圈，最让人兴奋的莫过于 Spring Boot 框架。或许从命名上就能看出这个框架的设计初衷：快速的启动 Spring 应用。因而 Spring Boot 应用本质上就是一个基于 Spring 框架的应用，它是 Spring 对 “约定优先于配置” 理念的最佳实践产物，它能够帮助开发者更快速高效地构建基于 Spring 生态圈的应用。</p>

<p>那 Spring Boot 有何魔法？<strong>自动配置</strong>、<strong>起步依赖</strong>、<strong>Actuator</strong>、<strong>命令行界面 (CLI)</strong> 是 Spring Boot 最重要的 4 大核心特性，其中 CLI 是 Spring Boot 的可选特性，虽然它功能强大，但也引入了一套不太常规的开发模型，因而这个系列的文章仅关注其它 3 种特性。如文章标题，本文是这个系列的第一部分，将为你打开 Spring Boot 的大门，重点为你剖析其启动流程以及自动配置实现原理。要掌握这部分核心内容，理解一些 Spring 框架的基础知识，将会让你事半功倍。</p>

<h2 id="toc_0">一、抛砖引玉：探索 Spring IoC 容器</h2>

<p>如果有看过<code>SpringApplication.run()</code>方法的源码，Spring Boot 冗长无比的启动流程一定会让你抓狂，透过现象看本质，SpringApplication 只是将一个典型的 Spring 应用的启动流程进行了扩展，因此，透彻理解 Spring 容器是打开 Spring Boot 大门的一把钥匙。</p>

<h3 id="toc_1">1.1、Spring IoC 容器</h3>

<p>可以把 Spring IoC 容器比作一间餐馆，当你来到餐馆，通常会直接招呼服务员：点菜！至于菜的原料是什么？如何用原料把菜做出来？可能你根本就不关心。IoC 容器也是一样，你只需要告诉它需要某个 bean，它就把对应的实例（instance）扔给你，至于这个 bean 是否依赖其他组件，怎样完成它的初始化，根本就不需要你关心。</p>

<p>作为餐馆，想要做出菜肴，得知道菜的原料和菜谱，同样地，IoC 容器想要管理各个业务对象以及它们之间的依赖关系，需要通过某种途径来记录和管理这些信息。<code>BeanDefinition</code>对象就承担了这个责任：容器中的每一个 bean 都会有一个对应的 BeanDefinition 实例，该实例负责保存 bean 对象的所有必要信息，包括 bean 对象的 class 类型、是否是抽象类、构造方法和参数、其它属性等等。当客户端向容器请求相应对象时，容器就会通过这些信息为客户端返回一个完整可用的 bean 实例。</p>

<p>原材料已经准备好（把 BeanDefinition 看着原料），开始做菜吧，等等，你还需要一份菜谱，<code>BeanDefinitionRegistry</code>和<code>BeanFactory</code>就是这份菜谱，BeanDefinitionRegistry 抽象出 bean 的注册逻辑，而 BeanFactory 则抽象出了 bean 的管理逻辑，而各个 BeanFactory 的实现类就具体承担了 bean 的注册以及管理工作。它们之间的关系就如下图：</p>

<p><img src="media/15376918786410/15376919608832.png" alt=""/></p>

<p><code>DefaultListableBeanFactory</code>作为一个比较通用的 BeanFactory 实现，它同时也实现了 BeanDefinitionRegistry 接口，因此它就承担了 Bean 的注册管理工作。从图中也可以看出，BeanFactory 接口中主要包含 getBean、containBean、getType、getAliases 等管理 bean 的方法，而 BeanDefinitionRegistry 接口则包含 registerBeanDefinition、removeBeanDefinition、getBeanDefinition 等注册管理 BeanDefinition 的方法。</p>

<p>下面通过一段简单的代码来模拟 BeanFactory 底层是如何工作的：</p>

<pre><code class="language-java">// 默认容器实现
DefaultListableBeanFactory beanRegistry = new DefaultListableBeanFactory();
// 根据业务对象构造相应的BeanDefinition
AbstractBeanDefinition definition = new RootBeanDefinition(Business.class,true);
// 将bean定义注册到容器中
beanRegistry.registerBeanDefinition(&quot;beanName&quot;,definition);
// 如果有多个bean，还可以指定各个bean之间的依赖关系
// ........

// 然后可以从容器中获取这个bean的实例
// 注意：这里的beanRegistry其实实现了BeanFactory接口，所以可以强转，
// 单纯的BeanDefinitionRegistry是无法强制转换到BeanFactory类型的
BeanFactory container = (BeanFactory)beanRegistry;
Business business = (Business)container.getBean(&quot;beanName&quot;);
</code></pre>

<p>这段代码仅为了说明 BeanFactory 底层的大致工作流程，实际情况会更加复杂，比如 bean 之间的依赖关系可能定义在外部配置文件 (XML/Properties) 中、也可能是注解方式。Spring IoC 容器的整个工作流程大致可以分为两个阶段：</p>

<p>①、容器启动阶段</p>

<p>容器启动时，会通过某种途径加载<code>Configuration MetaData</code>。除了代码方式比较直接外，在大部分情况下，容器需要依赖某些工具类，比如：<code>BeanDefinitionReader</code>，BeanDefinitionReader 会对加载的<code>Configuration MetaData</code>进行解析和分析，并将分析后的信息组装为相应的 BeanDefinition，最后把这些保存了 bean 定义的 BeanDefinition，注册到相应的 BeanDefinitionRegistry，这样容器的启动工作就完成了。这个阶段主要完成一些准备性工作，更侧重于 bean 对象管理信息的收集，当然一些验证性或者辅助性的工作也在这一阶段完成。</p>

<p>来看一个简单的例子吧，过往，所有的 bean 都定义在 XML 配置文件中，下面的代码将模拟 BeanFactory 如何从配置文件中加载 bean 的定义以及依赖关系：</p>

<pre><code class="language-java">// 通常为BeanDefinitionRegistry的实现类，这里以DeFaultListabeBeanFactory为例
BeanDefinitionRegistry beanRegistry = new DefaultListableBeanFactory(); 
// XmlBeanDefinitionReader实现了BeanDefinitionReader接口，用于解析XML文件
XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReaderImpl(beanRegistry);
// 加载配置文件
beanDefinitionReader.loadBeanDefinitions(&quot;classpath:spring-bean.xml&quot;);

// 从容器中获取bean实例
BeanFactory container = (BeanFactory)beanRegistry;
Business business = (Business)container.getBean(&quot;beanName&quot;);
</code></pre>

<p>②、Bean 的实例化阶段</p>

<p>经过第一阶段，所有 bean 定义都通过 BeanDefinition 的方式注册到 BeanDefinitionRegistry 中，当某个请求通过容器的 getBean 方法请求某个对象，或者因为依赖关系容器需要隐式的调用 getBean 时，就会触发第二阶段的活动：容器会首先检查所请求的对象之前是否已经实例化完成。如果没有，则会根据注册的 BeanDefinition 所提供的信息实例化被请求对象，并为其注入依赖。当该对象装配完毕后，容器会立即将其返回给请求方法使用。</p>

<p>BeanFactory 只是 Spring IoC 容器的一种实现，如果没有特殊指定，它采用采用延迟初始化策略：只有当访问容器中的某个对象时，才对该对象进行初始化和依赖注入操作。而在实际场景下，我们更多的使用另外一种类型的容器：<code>ApplicationContext</code>，它构建在 BeanFactory 之上，属于更高级的容器，除了具有 BeanFactory 的所有能力之外，还提供对事件监听机制以及国际化的支持等。它管理的 bean，在容器启动时全部完成初始化和依赖注入操作。</p>

<h3 id="toc_2">1.2、Spring 容器扩展机制</h3>

<p>IoC 容器负责管理容器中所有 bean 的生命周期，而在 bean 生命周期的不同阶段，Spring 提供了不同的扩展点来改变 bean 的命运。在容器的启动阶段，<code>BeanFactoryPostProcessor</code>允许我们在容器实例化相应对象之前，对注册到容器的 BeanDefinition 所保存的信息做一些额外的操作，比如修改 bean 定义的某些属性或者增加其他信息等。</p>

<p>如果要自定义扩展类，通常需要实现<code>org.springframework.beans.factory.config.BeanFactoryPostProcessor</code>接口，与此同时，因为容器中可能有多个 BeanFactoryPostProcessor，可能还需要实现<code>org.springframework.core.Ordered</code>接口，以保证 BeanFactoryPostProcessor 按照顺序执行。Spring 提供了为数不多的 BeanFactoryPostProcessor 实现，我们以<code>PropertyPlaceholderConfigurer</code>来说明其大致的工作流程。</p>

<p>在 Spring 项目的 XML 配置文件中，经常可以看到许多配置项的值使用占位符，而将占位符所代表的值单独配置到独立的 properties 文件，这样可以将散落在不同 XML 文件中的配置集中管理，而且也方便运维根据不同的环境进行配置不同的值。这个非常实用的功能就是由 PropertyPlaceholderConfigurer 负责实现的。</p>

<p>根据前文，当 BeanFactory 在第一阶段加载完所有配置信息时，BeanFactory 中保存的对象的属性还是以占位符方式存在的，比如<code>${jdbc.mysql.url}</code>。当 PropertyPlaceholderConfigurer 作为 BeanFactoryPostProcessor 被应用时，它会使用 properties 配置文件中的值来替换相应的 BeanDefinition 中占位符所表示的属性值。当需要实例化 bean 时，bean 定义中的属性值就已经被替换成我们配置的值。当然其实现比上面描述的要复杂一些，这里仅说明其大致工作原理，更详细的实现可以参考其源码。</p>

<p>与之相似的，还有<code>BeanPostProcessor</code>，其存在于对象实例化阶段。跟 BeanFactoryPostProcessor 类似，它会处理容器内所有符合条件并且已经实例化后的对象。简单的对比，BeanFactoryPostProcessor 处理 bean 的定义，而 BeanPostProcessor 则处理 bean 完成实例化后的对象。BeanPostProcessor 定义了两个接口：</p>

<pre><code class="language-java">public interface BeanPostProcessor {
    // 前置处理
    Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException;
    // 后置处理
    Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException;
}

</code></pre>

<p>为了理解这两个方法执行的时机，简单的了解下 bean 的整个生命周期：</p>

<p><img src="media/15376918786410/15376920059717.png" alt=""/></p>

<p><code>postProcessBeforeInitialization()</code>方法与<code>postProcessAfterInitialization()</code>分别对应图中前置处理和后置处理两个步骤将执行的方法。这两个方法中都传入了 bean 对象实例的引用，为扩展容器的对象实例化过程提供了很大便利，在这儿几乎可以对传入的实例执行任何操作。注解、AOP 等功能的实现均大量使用了<code>BeanPostProcessor</code>，比如有一个自定义注解，你完全可以实现 BeanPostProcessor 的接口，在其中判断 bean 对象的脑袋上是否有该注解，如果有，你可以对这个 bean 实例执行任何操作，想想是不是非常的简单？</p>

<p>再来看一个更常见的例子，在 Spring 中经常能够看到各种各样的 Aware 接口，其作用就是在对象实例化完成以后将 Aware 接口定义中规定的依赖注入到当前实例中。比如最常见的<code>ApplicationContextAware</code>接口，实现了这个接口的类都可以获取到一个 ApplicationContext 对象。当容器中每个对象的实例化过程走到 BeanPostProcessor 前置处理这一步时，容器会检测到之前注册到容器的 ApplicationContextAwareProcessor，然后就会调用其 postProcessBeforeInitialization() 方法，检查并设置 Aware 相关依赖。看看代码吧，是不是很简单：</p>

<pre><code class="language-java">// 代码来自：org.springframework.context.support.ApplicationContextAwareProcessor
// 其postProcessBeforeInitialization方法调用了invokeAwareInterfaces方法
private void invokeAwareInterfaces(Object bean) {
    if (bean instanceof EnvironmentAware) {
        ((EnvironmentAware) bean).setEnvironment(this.applicationContext.getEnvironment());
    }
    if (bean instanceof ApplicationContextAware) {
        ((ApplicationContextAware) bean).setApplicationContext(this.applicationContext);
    }
    // ......
}
</code></pre>

<p>最后总结一下，本小节内容和你一起回顾了 Spring 容器的部分核心内容，限于篇幅不能写更多，但理解这部分内容，足以让您轻松理解 Spring Boot 的启动原理，如果在后续的学习过程中遇到一些晦涩难懂的知识，再回过头来看看 Spring 的核心知识，也许有意想不到的效果。也许 Spring Boot 的中文资料很少，但 Spring 的中文资料和书籍有太多太多，总有东西能给你启发。</p>

<h2 id="toc_3">二、夯实基础：JavaConfig 与常见 Annotation</h2>

<h3 id="toc_4">2.1、JavaConfig</h3>

<p>我们知道<code>bean</code>是 Spring IOC 中非常核心的概念，Spring 容器负责 bean 的生命周期的管理。在最初，Spring 使用 XML 配置文件的方式来描述 bean 的定义以及相互间的依赖关系，但随着 Spring 的发展，越来越多的人对这种方式表示不满，因为 Spring 项目的所有业务类均以 bean 的形式配置在 XML 文件中，造成了大量的 XML 文件，使项目变得复杂且难以管理。</p>

<p>后来，基于纯 Java Annotation 依赖注入框架<code>Guice</code>出世，其性能明显优于采用 XML 方式的 Spring，甚至有部分人认为，<code>Guice</code>可以完全取代 Spring（<code>Guice</code>仅是一个轻量级 IOC 框架，取代 Spring 还差的挺远）。正是这样的危机感，促使 Spring 及社区推出并持续完善了<code>JavaConfig</code>子项目，它基于 Java 代码和 Annotation 注解来描述 bean 之间的依赖绑定关系。比如，下面是使用 XML 配置方式来描述 bean 的定义：</p>

<pre><code class="language-xml">&lt;bean id=&quot;bookService&quot; class=&quot;cn.moondev.service.BookServiceImpl&quot;&gt;&lt;/bean&gt;
</code></pre>

<p>而基于 JavaConfig 的配置形式是这样的：</p>

<pre><code class="language-java">@Configuration
public class MoonBookConfiguration {

    // 任何标志了@Bean的方法，其返回值将作为一个bean注册到Spring的IOC容器中
    // 方法名默认成为该bean定义的id
    @Bean
    public BookService bookService() {
        return new BookServiceImpl();
    }
}
</code></pre>

<p>如果两个 bean 之间有依赖关系的话，在 XML 配置中应该是这样：</p>

<pre><code class="language-xml">&lt;bean id=&quot;bookService&quot; class=&quot;cn.moondev.service.BookServiceImpl&quot;&gt;
    &lt;property /&gt;
&lt;/bean&gt;

&lt;bean id=&quot;otherService&quot; class=&quot;cn.moondev.service.OtherServiceImpl&quot;&gt;
    &lt;property /&gt;
&lt;/bean&gt;

&lt;bean id=&quot;dependencyService&quot; class=&quot;DependencyServiceImpl&quot;/&gt;
</code></pre>

<p>而在 JavaConfig 中则是这样：</p>

<pre><code class="language-java">@Configuration
public class MoonBookConfiguration {

    // 如果一个bean依赖另一个bean，则直接调用对应JavaConfig类中依赖bean的创建方法即可
    // 这里直接调用dependencyService()
    @Bean
    public BookService bookService() {
        return new BookServiceImpl(dependencyService());
    }

    @Bean
    public OtherService otherService() {
        return new OtherServiceImpl(dependencyService());
    }

    @Bean
    public DependencyService dependencyService() {
        return new DependencyServiceImpl();
    }
}
</code></pre>

<p>你可能注意到这个示例中，有两个 bean 都依赖于 dependencyService，也就是说当初始化 bookService 时会调用<code>dependencyService()</code>，在初始化 otherService 时也会调用<code>dependencyService()</code>，那么问题来了？这时候 IOC 容器中是有一个 dependencyService 实例还是两个？这个问题留着大家思考吧，这里不再赘述。</p>

<h3 id="toc_5">2.2、@ComponentScan</h3>

<p><code>@ComponentScan</code>注解对应 XML 配置形式中的<code>&lt;context:component-scan&gt;</code>元素，表示启用组件扫描，Spring 会自动扫描所有通过注解配置的 bean，然后将其注册到 IOC 容器中。我们可以通过<code>basePackages</code>等属性来指定<code>@ComponentScan</code>自动扫描的范围，如果不指定，默认从声明<code>@ComponentScan</code>所在类的<code>package</code>进行扫描。正因为如此，SpringBoot 的启动类都默认在<code>src/main/java</code>下。</p>

<h3 id="toc_6">2.3、@Import</h3>

<p><code>@Import</code>注解用于导入配置类，举个简单的例子：</p>

<pre><code class="language-java">@Configuration
public class MoonBookConfiguration {
    @Bean
    public BookService bookService() {
        return new BookServiceImpl();
    }
}
</code></pre>

<p>现在有另外一个配置类，比如：<code>MoonUserConfiguration</code>，这个配置类中有一个 bean 依赖于<code>MoonBookConfiguration</code>中的 bookService，如何将这两个 bean 组合在一起？借助<code>@Import</code>即可：</p>

<pre><code class="language-java">@Configuration
// 可以同时导入多个配置类，比如：@Import({A.class,B.class})
@Import(MoonBookConfiguration.class)
public class MoonUserConfiguration {
    @Bean
    public UserService userService(BookService bookService) {
        return new BookServiceImpl(bookService);
    }
}
</code></pre>

<p>需要注意的是，在 4.2 之前，<code>@Import</code>注解只支持导入配置类，但是在 4.2 之后，它支持导入普通类，并将这个类作为一个 bean 的定义注册到 IOC 容器中。</p>

<h3 id="toc_7">2.4、@Conditional</h3>

<p><code>@Conditional</code>注解表示在满足某种条件后才初始化一个 bean 或者启用某些配置。它一般用在由<code>@Component</code>、<code>@Service</code>、<code>@Configuration</code>等注解标识的类上面，或者由<code>@Bean</code>标记的方法上。如果一个<code>@Configuration</code>类标记了<code>@Conditional</code>，则该类中所有标识了<code>@Bean</code>的方法和<code>@Import</code>注解导入的相关类将遵从这些条件。</p>

<p>在 Spring 里可以很方便的编写你自己的条件类，所要做的就是实现<code>Condition</code>接口，并覆盖它的<code>matches()</code>方法。举个例子，下面的简单条件类表示只有在<code>Classpath</code>里存在<code>JdbcTemplate</code>类时才生效：</p>

<pre><code class="language-java">public class JdbcTemplateCondition implements Condition {

    @Override
    public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata) {
        try {
        conditionContext.getClassLoader().loadClass(&quot;org.springframework.jdbc.core.JdbcTemplate&quot;);
            return true;
        } catch (ClassNotFoundException e) {
            e.printStackTrace();
        }
        return false;
    }
}
</code></pre>

<p>当你用 Java 来声明 bean 的时候，可以使用这个自定义条件类：</p>

<pre><code class="language-java">@Conditional(JdbcTemplateCondition.class)
@Service
public MyService service() {
    ......
}
</code></pre>

<p>这个例子中只有当<code>JdbcTemplateCondition</code>类的条件成立时才会创建 MyService 这个 bean。也就是说 MyService 这 bean 的创建条件是<code>classpath</code>里面包含<code>JdbcTemplate</code>，否则这个 bean 的声明就会被忽略掉。</p>

<p><code>Spring Boot</code>定义了很多有趣的条件，并把他们运用到了配置类上，这些配置类构成了<code>Spring Boot</code>的自动配置的基础。<code>Spring Boot</code>运用条件化配置的方法是：定义多个特殊的条件化注解，并将它们用到配置类上。下面列出了<code>Spring Boot</code>提供的部分条件化注解：</p>

<table>
<thead>
<tr>
<th>条件化注解</th>
<th>配置生效条件</th>
</tr>
</thead>

<tbody>
<tr>
<td>@ConditionalOnBean</td>
<td>配置了某个特定 bean</td>
</tr>
<tr>
<td>@ConditionalOnMissingBean</td>
<td>没有配置特定的 bean</td>
</tr>
<tr>
<td>@ConditionalOnClass</td>
<td>Classpath 里有指定的类</td>
</tr>
<tr>
<td>@ConditionalOnMissingClass</td>
<td>Classpath 里没有指定的类</td>
</tr>
<tr>
<td>@ConditionalOnExpression</td>
<td>给定的 Spring Expression Language 表达式计算结果为 true</td>
</tr>
<tr>
<td>@ConditionalOnJava</td>
<td>Java 的版本匹配特定指或者一个范围值</td>
</tr>
<tr>
<td>@ConditionalOnProperty</td>
<td>指定的配置属性要有一个明确的值</td>
</tr>
<tr>
<td>@ConditionalOnResource</td>
<td>Classpath 里有指定的资源</td>
</tr>
<tr>
<td>@ConditionalOnWebApplication</td>
<td>这是一个 Web 应用程序</td>
</tr>
<tr>
<td>@ConditionalOnNotWebApplication</td>
<td>这不是一个 Web 应用程序</td>
</tr>
</tbody>
</table>

<h3 id="toc_8">2.5、@ConfigurationProperties 与 @EnableConfigurationProperties</h3>

<p>当某些属性的值需要配置的时候，我们一般会在<code>application.properties</code>文件中新建配置项，然后在 bean 中使用<code>@Value</code>注解来获取配置的值，比如下面配置数据源的代码。</p>

<pre><code class="language-properties">// jdbc config
jdbc.mysql.url=jdbc:mysql://localhost:3306/sampledb
jdbc.mysql.username=root
jdbc.mysql.password=123456
......
</code></pre>

<pre><code class="language-java">// 配置数据源
@Configuration
public class HikariDataSourceConfiguration {

    @Value(&quot;jdbc.mysql.url&quot;)
    public String url;
    @Value(&quot;jdbc.mysql.username&quot;)
    public String user;
    @Value(&quot;jdbc.mysql.password&quot;)
    public String password;

    @Bean
    public HikariDataSource dataSource() {
        HikariConfig hikariConfig = new HikariConfig();
        hikariConfig.setJdbcUrl(url);
        hikariConfig.setUsername(user);
        hikariConfig.setPassword(password);
        // 省略部分代码
        return new HikariDataSource(hikariConfig);
    }
}
</code></pre>

<p>使用<code>@Value</code>注解注入的属性通常都比较简单，如果同一个配置在多个地方使用，也存在不方便维护的问题（考虑下，如果有几十个地方在使用某个配置，而现在你想改下名字，你改怎么做？）。对于更为复杂的配置，Spring Boot 提供了更优雅的实现方式，那就是<code>@ConfigurationProperties</code>注解。我们可以通过下面的方式来改写上面的代码：</p>

<pre><code class="language-java">@Component
//  还可以通过@PropertySource(&quot;classpath:jdbc.properties&quot;)来指定配置文件
@ConfigurationProperties(&quot;jdbc.mysql&quot;)
// 前缀=jdbc.mysql，会在配置文件中寻找jdbc.mysql.*的配置项
pulic class JdbcConfig {
    public String url;
    public String username;
    public String password;
}

@Configuration
public class HikariDataSourceConfiguration {

    @AutoWired
    public JdbcConfig config;

    @Bean
    public HikariDataSource dataSource() {
        HikariConfig hikariConfig = new HikariConfig();
        hikariConfig.setJdbcUrl(config.url);
        hikariConfig.setUsername(config.username);
        hikariConfig.setPassword(config.password);
        // 省略部分代码
        return new HikariDataSource(hikariConfig);
    }
}
</code></pre>

<p><code>@ConfigurationProperties</code>对于更为复杂的配置，处理起来也是得心应手，比如有如下配置文件：</p>

<pre><code class="language-properties">#App
app.menus[0].title=Home
app.menus[0].name=Home
app.menus[0].path=/
app.menus[1].title=Login
app.menus[1].name=Login
app.menus[1].path=/login

app.compiler.timeout=5
app.compiler.output-folder=/temp/

app.error=/error/
</code></pre>

<p>可以定义如下配置类来接收这些属性</p>

<pre><code class="language-java">@Component
@ConfigurationProperties(&quot;app&quot;)
public class AppProperties {

    public String error;
    public List&lt;Menu&gt; menus = new ArrayList&lt;&gt;();
    public Compiler compiler = new Compiler();

    public static class Menu {
        public String name;
        public String path;
        public String title;
    }

    public static class Compiler {
        public String timeout;
        public String outputFolder;
    }
}
</code></pre>

<p><code>@EnableConfigurationProperties</code>注解表示对<code>@ConfigurationProperties</code>的内嵌支持，默认会将对应 Properties Class 作为 bean 注入的 IOC 容器中，即在相应的 Properties 类上不用加<code>@Component</code>注解。</p>

<h2 id="toc_9">三、削铁如泥：SpringFactoriesLoader 详解</h2>

<p>JVM 提供了 3 种类加载器：<code>BootstrapClassLoader</code>、<code>ExtClassLoader</code>、<code>AppClassLoader</code>分别加载 Java 核心类库、扩展类库以及应用的类路径 (<code>CLASSPATH</code>) 下的类库。JVM 通过双亲委派模型进行类的加载，我们也可以通过继承<code>java.lang.classloader</code>实现自己的类加载器。</p>

<p>何为双亲委派模型？当一个类加载器收到类加载任务时，会先交给自己的父加载器去完成，因此最终加载任务都会传递到最顶层的 BootstrapClassLoader，只有当父加载器无法完成加载任务时，才会尝试自己来加载。</p>

<p>采用双亲委派模型的一个好处是保证使用不同类加载器最终得到的都是同一个对象，这样就可以保证 Java 核心库的类型安全，比如，加载位于 rt.jar 包中的<code>java.lang.Object</code>类，不管是哪个加载器加载这个类，最终都是委托给顶层的 BootstrapClassLoader 来加载的，这样就可以保证任何的类加载器最终得到的都是同样一个 Object 对象。查看 ClassLoader 的源码，对双亲委派模型会有更直观的认识：</p>

<pre><code class="language-java">protected Class&lt;?&gt; loadClass(String name, boolean resolve) {
    synchronized (getClassLoadingLock(name)) {
    // 首先，检查该类是否已经被加载，如果从JVM缓存中找到该类，则直接返回
    Class&lt;?&gt; c = findLoadedClass(name);
    if (c == null) {
        try {
            // 遵循双亲委派的模型，首先会通过递归从父加载器开始找，
            // 直到父类加载器是BootstrapClassLoader为止
            if (parent != null) {
                c = parent.loadClass(name, false);
            } else {
                c = findBootstrapClassOrNull(name);
            }
        } catch (ClassNotFoundException e) {}
        if (c == null) {
            // 如果还找不到，尝试通过findClass方法去寻找
            // findClass是留给开发者自己实现的，也就是说
            // 自定义类加载器时，重写此方法即可
           c = findClass(name);
        }
    }
    if (resolve) {
        resolveClass(c);
    }
    return c;
    }
}
</code></pre>

<p>但双亲委派模型并不能解决所有的类加载器问题，比如，Java 提供了很多服务提供者接口 (<code>Service Provider Interface</code>，SPI)，允许第三方为这些接口提供实现。常见的 SPI 有 JDBC、JNDI、JAXP 等，这些 SPI 的接口由核心类库提供，却由第三方实现，这样就存在一个问题：SPI 的接口是 Java 核心库的一部分，是由 BootstrapClassLoader 加载的；SPI 实现的 Java 类一般是由 AppClassLoader 来加载的。BootstrapClassLoader 是无法找到 SPI 的实现类的，因为它只加载 Java 的核心库。它也不能代理给 AppClassLoader，因为它是最顶层的类加载器。也就是说，双亲委派模型并不能解决这个问题。</p>

<p>线程上下文类加载器 (<code>ContextClassLoader</code>) 正好解决了这个问题。从名称上看，可能会误解为它是一种新的类加载器，实际上，它仅仅是 Thread 类的一个变量而已，可以通过<code>setContextClassLoader(ClassLoader cl)</code>和<code>getContextClassLoader()</code>来设置和获取该对象。如果不做任何的设置，Java 应用的线程的上下文类加载器默认就是 AppClassLoader。在核心类库使用 SPI 接口时，传递的类加载器使用线程上下文类加载器，就可以成功的加载到 SPI 实现的类。线程上下文类加载器在很多 SPI 的实现中都会用到。但在 JDBC 中，你可能会看到一种更直接的实现方式，比如，JDBC 驱动管理<code>java.sql.Driver</code>中的<code>loadInitialDrivers()</code>方法中，你可以直接看到 JDK 是如何加载驱动的：</p>

<pre><code class="language-java">for (String aDriver : driversList) {
    try {
        // 直接使用AppClassLoader
        Class.forName(aDriver, true, ClassLoader.getSystemClassLoader());
    } catch (Exception ex) {
        println(&quot;DriverManager.Initialize: load failed: &quot; + ex);
    }
}
</code></pre>

<p>其实讲解线程上下文类加载器，最主要是让大家在看到<code>Thread.currentThread().getClassLoader()</code>和<code>Thread.currentThread().getContextClassLoader()</code>时不会一脸懵逼，这两者除了在许多底层框架中取得的 ClassLoader 可能会有所不同外，其他大多数业务场景下都是一样的，大家只要知道它是为了解决什么问题而存在的即可。</p>

<p>类加载器除了加载 class 外，还有一个非常重要功能，就是加载资源，它可以从 jar 包中读取任何资源文件，比如，<code>ClassLoader.getResources(String name)</code>方法就是用于读取 jar 包中的资源文件，其代码如下：</p>

<pre><code class="language-java">public Enumeration&lt;URL&gt; getResources(String name) throws IOException {
    Enumeration&lt;URL&gt;[] tmp = (Enumeration&lt;URL&gt;[]) new Enumeration&lt;?&gt;[2];
    if (parent != null) {
        tmp[0] = parent.getResources(name);
    } else {
        tmp[0] = getBootstrapResources(name);
    }
    tmp[1] = findResources(name);
    return new CompoundEnumeration&lt;&gt;(tmp);
}
</code></pre>

<p>是不是觉得有点眼熟，不错，它的逻辑其实跟类加载的逻辑是一样的，首先判断父类加载器是否为空，不为空则委托父类加载器执行资源查找任务，直到 BootstrapClassLoader，最后才轮到自己查找。而不同的类加载器负责扫描不同路径下的 jar 包，就如同加载 class 一样，最后会扫描所有的 jar 包，找到符合条件的资源文件。</p>

<p>类加载器的<code>findResources(name)</code>方法会遍历其负责加载的所有 jar 包，找到 jar 包中名称为 name 的资源文件，这里的资源可以是任何文件，甚至是. class 文件，比如下面的示例，用于查找 Array.class 文件：</p>

<pre><code class="language-java">// 寻找Array.class文件
public static void main(String[] args) throws Exception{
    // Array.class的完整路径
    String name = &quot;java/sql/Array.class&quot;;
    Enumeration&lt;URL&gt; urls = Thread.currentThread().getContextClassLoader().getResources(name);
    while (urls.hasMoreElements()) {
        URL url = urls.nextElement();
        System.out.println(url.toString());
    }
}
</code></pre>

<p>运行后可以得到如下结果：</p>

<pre><code class="language-log">$JAVA_HOME/jre/lib/rt.jar!/java/sql/Array.class
</code></pre>

<p>根据资源文件的 URL，可以构造相应的文件来读取资源内容。</p>

<p>看到这里，你可能会感到挺奇怪的，你不是要详解<code>SpringFactoriesLoader</code>吗？上来讲了一堆 ClassLoader 是几个意思？看下它的源码你就知道了：</p>

<pre><code class="language-java">public static final String FACTORIES_RESOURCE_LOCATION = &quot;META-INF/spring.factories&quot;;
// spring.factories文件的格式为：key=value1,value2,value3
// 从所有的jar包中找到META-INF/spring.factories文件
// 然后从文件中解析出key=factoryClass类名称的所有value值
public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryClass, ClassLoader classLoader) {
    String factoryClassName = factoryClass.getName();
    // 取得资源文件的URL
    Enumeration&lt;URL&gt; urls = (classLoader != null ? classLoader.getResources(FACTORIES_RESOURCE_LOCATION) : ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION));
    List&lt;String&gt; result = new ArrayList&lt;String&gt;();
    // 遍历所有的URL
    while (urls.hasMoreElements()) {
        URL url = urls.nextElement();
        // 根据资源文件URL解析properties文件
        Properties properties = PropertiesLoaderUtils.loadProperties(new UrlResource(url));
        String factoryClassNames = properties.getProperty(factoryClassName);
        // 组装数据，并返回
        result.addAll(Arrays.asList(StringUtils.commaDelimitedListToStringArray(factoryClassNames)));
    }
    return result;
}
</code></pre>

<p>有了前面关于 ClassLoader 的知识，再来理解这段代码，是不是感觉豁然开朗：从<code>CLASSPATH</code>下的每个 Jar 包中搜寻所有<code>META-INF/spring.factories</code>配置文件，然后将解析 properties 文件，找到指定名称的配置后返回。需要注意的是，其实这里不仅仅是会去 ClassPath 路径下查找，会扫描所有路径下的 Jar 包，只不过这个文件只会在 Classpath 下的 jar 包中。来简单看下<code>spring.factories</code>文件的内容吧：</p>

<pre><code class="language-spring.factories">// 来自 org.springframework.boot.autoconfigure下的META-INF/spring.factories
// EnableAutoConfiguration后文会讲到，它用于开启Spring Boot自动配置功能
org.springframework.boot.autoconfigure.EnableAutoConfiguration=\
org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\
org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\
org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration\
</code></pre>

<p>执行<code>loadFactoryNames(EnableAutoConfiguration.class, classLoader)</code>后，得到对应的一组<code>@Configuration</code>类，<br/>
我们就可以通过反射实例化这些类然后注入到 IOC 容器中，最后容器里就有了一系列标注了<code>@Configuration</code>的 JavaConfig 形式的配置类。</p>

<p>这就是<code>SpringFactoriesLoader</code>，它本质上属于 Spring 框架私有的一种扩展方案，类似于 SPI，Spring Boot 在 Spring 基础上的很多核心功能都是基于此，希望大家可以理解。</p>

<h2 id="toc_10">四、另一件武器：Spring 容器的事件监听机制</h2>

<p>过去，事件监听机制多用于图形界面编程，比如：<strong>点击</strong>按钮、在文本框<strong>输入</strong>内容等操作被称为事件，而当事件触发时，应用程序作出一定的响应则表示应用监听了这个事件，而在服务器端，事件的监听机制更多的用于异步通知以及监控和异常处理。Java 提供了实现事件监听机制的两个基础类：自定义事件类型扩展自<code>java.util.EventObject</code>、事件的监听器扩展自<code>java.util.EventListener</code>。来看一个简单的实例：简单的监控一个方法的耗时。</p>

<p>首先定义事件类型，通常的做法是扩展 EventObject，随着事件的发生，相应的状态通常都封装在此类中：</p>

<pre><code class="language-java">public class MethodMonitorEvent extends EventObject {
    // 时间戳，用于记录方法开始执行的时间
    public long timestamp;

    public MethodMonitorEvent(Object source) {
        super(source);
    }
}
</code></pre>

<p>事件发布之后，相应的监听器即可对该类型的事件进行处理，我们可以在方法开始执行之前发布一个 begin 事件，在方法执行结束之后发布一个 end 事件，相应地，事件监听器需要提供方法对这两种情况下接收到的事件进行处理：</p>

<pre><code class="language-java">// 1、定义事件监听接口
public interface MethodMonitorEventListener extends EventListener {
    // 处理方法执行之前发布的事件
    public void onMethodBegin(MethodMonitorEvent event);
    // 处理方法结束时发布的事件
    public void onMethodEnd(MethodMonitorEvent event);
}
// 2、事件监听接口的实现：如何处理
public class AbstractMethodMonitorEventListener implements MethodMonitorEventListener {

    @Override
    public void onMethodBegin(MethodMonitorEvent event) {
        // 记录方法开始执行时的时间
        event.timestamp = System.currentTimeMillis();
    }

    @Override
    public void onMethodEnd(MethodMonitorEvent event) {
        // 计算方法耗时
        long duration = System.currentTimeMillis() - event.timestamp;
        System.out.println(&quot;耗时：&quot; + duration);
    }
}

</code></pre>

<p>事件监听器接口针对不同的事件发布实际提供相应的处理方法定义，最重要的是，其方法只接收 MethodMonitorEvent 参数，说明这个监听器类只负责监听器对应的事件并进行处理。有了事件和监听器，剩下的就是发布事件，然后让相应的监听器监听并处理。通常情况，我们会有一个事件发布者，它本身作为事件源，在合适的时机，将相应的事件发布给对应的事件监听器：</p>

<pre><code class="language-java">public class MethodMonitorEventPublisher {

    private List&lt;MethodMonitorEventListener&gt; listeners = new ArrayList&lt;MethodMonitorEventListener&gt;();

    public void methodMonitor() {
        MethodMonitorEvent eventObject = new MethodMonitorEvent(this);
        publishEvent(&quot;begin&quot;,eventObject);
        // 模拟方法执行：休眠5秒钟
        TimeUnit.SECONDS.sleep(5);
        publishEvent(&quot;end&quot;,eventObject);

    }

    private void publishEvent(String status,MethodMonitorEvent event) {
        // 避免在事件处理期间，监听器被移除，这里为了安全做一个复制操作
        List&lt;MethodMonitorEventListener&gt; copyListeners = ➥ new ArrayList&lt;MethodMonitorEventListener&gt;(listeners);
        for (MethodMonitorEventListener listener : copyListeners) {
            if (&quot;begin&quot;.equals(status)) {
                listener.onMethodBegin(event);
            } else {
                listener.onMethodEnd(event);
            }
        }
    }

    public static void main(String[] args) {
        MethodMonitorEventPublisher publisher = new MethodMonitorEventPublisher();
        publisher.addEventListener(new AbstractMethodMonitorEventListener());
        publisher.methodMonitor();
    }
    // 省略实现
    public void addEventListener(MethodMonitorEventListener listener) {}
    public void removeEventListener(MethodMonitorEventListener listener) {}
    public void removeAllListeners() {}

</code></pre>

<p>对于事件发布者（事件源）通常需要关注两点：</p>

<ol>
<li> 在合适的时机发布事件。此例中的 methodMonitor() 方法是事件发布的源头，其在方法执行之前和结束之后两个时间点发布 MethodMonitorEvent 事件，每个时间点发布的事件都会传给相应的监听器进行处理。在具体实现时需要注意的是，事件发布是顺序执行，为了不影响处理性能，事件监听器的处理逻辑应尽量简单。</li>
<li> 事件监听器的管理。publisher 类中提供了事件监听器的注册与移除方法，这样客户端可以根据实际情况决定是否需要注册新的监听器或者移除某个监听器。如果这里没有提供 remove 方法，那么注册的监听器示例将一直被 MethodMonitorEventPublisher 引用，即使已经废弃不用了，也依然在发布者的监听器列表中，这会导致隐性的内存泄漏。</li>
</ol>

<h4 id="toc_11">Spring 容器内的事件监听机制</h4>

<p>Spring 的 ApplicationContext 容器内部中的所有事件类型均继承自<code>org.springframework.context.AppliationEvent</code>，容器中的所有监听器都实现<code>org.springframework.context.ApplicationListener</code>接口，并且以 bean 的形式注册在容器中。一旦在容器内发布 ApplicationEvent 及其子类型的事件，注册到容器的 ApplicationListener 就会对这些事件进行处理。</p>

<p>你应该已经猜到是怎么回事了。</p>

<p>ApplicationEvent 继承自 EventObject，Spring 提供了一些默认的实现，比如：<code>ContextClosedEvent</code>表示容器在即将关闭时发布的事件类型，<code>ContextRefreshedEvent</code>表示容器在初始化或者刷新的时候发布的事件类型......</p>

<p>容器内部使用 ApplicationListener 作为事件监听器接口定义，它继承自 EventListener。ApplicationContext 容器在启动时，会自动识别并加载 EventListener 类型的 bean，一旦容器内有事件发布，将通知这些注册到容器的 EventListener。</p>

<p>ApplicationContext 接口继承了 ApplicationEventPublisher 接口，该接口提供了<code>void publishEvent(ApplicationEvent event)</code>方法定义，不难看出，ApplicationContext 容器担当的就是事件发布者的角色。如果有兴趣可以查看<code>AbstractApplicationContext.publishEvent(ApplicationEvent event)</code>方法的源码：ApplicationContext 将事件的发布以及监听器的管理工作委托给<code>ApplicationEventMulticaster</code>接口的实现类。在容器启动时，会检查容器内是否存在名为 applicationEventMulticaster 的 ApplicationEventMulticaster 对象实例。如果有就使用其提供的实现，没有就默认初始化一个 SimpleApplicationEventMulticaster 作为实现。</p>

<p>最后，如果我们业务需要在容器内部发布事件，只需要为其注入 ApplicationEventPublisher 依赖即可：实现 ApplicationEventPublisherAware 接口或者 ApplicationContextAware 接口 (Aware 接口相关内容请回顾上文)。</p>

<h2 id="toc_12">五、出神入化：揭秘自动配置原理</h2>

<p>典型的 Spring Boot 应用的启动类一般均位于<code>src/main/java</code>根路径下，比如<code>MoonApplication</code>类：</p>

<pre><code>@SpringBootApplication
public class MoonApplication {

    public static void main(String[] args) {
        SpringApplication.run(MoonApplication.class, args);
    }
}

</code></pre>

<p>其中<code>@SpringBootApplication</code>开启组件扫描和自动配置，而<code>SpringApplication.run</code>则负责启动引导应用程序。<code>@SpringBootApplication</code>是一个复合<code>Annotation</code>，它将三个有用的注解组合在一起：</p>

<pre><code>@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Inherited
@SpringBootConfiguration
@EnableAutoConfiguration
@ComponentScan(excludeFilters = {
        @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class),
        @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) })
public @interface SpringBootApplication {
    // ......
}

</code></pre>

<p><code>@SpringBootConfiguration</code>就是<code>@Configuration</code>，它是 Spring 框架的注解，标明该类是一个<code>JavaConfig</code>配置类。而<code>@ComponentScan</code>启用组件扫描，前文已经详细讲解过，这里着重关注<code>@EnableAutoConfiguration</code>。</p>

<p><code>@EnableAutoConfiguration</code>注解表示开启 Spring Boot 自动配置功能，Spring Boot 会根据应用的依赖、自定义的 bean、classpath 下有没有某个类 等等因素来猜测你需要的 bean，然后注册到 IOC 容器中。那<code>@EnableAutoConfiguration</code>是如何推算出你的需求？首先看下它的定义：</p>

<pre><code>@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Inherited
@AutoConfigurationPackage
@Import(EnableAutoConfigurationImportSelector.class)
public @interface EnableAutoConfiguration {
    // ......
}

</code></pre>

<p>你的关注点应该在<code>@Import(EnableAutoConfigurationImportSelector.class)</code>上了，前文说过，<code>@Import</code>注解用于导入类，并将这个类作为一个 bean 的定义注册到容器中，这里它将把<code>EnableAutoConfigurationImportSelector</code>作为 bean 注入到容器中，而这个类会将所有符合条件的 @Configuration 配置都加载到容器中，看看它的代码：</p>

<pre><code>public String[] selectImports(AnnotationMetadata annotationMetadata) {
    // 省略了大部分代码，保留一句核心代码
    // 注意：SpringBoot最近版本中，这句代码被封装在一个单独的方法中
    // SpringFactoriesLoader相关知识请参考前文
    List&lt;String&gt; factories = new ArrayList&lt;String&gt;(new LinkedHashSet&lt;String&gt;(  
        SpringFactoriesLoader.loadFactoryNames(EnableAutoConfiguration.class, this.beanClassLoader)));
}

</code></pre>

<p>这个类会扫描所有的 jar 包，将所有符合条件的 @Configuration 配置类注入的容器中，何为符合条件，看看<code>META-INF/spring.factories</code>的文件内容：</p>

<pre><code>// 来自 org.springframework.boot.autoconfigure下的META-INF/spring.factories
// 配置的key = EnableAutoConfiguration，与代码中一致
org.springframework.boot.autoconfigure.EnableAutoConfiguration=\
org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration,\
org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\
org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration\
.....

</code></pre>

<p>以<code>DataSourceAutoConfiguration</code>为例，看看 Spring Boot 是如何自动配置的：</p>

<pre><code>@Configuration
@ConditionalOnClass({ DataSource.class, EmbeddedDatabaseType.class })
@EnableConfigurationProperties(DataSourceProperties.class)
@Import({ Registrar.class, DataSourcePoolMetadataProvidersConfiguration.class })
public class DataSourceAutoConfiguration {
}

</code></pre>

<p>分别说一说：</p>

<ul>
<li>  <code>@ConditionalOnClass({ DataSource.class, EmbeddedDatabaseType.class })</code>：当 Classpath 中存在 DataSource 或者 EmbeddedDatabaseType 类时才启用这个配置，否则这个配置将被忽略。</li>
<li>  <code>@EnableConfigurationProperties(DataSourceProperties.class)</code>：将 DataSource 的默认配置类注入到 IOC 容器中，DataSourceproperties 定义为：</li>
</ul>

<pre><code>// 提供对datasource配置信息的支持，所有的配置前缀为：spring.datasource
@ConfigurationProperties(prefix = &quot;spring.datasource&quot;)
public class DataSourceProperties  {
    private ClassLoader classLoader;
    private Environment environment;
    private String name = &quot;testdb&quot;;
    ......
}

</code></pre>

<ul>
<li>  <code>@Import({ Registrar.class, DataSourcePoolMetadataProvidersConfiguration.class })</code>：导入其他额外的配置，就以<code>DataSourcePoolMetadataProvidersConfiguration</code>为例吧。</li>
</ul>

<pre><code>@Configuration
public class DataSourcePoolMetadataProvidersConfiguration {

    @Configuration
    @ConditionalOnClass(org.apache.tomcat.jdbc.pool.DataSource.class)
    static class TomcatDataSourcePoolMetadataProviderConfiguration {
        @Bean
        public DataSourcePoolMetadataProvider tomcatPoolDataSourceMetadataProvider() {
            .....
        }
    }
  ......
}

</code></pre>

<p>DataSourcePoolMetadataProvidersConfiguration 是数据库连接池提供者的一个配置类，即 Classpath 中存在<code>org.apache.tomcat.jdbc.pool.DataSource.class</code>，则使用 tomcat-jdbc 连接池，如果 Classpath 中存在<code>HikariDataSource.class</code>则使用 Hikari 连接池。</p>

<p>这里仅描述了 DataSourceAutoConfiguration 的冰山一角，但足以说明 Spring Boot 如何利用条件话配置来实现自动配置的。回顾一下，<code>@EnableAutoConfiguration</code>中导入了 EnableAutoConfigurationImportSelector 类，而这个类的<code>selectImports()</code>通过 SpringFactoriesLoader 得到了大量的配置类，而每一个配置类则根据条件化配置来做出决策，以实现自动配置。</p>

<p>整个流程很清晰，但漏了一个大问题：<code>EnableAutoConfigurationImportSelector.selectImports()</code>是何时执行的？其实这个方法会在容器启动过程中执行：<code>AbstractApplicationContext.refresh()</code>，更多的细节在下一小节中说明。</p>

<h2 id="toc_13">六、启动引导：Spring Boot 应用启动的秘密</h2>

<h3 id="toc_14">6.1 SpringApplication 初始化</h3>

<p>SpringBoot 整个启动流程分为两个步骤：初始化一个 SpringApplication 对象、执行该对象的 run 方法。看下 SpringApplication 的初始化流程，SpringApplication 的构造方法中调用 initialize(Object[] sources) 方法，其代码如下：</p>

<pre><code>private void initialize(Object[] sources) {
     if (sources != null &amp;&amp; sources.length &gt; 0) {
         this.sources.addAll(Arrays.asList(sources));
     }
     // 判断是否是Web项目
     this.webEnvironment = deduceWebEnvironment();
     setInitializers((Collection) getSpringFactoriesInstances(ApplicationContextInitializer.class));
     setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class));
     // 找到入口类
     this.mainApplicationClass = deduceMainApplicationClass();
}

</code></pre>

<p>初始化流程中最重要的就是通过 SpringFactoriesLoader 找到<code>spring.factories</code>文件中配置的<code>ApplicationContextInitializer</code>和<code>ApplicationListener</code>两个接口的实现类名称，以便后期构造相应的实例。<code>ApplicationContextInitializer</code>的主要目的是在<code>ConfigurableApplicationContext</code>做 refresh 之前，对 ConfigurableApplicationContext 实例做进一步的设置或处理。ConfigurableApplicationContext 继承自 ApplicationContext，其主要提供了对 ApplicationContext 进行设置的能力。</p>

<p>实现一个 ApplicationContextInitializer 非常简单，因为它只有一个方法，但大多数情况下我们没有必要自定义一个 ApplicationContextInitializer，即便是 Spring Boot 框架，它默认也只是注册了两个实现，毕竟 Spring 的容器已经非常成熟和稳定，你没有必要来改变它。</p>

<p>而<code>ApplicationListener</code>的目的就没什么好说的了，它是 Spring 框架对 Java 事件监听机制的一种框架实现，具体内容在前文 Spring 事件监听机制这个小节有详细讲解。这里主要说说，如果你想为 Spring Boot 应用添加监听器，该如何实现？</p>

<p>Spring Boot 提供两种方式来添加自定义监听器：</p>

<ul>
<li>  通过<code>SpringApplication.addListeners(ApplicationListener&lt;?&gt;... listeners)</code>或者<code>SpringApplication.setListeners(Collection&lt;? extends ApplicationListener&lt;?&gt;&gt; listeners)</code>两个方法来添加一个或者多个自定义监听器</li>
<li>  既然 SpringApplication 的初始化流程中已经从<code>spring.factories</code>中获取到<code>ApplicationListener</code>的实现类，那么我们直接在自己的 jar 包的<code>META-INF/spring.factories</code>文件中新增配置即可：</li>
</ul>

<pre><code>org.springframework.context.ApplicationListener=\
cn.moondev.listeners.xxxxListener\

</code></pre>

<p>关于 SpringApplication 的初始化，我们就说这么多。</p>

<h3 id="toc_15">6.2 Spring Boot 启动流程</h3>

<p>Spring Boot 应用的整个启动流程都封装在 SpringApplication.run 方法中，其整个流程真的是太长太长了，但本质上就是在 Spring 容器启动的基础上做了大量的扩展，按照这个思路来看看源码：</p>

<pre><code>public ConfigurableApplicationContext run(String... args) {
        StopWatch stopWatch = new StopWatch();
        stopWatch.start();
        ConfigurableApplicationContext context = null;
        FailureAnalyzers analyzers = null;
        configureHeadlessProperty();
        // ①
        SpringApplicationRunListeners listeners = getRunListeners(args);
        listeners.starting();
        try {
            // ②
            ApplicationArguments applicationArguments = new DefaultApplicationArguments(args);
            ConfigurableEnvironment environment = prepareEnvironment(listeners,applicationArguments);
            // ③
            Banner printedBanner = printBanner(environment);
            // ④
            context = createApplicationContext();
            // ⑤
            analyzers = new FailureAnalyzers(context);
            // ⑥
            prepareContext(context, environment, listeners, applicationArguments,printedBanner);
            // ⑦ 
            refreshContext(context);
            // ⑧
            afterRefresh(context, applicationArguments);
            // ⑨
            listeners.finished(context, null);
            stopWatch.stop();
            return context;
        }
        catch (Throwable ex) {
            handleRunFailure(context, listeners, analyzers, ex);
            throw new IllegalStateException(ex);
        }
    }

</code></pre>

<p>① 通过 SpringFactoriesLoader 查找并加载所有的<code>SpringApplicationRunListeners</code>，通过调用 starting()方法通知所有的 SpringApplicationRunListeners：应用开始启动了。SpringApplicationRunListeners 其本质上就是一个事件发布者，它在 SpringBoot 应用启动的不同时间点发布不同应用事件类型 (ApplicationEvent)，如果有哪些事件监听者(ApplicationListener) 对这些事件感兴趣，则可以接收并且处理。还记得初始化流程中，SpringApplication 加载了一系列 ApplicationListener 吗？这个启动流程中没有发现有发布事件的代码，其实都已经在 SpringApplicationRunListeners 这儿实现了。</p>

<p>简单的分析一下其实现流程，首先看下 SpringApplicationRunListener 的源码：</p>

<pre><code>public interface SpringApplicationRunListener {

    // 运行run方法时立即调用此方法，可以用户非常早期的初始化工作
    void starting();

    // Environment准备好后，并且ApplicationContext创建之前调用
    void environmentPrepared(ConfigurableEnvironment environment);

    // ApplicationContext创建好后立即调用
    void contextPrepared(ConfigurableApplicationContext context);

    // ApplicationContext加载完成，在refresh之前调用
    void contextLoaded(ConfigurableApplicationContext context);

    // 当run方法结束之前调用
    void finished(ConfigurableApplicationContext context, Throwable exception);

}

</code></pre>

<p>SpringApplicationRunListener 只有一个实现类：<code>EventPublishingRunListener</code>。①处的代码只会获取到一个 EventPublishingRunListener 的实例，我们来看看 starting() 方法的内容：</p>

<pre><code>public void starting() {
    // 发布一个ApplicationStartedEvent
    this.initialMulticaster.multicastEvent(new ApplicationStartedEvent(this.application, this.args));
}

</code></pre>

<p>顺着这个逻辑，你可以在②处的<code>prepareEnvironment()</code>方法的源码中找到<code>listeners.environmentPrepared(environment);</code>即 SpringApplicationRunListener 接口的第二个方法，那不出你所料，<code>environmentPrepared()</code>又发布了另外一个事件<code>ApplicationEnvironmentPreparedEvent</code>。接下来会发生什么，就不用我多说了吧。</p>

<p>② 创建并配置当前应用将要使用的<code>Environment</code>，Environment 用于描述应用程序当前的运行环境，其抽象了两个方面的内容：配置文件 (profile) 和属性 (properties)，开发经验丰富的同学对这两个东西一定不会陌生：不同的环境(eg：生产环境、预发布环境) 可以使用不同的配置文件，而属性则可以从配置文件、环境变量、命令行参数等来源获取。因此，当 Environment 准备好后，在整个应用的任何时候，都可以从 Environment 中获取资源。</p>

<p>总结起来，②处的两句代码，主要完成以下几件事：</p>

<ul>
<li>  判断 Environment 是否存在，不存在就创建（如果是 web 项目就创建<code>StandardServletEnvironment</code>，否则创建<code>StandardEnvironment</code>）</li>
<li>  配置 Environment：配置 profile 以及 properties</li>
<li>  调用 SpringApplicationRunListener 的<code>environmentPrepared()</code>方法，通知事件监听者：应用的 Environment 已经准备好</li>
</ul>

<p>③、SpringBoot 应用在启动时会输出这样的东西：</p>

<pre><code>  .   ____          _            __ _ _
 /\\ / ___&#39;_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | &#39;_ | &#39;_| | &#39;_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  &#39;  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::        (v1.5.6.RELEASE)

</code></pre>

<p>如果想把这个东西改成自己的涂鸦，你可以研究以下 Banner 的实现，这个任务就留给你们吧。</p>

<p>④、根据是否是 web 项目，来创建不同的 ApplicationContext 容器。</p>

<p>⑤、创建一系列<code>FailureAnalyzer</code>，创建流程依然是通过 SpringFactoriesLoader 获取到所有实现 FailureAnalyzer 接口的 class，然后在创建对应的实例。FailureAnalyzer 用于分析故障并提供相关诊断信息。</p>

<p>⑥、初始化 ApplicationContext，主要完成以下工作：</p>

<ul>
<li>  将准备好的 Environment 设置给 ApplicationContext</li>
<li>  遍历调用所有的 ApplicationContextInitializer 的<code>initialize()</code>方法来对已经创建好的 ApplicationContext 进行进一步的处理</li>
<li>  调用 SpringApplicationRunListener 的<code>contextPrepared()</code>方法，通知所有的监听者：ApplicationContext 已经准备完毕</li>
<li>  将所有的 bean 加载到容器中</li>
<li>  调用 SpringApplicationRunListener 的<code>contextLoaded()</code>方法，通知所有的监听者：ApplicationContext 已经装载完毕</li>
</ul>

<p>⑦、调用 ApplicationContext 的<code>refresh()</code>方法，完成 IoC 容器可用的最后一道工序。从名字上理解为刷新容器，那何为刷新？就是插手容器的启动，联系一下第一小节的内容。那如何刷新呢？且看下面代码：</p>

<pre><code>// 摘自refresh()方法中一句代码
invokeBeanFactoryPostProcessors(beanFactory);

</code></pre>

<p>看看这个方法的实现：</p>

<pre><code>protected void invokeBeanFactoryPostProcessors(ConfigurableListableBeanFactory beanFactory) {
    PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(beanFactory, getBeanFactoryPostProcessors());
    ......
}

</code></pre>

<p>获取到所有的<code>BeanFactoryPostProcessor</code>来对容器做一些额外的操作。BeanFactoryPostProcessor 允许我们在容器实例化相应对象之前，对注册到容器的 BeanDefinition 所保存的信息做一些额外的操作。这里的 getBeanFactoryPostProcessors() 方法可以获取到 3 个 Processor：</p>

<pre><code>ConfigurationWarningsApplicationContextInitializer$ConfigurationWarningsPostProcessor
SharedMetadataReaderFactoryContextInitializer$CachingMetadataReaderFactoryPostProcessor
ConfigFileApplicationListener$PropertySourceOrderingPostProcessor

</code></pre>

<p>不是有那么多 BeanFactoryPostProcessor 的实现类，为什么这儿只有这 3 个？因为在初始化流程获取到的各种 ApplicationContextInitializer 和 ApplicationListener 中，只有上文 3 个做了类似于如下操作：</p>

<pre><code>public void initialize(ConfigurableApplicationContext context) {
    context.addBeanFactoryPostProcessor(new ConfigurationWarningsPostProcessor(getChecks()));
}

</code></pre>

<p>然后你就可以进入到<code>PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors()</code>方法了，这个方法除了会遍历上面的 3 个 BeanFactoryPostProcessor 处理外，还会获取类型为<code>BeanDefinitionRegistryPostProcessor</code>的 bean：<code>org.springframework.context.annotation.internalConfigurationAnnotationProcessor</code>，对应的 Class 为<code>ConfigurationClassPostProcessor</code>。<code>ConfigurationClassPostProcessor</code>用于解析处理各种注解，包括：@Configuration、@ComponentScan、@Import、@PropertySource、@ImportResource、@Bean。当处理<code>@import</code>注解的时候，就会调用 &lt;自动配置&gt; 这一小节中的<code>EnableAutoConfigurationImportSelector.selectImports()</code>来完成自动配置功能。其他的这里不再多讲，如果你有兴趣，可以查阅参考资料 6。</p>

<p>⑧、查找当前 context 中是否注册有 CommandLineRunner 和 ApplicationRunner，如果有则遍历执行它们。</p>

<p>⑨、执行所有 SpringApplicationRunListener 的 finished() 方法。</p>

<p>这就是 Spring Boot 的整个启动流程，其核心就是在 Spring 容器初始化并启动的基础上加入各种扩展点，这些扩展点包括：ApplicationContextInitializer、ApplicationListener 以及各种 BeanFactoryPostProcessor 等等。你对整个流程的细节不必太过关注，甚至没弄明白也没有关系，你只要理解这些扩展点是在何时如何工作的，能让它们为你所用即可。</p>

<p>整个启动流程确实非常复杂，可以查询参考资料中的部分章节和内容，对照着源码，多看看，我想最终你都能弄清楚的。言而总之，Spring 才是核心，理解清楚 Spring 容器的启动流程，那 Spring Boot 启动流程就不在话下了。</p>

<h2 id="toc_16">参考资料</h2>

<p>[1] <a href="https://link.jianshu.com?t=http%3A%2F%2Funion-click.jd.com%2Fjdc%3Fd%3D4jESQ9">王福强 著；SpringBoot 揭秘：快速构建微服务体系; 机械工业出版社, 2016</a><br/>
[2] <a href="https://link.jianshu.com?t=http%3A%2F%2Funion-click.jd.com%2Fjdc%3Fd%3DyzfgeF">王福强 著；Spring 揭秘; 人民邮件出版社, 2009</a><br/>
[3] <a href="https://link.jianshu.com?t=http%3A%2F%2Funion-click.jd.com%2Fjdc%3Fd%3DAQ6oHO">Craig Walls 著；丁雪丰 译；Spring Boot 实战；中国工信出版集团 人民邮电出版社，2016</a><br/>
[4] <a href="https://link.jianshu.com?t=https%3A%2F%2Fwww.ibm.com%2Fdeveloperworks%2Fcn%2Fjava%2Fj-lo-classloader%2F">深入探讨 Java 类加载器</a> : <a href="https://link.jianshu.com?t=https%3A%2F%2Fwww.ibm.com%2Fdeveloperworks%2Fcn%2Fjava%2Fj-lo-classloader%2F">https://www.ibm.com/developerworks/cn/java/j-lo-classloader/</a><br/>
[5] <a href="https://link.jianshu.com?t=http%3A%2F%2Fblog.csdn.net%2Fliaokailin%2Farticle%2Fdetails%2F49559951">spring boot 实战：自动配置原理分析</a> : <a href="https://link.jianshu.com?t=http%3A%2F%2Fblog.csdn.net%2Fliaokailin%2Farticle%2Fdetails%2F49559951">http://blog.csdn.net/liaokailin/article/details/49559951</a><br/>
[6]<a href="https://link.jianshu.com?t=http%3A%2F%2Fblog.csdn.net%2Fliaokailin%2Farticle%2Fdetails%2F49107209">spring boot 实战：Spring boot Bean 加载源码分析</a>：<a href="https://link.jianshu.com?t=http%3A%2F%2Fblog.csdn.net%2Fliaokailin%2Farticle%2Fdetails%2F49107209">http://blog.csdn.net/liaokailin/article/details/49107209</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Caffeine Cache 进程缓存之王]]></title>
    <link href="http://panlw.github.io/15376909096951.html"/>
    <updated>2018-09-23T16:21:49+08:00</updated>
    <id>http://panlw.github.io/15376909096951.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://mp.weixin.qq.com/s/Tv6Vv_bILX74OtRRYM06Mg">https://mp.weixin.qq.com/s/Tv6Vv_bILX74OtRRYM06Mg</a></p>
</blockquote>

<h3 id="toc_0">前言</h3>

<p>互联网软件神速发展，用户的体验度是判断一个软件好坏的重要原因，所以缓存就是必不可少的一个神器。在多线程高并发场景中往往是离不开 cache 的，需要根据不同的应用场景来需要选择不同的 cache，比如分布式缓存如 redis、memcached，还有本地（进程内）缓存如 ehcache、GuavaCache、Caffeine。</p>

<p>说起 Guava Cache，很多人都不会陌生，它是 Google Guava 工具包中的一个非常方便易用的本地化缓存实现，基于 LRU 算法实现，支持多种缓存过期策略。由于 Guava 的大量使用，Guava Cache 也得到了大量的应用。但是，Guava Cache 的性能一定是最好的吗？也许，曾经，它的性能是非常不错的。但所谓长江后浪推前浪，总会有更加优秀的技术出现。今天，我就来介绍一个比 Guava Cache 性能更高的缓存框架：Caffeine。</p>

<h3 id="toc_1">比较</h3>

<p>Google Guava 工具包中的一个非常方便易用的本地化缓存实现，基于 LRU 算法实现，支持多种缓存过期策略。</p>

<p>EhCache 是一个纯 Java 的进程内缓存框架，具有快速、精干等特点，是 Hibernate 中默认的 CacheProvider。</p>

<p>Caffeine 是使用 Java8 对 Guava 缓存的重写版本，在 Spring Boot 2.0 中将取代，基于 LRU 算法实现，支持多种缓存过期策略。</p>

<h4 id="toc_2">官方性能比较</h4>

<p>场景 1：8 个线程读，100% 的读操作</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/7XicP8RicaB3zAgjnsVcHgfVPZdwianHm1TD7gIjo4AX5MyKPTlkMz5y7mZBfZlNH9ic9RKickZic9weiceOGIZrwK1cw/640?wx_fmt=jpeg" alt=""/></p>

<p>场景二：6 个线程读，2 个线程写，也就是 75% 的读操作，25% 的写操作</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/7XicP8RicaB3zAgjnsVcHgfVPZdwianHm1TvibAnicLNEdiay37aXrRlgRibq1pk1MTxWetV0O2AfzJ0B2apWg17OcUIA/640?wx_fmt=jpeg" alt=""/></p>

<p>场景三：8 个线程写，100% 的写操作</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/7XicP8RicaB3zAgjnsVcHgfVPZdwianHm1Teb0NvHsZEHuBrWLte8sgMNLoUZrPia3jx8MzGOco5U68k24dfHxbx2w/640?wx_fmt=jpeg" alt=""/></p>

<p>可以清楚的看到 Caffeine 效率明显的高于其他缓存。</p>

<h3 id="toc_3">如何使用</h3>

<pre><code class="language-java">public static void main(String[] args) {
    LoadingCache&lt;String, String&gt; build = CacheBuilder.newBuilder().initialCapacity().maximumSize().expireAfterWrite(, TimeUnit.DAYS)
            .build(new CacheLoader&lt;String, String&gt;() {
                //默认的数据加载实现，当调用get取值的时候，如果key没有对应的值，就调用这个方法进行加载
                @Override
                public String load(String key)  {
                    return &quot;&quot;;
                }
            });
    }
}
</code></pre>

<h4 id="toc_4">参数方法</h4>

<ul>
<li>  initialCapacity(1) 初始缓存长度为 1</li>
<li>  maximumSize(100) 最大长度为 100</li>
<li>  expireAfterWrite(1, TimeUnit.DAYS) 设置缓存策略在 1 天未写入过期缓存（后面讲缓存策略）</li>
</ul>

<h3 id="toc_5">过期策略</h3>

<p>在 Caffeine 中分为两种缓存，一个是有界缓存，一个是无界缓存，无界缓存不需要过期并且没有界限。在有界缓存中提供了三个过期 API:<br/>
expireAfterWrite：代表着写了之后多久过期。（上面列子就是这种方式）<br/>
expireAfterAccess: 代表着最后一次访问了之后多久过期。<br/>
expireAfter: 在 expireAfter 中需要自己实现 Expiry 接口，这个接口支持 create,update, 以及 access 了之后多久过期。注意这个 API 和前面两个 API 是互斥的。这里和前面两个 API 不同的是，需要你告诉缓存框架，他应该在具体的某个时间过期，也就是通过前面的重写 create,update, 以及 access 的方法，获取具体的过期时间。</p>

<h3 id="toc_6">更新策略</h3>

<p>何为更新策略？就是在设定多长时间后会自动刷新缓存。<br/>
Caffeine 提供了 refreshAfterWrite() 方法来让我们进行写后多久更新策略:</p>

<pre><code class="language-java">    LoadingCache&lt;String, String&gt; build = CacheBuilder.newBuilder().refreshAfterWrite(1, TimeUnit.DAYS)
            .build(new CacheLoader&lt;String, String&gt;() {
                @Override
                public String load(String key)  {
                    return &quot;&quot;;
                }
            });
</code></pre>

<p>上面的代码我们需要建立一个 CacheLodaer 来进行刷新, 这里是同步进行的，可以通过 buildAsync 方法进行异步构建。在实际业务中这里可以把我们代码中的 mapper 传入进去，进行数据源的刷新。</p>

<p>但是实际使用中，你设置了一天刷新，但是一天后你发现缓存并没有刷新。这是因为必有在 1 天后这个缓存再次访问才能刷新，如果没人访问，那么永远也不会刷新。你明白了吗？</p>

<p>我们来看看自动刷新他是怎么做的呢？自动刷新只存在读操作之后，也就是我们 afterRead() 这个方法，其中有个方法叫 refreshIfNeeded，他会根据你是同步还是异步然后进行刷新处理。</p>

<h3 id="toc_7">填充策略（Population）</h3>

<p>Caffeine 为我们提供了三种填充策略：手动、同步和异步</p>

<h4 id="toc_8">手动加载（Manual）</h4>

<pre><code class="language-java">Cache&lt;String, Object&gt; manualCache = Caffeine.newBuilder()
        .expireAfterWrite(, TimeUnit.MINUTES)
        .maximumSize(_)
        .build();

String key = &quot;name&quot;;
// 根据key查询一个缓存，如果没有返回NULL
graph = manualCache.getIfPresent(key);
// 根据Key查询一个缓存，如果没有调用createExpensiveGraph方法，并将返回值保存到缓存。
// 如果该方法返回Null则manualCache.get返回null，如果该方法抛出异常则manualCache.get抛出异常
graph = manualCache.get(key, k -&gt; createExpensiveGraph(k));
// 将一个值放入缓存，如果以前有值就覆盖以前的值
manualCache.put(key, graph);
// 删除一个缓存
manualCache.invalidate(key);

ConcurrentMap&lt;String, Object&gt; map = manualCache.asMap();
cache.invalidate(key);
</code></pre>

<p>Cache 接口允许显式的去控制缓存的检索，更新和删除。我们可以通过 cache.getIfPresent(key) 方法来获取一个 key 的值，通过 cache.put(key, value) 方法显示的将数控放入缓存，但是这样子会覆盖缓原来 key 的数据。更加建议使用 cache.get(key，k - &gt; value) 的方式，get 方法将一个参数为 key 的 Function (createExpensiveGraph) 作为参数传入。如果缓存中不存在该键，则调用这个 Function 函数，并将返回值作为该缓存的值插入缓存中。get 方法是以阻塞方式执行调用，即使多个线程同时请求该值也只会调用一次 Function 方法。这样可以避免与其他线程的写入竞争，这也是为什么使用 get 优于 getIfPresent 的原因。</p>

<p>注意：如果调用该方法返回 NULL（如上面的 createExpensiveGraph 方法），则 cache.get 返回 null，如果调用该方法抛出异常，则 get 方法也会抛出异常。</p>

<p>可以使用 Cache.asMap() 方法获取 ConcurrentMap 进而对缓存进行一些更改。</p>

<h4 id="toc_9">同步加载（Loading）</h4>

<pre><code class="language-java">LoadingCache&lt;String, Object&gt; loadingCache = Caffeine.newBuilder()
        .maximumSize(_)
        .expireAfterWrite(, TimeUnit.MINUTES)
        .build(key -&gt; createExpensiveGraph(key));

String key = &quot;name&quot;;
// 采用同步方式去获取一个缓存和上面的手动方式是一个原理。在build Cache的时候会提供一个createExpensiveGraph函数。
// 查询并在缺失的情况下使用同步的方式来构建一个缓存
Object graph = loadingCache.get(key);

// 获取组key的值返回一个Map
List&lt;String&gt; keys = new ArrayList&lt;&gt;();
keys.add(key);
Map&lt;String, Object&gt; graphs = loadingCache.getAll(keys);
</code></pre>

<p>LoadingCache 是使用 CacheLoader 来构建的缓存的值。批量查找可以使用 getAll 方法。默认情况下，getAll 将会对缓存中没有值的 key 分别调用 CacheLoader.load 方法来构建缓存的值。我们可以重写 CacheLoader.loadAll 方法来提高 getAll 的效率。</p>

<p>注意：您可以编写一个 CacheLoader.loadAll 来实现为特别请求的 key 加载值。例如，如果计算某个组中的任何键的值将为该组中的所有键提供值，则 loadAll 可能会同时加载该组的其余部分。</p>

<h4 id="toc_10">异步加载（Asynchronously Loading）</h4>

<pre><code class="language-java">AsyncLoadingCache&lt;String, Object&gt; asyncLoadingCache = Caffeine.newBuilder()
            .maximumSize(_)
            .expireAfterWrite(, TimeUnit.MINUTES)
            // Either: Build with a synchronous computation that is wrapped as asynchronous
            .buildAsync(key -&gt; createExpensiveGraph(key));
            // Or: Build with a asynchronous computation that returns a future
            // .buildAsync((key, executor) -&gt; createExpensiveGraphAsync(key, executor));

 String key = &quot;name&quot;;

// 查询并在缺失的情况下使用异步的方式来构建缓存
CompletableFuture&lt;Object&gt; graph = asyncLoadingCache.get(key);
// 查询一组缓存并在缺失的情况下使用异步的方式来构建缓存
List&lt;String&gt; keys = new ArrayList&lt;&gt;();
keys.add(key);
CompletableFuture&lt;Map&lt;String, Object&gt;&gt; graphs = asyncLoadingCache.getAll(keys);
// 异步转同步
loadingCache = asyncLoadingCache.synchronous();
</code></pre>

<p>AsyncLoadingCache 是继承自 LoadingCache 类的，异步加载使用 Executor 去调用方法并返回一个 CompletableFuture。异步加载缓存使用了响应式编程模型。</p>

<p>如果要以同步方式调用时，应提供 CacheLoader。要以异步表示时，应该提供一个 AsyncCacheLoader，并返回一个 CompletableFuture。</p>

<p>synchronous() 这个方法返回了一个 LoadingCacheView 视图，LoadingCacheView 也继承自 LoadingCache。调用该方法后就相当于你将一个异步加载的缓存 AsyncLoadingCache 转换成了一个同步加载的缓存 LoadingCache。</p>

<p>默认使用 ForkJoinPool.commonPool() 来执行异步线程，但是我们可以通过 Caffeine.executor(Executor) 方法来替换线程池。</p>

<h3 id="toc_11">驱逐策略（eviction）</h3>

<p>Caffeine 提供三类驱逐策略：基于大小（size-based），基于时间（time-based）和基于引用（reference-based）。</p>

<h4 id="toc_12">基于大小（size-based）</h4>

<p>基于大小驱逐，有两种方式：一种是基于缓存大小，一种是基于权重。</p>

<pre><code class="language-java">// Evict based on the number of entries in the cache
// 根据缓存的计数进行驱逐
LoadingCache&lt;Key, Graph&gt; graphs = Caffeine.newBuilder()
    .maximumSize(_)
    .build(key -&gt; createExpensiveGraph(key));

// Evict based on the number of vertices in the cache
// 根据缓存的权重来进行驱逐（权重只是用于确定缓存大小，不会用于决定该缓存是否被驱逐）
LoadingCache&lt;Key, Graph&gt; graphs = Caffeine.newBuilder()
    .maximumWeight(_)
    .weigher((Key key, Graph graph) -&gt; graph.vertices().size())
    .build(key -&gt; createExpensiveGraph(key));
</code></pre>

<p>我们可以使用 Caffeine.maximumSize(long) 方法来指定缓存的最大容量。当缓存超出这个容量的时候，会使用 Window TinyLfu 策略来删除缓存。我们也可以使用权重的策略来进行驱逐，可以使用 Caffeine.weigher(Weigher) 函数来指定权重，使用 Caffeine.maximumWeight(long) 函数来指定缓存最大权重值。</p>

<p>注意：maximumWeight 与 maximumSize 不可以同时使用。</p>

<h4 id="toc_13">基于时间（Time-based）</h4>

<pre><code class="language-java">// Evict based on a fixed expiration policy
// 基于固定的到期策略进行退出
LoadingCache&lt;Key, Graph&gt; graphs = Caffeine.newBuilder()
    .expireAfterAccess(, TimeUnit.MINUTES)
    .build(key -&gt; createExpensiveGraph(key));
LoadingCache&lt;Key, Graph&gt; graphs = Caffeine.newBuilder()
    .expireAfterWrite(, TimeUnit.MINUTES)
    .build(key -&gt; createExpensiveGraph(key));

// Evict based on a varying expiration policy
// 基于不同的到期策略进行退出
LoadingCache&lt;Key, Graph&gt; graphs = Caffeine.newBuilder()
    .expireAfter(new Expiry&lt;Key, Graph&gt;() {
      @Override
      public long expireAfterCreate(Key key, Graph graph, long currentTime) {
        // Use wall clock time, rather than nanotime, if from an external resource
        long seconds = graph.creationDate().plusHours()
            .minus(System.currentTimeMillis(), MILLIS)
            .toEpochSecond();
        return TimeUnit.SECONDS.toNanos(seconds);
      }

      @Override
      public long expireAfterUpdate(Key key, Graph graph, 
          long currentTime, long currentDuration) {
        return currentDuration;
      }

      @Override
      public long expireAfterRead(Key key, Graph graph,
          long currentTime, long currentDuration) {
        return currentDuration;
      }
    })
    .build(key -&gt; createExpensiveGraph(key));
</code></pre>

<h4 id="toc_14">基于引用（reference-based）</h4>

<p>强引用，软引用，弱引用概念说明请点击连接，这里说一下各各引用的区别：</p>

<p>Java4 种引用的级别由高到低依次为：强引用 &gt; 软引用 &gt; 弱引用 &gt; 虚引用</p>

<p><img src="media/15376909096951/15376914545482.jpg" alt=""/></p>

<pre><code class="language-java">// Evict when neither the key nor value are strongly reachable
// 当key和value都没有引用时驱逐缓存
LoadingCache&lt;Key, Graph&gt; graphs = Caffeine.newBuilder()
    .weakKeys()
    .weakValues()
    .build(key -&gt; createExpensiveGraph(key));

// Evict when the garbage collector needs to free memory
// 当垃圾收集器需要释放内存时驱逐
LoadingCache&lt;Key, Graph&gt; graphs = Caffeine.newBuilder()
    .softValues()
    .build(key -&gt; createExpensiveGraph(key));
</code></pre>

<p>我们可以将缓存的驱逐配置成基于垃圾回收器。为此，我们可以将 key 和 value 配置为弱引用或只将值配置成软引用。</p>

<p>注意：AsyncLoadingCache 不支持弱引用和软引用。</p>

<h3 id="toc_15">移除监听器（Removal）</h3>

<h4 id="toc_16">概念：</h4>

<p>驱逐（eviction）：由于满足了某种驱逐策略，后台自动进行的删除操作<br/>
无效（invalidation）：表示由调用方手动删除缓存<br/>
移除（removal）：监听驱逐或无效操作的监听器<br/>
手动删除缓存：</p>

<p>在任何时候，您都可能明确地使缓存无效，而不用等待缓存被驱逐。</p>

<pre><code class="language-java">// individual key
cache.invalidate(key)
// bulk keys
cache.invalidateAll(keys)
// all keys
cache.invalidateAll()
</code></pre>

<h4 id="toc_17">Removal 监听器：</h4>

<pre><code class="language-java">Cache&lt;Key, Graph&gt; graphs = Caffeine.newBuilder()
    .removalListener((Key key, Graph graph, RemovalCause cause) -&gt;
        System.out.printf(&quot;Key %s was removed (%s)%n&quot;, key, cause))
    .build();
</code></pre>

<p>您可以通过 Caffeine.removalListener(RemovalListener) 为缓存指定一个删除侦听器，以便在删除数据时执行某些操作。 RemovalListener 可以获取到 key、value 和 RemovalCause（删除的原因）。</p>

<p>删除侦听器的里面的操作是使用 Executor 来异步执行的。默认执行程序是 ForkJoinPool.commonPool()，可以通过 Caffeine.executor(Executor) 覆盖。当操作必须与删除同步执行时，请改为使用 CacheWrite，CacheWrite 将在下面说明。</p>

<p>注意：由 RemovalListener 抛出的任何异常都会被记录（使用 Logger）并不会抛出。</p>

<h3 id="toc_18">统计（Statistics）</h3>

<pre><code class="language-java">Cache&lt;Key, Graph&gt; graphs = Caffeine.newBuilder()
    .maximumSize(_)
    .recordStats()
    .build();
</code></pre>

<p>使用 Caffeine.recordStats()，您可以打开统计信息收集。Cache.stats() 方法返回提供统计信息的 CacheStats，如：</p>

<ul>
<li>  hitRate()：返回命中与请求的比率</li>
<li>  hitCount(): 返回命中缓存的总数</li>
<li>  evictionCount()：缓存逐出的数量</li>
<li>  averageLoadPenalty()：加载新值所花费的平均时间</li>
</ul>

<h3 id="toc_19">总结</h3>

<p>Caffeine 的调整不只有算法上面的调整，内存方面的优化也有很大进步，Caffeine 的 API 的操作功能和 Guava 是基本保持一致的，并且 Caffeine 为了兼容之前是 Guava 的用户，所以使用或者重写缓存到 Caffeine 应该没什么问题，但是也要看项目情况，不要盲目使用。 </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[New Architecture of OAuth 2.0 and OpenID Connect Implementation]]></title>
    <link href="http://panlw.github.io/15374959996961.html"/>
    <updated>2018-09-21T10:13:19+08:00</updated>
    <id>http://panlw.github.io/15374959996961.html</id>
    <content type="html"><![CDATA[
<p><a href="https://medium.com/@darutk?source=post_header_lockup">Takahiko Kawasaki</a> Oct 23, 2017</p>

<blockquote>
<p><a href="https://medium.com/@darutk/new-architecture-of-oauth-2-0-and-openid-connect-implementation-18f408f9338d">https://medium.com/@darutk/new-architecture-of-oauth-2-0-and-openid-connect-implementation-18f408f9338d</a></p>
</blockquote>

<h2 id="toc_0">~Semi-Hosted Service Pattern~</h2>

<h3 id="toc_1">1. Semi-Hosted Service Pattern</h3>

<p>This article describes details about a new architecture of OAuth 2.0 and OpenID Connect implementation which is categorized as &quot;Semi-Hosted Service&quot; pattern in <u>“</u><a href="https://medium.com/@justinsecurity/deployment-and-hosting-patterns-in-oauth-a1666dc0d966">_Deployment and Hosting Patterns in OAuth_</a>_”_.</p>

<p>In the pattern, a frontend server (an authorization server and an OpenID provider) utilizes a backend service which provides APIs to help the frontend server implement OAuth 2.0 and OpenID Connect. <a href="https://www.authlete.com/"><strong>Authlete</strong></a> is a real-world example of such backend services. The figure below illustrates the relationship between a frontend server and a backend service (Authlete).</p>

<p><img src="media/15374959996961/15374962758065.png" alt=""/></p>

<p>The primary advantage of this architecture is in that <strong>the backend service can focus on implementing OAuth 2.0 and OpenID Connect</strong> without caring about other components such as identity management, user authentication, login session management, API management and fraud detection. And, consequently, it leads to another major advantage which enables the backend service (implementation of OAuth 2.0 and OpenID Connect) to be <strong>combined with any solution</strong> of other components and thus gives flexibility to frontend server implementations.</p>

<h3 id="toc_2">2. User Authentication</h3>

<p>Although <a href="https://tools.ietf.org/html/rfc6749">RFC 6749</a> (The OAuth 2.0 Authorization Framework) explicitly states as follows:</p>

<blockquote>
<p>The way in which the authorization server <strong>authenticates the resource owner</strong> (e.g., username and password login, session cookies) is <strong>beyond the scope of this specification</strong>.</p>
</blockquote>

<p>, most implementations provide both user authentication and authorization combinedly as a package solution because user authentication is included as a step in authorization process as illustrated below (see <u>“3. Authentication and Authorization”</u> in <u>“</u><a href="https://medium.com/@darutk/full-scratch-implementor-of-oauth-and-openid-connect-talks-about-findings-55015f36d1c3">_Full-Scratch Implementor of OAuth and OpenID Connect Talks About Findings_</a>_”_ for details). To put the other way around, rather, it&#39;s because it is difficult to separate user authentication from OAuth 2.0 and OpenID Connect implementation.</p>

<p><img src="media/15374959996961/15374962865273.png" alt=""/></p>

<p>Such package solutions often offer mechanisms to customize user authentication process (e.g. editable authorization page, common interface over underlying identity management system, hooks in page transitions). However, this approach makes it difficult to adopt a new user authentication mechanism whose flow is considerably different from the ones assumed by the package solutions.</p>

<p>On the other hand, Authlete has adopted the semi-hosted service pattern in order to eliminate the need itself to abstract the way of user authentication. Authlete requires only a result of user authentication and does not care about how the result has been obtained. Consequently, Authlete can be combined with any user authentication solution.</p>

<p>You may wonder what is a result of user authentication. Regardless of how a user is authenticated (e.g. by ID and password, fingerprint, iris, hardware token, random table, and whatever), from a technical point of view, <strong>user authentication is a process to identify a unique user identifier</strong>. That is, a result of user authentication is a user ID.</p>

<h4 id="toc_3">2.1. How to Push Out User Authentication?</h4>

<p>The figure below is a diagram of the <strong>Authorization Code Flow</strong> defined in <u>“</u><a href="https://tools.ietf.org/html/rfc6749#section-4.1">_4.1. Authorization Code Grant_</a>_”_ in RFC 6749. (You can find the same figure in <u>“</u><a href="https://medium.com/@darutk/diagrams-and-movies-of-all-the-oauth-2-0-flows-194f3c3ade85">_Diagrams And Movies Of All The OAuth 2.0 Flows_</a>_”_.)</p>

<p><img src="media/15374959996961/15374962999321.png" alt=""/></p>

<p>The client application makes an authorization request in the step (2), and the authorization server returns an authorization code in the step (6). User authentication is performed in between (2) and (6). User authentication, however, may be omitted if the user has already been authenticated. In either case, the authorization server has to obtain the user ID <u>before</u> issuing an authorization code because the user ID has to be associated with the authorization code.</p>

<p>To push out user authentication completely from the implementation of OAuth 2.0 and OpenID Connect, Authlete has divided the authorization flow into the following three parts:</p>

<p><u>(a) Processing the authorization request</u></p>

<p><u>(b) Authenticating the user</u></p>

<p><u>(c) Making the authorization response</u></p>

<p>and provides two separate APIs for (a) and (c) only. Authlete does nothing for (b) and leaves it to customers.</p>

<p>The point is in that the API for (c) requires a result of (b). In other words, API callers must pass a unique user identifier to the API.</p>

<p>The figure below illustrates how a frontend server and a backend service (Authlete) work together. You can see that user authentication is performed at the frontend server in the step (11) and that the user ID is passed to an Authlete&#39;s API (<a href="https://www.authlete.com/documents/apis/reference#auth_authorization_issue">/api/auth/authorization/issue</a>) in the step (12).</p>

<p><img src="media/15374959996961/15374963122944.png" alt=""/><br/>
Authorization Code Flow + AUTHLETE</p>

<h4 id="toc_4">2.2. Other OAuth flows + Authlete</h4>

<p>Just for references.</p>

<p><img src="media/15374959996961/15374963469412.jpg" alt=""/><br/>
Implicit Flow + AUTHLETE<br/>
<img src="media/15374959996961/15374963577879.jpg" alt=""/><br/>
Resource Owner Password Credentials Flow + AUTHLETE<br/>
<img src="media/15374959996961/15374963816512.jpg" alt=""/><br/>
Client Credentials Flow + AUTHLETE<br/>
<img src="media/15374959996961/15374963937800.jpg" alt=""/>Refresh Token Flow + AUTHLETE</p>

<h3 id="toc_5">3. API Management</h3>

<p>The growth of the API economy has attracted many companies into the market of <strong>API management</strong>. Most solutions in the market provide some mechanisms to protect APIs, including protection by OAuth 2.0 access tokens.</p>

<p>Instead of incorporating functionality of OAuth 2.0 and OpenID Connect in a complicated manner, some API management solutions have decided to delegate the functionality to external third-party solutions.</p>

<h4 id="toc_6">3.1. AWS API Gateway</h4>

<p>One example is <a href="https://aws.amazon.com/api-gateway/">AWS API Gateway</a>. It provides a mechanism to delegate validation of bearer tokens (such as OAuth 2.0 tokens) presented by client applications to an external authorizer. The figure below excerpted from <u>“</u><a href="http://docs.aws.amazon.com/apigateway/latest/developerguide/use-custom-authorizer.html">_Enable Amazon API Gateway Custom Authorization_</a>_”_ illustrates the mechanism.</p>

<p><img src="media/15374959996961/15374964185785.jpg" alt=""/><br/>
Custom Authorizer; mechanism to delegate validation of bearer tokens</p>

<p>“Lambda Auth function” at the top position in the figure is an authorizer. The implementation of the function receives bearer tokens from API Gateway, validates them, and returns the result of the validation to API Gateway. Based on the result, API Gateway determines whether to accept the request from the client or reject it.</p>

<p>The implementation of the function itself may in turn delegate the validation to an external authorizer. The figure below is an example which uses Authlete as the external authorizer. Technical details about this are written in <u>“</u><a href="https://www.authlete.com/documents/article/custom_authorizer">_Amazon API Gateway + AWS Lambda + OAuth_</a>_”_.</p>

<p><img src="media/15374959996961/15374964288998.jpg" alt=""/><br/>
Custom Authorizer using Authlete</p>

<h4 id="toc_7">3.2. IBM API Connect</h4>

<p><a href="https://www.ibm.com/software/products/api-connect">IBM API Connect</a> is another example. It has OAuth implementation, but at the same time, it can delegate validation of access tokens to an external authorization server if the server supports <a href="https://tools.ietf.org/html/rfc7662">RFC 7662</a> (OAuth 2.0 Token Introspection). Details are written in <u>“</u><a href="https://www.ibm.com/support/knowledgecenter/en/SSMNED_5.0.0/com.ibm.apic.toolkit.doc/con_oauth_introspection.html">_Integrating third party OAuth provider_</a>_”_ (in IBM Knowledge Center).</p>

<p><strong>Note for developers:</strong></p>

<p>APIs built using IBM API Connect require a custom HTTP header, <code>X-IBM-Client-Id</code>, in addition to <code>Authorization</code> header which includes an access token in the way defined in <u>“</u><a href="https://tools.ietf.org/html/rfc6750#section-2.1">_2.1. Authorization Request Header Field_</a>_”_ in <a href="https://tools.ietf.org/html/rfc6750">RFC 6750</a>. The custom header is required even if access token validation is delegated to a third-party authorization server.</p>

<p>The following is the command line excerpted from <u>“</u><a href="https://www.ibm.com/support/knowledgecenter/SSFS6T/com.ibm.apic.toolkit.doc/tutorial_apionprem_security_OAuth.html#OAuth_Tute__Using_Tokens">_Using the access token_</a>_”_ (in <u>“</u><a href="https://www.ibm.com/support/knowledgecenter/SSFS6T/com.ibm.apic.toolkit.doc/tutorial_apionprem_security_OAuth.html">_Tutorial: Securing an API by using OAuth 2.0_</a>_”_ in IBM Knowledge Center) (with extra line breaks added for display purpose only).</p>

<pre><code>curl -k -v
</code></pre>

<p>A certain major bank in Japan has adopted IBM&#39;s solution for its bank API, and now <code>X-IBM-Client-Id</code> is a part of the bank&#39;s official API specification (<a href="https://developer.portal.bk.mufg.jp/node/1947">example</a>).</p>

<h3 id="toc_8">4. Login Session Management</h3>

<p>Before an OpenID provider issues an ID token, it has to authenticate the user. However, user authentication may be skipped if the user has already logged in the server.</p>

<p>In the semi-hosted service pattern, login session management is handled by the frontend server (OpenID provider), and the backend service does nothing for it. Because login session management is separated from OAuth and OpenID Connect implementation, developers can choose any solution for login session management (e.g. <a href="https://shiro.apache.org/index.html">Apache Shiro</a>) as they like.</p>

<p><a href="https://github.com/authlete/java-oauth-server">java-oauth-server</a> is a good example that demonstrates login session management can be handled only in the frontend server. The open-source software is an implementation of authorization server and OpenID provider written in Java. It uses Authlete as the backend service.</p>

<p>When we ran the <a href="http://openid.net/certification/">OpenID Certification</a> test for java-oauth-server for the first time, the test reported some errors related to login session management. We could solve the errors by adding login session management to java-oauth-server. The point is that we didn&#39;t have to change any code of the backend service (Authlete) to solve the errors. This has proved login session management can be implemented in the frontend server independently of the backend service.</p>

<h3 id="toc_9">5. Identity Management</h3>

<p><strong>Authorization</strong> in the context of <strong>identity management</strong> and authorization in the context of <strong>OAuth</strong> are different. In the former context, authorization means <strong>_“who has what permissions”_</strong>. In the latter context, authorization means <strong>_“who grants what permissions to whom”_</strong>. They are different but in some cases you have to handle both simultaneously. <a href="https://stackoverflow.com/q/34377012/1174054">This question</a> (_“How to verify which resources each user can access with OAuth and OpenID Connect?”_) and <a href="https://stackoverflow.com/a/34378565/1174054">this answer</a> in Stack Overflow show one of such use cases.</p>

<p>Some identity management solutions support authorization in the context of OAuth (which may make people confused). However, the semi-hosted service pattern removes the need itself for identity management solutions to support OAuth.</p>

<h4 id="toc_10">5.1. Shared User Database</h4>

<p>Suppose there is a system for music service. If we develop APIs of the system with an authorization server which is tightly combined with identity management, the system will look like the figure below. The authorization server holds both a user database and an authorization database.</p>

<p><img src="media/15374959996961/15374964425934.jpg" alt=""/><br/>
An authorization server combined tightly with identity management</p>

<p>If the company running the music service expands its business and starts a healthcare service and a travel service, and if APIs of the new services are built on top of the existing system, the authorization server is shared as illustrated below even though API servers are prepared independently.</p>

<p><img src="media/15374959996961/15374964538750.jpg" alt=""/><br/>
Multiple services share one authorization server</p>

<p>Sharing an authorization server among services means that scopes (permissions) and client applications of the services are managed at one place. For example, a permission to create play lists (music service), a permission to refer to the record of body weight (healthcare service) and a permission to reserve hotels (travel service) are managed at one place.</p>

<p>Because each service usually has a different development team, a different schedule, a different target for API exposure and different client applications, it is desirable to make each service have its own authorization server. At the same time, it is also desirable to share the user pool among services. However, if an authorization server is tightly combined with identity management, it is difficult to have multiple authorization servers for multiple services which share the same user pool.</p>

<p>On the contrary, what if there exists an authorization server which is not tied to identity management? If you have such an authorization server, you can build a system where each service has its own authorization server but shares the same user pool with other services. The semi-hosted service pattern which clearly separates authorization from identity management enables you to adopt such a system architecture.</p>

<p><img src="media/15374959996961/15374964737117.jpg" alt=""/><br/>
Each service has an authorization server but shares one user pool with other services</p>

<h4 id="toc_11">5.2. Multiple Authorization Servers</h4>

<p>Even if the architecture where each service can have its authorization server is beautiful, if it requires many man-hours to develop one authorization server, it is difficult to adopt the architecture. But, it is almost okay to expect that implementations (such as Authlete) which by design take multiple authorization servers into consideration offer a mechanism to easily create and delete authorization server instances.</p>

<p>As an example, the figure below illustrates the steps to create a new instance of authorization server / OpenID provider in Authlete&#39;s web console (<a href="https://www.authlete.com/documents/so_console">Service Owner Console</a>). Just three clicks. If it is easy to create an authorization server instance like this, system architects can pursue a better architecture for their OAuth and OpenID Connect implementations.</p>

<p><img src="media/15374959996961/15374964847880.jpg" alt=""/></p>

<h3 id="toc_12">6. Extensibility</h3>

<p>Frontend servers are expected to behave as defined in the standard specifications. On the other hand, backend services in the semi-hosted service pattern can design their APIs freely without any restraint.</p>

<p>The following sections show example extensions that backend services may provide in order to help developers implement authorization servers and OpenID providers.</p>

<h4 id="toc_13">6.1. Access Token Creation</h4>

<p>In some use cases, you may want to create access tokens using a different way than the standard flows defined in the specification (RFC 6749). Backend service may provide an API for that purpose.</p>

<p><strong>Example</strong></p>

<p>Authlete&#39;s <a href="https://www.authlete.com/documents/apis/reference#auth_token_create">/api/auth/token/create</a> API is an example. By using the API, developers can create access tokens without user interaction.</p>

<pre>$ curl ¥
  --user 4593494640:BBw0rner_-y1A6J9s20wjRCpkBvez3GxEBoL9jOJVR0 \
  https://api.authlete.com/api/auth/token/create \
  -d grantType**=**AUTHORIZATION_CODE \
  -d clientId**=**98282920604 \
  -d subject**=**user123 \
  -d scopes**=**photo</pre>

#### 6.2\. Extra Data of Access Token

The following is an excerpt from _“_[_5.1\. Successful Response_](https://tools.ietf.org/html/rfc6749#section-5.1)_”_ in RFC 6749.

<pre>{
  "access_token":"2YotnFZFEjr1zCsicMWpAA",
  "token_type":"example",
  "expires_in":3600,
  "refresh_token":"tGzv3JOkF0XG5Qx2TlKWIA",
  "example_parameter":"example_value"
}</pre>

<p>This shows a possibility that non-standard parameters such as <code>example_parameter</code> may be returned when an access token is issued. However, there is no standardized way to associate arbitrary data like <code>example_parameter</code> with an access token.</p>

<p>Backend services in the semi-hosted service pattern can provide a mechanism to associate arbitrary data with an access token without needing to add proprietary specifications to frontend servers.</p>

<p><strong>Example</strong></p>

<p><code>properties</code> request parameter of some Authlete APIs is an example. By passing an array of key-value pairs via the request parameter, developers can associate arbitrary data with access tokens.</p>

<p>The following is an example of <a href="https://www.authlete.com/documents/apis/reference#auth_authorization_issue">/api/auth/authorization/issue</a> API call with the <code>properties</code> request parameter.</p>

<pre>$ curl \
  --user 4593494640:BBw0rner_-y1A6J9s20wjRCpkBvez3GxEBoL9jOJVR0 \
  https://api.authlete.com/api/auth/authorization/issue \
  -H 'Content-Type:application/json' \
  -d "{\"ticket\":\"xKdGvPyPkLJRkmP6MSAJ1wISBmdnSbPG8pFzgTdZh4U\",
       \"subject\":\"user123\",
       \"properties\":[
          {\"key\":\"example_parameter\",
           \"value\":\"example_value\"},
          {\"key\":\"hidden_parameter\",
           \"value\":\"hidden_value\",
           \"hidden\":true}]}"</pre>

<h4 id="toc_14">6.3. Operations on a User-Client Basis</h4>

<p>In order to enable a user to revoke permissions given to client applications, the service has to display client applications to which the user has given permissions, let the user select client applications, and delete all the access tokens issued to the selected client applications by the user.</p>

<p>Some authorization server implementations may provide UI for the purpose. On the other hand, backend services in the semi-hosted service pattern would take a different approach - provide APIs instead of UI.</p>

<p><strong>Example</strong></p>

<p>Developers can support the use case above by using the following Authlete APIs.</p>

<ol>
<li> <a href="https://www.authlete.com/documents/apis/reference#client_authorization_get_list">/api/client/authorization/get/list</a></li>
<li> <a href="https://www.authlete.com/documents/apis/reference#client_authorization_delete">/api/client/authorization/delete</a></li>
<li> <a href="https://www.authlete.com/documents/apis/reference#client_authorization_update">/api/client/authorization/update</a></li>
</ol>

<h4 id="toc_15">6.4. Record of Granted Permissions</h4>

<p>When a client application wants new permissions in addition to the ones it already has, it sends an authorization request to the authorization server again. As a response to the request, the authorization server will return an authorization page which includes the list of permissions requested by the client application.</p>

<p>A simple implementation will list all the permissions in the authorization page. On the other hand, a user-friendly implementation may list the new permissions only.</p>

<p>To implement the user-friendly authorization page, the authorization server has to remember sets of permissions granted to client applications by users. It should be noted that the records of granted permissions cannot be deleted even after all the associated access tokens expire. Otherwise, permissions would be displayed to users again if an authorization request is made after all access tokens expire.</p>

<p>Some authorization server implementations may provide UI for the purpose. On the other hand, backend services in the semi-hosted service pattern would take a different approach — provide APIs instead of UI.</p>

<p><strong>Example</strong></p>

<p>Developers can support the use case above by using the following Authlete APIs. Note that these APIs work on dedicated servers only (don&#39;t work on the shared server, api.authlete.com).</p>

<ol>
<li> <a href="https://www.authlete.com/documents/apis/reference#client_granted_scopes_get">/api/client/granted_scopes/get</a></li>
<li> <a href="https://www.authlete.com/documents/apis/reference#client_granted_scopes_delete">/api/client/granted_scopes/delete</a></li>
</ol>

<h3 id="toc_16">Summary</h3>

<p>This article explained a new architecture that, instead of providing an authorization server and OpenID provider itself, provides technical components as Web APIs with which developers can develop authorization servers and OpenID providers. This architecture has been named “Semi-Hosted Service Pattern”.</p>

<p>This architecture draws a clear line between an implementation of OAuth 2.0 / OpenID Connect and other technical components such as user authentication and identity management. I hope developers who seek for better system architectures will notice the advantages of the semi-hosted service pattern.</p>

<p>Thank you for reading this long article to the end.</p>

<ul>
<li>  <a href="https://medium.com/tag/api?source=post">API</a></li>
<li>  <a href="https://medium.com/tag/oauth?source=post">Oauth</a></li>
<li>  <a href="https://medium.com/tag/openid-connect?source=post">Openid Connect</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[全面对比，深度解析 Ignite 与 Spark]]></title>
    <link href="http://panlw.github.io/15371530078234.html"/>
    <updated>2018-09-17T10:56:47+08:00</updated>
    <id>http://panlw.github.io/15371530078234.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://my.oschina.net/editorial-story/blog/2050881">https://my.oschina.net/editorial-story/blog/2050881</a></p>

<p>作者：李玉珏</p>
</blockquote>

<p>经常有人拿 Ignite 和 Spark 进行比较，然后搞不清两者的区别和联系。Ignite 和 Spark，如果笼统归类，都可以归于内存计算平台，然而两者功能上虽然有交集，并且 Ignite 也会对 Spark 进行支持，但是不管是从定位上，还是从功能上来说，它们差别巨大，适用领域有显著的区别。本文从各个方面对此进行对比分析，供各位技术选型参考。</p>

<h1 id="toc_0"><strong>一、综述</strong></h1>

<p>Ignite 和 Spark 都为 Apache 的顶级开源项目，遵循 Apache 2.0 开源协议，经过多年的发展，二者都已经脱离了单一的技术组件或者框架的范畴，向着多元化的生态圈发展，并且发展速度都很快。</p>

<p><strong>Ignite</strong></p>

<p>Ignite 技术来源于 GridGain 公司的商业产品，于 2014 年将绝大部分功能捐赠给 Apache 社区，并于 2015 年 8 月毕业成为 Apache 的顶级项目。Ignite 目前一直保持着高强度的快速迭代式开发，基本一个季度发布一个大版本，从提交数量、版本发布数量等若干指标来评估，一直保持在 Apache 社区 300 多个开源项目的前五位。目前已经聚拢了来自多家组织或公司的众多开发者，处于非常活跃的状态，开发者社区和产品生态正在形成中。</p>

<p><strong>Spark</strong></p>

<p>作为 Hadoop 生态圈重要成员的 Spark 于 2009 年由 Matei Zaharia 在加州大学伯克利分校 AMPLab 开发，于 2013 年 6 月捐赠给 Apache 基金会并切换协议至 Apache2.0，2014 年 2 月毕业成为 Apache 的顶级项目。鉴于 Spark 核心计算模型的先进性，它吸引了众多大企业和组织的积极参与，促成了 Spark 的高速发展和社区的空前繁荣，随着 Spark 技术不断地向纵深发展以及向外延伸，形成了庞大的 Spark 社区和生态圈，目前几乎成为了大数据领域影响力最大的开源项目。</p>

<h1 id="toc_1"><strong>二、定位</strong></h1>

<p>Ignite 和 Spark 都是分布式架构，都归类于目前的大数据技术类别，二者都是利用大量内存的高性能，为原有的技术方案进行提速，但是定位差别很大。</p>

<p><strong>Ignite</strong></p>

<p>Ignite 的核心定位是一个分布式的内存缓存解决方案，通过将数据保存在内存中，提供比传统的基于磁盘的方案更快的性能。然后在分布式缓存的基础上，一方面进一步深入，通过标准 SQL 功能的引入，向分布式内存数据库的方向发展，一方面功能不断扩展，引入了内存计算、流数据处理、机器学习等功能。Ignite 部署灵活，可以轻易地集成进已有的系统，非常方便地与已有的数据库系统集成（NoSQL、HDFS 也支持），为已有的业务进行加速服务。不颠覆已有架构，是 Ignite 很重要的逻辑。</p>

<p><strong>Spark</strong></p>

<p>Spark 的核心定位是一个分布式统一大数据分析引擎，通过先进的 RDD 模型和大量内存的使用，解决了使用 Hadoop 的 MapReduce 进行多轮迭代式计算的性能问题。然后在 RDD 的基础上不断完善，引入了 Dataset 和 DataFrame、SparkSQL、Spark Streaming、SparkML 等更高级的功能。Spark 对 Hadoop 技术栈有非常好的支持，很多可以直接集成，虽然也可以支持 RDBMS 的读写，但是这不是 Spark 主要的关注方向。</p>

<h1 id="toc_2"><strong>三、核心技术</strong></h1>

<p>Ignite 和 Spark 核心技术截然不同。</p>

<p><strong>Ignite</strong></p>

<p>Ignite 的核心数据结构为分布式哈希，即键 - 值型存储，和 Redis 等可以归于同一类，对于分布式内存数据库，核心技术来源于 H2 数据库，也即 Ignite 对 SQL 的支持来源于 H2 的 SQL 引擎。Ignite 的核心计算模型为 MapReduce + 支持 SQL 查询的缓存优化。</p>

<p>Ignite 的内存数据模型为固化内存架构，同时支持内存存储和磁盘存储（可选）。数据保存在堆外，因此只要内存够用，不用担心内存溢出，也不用担心大量占用内存导致垃圾回收暂停。</p>

<p><strong>Spark</strong></p>

<p>Spark 的核心是建立在统一的抽象 RDD 之上，使得 Spark 的各个组件可以无缝进行集成，在同一个应用程序中完成大数据计算任务。RDD 的设计理念源自 AMP 实验室发表的论文《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》。RDD 可以认为是 MapReduce 的超集，也即 RDD 也可以实现传统的 MapReduce 计算机制。</p>

<h1 id="toc_3"><strong>四、部署模型</strong></h1>

<p>Ignite 和 Spark 的组网基本模式有很大的不同，但在更高层面的资源管理上，支持能力是差不多的。</p>

<p><strong>Ignite</strong></p>

<p>Ignite 集群基于无共享架构，所有的集群节点都是平等的、独立的，整个集群不存在单点故障。 通过灵活的 Discovery SPI 组件，Ignite 节点可以自动地发现对方，因此只要需要，可以轻易地对集群进行缩放。</p>

<p>Ignite 可以独立运行，可以组成集群，可以运行于 Kubernetes 和 Docker 容器中，也可以运行在 Apache Mesos 以及 Hadoop Yarn 上，可以运行于虚拟机和云环境，也可以运行于物理机，从技术上来说，集群部署在哪里，是没有限制的。</p>

<p>Ignite 还支持嵌入式部署，也就是和应用集成在一起。</p>

<p><strong>Spark</strong></p>

<p>Spark 支持四种分布式部署方式：分别是 Standalone、Spark on Mesos、Spark on YARN 和 Kubernetes。</p>

<p>Spark 的部署属于 Master/Slave 模式，可能存在单点故障问题，但是可以通过 ZooKeeper 解决。</p>

<h1 id="toc_4"><strong>五、功能</strong></h1>

<h2 id="toc_5"><strong>内存计算</strong></h2>

<p>Ignite 和 Spark 都有内存计算的能力，尤其内存计算是 Spark 的主打功能，从技术原理上来看它们的能力：SparkRDD &gt; Ignite MapReduce+Cache &gt; Hadoop MapReduce。</p>

<p>但具体来说，Ignite 的计算模型优于 Hadoop 毋庸置疑。但是 Ignite 和 Spark，虽然 Ignite 技术原理上不如 SparkRDD 先进，但是落实到具体的实践中，则要看具体的业务场景、技术人员对技术和设计的掌控力、代码优化程度等，无法直接下结论，这个要具体问题具体分析。</p>

<p>Spark 擅长的多轮迭代式计算、交互式计算、图计算等，Ignite 则没有对应的解决方案。</p>

<p><strong>Ignite</strong></p>

<p>Ignite 的计算功能原理与 Hadoop 一致，都是 MapReduce 范式，即可以将一个批量任务拆分为多个部分，然后在不同的节点并行执行，这样就可以并行地利用所有节点的资源，来减少计算任务的整体执行时间。</p>

<p>但是 Ignite 的计算有两个重要的独特之处，一个是鉴于 Ignite 灵活的部署模型，Ignite 可以是离线计算，也可以是在线计算，对于在线的场景，比如 OLTP 业务，它可以通过将请求中的计算负载同步地放在多个可用节点上，然后将结果返回，这样可以提高整个系统的扩展性和容错能力。 另一个是计算可以和数据并置，即计算会被发送到要处理的数据所在的节点，这样会使开销最小化。</p>

<p><strong>Spark</strong></p>

<p>Spark 的计算模型从原理上来说，作为 MapReduce 的超集是非常先进的，Spark 也具有 MapReduce 的机制和开发接口，所以用 Spark 实现 MapReduce 计算模型是可以的。</p>

<p>Spark 的核心概念 RDD，作为一个通用的数据抽象，着重解决了 MapReduce 模型在处理多轮迭代式算法（比如机器学习、图算法等）的性能瓶颈，避免了中间结果落盘导致的大量数据复制、磁盘 IO 和序列化开销。但是 Spark 的计算功能是按照离线系统设计的，无法实现 Ignite 的在线计算功能。</p>

<h2 id="toc_6"><strong>存储支持能力</strong></h2>

<p>Ignite 和 Spark 都可以将第三方存储作为数据来源用作后续的处理，两者对第三方存储的支持程度、侧重点完全不同。这里说的第三方存储，暂时划分为传统的 RDBMS 和 NoSQL（HDFS、Hive、Cassandra 等）。但是 Ignite 在支持第三方存储的同时，本身还具有原生持久化的能力。</p>

<p><strong>Ignite</strong></p>

<ul>
<li><p>RDBMS：Ignite 作为一个缓存系统，天然对 RDBMS 有良好的支持，基本上只要支持 JDBC/ODBC 协议的数据库都没有问题。对于数据的加载、数据的读写及其一致性（事务）保证、各种工具的支持、各种通信协议的支持都一应俱全，是一个完整的方案；</p></li>
<li><p>NoSQL：Ignite 对于各种 NoSQL 数据库的支持是有限的，因为功能定位的原因，不是任何 NoSQL 产品都适合和 Ignite 整合进而提升能力，就目前来说，Ignite 在不同的功能场景对 NoSQL 提供了支持，包括对 HDFS 的支持，也包括与 Cassandra 的原生集成；</p></li>
<li><p>原生持久化：Ignite 基于固化内存架构，提供了原生持久化，可以同时处理存储于内存和磁盘上的数据和索引，它将内存计算的性能和扩展性与磁盘持久化和强一致性整合到一个系统中。 原生持久化以有限的性能损失，透明地提供了更强大的功能，即使整个集群重启，内存不需要预热，数据可以直接访问。</p></li>
</ul>

<p><strong>Spark</strong></p>

<ul>
<li><p>RDBMS：SparkRDD 可以将 RDBMS 作为数据来源之一，支持 RDBMS 数据的批量读写，也支持各种类型的 RDBMS，但是 Spark 对 RDBMS 的读写，属于批量模式，Spark 更多地会将 RDBMS 作为分析型业务的数据来源之一，最后如有必要，则将业务分析的结果批量回写 RDBMS；</p></li>
<li><p>NoSQL：Spark 原生支持 JDBC、JSON、Parquet、csv、libsvm 以及 orcFile 等，也可以通过扩展接口自定义数据源。Spark 可以直接或者通过各种连接器读取 Hive、Hbase、Cassandra 中的数据，然后创建对应的 RDD，写入也是同理，这个能力是 Ignite 所不具备的；</p></li>
<li><p>原生持久化：Spark 不具备原生的持久化能力。</p></li>
</ul>

<h2 id="toc_7"><strong>SQL</strong></h2>

<p>Ignite 和 Spark 都支持 SQL，但是两者的定位和能力，有所不同。</p>

<p><strong>Ignite</strong></p>

<p>Ignite SQL 目前的语法兼容于 ANSI-99，支持查询、删除、更新与插入，但语法和功能与标准并不完全一致。Ignite 如果做好了数据并置，SQL 查询的性能是很好的，同时 Ignite 还支持索引，这都进一步提升了 Ignite SQL 的能力。另外，Ignite SQL 对缓存的功能进行了极大的增强，通常用于缓存的在线查询和计算，用于离线数据处理也是可以的。</p>

<p><strong>Spark</strong></p>

<p>SparkSQL 最初来源于 Shark 项目，后来两者进行了合并，SparkSQL 构建于 Dataset/DataFrame 机制基础上，目前只支持查询，主要适用于分析型业务以及对来自不同数据源的结构化数据进行处理。它也可以进行交互式查询，因为不支持索引等等原因，所以性能较差，响应时间可能较长。</p>

<h2 id="toc_8">数据一致性（事务）</h2>

<p><strong>Ignite</strong></p>

<p>Ignite 整体来说对事务的支持还不完善，具体来说，在键 - 值 API 层面，有完善的事务机制，主要原理来自于经过优化的二阶段提交协议，但是 SQL 层面的 DML 语句还不支持事务，未来版本会解决该问题。</p>

<p>在计算层面，因为支持丰富的编程接口，也可以非常容易地与各种开源的 ORM 框架集成，所以也可以方便地对事务进行细粒度的控制，比如 CRUD 都是没问题的。</p>

<p><strong>Spark</strong></p>

<p>SparkSQL 本身并不提供事务机制。Spark 本身也不适用于 RDBMS 的细粒度数据维护，RDBMS 对于 Spark 来说，只是数据的一个来源和存储地之一，通常都是批量操作，如果批量操作失败，Spark 有容错机制可以重来，以保证整体的一致性。</p>

<h2 id="toc_9"><strong>流计算</strong></h2>

<p>Spark 有 Spark Streaming，Ignite 也支持流数据处理。</p>

<p><strong>Ignite</strong></p>

<p>Ignite 可以与主流的流处理技术和框架进行集成，比如 Kafka、Camel、Storm 与 JMS，提供可扩展和容错的能力。流处理技术为 Ignite 提供了一种数据加载机制，针对流式数据，Ignite 也提供了各种处理和查询功能。Ignite 社区官方提供了 10 种流处理技术的集成实现，利用统一的 API，开发者也可以自行开发流处理技术实现。Ignite 为所有流入 Ignite 的数据以可扩展和容错的方式提供至少一次保证。</p>

<p><strong>Spark</strong></p>

<p>Spark Streaming 是基于 Spark 的流式批处理引擎，其基本原理是把输入数据以某一时间间隔批量的处理，即以时间为单位切分数据流，每个切片内的数据对应一个 RDD，进而可以采用 Spark 引擎进行快速计算。其同样支持众多的数据源，内部的数据表示形式为 DStream。Spark Streaming 吞吐量高，可以做复杂的业务逻辑，但是秒级别的延迟是否符合业务需求需要确认。Spark Streaming 可以与 Spark 其他技术完美集成，包括 SparkML、SparkSQL 等。</p>

<h2 id="toc_10"><strong>机器学习</strong></h2>

<p>Ignite 和 Spark 都支持机器学习。</p>

<p><strong>Ignite</strong></p>

<p>Ignite 从 2.5 版本开始，提供了完整的机器学习解决方案，Ignite 的机器学习有两个优点：一个是如果已经在 Ignite 中持有了大量的数据，那么继续在 Ignite 中进行机器学习的训练和推理，就不需要在不同系统间进行 ETL 的等待，提高效率。另一个是 Ignite 提供了一系列的机器学习和深度学习算法，对 Ignite 的分布式并置处理进行优化，这样在处理大规模的数据集或者不断增长的输入数据流时，提供了内存级的速度和近乎无限的扩展性，而不需要将数据移到另外的存储。目前支持的算法包括回归、分类、聚类以及对数据进行预处理等。另外 Ignite 还支持了一组遗传算法，该算法适合于以最优的方式检索大量复杂的数据集。</p>

<p><strong>Spark</strong></p>

<p>Spark 很早就包含了机器学习库，RDD 模型面向的一个主要场景就是机器学习这样的多轮迭代式计算。目前的 Spark 机器学习库有 2 个实现，正在逐步向 SparkML 过渡，SparkML 基于 DataFrame API，更强大更灵活，而传统的 MLlib 会处于维护状态。SparkML 基于 DataFrames 对 API 进行了统一，使用体验更友好。可以使用 SparkSQL 等更高级的功能，支持流水线，特别是特征变换。Spark 的机器学习因为 RDD 的原因性能更好，支持的算法也更多。</p>

<h2 id="toc_11"><strong>图计算</strong></h2>

<p><strong>Ignite</strong></p>

<p>暂不支持</p>

<p><strong>Spark</strong></p>

<p>Spark 中包含了 GraphX，这是一个图计算组件。它在 RDD 基础上引入了新的 Graph 抽象，为了支持图形计算，GraphX 公开了一组基本运算符（例如子图、连接顶点和聚合消息）以及 Pregel API 的优化变型。此外，GraphX 还包括了越来越多的图形算法和构造者，以简化图形分析任务。</p>

<h2 id="toc_12"><strong>开发语言和客户端协议</strong></h2>

<p><strong>Ignite</strong></p>

<p>Ignite 是以 Java 语言为主进行开发的，因此可以在 JVM 支持的任何操作系统和架构上部署和运行。Java 的 API 支持 Ignite 的所有功能，使用 Java 或者 Scala 开发的应用，相关的逻辑可以直接嵌入 Ignite，然后借助于 SQL 以及键 - 值操作与集群进行交互，执行分布式计算和机器学习算法等等。</p>

<p>除了 Java，Ignite 还支持 .NET 平台与 C++，Ignite.NET 和 Ignite C++ 使用 JNI，会把大部分的调用转发给 Java。</p>

<p>Ignite 还支持使用标准的 JDBC 或者 ODBC 连接，可以像其它 SQL 存储一样与 Ignite 进行交互。Ignite 还为 Java、.NET 和 C++ 开发者提供原生的 SQL API，性能更好。</p>

<p>Ignite 还支持其它的语言访问，比如 Python、Ruby、PHP 与 NodeJS，另外还可以考虑使用 Ignite 的二进制客户端协议接入集群。</p>

<p><strong>Spark</strong></p>

<p>Spark 使用 Scala 语言开发，目前支持使用 Scala、Java、Python、R 语言开发 Spark 程序。</p>

<h2 id="toc_13"><strong>监控运维工具支持</strong></h2>

<p><strong>Ignite</strong></p>

<p>Ignite 开源版没有提供图形化的监控工具，但是提供了简易的命令行工具，同时为了简化开发，Ignite 提供了图形化的 Web 控制台。</p>

<p>Ignite 运行时可以通过 API 接口获取大量的指标，通过编程的方式了解集群的状况。</p>

<p>如果需要强大的监控运维工具，可以购买 GridGain 的商业版软件和服务。如果搭建的是一个小规模的集群，鉴于 Ignite 的无共享架构，部署运维都是比较简单的。</p>

<p><strong>Spark</strong></p>

<p>Spark 启动后会有一个 Web 控制台，虽然不是很美观，但是可以从总体上看到 Spark 的当前运行状态。</p>

<p>Spark 属于 Master/Slave 模式，如果直接拿开源版本搭建大规模集群，部署运维还是非常麻烦的，但是国内有很多厂商开发包含 Spark 组件的大数据平台，为部署和运维提供了很大的便利。</p>

<h1 id="toc_14"><strong>六、总结</strong></h1>

<p>综上所述，Ignite 和 Spark 功能都很全面，已经脱离了简单开源技术组件的范围，都成为了自成体系的开源大数据平台。上面主要对 Ignite 和 Spark 的主要功能做了简单的梳理对比，不一定全面，也没有对其各自特有的功能进行梳理。但经过这么一些分析，还是可以得出这样一个结论：两者差别很大，定位不同，因此会有不同的适用领域。</p>

<p><strong>Ignite</strong></p>

<p>Ignite 以缓存为中心构建大数据体系，底层存储模型更偏向传统关系型数据架构，上层为应用开发的便利做了大量的工作，包括为各种常见语言和协议提供支持。中间核心层在缓存的基础上不断向外扩展，功能日趋丰富强大。</p>

<p>Ignite 从定位上来说有两个突出点，一是可以独立组网，构建独立的大数据平台，然后企业在其上开发全新的大数据应用，包括缓存、计算、流数据处理、机器学习应用等等。二是还可以与传统应用紧密整合，在不颠覆已有架构的前提下，帮助用户进行传统应用的分布式架构转型。为运行多年的复杂、运行缓慢、技术架构落后的业务系统，提供加速能力的同时，引入众多的先进功能，大幅提升原有系统的能力从而延长已有架构的寿命，产生更大的价值，保护客户原有投资。</p>

<p>Ignite 的定位和架构，与 Hadoop 体系大数据组件有很大的不同，但是并不冲突，即使企业已经部署了基于 Hadoop 技术体系的大数据平台，那么也可以继续引入 Ignite 作为补充。</p>

<p><strong>Spark</strong></p>

<p>Spark 以计算为中心构建大数据体系，底层存储对各种数据源进行了抽象，总体上更偏向非结构化的数据，上层应用支持多种语言，核心层基于 RDD 模型，然后进行了大量的扩展，支持了更多更高级的功能，比如 SparkSQL、Spark Streaming、SparkML 与 Spark GraphX 等。Spark 的核心优势是进行多轮迭代式计算、交互式计算以及图计算等。</p>

<p>Spark 是围绕 RDD 构建生态，用户可以以 Spark 为中心搭建大数据平台，满足大量数据的获取、清洗、处理、加载、计算、存储等需求，核心定位是解决大数据的分析问题。虽然 Spark 的计算能力也可以处理传统的关系型数据，但这并非 Spark 的强项，因此和传统业务系统并没有太多的交集。企业基于 Spark 搭建大数据平台之后，其上的应用基本需要全新开发。传统的数据处理业务，即使适合用 Spark 实现，原有的业务逻辑也无法直接、简单地移植进入 Spark 技术堆栈。Spark 技术堆栈更适合用于处理传统技术处理起来很麻烦、性能很差、数据量又很大的非结构化数据，Spark 适合对众多系统的相关数据进行整合，通过分析后能产生更大价值的业务场景。</p>

<h1 id="toc_15"><strong>作者</strong></h1>

<p>李玉珏，架构师，有丰富的架构设计和技术研发团队管理经验，社区技术翻译作者以及撰稿人，开源技术贡献者。Apache Ignite 技术中文文档翻译作者，长期在国内进行 Ignite 技术的推广 / 技术支持 / 咨询工作。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Understanding Storage Sizes for MySQL TEXT Data Types]]></title>
    <link href="http://panlw.github.io/15369925797384.html"/>
    <updated>2018-09-15T14:22:59+08:00</updated>
    <id>http://panlw.github.io/15369925797384.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://chartio.com/resources/tutorials/understanding-strorage-sizes-for-mysql-text-data-types/">https://chartio.com/resources/tutorials/understanding-strorage-sizes-for-mysql-text-data-types/</a></p>
</blockquote>

<p>TEXT data objects, as their namesake implies, are useful for storing long-form text strings in a MySQL database. The four TEXT data object types are built for storing and displaying substantial amounts of information as opposed to other data object types that are helpful with tasks like sorting and searching columns or handling smaller configuration-based options for a larger project. The different TEXT objects offer a range of storage space from 1 byte to 4 GB and are not designed for storing computational values. It’s common to see these used to store product descriptions for a sales site, property summaries for realty database, and long-form article text on a news website. TEXT objects are best used when VARCHAR and other string-based data objects are insufficient to handle storing the desired amount of information. However, the smallest TEXT type, TINYTEXT, shares the same character length as VARCHAR. TEXT objects differentiate themselves from other string storage types by removing the requirement to specify a storage length, not stripping bytes when selected, and do not pad unused character space for efficient disk storage. Since TEXT objects are not stored in the server’s memory, they require data overhead for retrieval. The following sizes assume the database is using the UTF-8 encoding.</p>

<h2 id="toc_0">TINYTEXT: 255 characters - 255 B</h2>

<p>The TINYTEXT data object is the smallest of the TEXT family and is built to efficiently store short information strings. This type can store up to 255 bytes (expressed as 2<sup>8</sup> -1) or 255 characters and requires a 1 byte overhead. This object can be used to store things like short summaries, URL links, and other shorter objects. TINYTEXT shines over VARCHAR when storing data that’s under 255 characters with an inconsistent length and no need to be used for sorting criteria.</p>

<h2 id="toc_1">TEXT: 65,535 characters - 64 KB</h2>

<p>The standard TEXT data object is sufficiently capable of handling typical long-form text content. TEXT data objects top out at 64 KB (expressed as 2<sup>16</sup> -1) or 65,535 characters and requires a 2 byte overhead. It is sufficiently large enough to hold text for something like an article, but would not be sufficient for holding the text of an entire book.</p>

<h2 id="toc_2">MEDIUMTEXT: 16,777,215 - 16 MB</h2>

<p>The MEDIUMTEXT data object is useful for storing larger text strings like white papers, books, and code backup. These data objects can be as large as 16 MB (expressed as 24<sup>2</sup> -1) or 16,777,215 characters and require 3 bytes of overhead storage.</p>

<h2 id="toc_3">LONGTEXT: 4,294,967,295 characters - 4 GB</h2>

<p>The LONGTEXT data object is for use in extreme text string storage use cases. It is a viable option when the MEDIUMTEXT object is not big enough. Computer programs and applications often reach text lengths in the LONGTEXT range. These data objects can be as large as 4 GB (expressed as 2<sup>32</sup> -1) and store up to 4,294,967,295 characters with 4 bytes of overhead storage,</p>

<h2 id="toc_4">TEXT vs. BLOB</h2>

<p>BLOBs are an alternative type of data storage that share matching naming and capacity mechanisms with TEXT objects. However, BLOBs are binary strings with no character set sorting, so they are treated as numeric values while TEXT objects are treated as character strings. This differentiation is important for sorting information. BLOBs are used to store data files like images, videos, and executables.</p>

<h2 id="toc_5">Usage Notes</h2>

<p>Using TEXT fields for select and search queries will incur performance hits because the server will call the objects individually and scan them during the query instead of paging data stored in the memory.</p>

<p>Enabling strict SQL will enforce the maximum character lengths and truncate any entered data that exceeds those limits.</p>

<p>TEXT columns require an index prefix length and can’t have DEFAULT values, unlike CHAR and VARCHAR objects.</p>

<p>Estimating size by word count: assume average English word is 4.5 letters long and needs 1 extra character for spacing. Example, a site that consists of 500 word articles would use about 2,750 characters on average for the article text data. TINYTEXT’s 255 character capacity is insufficient for this use case, while TEXT’s 65535 character capacity offers storage for articles that hit over 11,900 words based on the average criteria.</p>

]]></content>
  </entry>
  
</feed>
