<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Junkman]]></title>
  <link href="http://panlw.github.io/atom.xml" rel="self"/>
  <link href="http://panlw.github.io/"/>
  <updated>2018-12-27T19:07:11+08:00</updated>
  <id>http://panlw.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[The Art of Crafting Architectural Diagrams]]></title>
    <link href="http://panlw.github.io/15517999043443.html"/>
    <updated>2019-03-05T23:31:44+08:00</updated>
    <id>http://panlw.github.io/15517999043443.html</id>
    <content type="html"><![CDATA[
<pre><code>AUG 04, 2017 16 MIN READ
</code></pre>

<blockquote>
<p>原文地址 <a href="https://www.infoq.com/articles/crafting-architectural-diagrams">https://www.infoq.com/articles/crafting-architectural-diagrams</a></p>
</blockquote>

<h3 id="toc_0">Key Takeaways</h3>

<ul>
<li>  Designing architectural diagrams might not be an easy task; it can be tricky or error prone, even for the simplest ones. Creating consistent and meaningful diagrams brings clarity and consensus across different stakeholders.</li>
<li>  In most cases, the real issues are not strictly related to using a less efficient Architectural Description Language (e.g. UML), but the misunderstanding of diagrams importance, relying on improper or inconsistent guidelines or even the lack of architectural education.</li>
<li>  In the process of creating diagrams, try to blend automatically generated with manually created ones in order to minimize the work, to illustrate different set of concerns and to cover multiple abstraction levels of the system.</li>
<li>  As the system is evolving, maintaining diagrams up-to-date requires extra effort. We need to know how to efficiently proceed in such cases by still keeping consistency and robustness across architectural diagrams.</li>
<li>  Modern architectures bring extra complexities which are reflected in the diagrams. Additional concerns might emerge and could easily</li>
</ul>

<p>At some point in time, in every software project we are involved in, there might be a need to create architectural diagrams. Whether we are following a formal architectural model (e.g. Kruchten 4+1, Rozanski &amp; Woods, etc) or not, there is a need to document some parts of the application by creating diagrams. In software architecture, such diagrams are created in compliance with views which are related to a specific viewpoint that could be part of a model, but in the current article I prefer to stick to the term architectural diagram and not be very formal; all the other aspects are not intended to be covered here.</p>

<p>Based on my experience as a software architect and technical trainer, there are a lot of discrepancies between projects and inside the project team from developer to developer in the way architectural diagrams are created. I saw a lot of issues regarding inconsistency, fragmentation, and granularity of the rendered information and the look of the diagrams. In comparison to an architectural model which must be formal and standardized, the diagrams might not necessarily be formalized or follow a specific standard.</p>

<h2 id="toc_1">Current pitfalls when designing architectural diagrams</h2>

<p>Before going deeper into possible issues, I would like to have an analogy to an English idiom which says &quot;a picture is worth a thousand words&quot;. As per this <a href="https://en.wikipedia.org/wiki/A_picture_is_worth_a_thousand_words">wiki</a> explanation, &quot;it refers to the notion that a complex idea can be conveyed with just a single still image or that an image of a subject conveys its meaning or essence more effectively than a description does&quot;. The same concept applies for an architectural diagram: if it raises more questions than answers, the diagram is not well created. Do not let an architectural diagram require thousand of words or clarifications!<br/>
<img src="media/15517999043443/15518000095475.jpg" alt=""/></p>

<p><small><strong>Example of an improper architectural diagram. It suffers most of the issues described below</strong></small></p>

<p>Let’s now iterate through a list of pitfalls which might hinder the process of properly creating  architectural diagrams.</p>

<h3 id="toc_2">What does a box or shape denote?</h3>

<ul>
<li>  Using any kind of box or shape which is not properly documented might cause multiple interpretations. It might be associated with either a piece of data, a bunch of code, or a process. Just a simple box in a diagram might raise multiple doubts and it is very important to avoid them by explicitly adding details about the box or shape meaning in the diagram legend.</li>
</ul>

<h3 id="toc_3">What do different edges of a shape represent?</h3>

<ul>
<li>  Each edge of a shape (e.g. dashed, dotted, etc) can be misunderstood in the case of a poor diagram. Does a specific border refer to a specific component type (e.g. a dashed line refers to a container, a microservice, a layer, etc.), or it is just the designer’s preference to have a rich look and feel? Avoid such confusion by providing accurate details in the legend diagram when choosing multiple or non-standard edges.</li>
</ul>

<h3 id="toc_4">What does a line or an arrow denote?</h3>

<ul>
<li>  A line or an arrow can be interpreted either as a data flow (e.g. data flows from system A to system B) or as a relationship across elements (e.g. component A depends on component B). In most cases the relationships or data flows represented by arrows do not converge in the same directions and it is important to explicitly write this in the diagram legend.</li>
</ul>

<h3 id="toc_5">What is the communication/association type indicated by a line or arrow?</h3>

<ul>
<li>  Even if the line refers to a data flow or a relationship across components, the communication type (e.g. in case of data flow) or the association type (e.g. in case of relationship) denoted by that line or arrow must be detailed. For example, if the line represents a data flow, the communication might be synchronous or asynchronous, but if the line refers to a relationship, it might be represented by a dependency, inheritance, implementation, etc. All of these details must be present in the diagram legend.</li>
</ul>

<h3 id="toc_6">What does that color mean?</h3>

<ul>
<li>  Having a ‘perrot’ policolor diagram (e.g. multiple colors for boxes, lines) without any proper documented intention might raise multiple questions (e.g. why are some boxes green and others red? Why are some lines black and others blue?). The color scheme is less important in a diagram, and using a rich number of colors does not bring too much additional content or valuable information. A diagram could also be self explanatory and well designed just by using black and white colors, unless there is a stringent constraint to emphasize some parts of the diagram by using distinguishable colors. In any case, it is always better to stick to the simplicity in terms of colors used, but if it is not the case, do not forget to detail the choice.</li>
</ul>

<h3 id="toc_7">Missing relationships between diagram elements or isolated entities</h3>

<ul>
<li>  Missing relationships between elements or isolated entities in a diagram might be a clue of incompleteness. From both a structural and behavioural perspective, every element or entity should rely on / have a relationship (represented by a line or arrow) with another part of the system represented by a different element.</li>
</ul>

<h3 id="toc_8">Misleading/undocumented acronyms or too vague/generic terms</h3>

<ul>
<li><p>When using a label for an element in a diagram, it is recommended to not use any misleading or undocumented acronym which might cause confusions. Just a sequence of letters (e.g. TFH, RBPM) do not mean anything without a proper explanation on the diagram element or, even better, in the diagram legend (e.g. TFH - ticket feed handler, RBPM - rates business process manager).</p></li>
<li><p>Another characteristic of naming diagram elements relates to extremely vague or generic terms (e.g. business logic, integration logic) which do not bring too much valuable information because their names are not properly self-descriptive. This issue might reside at the code level as well, and the suggestion would be to always use self explanatory and suggestive names by following on clean code principles.</p></li>
</ul>

<h3 id="toc_9">Emphasize technologies, frameworks, programming or scripting languages, IDE or development methodology on diagrams</h3>

<ul>
<li>  Architectural design is not related or fundamentally based on any technology, framework, programming or scripting language, IDE or development methodology. All of these come later on in the process in order to help build the architecture, but they are not the central point. They should not be included in the diagrams, but stated in the architectural description including the rationale around choosing them.</li>
</ul>

<h3 id="toc_10">Mix runtime and static elements in the same diagram</h3>

<ul>
<li>  Runtime elements (e.g. threads, processes, virtual machines, containers, services, firewalls, data repositories, etc.) are not present at compile time and it is recommended to avoid mixing these elements with the static ones (e.g. components, packages, classes) in the same diagram. There are dedicated diagram types (e.g. concurrency diagram, deployment diagram) which are primarily focused on runtime elements and it is important to distinguish between these two elements categories and to avoid mixing them as much as possible.</li>
</ul>

<h3 id="toc_11">Make assumptions like &quot;I will verbally describe this&quot;, and &quot;I will explain it later&quot;</h3>

<ul>
<li>  Everything which is not described by the diagram itself is missing, and there is no room to provide verbal details to complement a diagram. Why? Because all explanations orally mentioned but not captured in the diagram are lost, and later on, when some other stakeholders (e.g. developer, architect) will read the diagram, they will not be aware of these explanations. Try to include all necessary details in a diagram to avoid any need for further clarifications.</li>
</ul>

<h3 id="toc_12">Conflicting levels of details or mixed abstractions</h3>

<ul>
<li>  Adding elements related to different levels of abstraction in the same diagram might conflict, since they are seen from different perspectives. For example, adding components to an architectural context diagram or classes to a deployment diagram might diverge the purpose of the diagram itself. When creating a diagram, try to stick with the same level of abstraction.</li>
</ul>

<h3 id="toc_13">Cluttered or too vague diagrams trying to show too much or insufficient level of detail</h3>

<ul>
<li>  &quot;Everything should be made as simple as possible, but no simpler&quot; is a well known quote belonging to Albert Einstein. This is valid for architectural diagrams as well; the level and the granularity of captured information should be meaningfully elected. This is not an easy thing; it depends on the architectural model used, the experience of the architect and the complexity of the system.</li>
</ul>

<h2 id="toc_14">Guidelines to follow when creating architectural diagrams</h2>

<p>Apart the above pitfalls, which must be part of a prerequisite checklist in order to avoid them, there are also general guidelines on how to properly create diagrams:</p>

<h3 id="toc_15">Choose the optimal number of diagrams</h3>

<ul>
<li>  As Philippe Kruchten said, &quot;architecture is a complex beast. Using a single blueprint to represent architecture results in an unintelligible semantic mess.&quot; To document modern systems we cannot end up with only one sort of diagram, but when creating architectural diagrams it is not always straightforward what diagrams to choose and how many of them to create. There are multiple factors to take into consideration before making a decision; for example, the nature and the complexity of the architecture, the skills and experience of the software architect, time available, amount of work needed to maintain them, and what makes sense or is useful for meeting stakeholders concerns. For example, a network engineer will probably want to see an explicit network model including hosts, communication ports and protocols; a database administrator is concerned about how the system manipulates, manages and distributes the data, etc. Based on all of these aspects, it is recommended to pick up the optimal number of diagrams, whatever that number is.</li>
<li>  If there are insufficient diagrams (e.g. under-documenting), parts of the architecture might be hidden or undocumented; on the other hand, if there are too many (e.g. over-documenting), the effort needed to keep them consistent, updated and not fragmented might considerably increase.</li>
</ul>

<h3 id="toc_16">Keep structural and semantical consistency across diagrams</h3>

<ul>
<li>  Every diagram should be consistent with the others in terms of boxes, shapes, borders, lines, colors, etc. The structural look and feel should be the same and every stakeholder should have no difficulties in understanding diagrams created by different developers inside a team. Ideally, stick to a common diagramming tool and reuse it across all projects.</li>
<li>  From the semantical point of view, all of these diagrams should be periodically synchronized to latest code changes and between them, since a change in one diagram might impact others. This process might be manually or automatically triggered by using a modeling tool. The latter is the preferred mechanism but this depends from project to project, in all cases the idea is to maintain consistency between diagrams and code, independent of the method or tool. Simon Brown said &quot;diagrams are not useful for architectural improvement if they are not connected to the code&quot;, which emphasizes the idea of semantical consistency.</li>
</ul>

<h3 id="toc_17">Prevent diagrams fragmentation</h3>

<ul>
<li>  Having multiple diagrams might make the architectural description difficult to understand but also a significant effort in maintaining them. As a side effect, fragmentation might appear (e.g. for example two or more diagrams illustrate the same quality attribute - performance, scalability, etc. - but each of them is individually incomplete). In such cases it is recommended to either remove the diagrams which do not reflect relevant quality attributes (linked to architecturally significant requirements) or, even better, to merge diagrams (e.g. concurrency and deployment).</li>
</ul>

<h3 id="toc_18">Keep traceability across diagrams</h3>

<ul>
<li>  To be able to check the history, making comparisons between different diagram versions plus easily reverting to a previous version is also important. Using a modeling tool which does not allow that might be an impediment. The latest trends in the industry rely on using a simple and intuitive plain text language to generate the diagrams out of it, which seems to solve the traceability concern. Another advantage of such an approach is that it implicitly ensures a homogeneous structural consistency between diagrams.</li>
</ul>

<h3 id="toc_19">Add legends next to architectural diagrams</h3>

<ul>
<li>  If you do not follow a standard architectural description language (e.g. UML, ArchiMate), detail every piece of the diagram in the legend (e.g. boxes, shapes, borders, lines, colors, acronyms, etc).</li>
<li>  If this is not the case, in the legend just add the architectural description language as a key and there is no need for additional explanations, since every reader will follow on that language specifics to understand the diagram.</li>
</ul>

<h2 id="toc_20">Does the Architectural Description Language (e.g. UML, ArchiMate, etc.) make a difference?</h2>

<p>There are a lot of opinions regarding which is the right description language to be adopted in the project. Some people might argue that UML is rigid and not flexible enough to model the architectural design, a point of view which I agree with. Nevertheless, in some cases it might be more than sufficient for documenting the fundamentals of an architecture without relying on any UML extensibility features like profiles and stereotypes. By taking a look at other description languages, we can see that <a href="http://www.opengroup.org/subjectareas/enterprise/archimate">ArchiMate</a> is more powerful and suitable for modeling enterprise systems in comparison to UML; there is also <a href="http://www.bpmn.org/">BPMN</a> which is particularly targeted to business processes, etc. The comparisons might continue, but I do not intent to make any deep review across them, since this is not the goal of this article.</p>

<p>Having an architectural description language comprehensive and flexible enough is a big step forward and this should be a solid criteria when choosing it. But from my perspective, the real cause resides somewhere else and is related to the fact that architectural documentation is not created at all. People often find creating it boring, useless or pointless. The number of software projects without, or with improper documentation, is huge. I do not think people are intensively creating or involved in the creation of architectural diagrams using an improper description language, and if they were to replace them with a better one the results would be very different. No, people are not creating any architectural documentation (including architectural diagrams), and even worse, most of them have no idea about how to properly create it. These are the things we need to address first- to understand why documentation matters and how to properly create it (by training software engineers); then the selection of proper tools comes naturally.</p>

<h2 id="toc_21">How can diagrams be kept up-to-date as the system is developed, and changes to the architecture materialize</h2>

<p>There are few approaches to keeping diagrams updated; below I will express three of them. The first option, and the easiest one, would be to automatically generate diagrams out of the source code, which is the ground truth. This guarantees they are all consistent to the code. Unfortunately, with existing tools this is not yet fully possible (at least to my knowledge), since actual tools cannot create any type of accurate and meaningful diagram only based on the source code, without significant manual intervention. Len Bass said &quot;the ideal development environment is one for which the documentation is available for essentially free with the push of a button&quot;, implicitly auto generating the diagrams, but we have not reached that point.</p>

<p>The second approach would be to first design the diagrams using a dedicated tool which then generate the source code skeletons (e.g. components/packages with boundaries, APIs) used later on by developers to fill in the code. This way, every change in the architecture needs to be triggered from the diagram itself which automatically might regenerate or update the code skeleton.</p>

<p>The last case involves manually updating the diagrams every time a new feature - which has an impact on the architectural design - is implemented. To be sure all code changes are reflected in the diagrams, it is recommended that updating diagrams to be part of the definition of done in the development process. This scenario is less recommended because it could easily cause outdated or inconsistent diagrams (e.g. developers often forget or are not in the mood to update diagrams) and unfortunately this still happens in a majority of the projects.</p>

<p>Taking into account existing tools, my recommendation is to have a mix; to blend automatically and manually create diagrams. For example, try to auto generate diagrams, which can be reasonably rendered by tools based on source code without too much noise (e.g. too cluttered or meaningless information). In this category we can include either diagrams with a high degree of volatility (e.g. more prone to frequent development changes, usually having a lower abstraction) or, on the contrary, static diagrams. Some such diagrams might refer to context diagrams, reference architecture diagrams, package diagrams, class diagrams, entity diagrams, etc. Nevertheless, in some cases, it is not obvious based only on the source code how the system meets some quality attributes (e.g. availability, scalability, performance), hence the automatic creation of diagrams is not a sufficient option. It needs to be complemented by manually modeled diagrams. Some examples of such diagrams include sequence diagrams, state diagrams, concurrency diagrams, deployment diagrams, operational diagrams, etc.</p>

<h2 id="toc_22">What complications (or simplifications) emerge for architectural diagrams when dealing with modern architectures (e.g. microservices)?</h2>

<p>Microservices or any other modern architectural style (e.g. serverless, event driven) only drives the structure of the system, how the components communicates each other (e.g. relationships between them) and what principles govern them. Personally, I don&#39;t think the architectural style should change the rationale or concepts around creating the diagrams (and implicitly the architectural description), neither what they should capture. Nevertheless, when we talk about modern systems architectures, usually having higher levels of complexities in comparison to old and classical systems (e.g. monolith), they definitely have an impact on the architectural description and implicitly on the diagrams, in the sense that there are multiple considerations to take care of. Such considerations might be in regards to understanding the number of distributed components (e.g. distributed micro-services), the type of each component, how components communicate to each other (e.g. boundaries, APIs, messages), their lifecycle and who owns each component.</p>

<p>Taking all of these into account, views capturing system decomposition, development, deployment and operability should be considered by default. Imagine a system with an impressive number of micro-services, for example; in such a case the number of diagrams might significantly increase because each microservice might end up in having its own set of diagrams. Issues regarding consistency (e.g. changing the API of one service impacts other X services, therefore all impacted diagrams needs to be updated), fragmentation (e.g. highly availability or performance between distributed services is not consolidated in one diagram) or cross-cutting concerns (e.g. who is in charge to illustrate, in a consolidated manner, aspects like monitoring or security across entire system elements) might not be easily handled. On top of this there might be challenges related to teams’ coexistence and collaboration during project development, and even afterwards, in order to maintain it.</p>

<p>To summarize, moderns systems with complex architectures might bring additional concerns which could lead to complications even at the diagrams level. </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[为什么说我们需要软件架构图？]]></title>
    <link href="http://panlw.github.io/15517997955971.html"/>
    <updated>2019-03-05T23:29:55+08:00</updated>
    <id>http://panlw.github.io/15517997955971.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://mp.weixin.qq.com/s/QTAo6M3u1ouoQYXOFgPddQ">https://mp.weixin.qq.com/s/QTAo6M3u1ouoQYXOFgPddQ</a></p>
</blockquote>

<ul>
<li><p>通过创建和维护架构图来提供准确且有价值的内容并非易事。大多数情况下，我们要么创建了太多的文档，要么太少，或者不相关，因为我们没能准确地定位文档的受益人及其实际的需求。</p></li>
<li><p>我们常犯的最大的一个错误是为系统中具有高波动性的部分创建详细的架构图。除非是自动生成的，否则手动维护它们对我们来说就是一种负担。</p></li>
<li><p>在实践中，大多数利益相关者对详细架构图不感兴趣，但会对一两个反映系统模块和边界的高级架构图感兴趣。除此之外，要深入理解系统，代码才是事实的来源，但在大多数情况下，只有开发人员会对代码感兴趣。</p></li>
<li><p>为了创建具备一定质量的架构图，可以进行头脑风暴，并与团队就什么对他们来说才是真正有用的东西上达成一致。不要尝试为源代码中不言自明的东西或为了迎合架构方法而创建架构图。</p></li>
<li><p>架构图的主要目的应该是促进协作、增强沟通、提供愿景和指导。</p></li>
<li><p>在墙上绘制一两个高级架构图并在会议（站会等）期间使用它们。作为一名架构师，你应该让它们可见，变得有价值，并作为项目文化的一部分。不要将它们隐藏起来或放在利益相关者不易接触到的地方。</p></li>
</ul>

<p>我们尝试通过创建架构图（作为技术文档的一部分）来反映应用程序的内部状态，但大多数时候我们都没能做对。由此产生的架构图可能非常全面，也可能非常模糊。有时，架构图根本就是不相关的。我之前写过一些关于如何创建有用架构图 (<a href="https://www.infoq.com/articles/crafting-architectural-diagrams">https://www.infoq.com/articles/crafting-architectural-diagrams</a>) 的技巧。</p>

<p>即使创建了相关的架构图，我们也很少更新它们，作为持续开发过程的一部分。实际上，我们只是时不时地更新文档，可能是在某些 sprint 期间（当有时间更新文档时）或在发布特定版本之前。另一方面，大多数开发人员（参加我的软件架构课程的同事或学生）不赞成创建和维护技术文档，他们认为这些任务乏味、耗时，而且价值不如其他任务，他们甚至认为如果源代码写得足够好，文档不是必需的。虽然总会有例外，但我很确定，在架构图方面，对于大多数项目来说几乎都是一样的。</p>

<p><section class="" style="color: rgb(63, 63, 63);font-family: Avenir, -apple-system-font, 微软雅黑, sans-serif;white-space: normal;background-color: rgb(255, 255, 255);box-sizing: border-box;text-align: center;font-size: 20px;">我们做错了什么以及如何改进</section></p>

<p>首先，最重要的是 <strong>要了解谁是架构图和技术文档的真正受益者</strong>。文档的数量和质量应该反映出利益相关者的需求，因为只有这样，我们才能创建准确且恰到好处的文档。</p>

<p><strong>主要受益者应该是直接参与项目的团队（开发人员、测试工程师、业务分析师、DevOps 工程师，等等）</strong>。根据我的经验，在团队之外，很少有利益相关者真正关心文档。在最好的情况下，他们可能对一两个高级架构图（例如上下文图、应用程序或软件组件图）感兴趣，这些图粗略地描述了系统的结构并提供了高层次的系统视图。</p>

<p>但是，在大多数情况下，我们并没有确定真正的受益者及其真正的需求，直接就创建了过多的文档。这些文档很快就会成为维护负担，并且很快就会过时。而在其他一些情况下，我们直接省略了架构图，因为没有时间，或者没有兴趣，或者没有人愿意接受这个任务。除此之外，敏捷宣言宣称，团队应该更加重视软件本身而不是文档，也就是不鼓励繁琐的文档处理过程。</p>

<p>为了找到恰当的文档级别平衡点，可以尝试在团队中这么做：</p>

<blockquote>
<p>询问每个同事，他们需要文档为他们提供怎样的内容，以及应该包含哪些类型的架构图。收集他们的意见，然后进行集体讨论，并就团队真正需要哪些的东西达成一致。团队之外可能会有一两个有影响力的利益相关者，他们会提出额外的需求，架构师也有责任将这些人的需求考虑在内。在这个基础上，创建适当数量和质量的技术文档，以满足利益相关者的需求。如果开发人员能够了解文档的真正价值，并对其剩余的价值感兴趣，可以让他们参与更新和维护文档。最后，每个人都会变得很愉快。但是，如果他们不了解文档的必要性或者他们根本不在乎，你几乎可以忽略它，因为很难由一个人（架构师）来维护文档，这应该是团队成员的共同责任。</p>
</blockquote>

<p>过去，在瀑布式项目中，因为采用了综合性的企业架构方法（我故意不说出是哪些方法），或者是一些象牙塔架构师提出的要求，我们创建了太多的文档。当软件项目开始大规模拥抱敏捷方法时，一个常见的误解是人们认为他们不需要文档，因为软件比文档更重要。当然，这是两个极端的情况。并不存在什么精确的方法或科学的过程来明确地指定项目需要多少文档才是恰当的。所有当前的软件架构方法都是纯建议或指南。过去遵循的那些综合性的架构过程在现今的项目中被大大简化，甚至已经不存在了。这并不意味着我们应该创建更少的文档，或者根本不创建文档，而是应该专注于创建具备真正价值的文档，同时不妨碍团队的进展。除此之外，并不是所有的文档都会提供价值。但这并不等同于 “所有的文档都没有价值”。此外，因为不同的环境（如经济、政治等）、业务目标和利益相关者等因素，对一个项目有意义的文档对于另一个项目来说可能并没有那么有用。</p>

<p>在这些情况下，很难得出这个问题的正确答案：多少文档（即架构图）才算是适当的？最后，它关系到每个项目和架构师的经验，可以说是 “视情况而定”。适当的能够提供价值的文档数量取决于团队需要什么。我的建议是与团队一起决定需要创建多少技术文档，无论这对团队来说意味着什么。如果文档对你的项目来说毫无意义（为什么会这样？），这也是可以接受的。将团队做出的决定记录下来，让所有利益相关者都知晓。如果有两三个架构图是有用的，那么请确保随时更新它们，保持它们的一致性，并且总是能反映系统的状态。不要专注于任何不会带来价值的事情。</p>

<p><section class="" style="color: rgb(63, 63, 63);font-family: Avenir, -apple-system-font, 微软雅黑, sans-serif;white-space: normal;background-color: rgb(255, 255, 255);box-sizing: border-box;text-align: center;font-size: 20px;">那么，我们用架构图来做什么？</section></p>

<p>一般而言，架构图和文档应该主要用于团队内部和团队之间的协作、沟通、愿景和指导。它们还应该包含项目的重要设计决策（在某个特定时刻采取）。</p>

<p>架构图应该帮助每个人看到大局，了解周围的环境。在我看来，这应该是创建和维护架构图背后的根本原因。</p>

<p>例如，上下文架构图完全满足了这种需求，并提供了关于系统边界的大量细节，从而可以看到全局。它有助于团队在不同的利益相关者之间达成共识，并简化沟通。我参加了很多会议，当大屏幕上出现这样的上下文架构图时，省去了很多问题，并消除了关于高级系统架构的不确定性。</p>

<p>不过，我们经常会尝试创建综合性的文档来反映系统的内部状态，它们可以是状态图、活动图、类图、实体图、并发图等形式。但这些很快就会过时，除非它们是基于源代码通过一些 “神奇” 的工具自动生成的。</p>

<p>如果人们不需要它们，那么创建这些详细的架构图有什么意义呢？业务利益相关者的抽象图绰绰有余了。在大多数情况下，对于开发人员来说，源代码（即单一事实来源）才是他们真正需要的。因此，请停止为代码中自解释的内容创建详细的架构图，或者当没有真正受众时。</p>

<p>因此，创建有意义的小型架构图，并将它们加到技术文档中。对于大多数应用程序，可能需要两三种架构图。最常见的是上下文图、组件图、系统图或部署图。</p>

<p><section class="" style="color: rgb(63, 63, 63);font-family: Avenir, -apple-system-font, 微软雅黑, sans-serif;white-space: normal;background-color: rgb(255, 255, 255);box-sizing: border-box;text-align: center;font-size: 20px;">我的真实项目示例</section></p>

<p>在我的项目中，我主要使用两种类型的架构图：</p>

<p><img src="media/15517997955971/15517998447312.jpg" alt=""/></p>

<p><center style="color: rgb(63, 63, 63);font-family: Avenir, -apple-system-font, 微软雅黑, sans-serif;font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);">上下文图</center></p>

<p><img src="media/15517997955971/15517998584585.jpg" alt=""/></p>

<p><center style="color: rgb(63, 63, 63);font-family: Avenir, -apple-system-font, 微软雅黑, sans-serif;font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);">应用程序或软件组件图</center></p>

<p>请将这些图视为简单的示例，主要作为每种图应该提供哪些合理信息的指导。图中的信息应该与相应的抽象级别相关，还必须满足利益相关者的需求。</p>

<p>在实践中，你可能倾向于向图中添加越来越多的细节，但是如果它们对利益相关者没有真正的用处，就会导致额外的噪音，增加维护成本，而且容易过时。这些图中的一些细节，例如协议和数据格式，可能对技术利益相关者来说比较方便，因为它们是必要的实现细节。然而，正如之前所述，并不存在精确的方法来确定图中包含多少数量的细节才算是恰当的，这完全取决于不同的项目。不过，架构师需要知道对利益相关者来说真正有用的是什么，并创建和维护架构图来正确地反映这一点。</p>

<p>除了这些架构图之外的任何额外细节，我可以在源代码中找到，或者通过某些工具自动生成（例如运行时视图、开发视图、系统或基础设施视图等）。</p>

<p>我还在会议室中绘制软件架构图（包括所有应用程序组件）。在我们的站会和其他会议期间，人们边指着墙上的这些架构图边谈论他们的任务、状态和遇到的问题。这样，每个团队成员，从产品所有者到开发人员，都能理解系统的全局，并预见到整体障碍和其他集成挑战。除此之外，在 Sprint 期间，它还为整个团队提供了更准确的进度状态，尤其是在分布式架构中，且人与人之间存在依赖关系时。</p>

<p>我建议你也在团队中这么做。通过使用足够的架构图继续加强协作、沟通、愿景和指导，否则就不要创建它们，特别是如果团队不使用它们的话。在大多数情况下，手动创建和维护架构图，以此来反映代码行为纯粹是在浪费时间。如果你这样做了，随着源代码的演变，你可能会想要添加越来越多这样的架构图，这是一个危险的陷阱。与其创建大量令人精疲力尽的架构图，不如坚持使用两到三个从不同抽象层次描述系统的架构图，这对于团队来说是非常必要的。经常性地更新它们，如果这个任务不包含太多的细节，并且是团队文化的一部分，那么它将变得更容易。</p>

<p>另外，<strong>请记住，团队应该是架构图的主要受益者</strong>。如果它们没有表现出任何兴趣，那么你应该停止创建它们，因为这可能是在浪费时间。我们不应该只是 “为了拥有它们”，或者为了遵循综合性的架构方法，或者纯粹为了证明我们作为架构师的角色而去创建架构图。</p>

<p><strong>关于作者</strong></p>

<p>Ionut Balosin 是 Luxoft 的软件架构师，在各种商业应用程序方面拥有超过 10 年的经验，热衷于性能和调优以及软件架构。他经常在各种技术大会上发表演讲，是一个技术培训师。</p>

<p>查看英文原文：</p>

<p><a href="https://www.infoq.com/articles/why-architectural-diagrams">https://www.infoq.com/articles/why-architectural-diagrams</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DNS Servers That Offer Privacy and Filtering]]></title>
    <link href="http://panlw.github.io/15516128677869.html"/>
    <updated>2019-03-03T19:34:27+08:00</updated>
    <id>http://panlw.github.io/15516128677869.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://danielmiessler.com/blog/dns-servers-you-should-have-memorized/">https://danielmiessler.com/blog/dns-servers-you-should-have-memorized/</a></p>
</blockquote>

<p><img src="https://danielmiessler.com/images/DNS.png" alt=""/></p>

<p>If you’re a programmer, a systems administrator, or really any type of IT worker, you probably have your favorite go-to IP addresses for troubleshooting. And if you’re like me, you’ve probably been using the same ones for years.</p>

<p>Such IPs can be used for:</p>

<ul>
<li>  Testing <code>ping</code> connectivity</li>
<li>  Checking DNS resolution using <code>dig</code> or <code>nslookup</code></li>
<li>  Updating a system’s permanent DNS settings</li>
</ul>

<p>Most DNS servers allow you to ping them.</p>

<p>I like using DNS servers for this because you can use them for both connectivity and name resolution testing, and for the longest time I used the Google DNS servers:</p>

<p>8.8.8.8<br/>
8.8.4.4</p>

<p>…but they don’t have any filtering enabled, and in recent years I’ve become less thrilled about sending Google all my DNS queries.</p>

<p>Cisco bought OpenDNS, which is where Umbrella came from.</p>

<h2 id="toc_0">Alternatives to Google DNS</h2>

<p>At some point I switched to using Cisco’s Umbrella servers because they do URL filtering for you. They maintain a list of dangerous URLs and block them automatically for you, which can help protect from malware.</p>

<p>208.67.222.222<br/>
208.67.220.220</p>

<p>The OpenDNS servers are great, but I always have to look them up. Then, a few years ago, a new set of DNS servers came out that focused not only on speed and functionality, but also <u>memorability</u>.</p>

<p>One of the first easy-to-remember options with filtering that came out was IBM’s Quad 9—which as you might expect has an IP address of four nines:</p>

<p>9.9.9.9</p>

<p>I figured they were being overwhelmed at launch time, or their filtering wasn’t tweaked yet.</p>

<p>I tried to use Quad9 one for a bit when it first came out, but found it a bit slow. I imagine they have probably fixed that by now, but more on performance below.</p>

<h2 id="toc_1">Enter CloudFlare</h2>

<p><img src="https://danielmiessler.com/images/Screen-Shot-2019-01-27-at-11.49.14-PM-300x300.png" alt=""/></p>

<p>So with Google, Cisco, and IBM providing interesting options with various functionality, we then saw CloudFlare enter the arena.</p>

<p>But rather than provide filtering, they instead focused on privacy.</p>

<blockquote>
<p>Some other recursive DNS services may claim that their services are secure because they support DNSSEC. While this is a good security practice, users of these services are ironically not protected from the DNS companies themselves. Many of these companies collect data from their DNS customers to use for commercial purposes. Alternatively, 1.1.1.1 does not mine any user data. Logs are kept for 24 hours for debugging purposes, then they are purged.<br/>
<cite>CloudFlare Website</cite></p>
</blockquote>

<p>And perhaps coolest of all for me was their memorability rating, which is basically flawless:</p>

<p>1.0.0.1 abbreviates to 1.1, so you can literally test by typing <code>ping 1.1</code>.</p>

<p>1.1.1.1<br/>
1.0.0.1</p>

<p>How cool is that?</p>

<p>So with them they’re not filtering your URLs, but they are consciously avoiding logging or tracking you in any way, which is excellent.</p>

<h2 id="toc_2">Norton ConnectSafe DNS</h2>

<p>Norton also has a public DNS service, which has an interesting feature of multiple levels of URL content filtering.</p>

<h3 id="toc_3">Block malicious and fraudulent sites</h3>

<p>199.85.126.10<br/>
199.85.127.10</p>

<h3 id="toc_4">Block sexual content</h3>

<p>199.85.126.20<br/>
199.85.127.20</p>

<h3 id="toc_5">Block mature content of many types</h3>

<p>199.85.126.30<br/>
199.85.127.30</p>

<h2 id="toc_6">My recommendation</h2>

<p>Performance also matters here, and that will vary based on where you are, but in recent testing I found all of these options to be fairly responsive.</p>

<p>To me it comes down to this:</p>

<ul>
<li>  If you care about privacy and speed and maximum memorability, I recommend CloudFlare:</li>
</ul>

<p>1.1.1.1<br/>
1.0.0.1</p>

<p>I find the filtering claims by both companies to be too opaque for my tastes, with both of them feeling like borderline marketing to be honest.</p>

<ul>
<li>  If you want URL filtering I recommend Quad9 over Umbrella simply because it’s easier to remember and seems to focus on having multiple threat intelligence sources.</li>
</ul>

<p>9.9.9.9</p>

<ul>
<li>  And if you want multiple levels of URL filtering, you can go with the Norton offering, but I think I personally prefer to just use Quad9 for that and be done with it. But I think Norton is still a cool option for like protecting an entire school or something by forcing their DNS through the strictest option.</li>
</ul>

<h2 id="toc_7">Summary</h2>

<p>Final answer—if pressed—here are the two I recommend you remember.</p>

<ol>
<li> For speed and privacy: <code>1.1.1.1</code></li>
<li> For filtering: <code>9.9.9.9</code></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Airbnb's Migration from Monolith to Services]]></title>
    <link href="http://panlw.github.io/15516123108194.html"/>
    <updated>2019-03-03T19:25:10+08:00</updated>
    <id>http://panlw.github.io/15516123108194.html</id>
    <content type="html"><![CDATA[
<pre><code>Feb 27, 2019 3 min read
</code></pre>

<blockquote>
<p>原文地址 <a href="https://www.infoq.com/news/2019/02/airbnb-monolith-migration-soa">https://www.infoq.com/news/2019/02/airbnb-monolith-migration-soa</a></p>
</blockquote>

<p>Jessica Tai, a self-described &quot;ex-monolith developer&quot; at Airbnb, spoke at the <a href="https://qconsf.com/">2018 QCon San Francisco</a> about her company&#39;s move from a Ruby on Rails monolith architecture to a service-oriented architecture. The company has expanded from 200 engineers in 2015 to 1,000 and has less downtime due to rollbacks and, Tai said, has improved performance with page load times up to 10x faster.</p>

<p>As the company grew, the deployments became more complex, providing the motivation for change. Airbnb engineers were delayed from deploying their code to production on average 15 hours each week due to reverts and rollbacks of code.</p>

<p>Airbnb managed the complexity of splitting a monolith, which the company called Monorail, by phasing the migration, comparing Monorail functionality with that of the new services. They would take 1% of the load in new world, and compared the results down both paths. They progressively increased the load towards the services, until comparison is clean against 100% of load.</p>

<p>This was a relatively straightforward process with reads, because they are idempotent–simply doing the same read query twice and comparing the results could be done with no side effects. However, write comparisons required a different approach. Airbnb achieved this by having the service write to a shadow database and then issue a read request to both the production database and the shadow one, and compare the results.</p>

<p>Airbnb used open-source tools as part of the new architecture. They developed <a href="https://github.com/airbnb/SpinalTap">Spinaltap</a>, which they have open-sourced, to listen to changes in databases and put them onto a Kafka queue.</p>

<p>Airbnb ensured standardisation of best practices in coding their services. Airbnb looked to auto-generate the boilerplate code using <a href="https://thrift.apache.org/">Apache Thrift</a>, an open-source framework that auto-generates code in multiple languages, including Ruby and Java.</p>

<p>Tai interchanged between the phrases SOA and microservices during her presentation and it&#39;s clear there are a number of concepts borrowed from each architectural style.&quot;Airbnb&#39;s approach is closer to service-oriented architecture (SOA),&quot; Tai told InfoQ after the presentation. &quot;Like traditional SOA, we focus on reusing common components as much as possible, including business functionality and storage,&quot; while more akin to a microservices architecture was the ability of Airbnb engineers to code and deploy their own services independently.</p>

<p>Airbnb built their architecture so that all client requests go through an API gateway which routes to the relevant service. Every service is built, deployed and scaled independently. While initially run on EC2 instances in AWS, they moved to a Kubernetes cluster to simplify scaling needs.</p>

<p>To ensure a standard approach to building services, Airbnb defined a set of tenets to design the architecture and services, as well as a service interaction design. These were created up front before the execution phase of the migration. According to Tai, &quot;these service design principles were inspired by the widespread problems we were seeing with our monolith, especially regarding unclear ownership and tight coupling&quot;.</p>

<p>Tai describes four <strong>tenets</strong> in her talk:</p>

<ol>
<li> Services should own both reads and writes to their own data. If several services need access to the same data, this is done via the owning service&#39;s API.</li>
<li> Services should address a specific concern. For Airbnb, this was about trying to find the right granularity when creating services. They wished to avoid services having too much functionality, becoming monoliths in their own right, while also avoiding such fine-grained services that there would be a &quot;polylith&quot; or distributed monolith.</li>
<li> Avoid duplicate functionality by using shared services and libraries.</li>
<li> Data mutations should publish via standard events.</li>
</ol>

<p>In a nod to traditional SOA service design, Airbnb designed their services to &quot;interact with a specific direction as well to have a strict flow of dependencies&quot;. Below is a diagram for the interaction design they ended up with. Airbnb added the middle tier services type at a later date, as a need for shared validation logic was identified.</p>

<p><small>[Click on the image to enlarge it]</small></p>

<p><a href="https://res.infoq.com/news/2019/02/airbnb-monolith-migration-soa/en/resources/airbnb_service_types-large-1550818833059.jpg"><img src="https://res.infoq.com/news/2019/02/airbnb-monolith-migration-soa/en/resources/1airbnb_service_types-small-1550818833582.jpg" alt=""/></a></p>

<p>To avoid changing every database query within the monolith, Airbnb created a custom ActiveRecord adapter that effectively diverts database calls to the new service. Tai described the benefits in her presentation, &quot;by having it at the bottom layer, this allowed product engineers to continue with their interactions of the ActiveRecord methods and not need to change the workflow, while under the hood, we were calling our new service.&quot;</p>

<p>While <a href="https://github.com/twitter/diffy">Diffy</a> is not a specific SOA tool, Airbnb decided to use it to replay traffic to compare releases. By replaying production traffic to their staging environment they could then compare the effect of the new service against real world traffic, and confirm new services would not degrade customer experience.</p>

<p>A video recording of the presentation, slides, and a full transcript are <a href="https://www.infoq.com/presentations/airbnb-soa-migration">available on InfoQ</a>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Events As First-Class Citizens]]></title>
    <link href="http://panlw.github.io/15516097487470.html"/>
    <updated>2019-03-03T18:42:28+08:00</updated>
    <id>http://panlw.github.io/15516097487470.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://hackernoon.com/events-as-first-class-citizens-8633e8479493">https://hackernoon.com/events-as-first-class-citizens-8633e8479493</a></p>
</blockquote>

<p>We’ve all heard of events and event-driven programming, but in my experience, events are not used nearly enough in our (software) lives. We often don’t appreciate how powerful this tool can be in our toolbox, and consequently we don’t take advantage of it when we really should.</p>

<p>In this post, I’ll discuss the lifecycle of a “fix” at Stitch Fix and how we use events to model it. I’ll suggest that we should think of events as a first-class citizen in our system, because they help us both to decouple parts of the system and to reason about them independently. Lastly, I’ll talk about how common it is to use events in the real world, and use the metaphor of software development itself to help develop intuition about event-based systems.</p>

<h3 id="toc_0"><strong>The Lifecycle of a Fix</strong></h3>

<p>At Stitch Fix, the engineering team builds and operates more than 70 individual applications and services, serving every aspect of Stitch Fix’s business. We have applications for our Merchandising team which is responsible for buying the clothes, our Warehouse Operations team which stores and ships them, and our thousands-strong Styling team which chooses them for our clients. We have applications used by our clients to schedule their Fixes, rate them, and pay for them. And we have applications used by our Client Experience team to help give our clients the best clothing purchase experience we can. Almost all of these applications and services operate on one or more of the core entities in our business — clients, items, fixes, etc. — in one way or another.</p>

<p>To walk through just one motivating example, a nascent fix is created when a client tells us she’d like to receive it on a particular day (scheduling). Based on where and when that fix is shipping, we assign it to one of our warehouses around the US (warehouse assignment), and we make it available to be styled by one of our 3500 stylists (stylist assignment). The stylist selects 5 items she expects the client to enjoy (styling), and we’re ready to send it out. The warehouse team picks, packs, and ships the fix (shipping), and the client receives it on her doorstep. She keeps what she wants, and returns what she doesn’t (checkout). And the cycle is complete.</p>

<p>We just described a moderately complicated workflow, with many individual steps, all operating on a single fix. Looked at through the lens of software engineering, the straightforward way to model this is as a state machine, with individual events that indicate that the fix has transitioned from one state to another. And that’s exactly how we implement it. Here’s a simplified representation of this workflow:</p>

<ul>
<li>  Request a fix</li>
<li>  -&gt; Fix is __scheduled_</li>
<li>  Assign fix to warehouse</li>
<li>  -&gt; Fix is __hizzy_assigned_ (we call our warehouses “hizzies”; don’t ask)</li>
<li>  Assign fix to a stylist</li>
<li>  -&gt; Fix is __stylist_assigned_</li>
<li>  Style the fix</li>
<li>  -&gt; Fix is __styled_</li>
<li>  Pick the items for the fix</li>
<li>  -&gt; Fix is __picked_</li>
<li>  Pack the items into a box</li>
<li>  -&gt; Fix is __packed_</li>
<li>  Ship the fix via a shipping carrier</li>
<li>  -&gt; Fix is __shipped_</li>
<li>  Fix travels (as actual atoms!) to the client</li>
<li>  -&gt; Fix is __delivered_</li>
<li>  Client decides what to keep and return, pays for her fix</li>
<li>  -&gt; Fix is __checked_out_</li>
</ul>

<p>Several things immediately come to mind:</p>

<ul>
<li>  As the fix moves happily along, different applications or services do something with it, by enriching it with more (meta)data, by connecting it up with something else, or by doing something physical with the goods or the packaging. That doing of the something will take the fix from one state to another (_state transitions_).</li>
<li>  The next application or service can only do its work when the previous one has done its, and consequently needs to know when to step up (_events_).</li>
<li>  We can’t skip any of these steps, or there would be something missing — in many cases, quite literally. Said another way, from a given state, it’s only possible to go to a subset of the other states (_state machine_).</li>
<li>  If we just remembered the current state of things — where the fix is right now — we’d be missing a lot. We want to be able to ask where the fix is right now, but we also want to know where it has been, how long it was there, and when it moved to the next step. So if we only stored its current-state in some database table and nothing else about it, we’d be stuck. Instead we need to also record all the steps along the way.</li>
</ul>

<h3 id="toc_1"><strong>Events as a First-Class Citizen</strong></h3>

<p>Notice that if we only had the standard tools of the classic 3-tier architecture at our disposal, we’d be in trouble. We’re all familiar with these three fundamental application building blocks:</p>

<ul>
<li>  Presentation: The user interface where the user interacts with the system</li>
<li>  Application: The “business logic” where we do the work, typically statelessly</li>
<li>  Persistence: The place where we store things, typically in a database</li>
</ul>

<p>I strongly believe that events represent a fourth fundamental building block:</p>

<ul>
<li>  Event: The statement that an interesting thing happened, or, according to Wikipedia, “_a significant change in state_”</li>
</ul>

<p>In a [micro]services architecture like we have at Stitch Fix, a given application or service might be a producer of events, a consumer of events, or both. For example, the styling application listens for the __stylist_assigned_ event, and displays all the information needed to the stylist for her to style the fix. When she is done with the fix, she clicks the “Ship It” button, which (among other things) publishes the __styled_ event. The warehouse services listen for that event, and can start their work, etc.</p>

<p>Consuming these events and producing these events are first-class parts of these applications and services; they need these events to do their jobs. So when we talk about the “interface” of one of these services, let’s make this explicit. A service interface includes:</p>

<ul>
<li>  Synchronous request-response operations (e.g., for use, this is REST/JSON, but it could just as easily be over gRPC, Thrift, etc.)</li>
<li>  Events the service produces</li>
<li>  Events the service consumes</li>
<li>  Bulk reads and writes (e.g., an ETL that extracts data from the service into an analytics system)</li>
</ul>

<p>More generally, a service’s interface includes any mechanism for getting data into or out of the service. As a service owner or a service consumer, we forget this at our own peril.</p>

<h3 id="toc_2"><strong>Events as Decoupling</strong></h3>

<p>The producer of an event publishes it, and zero or more consumers subscribe to it. Maybe no one is listening; maybe one is; maybe many are. The producer does not know or care. This gives the nice property that the producer and consumer are completely decoupled from one another. We can add more consumers, remove them, or scale them up and down — without the producer being any the wiser.</p>

<h3 id="toc_3"><strong>Events as Record</strong></h3>

<p>Once we represent all the interesting state transitions for our entity as events, we can use those events as a record of what happened to that entity, and when. This is hugely valuable when we want to go back and see what went on. It’s common for our client experience team (think “customer support”, but with more smiles and empathy) to look up the history of a fix when they are trying to help out a client. It’s common for our data science team to use the events around a fix to optimize various aspects of our workflow. And it’s common for our engineering team to use this as a debugging and diagnosis tool.</p>

<p>Taking this idea to its logical conclusion, we could imagine <em>only</em> retaining the events and never bothering to store the current state in any permanent way. After all we can always simply reconstruct it by playing the events forward. This is such a clever idea that people have already thought of it — it is called “event sourcing” (see the many writings of <a href="https://goodenoughsoftware.net/tag/event-sourcing/">Greg Young</a>, <a href="http://microservices.io/patterns/data/event-sourcing.html">Chris Richardson</a>, etc.), and it has a lot of wonderful resilience properties, particularly in distributed systems. In fact, there are entire software systems based on this idea (<a href="https://eventstore.org/">Event Store</a>, <a href="https://www.confluent.io/blog/apache-kafka-for-service-architectures/">Kafka</a>, <a href="https://doc.akka.io/docs/akka/2.5.4/scala/persistence.html#event-sourcing">Akka Persistence</a>, etc.). It’s also, of course, exactly how double-entry booking works in accounting, so even these (very smart) people were <a href="https://en.wikipedia.org/wiki/Double-entry_bookkeeping_system">preempted by the Medici 700 years ago</a>.</p>

<h3 id="toc_4"><strong>Events are How the Real World Works™</strong></h3>

<p>I often hear that it’s hard for developers to think in terms of events. It can feel a bit counterintuitive if you’re used to building the classic three-tier. What can trip people up is that at any given moment something might have happened in one part of our system, but the effects of that action are not yet visible in the other areas of our system. We use words like “eventual consistency” and “asynchrony” here, and they’ve earned their reputation for being hard to reason about. But I’d like to suggest that you have a lot more intuition about events than you think you do. If you can think about your problem as a workflow, you’re more than halfway there.</p>

<p>So let’s take an example that will be familiar to every software engineer. Imagine a typical modern software development process, where we write code, check it into source control, test it, stage it, and deploy it. We often talk about this as a “lifecycle” or “pipeline” or “workflow”. Well, that sounds a lot like events. Let’s see.</p>

<ul>
<li>  Write code</li>
<li>  -&gt; code is __submitted_</li>
<li>  Test code</li>
<li>  -&gt; code is __tested_</li>
<li>  Deploy it to a staging server</li>
<li>  -&gt; code is __staged_</li>
<li>  Deploy it to production</li>
<li>  -&gt; code is __deployed_</li>
</ul>

<p>This seems super-familiar — we do this every day! And it’s not just a tangential thing off in the corner; for most of us, this is our job.</p>

<p>Think if this did not behave like an asynchronous workflow, and we did all of this synchronously. Imagine that every time you hit return in your IDE, your code would be deployed to production. I’m all for continuous delivery, believe me, but this would be crazy.</p>

<p>So this idea that you can’t reason about systems where there is an “inconsistency” between one part of the system and another is a little silly. For much of the day, your code has one version on your laptop, and another “stale” version in production, and everything works just fine.</p>

<h3 id="toc_5"><strong>Conclusion</strong></h3>

<p>Thanks for reading this far. If you have, I hope I’ve got you thinking that you should reach for the “event” tool in your toolbox more often. It can help you in many ways.</p>

<p>And if you’re having trouble reasoning about events, just think about what you do for 8, 10, 12 hours every day.</p>

<h3 id="toc_6"><strong>Helpful Resources</strong></h3>

<p>If you are interested in deepening your knowledge of events and event-driven systems, check out:</p>

<ul>
<li>  <a href="https://www.infoq.com/presentations/microservices-data-centric">Managing Data in Microservices</a>, slides / video from my talk at QCon New York 2017</li>
<li>  <a href="https://www.youtube.com/watch?v=STKCRSUsyP0">The Many Meanings of Event-Driven Architecture</a>, video of <a href="https://martinfowler.com/">Martin Fowler</a>’s keynote at GOTO Chicago 2017</li>
<li>  <a href="http://microservices.io/patterns/data/event-sourcing.html">Microservices and the Event Sourcing Pattern</a>, by <a href="http://chrisrichardson.net/">Chris Richardson</a></li>
<li>  <a href="https://www.youtube.com/watch?v=JHGkaShoyNs">CQRS and Event Sourcing</a>, by <a href="https://goodenoughsoftware.net/">Greg Young</a></li>
<li>  <a href="http://www.enterpriseintegrationpatterns.com/">Enterprise Integration Patterns</a>, the classic oldie-but-still-very-much-goodie book and pattern collection by <a href="http://www.enterpriseintegrationpatterns.com/gregor.html">Gregor Hohpe</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Monitoring and Managing Workflows Across Collaborating Microservices]]></title>
    <link href="http://panlw.github.io/15516093053616.html"/>
    <updated>2019-03-03T18:35:05+08:00</updated>
    <id>http://panlw.github.io/15516093053616.html</id>
    <content type="html"><![CDATA[
<pre><code>Feb 28, 2019 13 min read
</code></pre>

<blockquote>
<p>原文地址 <a href="https://www.infoq.com/articles/monitor-workflow-collaborating-microservices">https://www.infoq.com/articles/monitor-workflow-collaborating-microservices</a></p>
</blockquote>

<h3 id="toc_0">Key Takeaways</h3>

<ul>
<li>  Peer-to-peer communication between components can lead to emergent behavior, which is challenging for developers, operators and business analysts to understand.</li>
<li>  You need to make sure to have the overview of all of the backwards-and-forwards communication that is going on in order to fulfill a business capability.</li>
<li>  Solutions that provide an overview range from distributed tracing, which typically misses the business perspective; data lakes, which require some effort to tune to what you need to know; process tracking, where you have to model a workflow for the tracking; process mining, which can discover the workflow; all the way through to orchestration, which comes with visibility built-in.</li>
<li>  In this article we argue that you need to balance orchestration and choreography in a microservices architecture in order to be able to understand, manage and change the system.</li>
</ul>

<p>Last year I met an architect from a big e-commerce organisation. He told me that they do all the right things and divide their functionality into smaller pieces along domain boundaries, even if they don’t call this architectural style “microservices”. Then we talked about how these services collaborate to carry out business logic that crosses service boundaries, as this is typically where the rubber meets the road. He told me their services interact via events published on an event bus, which is typically known as “<a href="https://blog.bernd-ruecker.com/why-service-collaboration-needs-choreography-and-orchestration-239c4f9700fa">choreography</a>” (and this concept will be explained in greater detail later). They considered this to be optimal in terms of decoupling. But the problem they face is that it becomes hard to understand what’s happening and even harder to change something. “This is not like that choreographed dances you see in slides of some microservices talks; this is unmanageable pogo jumping!” </p>

<p>This matches what other customers tell me, e.g. <a href="https://medium.com/@sitapati/node-js-client-for-zeebe-microservices-orchestration-engine-72287e4c7d94">Josh Wulf from Credit Sense said</a>, “the system we are replacing uses a complex peer-to-peer choreography that requires reasoning across multiple codebases to understand.”</p>

<p><img src="https://res.infoq.com/articles/monitor-workflow-collaborating-microservices/en/resources/1monitor-workflow-collaborating-microservices-1-1551187247423.jpg" alt=""/></p>

<p><small><strong><a href="https://medium.com/r/?url=https%3A%2F%2Fwww.flickr.com%2Fphotos%2F23447193%40N06%2F7849857232">Photo by</a><a href="https://commons.wikimedia.org/wiki/File:Moshing_BMTH_RAL_2013.jpg">Pedobear19</a><a href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">available under CC BY-SA 4.0</a></strong></small></p>

<p>Let’s investigate this further using a simplified example. Assume you build an order fulfillment application. You choose to implement the system using an event-driven architecture and you use, for example, Apache Kafka as an event bus. Whenever somebody places an order, an event is fired from the checkout service and picked up by a payment service. This payment service now collects money and fires an event which gets picked up by an inventory service. </p>

<p><img src="https://res.infoq.com/articles/monitor-workflow-collaborating-microservices/en/resources/1monitor-workflow-collaborating-microservices-2-1551187248305.jpg" alt=""/></p>

<p><small><strong>Choreographed event flow</strong></small></p>

<p>The advantage of this way of working is that you can easily include new components into the system. Assume you want to build a notification service to send emails to your customer; you can simply add a new service and subscribe to relevant events, without touching any of the other services. You can now manage communication settings and handle all complexity of <a href="https://eugdpr.org/">GDPR</a> compliant customer notifications in that central place. </p>

<p><img src="https://res.infoq.com/articles/monitor-workflow-collaborating-microservices/en/resources/1monitor-workflow-collaborating-microservices-3-1551187246590.jpg" alt=""/></p>

<p>This architectural style is called choreography, as there is no orchestrator required which tells others what to do. In contrast, every component emits events and others can be reactiveto them. The assumption is that this style reduces coupling between the components and systems become easier to develop and change, which is true for the sketched notification service. </p>

<h2 id="toc_1">Losing Sight of the Flow of Events</h2>

<p>In this article I want to concentrate on the most-often asked question whenever I discuss this architecture: How do we avoid losing sight (and probably control) of the flow of events?In <a href="https://blog.camunda.com/post/2018/09/microservices-orchestration-survey-results-2018/">a recent survey</a>, Camunda (the company I work for) asked about the adoption of microservices. 92% of all respondents at least consider microservices, and 64% already do microservices in some form. It is more than just hype. But in that survey we also asked about challenges , and found a clear confirmation of this risk; the top answer was about lack of visibility into end-to-end business processes that span multiple services.</p>

<p><img src="https://res.infoq.com/articles/monitor-workflow-collaborating-microservices/en/resources/1monitor-workflow-collaborating-microservices-4-1551187247761.jpg" alt=""/></p>

<p>Remember architectures based on a lot of database triggers? Architectures where you never exactly knew what would happen if you did this - and wait - why did that happen now? Challenges with reactive microservices sometimes remind me a bit of this, even if this comparison is clearly misplaced. </p>

<h2 id="toc_2">Establishing Visibility</h2>

<p>But what can we do about it? The following approaches can help you get back visibility, but each have their different pros and cons:</p>

<ol>
<li> Distributed tracing (e.g. Zipkin or Jaeger)</li>
<li> Data lakes or analytic tools (e.g. Elastic)</li>
<li> Process mining (e.g. ProM)</li>
<li> Tracking using workflow automation (e.g. Camunda)</li>
</ol>

<p>Please be aware that all approaches observe a running system and inspect instances flowing through it. I do not know any static analysis tool that yields useful information.</p>

<h3 id="toc_3">Distributed Tracing</h3>

<p>Distributed tracing wants to trace call-stacks across different systems and services. This is done by creating unique trace ids that are typically added to certain headers generically (e.g. HTTP or messaging headers). If everybody in your universe understands or at least forwards these headers, you can leave breadcrumbs while a request hops through different services. </p>

<p><img src="https://res.infoq.com/articles/monitor-workflow-collaborating-microservices/en/resources/1monitor-workflow-collaborating-microservices-5-1551187248588.jpg" alt=""/></p>

<p>Distributed tracing is typically used to understand how requests flow through the system, to pinpoint failures or to investigate the root of performance bottlenecks. The great thing about distributed tracing is that there are mature toolswith a lively ecosystem around. So it is relatively easy to get started, even if you typically have to (potentially invasively) instrument your applications or containers. </p>

<p>So why not use this to actually understand how business processes emergeby events flowing through our system? Well, basically two reasons make it hard to apply distributed tracing for this use case:</p>

<ul>
<li>  Traces are hard to understand for non-engineers. My personal experiments aiming to show traces to non tech people failed miserably. It was far better to invest some time to redraw the same information with boxes and arrows. And even if all the information about method calls and messages is totally useful to understand communication behaviors, it is too fine-grained to understand the essence of cross-service business processes.</li>
<li>  To manage the overwhelming mass of fine-grained data, distributed tracing uses so-called sampling. This means only a small portion of all requests are collected. Typically, more than 90% of the requests are never recorded. A good take on this is <a href="https://lightstep.com/blog/three-pillars-zero-answers-towards-new-scorecard-observability/">Three Pillars with Zero Answers - towards a New Scorecard for Observability</a>. So you never have a complete view of what’s happening. </li>
</ul>

<h3 id="toc_4">Data Lakes or Analytic Tools </h3>

<p>So, out-of-the-box tracing will probably not be the way to go. The logical next step is to do something comparable, but bespoke to the problem at hand. That basically means not collecting traces, but instead collecting meaningful business or domain eventsthat you might have flowing around already anyway. This often boils down to building a service to listen to all events and storing them in a data store that can take some load. Currently a lot of our customers use Elastic for this purpose. </p>

<p><img src="https://res.infoq.com/articles/monitor-workflow-collaborating-microservices/en/resources/1monitor-workflow-collaborating-microservices-6-1551187248965.jpg" alt=""/></p>

<p>This is a powerful mechanism which is relatively easy to build. Most customers who work in an event-driven manner have this setup already. The biggest barrier for introduction is often the question of who will operate such a tool within a large organization, as it definitely needs to be managed by some centralized facility. It is also easy to build you own user interfaces on top of this to find relevant information for certain questions easily.</p>

<p><img src="https://res.infoq.com/articles/monitor-workflow-collaborating-microservices/en/resources/1monitor-workflow-collaborating-microservices-7-1551187245374.jpg" alt=""/></p>

<p><small><strong><a href="http://github.com/berndruecker/flowing-retail">Example event monitor UI</a></strong></small></p>

<p>One shortcoming is the lack of graphicsto make sense out of a list of events. But you could build that into this infrastructure by for example projecting events onto some visualisation like BPMN. </p>

<p>Lightweight frameworks like bpmn.io allow to add information to such a diagram in simple HTML pages (<a href="https://github.com/berndruecker/flowing-retail/tree/master/kafka/java/monitor">an example can be found here</a>) which could also be packaged into a Kibana Plugin.)</p>

<p><img src="https://res.infoq.com/articles/monitor-workflow-collaborating-microservices/en/resources/1monitor-workflow-collaborating-microservices-8-1551187246331.jpg" alt=""/></p>

<p>This model is not executed by some workflow engine; it is a diagram used to visualize the captured events in a different way. In that sense you also have some freedom as to what granularity it will show, and it is also OK to have models that show events from different microservices in one diagram, as this is what you are especially interested in: the big picture. The good news is that this diagram does not stop you from deploying changes to individual services, so it does not hinder agility in your organization, but the tradeoff is that that introduces the risk of diagrams becoming outdated, when compared with the current state of the system operating in production.</p>

<h3 id="toc_5">Process Mining Tools</h3>

<p>In the above approach you have to explicitly model the diagram you use for visualization, but if the nature of the event-flow is not known in advance, it needs to be discovered first. </p>

<p>This process discovery could be done by process mining tools. They can derive the overall blueprint and show that graphically, often even allowing to dig into a lot of detailed data, especially around bottlenecks or optimization opportunities. </p>

<p><img src="https://res.infoq.com/articles/monitor-workflow-collaborating-microservices/en/resources/1monitor-workflow-collaborating-microservices-9-1551187245799.jpg" alt=""/></p>

<p><small><strong><a href="http://www.promtools.org/doku.php">image source</a></strong></small></p>

<p>This sounds like a perfect fit for our problem. Unfortunately, the tools are most often used to discover process flows within legacy architectures, so they focus on log file analysis and are not really good at ingesting live event streams. Another issue with these tools is that they are either very scientific and hard to use (like ProM) — or very much heavyweight (like Celonis). So in our experience it is often impractical to introduce these tools into typical microservice endeavors.</p>

<p><img src="https://res.infoq.com/articles/monitor-workflow-collaborating-microservices/en/resources/1monitor-workflow-collaborating-microservices-10-1551187247127.jpg" alt=""/></p>

<p><small><strong><a href="https://www.celonis.com/de/intelligent-business-cloud/process-discovery/">image source</a></strong></small></p>

<p>Nevertheless, process discovery and mining add interesting capabilities into the mix in order to get visibility into your event-flows and business processes. I hope that there will be technology emerging soon that offers comparable functionality but is also lightweight, developer-friendly and easily adoptable. </p>

<h3 id="toc_6">Tracking via Workflow Automation</h3>

<p>Another interesting approach is to model the workflow, but then deploy and run it on a real workflow engine. The workflow model is special in a sense, in that it is only tracking events, and not doing anything actively itself. So it does not steer anything -- it simply records. I talked about this at the <a href="https://www.confluent.io/kafka-summit-sf18/the_big_picture">Kafka Summit San Francisco 2018</a>, and the recording includes a live demo using <a href="https://kafka.apache.org/">Apache Kafka</a> and the open source workflow engine <a href="https://zeebe.io/">Zeebe</a>.</p>

<p><img src="https://res.infoq.com/articles/monitor-workflow-collaborating-microservices/en/resources/1monitor-workflow-collaborating-microservices-11-1551187246056.jpg" alt=""/></p>

<p>This option is especially interesting as there is a lot of innovation in the workflow engine market, which is resulting in the emergence of tools that are lightweight, developer-friendly and highly-scalable. I wrote about this in <a href="https://www.infoq.com/articles/events-workflow-automation">Events, Flows and Long-Running Services: A Modern Approach to Workflow Automation</a>. The obvious downside is that you have to model the workflow upfront. But in contrast to event monitoring, this model is executed on a workflow engine - in fact you start workflow instances for incoming events or correlate events to that workflow instance. This also allows conformance checks - does the reality fit into what you modeled? </p>

<p>And this approach allows you to leverage the complete tool chain of workflow automation platforms, which allows to see what’s currently going on, monitor SLA’s and detect stuck instances or do extensive analysis on historical audit data. </p>

<p><img src="https://res.infoq.com/articles/monitor-workflow-collaborating-microservices/en/resources/1monitor-workflow-collaborating-microservices-12-1551187246875.jpg" alt=""/></p>

<p><small><strong>Sample workflow monitoring (from camunda.com)</strong></small></p>

<p>When I validated this approach with customers it was easy to set up. We just had to build a generic component picking up events from the bus and correlate it to the workflow engine. Whenever an event could not be correlated, we used a small decision table to decide if it could be ignored or would yield in an incident to be checked later. We also instrumented <a href="https://www.slideshare.net/BerndRuecker/camundacon-the-role-of-workflows-in-microservices/40">workflow engines used within microservices to execute business logic</a> to generate certain events (e.g. workflow instance started, ended or milestone reached) to be used in the big picture.</p>

<p>This workflow tracking is a bit like event-monitoring, but with a business process focus. Unlike tracing, it can record 100% of your business events and provide a view that is suitable for the various stakeholders.</p>

<h2 id="toc_7">The Business Perspective</h2>

<p>One big advantage of having the business process available in monitoring is that you understand the context. For a certain instance you can always see how and why it ended up in the current state, which enables you to understand which path it did nottake (but other instances often do) and what events or data lead to certain decisions. You can also get an idea of what might happen in the near future. This is something you miss in other forms of monitoring. And even if it is often not currently hip to discuss the alignment between business and IT, it is absolutely necessary that non-engineers also understand business processes and how events flow through various microservices. </p>

<h2 id="toc_8">A Journey from Tracking to Managing</h2>

<p>Process tracking is fine, as this gives you operational monitoring, reporting, KPIs and visibility as an important pillar to keep agility. But in current projects this tracking approach is actually just the first step in a journey towards more management and orchestration in your microservice landscape.</p>

<p>A simple example could be that you start to monitor timeouts for your end-to-end process. Whenever this timeout is hit, some action is taken automatically. In the following example we would inform the customer of a delay after 14 days — but still keep waiting. After 21 days we give up and cancel the order.</p>

<p><img src="https://res.infoq.com/articles/monitor-workflow-collaborating-microservices/en/resources/1monitor-workflow-collaborating-microservices-13-1551187249236.jpg" alt=""/></p>

<p>One interesting aspect in the above picture is the sending of the command cancel order. This is orchestration -- and this is sometimes discussed in a controversial fashion. </p>

<h2 id="toc_9">Orchestration</h2>

<p>I often hear that orchestration should be avoided with the argument that it introduces coupling or violates autonomy of single microservices. And of course it is true that orchestration can be done badly, but it also can be done in a way that aligns with microservices principles while adding a lot of value to the business. At <a href="https://berndruecker.io/complex-event-flows-in-distributed-systems/">InfoQ New York 2018</a> I  talked specifically about this misconception. </p>

<p>In its essence, orchestration for me means that one service can command another to do something. That’s it.That’s not tighter coupling, it is just coupled the other way round.  Take the order example. It might be a good idea that the checkout service just emits an order placed event but does not know who processes it. The order service listens to that order placed event. The receiver knows about the event and decides to do something about it; the coupling is on the receiving side. </p>

<p>It is different with the payment, because it would be quite unnatural that the payment service knows what the payment is for. But it would need that knowledge in order to react on the right events, like order placed or order created. This also means it has to be changed whenever you want to receive payments for new products or services. Many projects work around this unfavorable coupling by issuing payment required events, but these are not events, as the sender wants somebody else to do something. This is a command! The order service commands the payment to retrieve money. In this case the sender knows about the command to send and decides to use it; the coupling is on the sending side. </p>

<p>Every communication between two services involves some degree of coupling in order to be effective, but depending on the problem at hand it may be more appropriate to implement the coupling within one side over the other.</p>

<p><img src="https://res.infoq.com/articles/monitor-workflow-collaborating-microservices/en/resources/1monitor-workflow-collaborating-microservices-14-1551187248047.jpg" alt=""/></p>

<p>The order service might even be responsible of orchestrating more services and keeping track of the sequence of steps in order fulfillment. I discussed the advantages in the above mentioned talk in detail. The tricky part is that a good architecture needs to balance orchestration and choreography, which is not always easy to do.</p>

<p>But for this article I wanted to focus on visibility. And there is an obvious advantage of orchestration using a workflow engine; the model is not only the code to execute the orchestration, it can also be used directly for visibility of the flow.</p>

<p><img src="https://res.infoq.com/articles/monitor-workflow-collaborating-microservices/en/resources/1monitor-workflow-collaborating-microservices-15-1551187249488.jpg" alt=""/></p>

<h2 id="toc_10">Summary</h2>

<p>It is essential to get visibility into your business processes, independently of the way they are implemented. I discussed multiple possibilities, and in most real-world situations it typically boils down to some event-monitoring using Elastic-like tools or process tracking using workflow engines. This might depend slightly on the use case and role involved, so business analysts need to understand the data collected from all instances on the correct granularity, whereas operations need to look into one specific instance in varying granularity and probably want to have tools to resolve system-level incidents quickly.</p>

<p>If you are a choreographed shop, process-tracking can lead you to a journey towards more orchestration, which I think is a very important step in keeping control of your business processes in the long run. Otherwise, you might “make nicely decoupled systems with event notification, without realizing that you’re losing sight of that larger-scale flow, and thus set yourself up for trouble in future years”, <a href="https://martinfowler.com/articles/201701-event-driven.html">as Martin Fowler puts it</a>. If you are working on a more greenfield system, you should find a good balance for orchestration and choreography right from the beginning.</p>

<p>However, regardless of the implementation details of your system, make sure you have a business-friendly view of your business processes implemented by collaborating services.</p>

<h2 id="toc_11">About the Author</h2>

<p><strong><img src="https://res.infoq.com/articles/monitor-workflow-collaborating-microservices/en/resources/1Bernd-ruecker-1551189452268.jpeg" alt=""/>Bernd Ruecker</strong> is co-founder and technologist at Camunda. Previously, he has helped automating highly scalable core workflows at global companies including T-Mobile, Lufthansa, Zalando. He is currently focused on new workflow automation paradigms that fit into modern architectures around distributed systems, microservices, domain-driven design, event-driven architecture and reactive systems.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[推荐一款很好用的内网穿透工具 FRP]]></title>
    <link href="http://panlw.github.io/15514075339420.html"/>
    <updated>2019-03-01T10:32:13+08:00</updated>
    <id>http://panlw.github.io/15514075339420.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://www.hi-linux.com/posts/25686.html">https://www.hi-linux.com/posts/25686.html</a></p>
</blockquote>

<p>对于没有公网 <code>IP</code> 的内网用户来说，远程管理或在外网访问内网机器上的服务是一个问题。通常解决方案就是用内网穿透工具将内网的服务穿透到公网中，便于远程管理和在外部访问。内网穿透的工具很多，之前也介绍过 <a href="https://www.hi-linux.com/posts/29097.html">Ngrok</a><a href="https://www.hi-linux.com/posts/24471.html">、Localtunnel</a>。</p>

<p>今天给大家介绍另一款好用内网穿透工具 <code>FRP</code>，<code>FRP</code> 全名：Fast Reverse Proxy。<code>FRP</code> 是一个使用 <code>Go</code> 语言开发的高性能的反向代理应用，可以帮助您轻松地进行内网穿透，对外网提供服务。<code>FRP</code> 支持 <code>TCP</code>、<code>UDP</code>、<code>HTTP</code>、<code>HTTPS</code>等协议类型，并且支持 <code>Web</code> 服务根据域名进行路由转发。</p>

<p>FRP 项目地址：<a href="https://github.com/fatedier/frp">https://github.com/fatedier/frp</a></p>

<p><strong>FRP 的作用</strong></p>

<ul>
<li>  利用处于内网或防火墙后的机器，对外网环境提供 <code>HTTP</code> 或 <code>HTTPS</code> 服务。</li>
<li>  对于 <code>HTTP</code>, <code>HTTPS</code> 服务支持基于域名的虚拟主机，支持自定义域名绑定，使多个域名可以共用一个 80 端口。</li>
<li>  利用处于内网或防火墙后的机器，对外网环境提供 <code>TCP</code> 和 <code>UDP</code> 服务，例如在家里通过 <code>SSH</code> 访问处于公司内网环境内的主机。</li>
</ul>

<p><strong>FRP 架构</strong></p>

<p><img src="https://www.hi-linux.com/img/linux/frp-architecture.png" alt=""/></p>

<h3 id="toc_0"><a href="#frp-%E5%AE%89%E8%A3%85"></a>FRP 安装</h3>

<p><code>FRP</code> 采用 <code>Go</code> 语言开发，支持 <code>Windows</code>、<code>Linux</code>、<code>MacOS</code>、<code>ARM</code>等多平台部署。<code>FRP</code> 安装非常容易，只需下载对应系统平台的软件包，并解压就可用了。</p>

<p>这里以 <code>Linux</code> 为例，为了方便管理我们把解压后的目录重命名为 frp ：</p>

<pre><code class="language-sh">$ wget https://github.com/fatedier/frp/releases/download/v0.15.1/frp_0.15.1_linux_amd64.tar.gz
$ tar xzvf frp_0.15.1_linux_amd64.tar.gz
$ mv frp_0.15.1_linux_amd64 frp
</code></pre>

<p>更多平台的软件包下载地址：<a href="https://github.com/fatedier/frp/releases">https://github.com/fatedier/frp/releases</a></p>

<h3 id="toc_1"><a href="#frp-%E9%85%8D%E7%BD%AE"></a>FRP 配置</h3>

<h4 id="toc_2"><a href="#frp-%E6%9C%8D%E5%8A%A1%E7%AB%AF%E9%85%8D%E7%BD%AE"></a>FRP 服务端配置</h4>

<p>配置 <code>FRP</code> 服务端的前提条件是需要一台具有公网 <code>IP</code> 的设备，得益于 <code>FRP</code> 是 <code>Go</code> 语言开发的，具有良好的跨平台特性。你可以在 <code>Windows</code>、<code>Linux</code>、<code>MacOS</code>、<code>ARM</code>等几乎任何可联网设备上部署。</p>

<p>这里以 <code>Linux</code> 为例，<code>FRP</code> 默认给出两个服务端配置文件，一个是简版的 frps.ini，另一个是完整版本 frps_full.ini。</p>

<p>我们先来看看简版的 frps.ini，通过这个配置可以快速的搭建起一个 FRP 服务端。</p>

<pre><code class="language-sh">$ cat frps.ini

[common]
bind_port = 7000
</code></pre>

<blockquote>
<ul>
<li>  默认配置中监听的是 7000 端口，可根据自己实际情况修改。</li>
</ul>
</blockquote>

<p>启动 FRP 服务端</p>

<pre><code class="language-sh">$ ./frps -c ./frps.ini
2018/01/25 10:52:45 [I] [service.go:96] frps tcp listen on 0.0.0.0:7000
2018/01/25 10:52:45 [I] [main.go:112] Start frps success
2018/01/25 10:52:45 [I] [main.go:114] PrivilegeMode is enabled, you should pay more attention to security issues
</code></pre>

<p>通过上面简单的两步就可以成功启动一个监听在 7000 端口的 <code>FRP</code> 服务端。</p>

<h4 id="toc_3"><a href="#frp-%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%85%8D%E7%BD%AE"></a>FRP 客户端配置</h4>

<p>和 FRP 服务端类似，<code>FRP</code> 默认也给出两个客户端配置文件，一个是简版的 frpc.ini，另一个是完整版本 frpc_full.ini。</p>

<p>这里同样以简版的 frpc.ini 文件为例，假设 FRP 服务端所在服务器的公网 <code>IP</code> 为 4.3.2.1。</p>

<pre><code class="language-sh">$ vim frpc.ini

[common]
# server_addr 为 FRP 服务端的公网 IP 
server_addr = 4.3.2.1
# server_port 为 FRP 服务端监听的端口 
server_port = 7000
</code></pre>

<p>启动 FRP 客户端</p>

<pre><code>$ ./frpc -c ./frpc.ini
2018/01/25 11:15:49 [I] [proxy_manager.go:284] proxy removed: []
2018/01/25 11:15:49 [I] [proxy_manager.go:294] proxy added: []
2018/01/25 11:15:49 [I] [proxy_manager.go:317] visitor removed: []
2018/01/25 11:15:49 [I] [proxy_manager.go:326] visitor added: []
2018/01/25 11:15:49 [I] [control.go:240] [83775d7388b8e7d9] login to server success, get run id [83775d7388b8e7d9], server udp port [0]
</code></pre>

<p>这样就可以成功在 <code>FRP</code> 服务端上成功建立一个客户端连接，当然现在还并不能对外提供任何内网机器上的服务，因为我们并还没有在 <code>FRP</code> 服务端注册任何内网服务的端口。</p>

<h3 id="toc_4"><a href="#frp-%E4%BD%BF%E7%94%A8%E5%AE%9E%E4%BE%8B"></a>FRP 使用实例</h3>

<p>下面我们就来看几个常用的例子，通过这些例子来了解下 FRP 是如何实现内网服务穿透的。</p>

<h4 id="toc_5"><a href="#%E9%80%9A%E8%BF%87-tcp-%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%E6%9C%BA%E5%99%A8"></a>通过 TCP 访问内网机器</h4>

<p>这里以访问 <code>SSH</code> 服务为例， 修改 FRP 客户端配置文件 frpc.ini 文件并增加如下内容：</p>

<pre><code class="language-sh">$ cat frpc.ini

[ssh]
type = tcp
local_ip = 127.0.0.1
local_port = 22
remote_port = 6000
</code></pre>

<p>启动 FRP 客户端</p>

<pre><code class="language-sh">$ ./frpc -c ./frpc.ini
2018/01/25 12:21:23 [I] [proxy_manager.go:284] proxy removed: []
2018/01/25 12:21:23 [I] [proxy_manager.go:294] proxy added: [ssh]
2018/01/25 12:21:23 [I] [proxy_manager.go:317] visitor removed: []
2018/01/25 12:21:23 [I] [proxy_manager.go:326] visitor added: []
2018/01/25 12:21:23 [I] [control.go:240] [3b468a55191341cb] login to server success, get run id [3b468a55191341cb], server udp port [0]
2018/01/25 12:21:23 [I] [control.go:165] [3b468a55191341cb] [ssh] start proxy success
</code></pre>

<p>这样就在 <code>FRP</code> 服务端上成功注册了一个端口为 6000 的服务，接下来我们就可以通过这个端口访问内网机器上 <code>SSH</code> 服务，假设用户名为 mike：</p>

<pre><code class="language-sh">$ ssh -oPort=6000 mike@4.3.2.1
</code></pre>

<h4 id="toc_6"><a href="#%E9%80%9A%E8%BF%87%E8%87%AA%E5%AE%9A%E4%B9%89%E5%9F%9F%E5%90%8D%E8%AE%BF%E9%97%AE%E9%83%A8%E7%BD%B2%E4%BA%8E%E5%86%85%E7%BD%91%E7%9A%84-web-%E6%9C%8D%E5%8A%A1"></a>通过自定义域名访问部署于内网的 Web 服务</h4>

<p>有时需要在公有网络通过域名访问我们在本地环境搭建的 <code>Web</code> 服务，但是由于本地环境机器并没有公网 <code>IP</code>，无法将域名直接解析到本地的机器。</p>

<p>现在通过 <code>FRP</code> 就可以很容易实现这一功能，这里以 <code>HTTP</code> 服务为例：首先修改 <code>FRP</code> 服务端配置文件，通过 <code>vhost_http_port</code> 参数来设置 <code>HTTP</code> 访问端口，这里将 <code>HTTP</code> 访问端口设为 8080。</p>

<pre><code class="language-sh">$ vim frps.ini
[common]
bind_port = 7000
vhost_http_port = 8080
</code></pre>

<p>启动 FRP 服务端</p>

<pre><code>$ ./frps -c ./frps.ini
2018/01/25 13:33:26 [I] [service.go:96] frps tcp listen on 0.0.0.0:7000
2018/01/25 13:33:26 [I] [service.go:125] http service listen on 0.0.0.0:8080
2018/01/25 13:33:26 [I] [main.go:112] Start frps success
2018/01/25 13:33:26 [I] [main.go:114] PrivilegeMode is enabled, you should pay more attention to security issues
</code></pre>

<p>其次我们在修改 <code>FRP</code> 客户端配置文件并增加如下内容：</p>

<pre><code class="language-sh">$ vim frpc.ini

[web]
type = http
local_port = 80
custom_domains = mike.hi-linux.com
</code></pre>

<p>这里通过 <code>local_port</code> 和 <code>custom_domains</code> 参数来设置本地机器上 <code>Web</code> 服务对应的端口和自定义的域名，这里我们分别设置端口为 80，对应域名为 <code>mike.hi-linux.com</code>。</p>

<p>启动 FRP 客户端</p>

<pre><code class="language-sh">$ ./frpc -c ./frpc.ini
2018/01/25 13:56:11 [I] [proxy_manager.go:284] proxy removed: []
2018/01/25 13:56:11 [I] [proxy_manager.go:294] proxy added: [web ssh]
2018/01/25 13:56:11 [I] [proxy_manager.go:317] visitor removed: []
2018/01/25 13:56:11 [I] [proxy_manager.go:326] visitor added: []
2018/01/25 13:56:11 [I] [control.go:240] [296fe9e31a551e07] login to server success, get run id [296fe9e31a551e07], server udp port [0]
2018/01/25 13:56:11 [I] [control.go:165] [296fe9e31a551e07] [web] start proxy success
2018/01/25 13:56:11 [I] [control.go:165] [296fe9e31a551e07] [ssh] start proxy success
</code></pre>

<p>最后将 <code>mike.hi-linux.com</code> 的域名 A 记录解析到 <code>FRP</code> 服务器的公网 <code>IP</code> 上，现在便可以通过 <code>http://mike.hi-linux.com:8080</code> 这个 <code>URL</code> 访问到处于内网机器上对应的 <code>Web</code> 服务。</p>

<blockquote>
<ul>
<li>  <code>HTTPS</code> 服务配置方法类似，只需将 <code>vhost_http_port</code> 替换为 <code>vhost_https_port</code>， type 设置为 <code>https</code> 即可。</li>
</ul>
</blockquote>

<h5 id="toc_7"><a href="#%E9%80%9A%E8%BF%87%E5%AF%86%E7%A0%81%E4%BF%9D%E6%8A%A4%E4%BD%A0%E7%9A%84-web-%E6%9C%8D%E5%8A%A1"></a>通过密码保护你的 Web 服务</h5>

<p>由于所有客户端共用一个 <code>FRP</code> 服务端的 <code>HTTP</code> 服务端口，任何知道你的域名和 <code>URL</code> 的人都能访问到你部署在内网的 <code>Web</code> 服务，但是在某些场景下需要确保只有限定的用户才能访问。</p>

<p><code>FRP</code> 支持通过 HTTP Basic Auth 来保护你的 <code>Web</code> 服务，使用户需要通过用户名和密码才能访问到你的服务。需要实现此功能主要需要在 <code>FRP</code> 客户端的配置文件中添加用户名和密码的设置。</p>

<pre><code class="language-sh">$ vim frpc.ini

[web]
type = http
local_port = 80
custom_domains = mike.hi-linux.com
# 设置认证的用户名
http_user = abc
# 设置认证的密码
http_pwd = abc
</code></pre>

<p>这时访问 <code>http://mike.hi-linux.com:8080</code> 这个 URL 时就需要输入配置的用户名和密码才能访问。</p>

<blockquote>
<ul>
<li>  该功能目前仅限于 HTTP 类型的代理。</li>
</ul>
</blockquote>

<h5 id="toc_8"><a href="#%E7%BB%99-web-%E6%9C%8D%E5%8A%A1%E5%A2%9E%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E4%BA%8C%E7%BA%A7%E5%9F%9F%E5%90%8D"></a>给 Web 服务增加自定义二级域名</h5>

<p>在多人同时使用一个 <code>FRP</code> 服务端实现 <code>Web</code> 服务时，通过自定义二级域名的方式来使用会更加方便。</p>

<p>通过在 <code>FRP</code> 服务端的配置文件中配置 <code>subdomain_host</code>参数就可以启用该特性。之后在 <code>FRP</code> 客户端的 http、https 类型的代理中可以不配置 <code>custom_domains</code>，而是配置一个 <code>subdomain</code> 参数。</p>

<p>然后只需要将 <code>*.{subdomain_host}</code> 解析到 <code>FRP</code> 服务端所在服务器。之后用户可以通过 <code>subdomain</code> 自行指定自己的 <code>Web</code> 服务所需要使用的二级域名，并通过 <code>{subdomain}.{subdomain_host}</code> 来访问自己的 <code>Web</code> 服务。</p>

<p>首先我们在 <code>FRP</code> 服务端配置 <code>subdomain_host</code> 参数：</p>

<pre><code class="language-sh">$ vim frps.ini
[common]
subdomain_host = hi-linux.com
</code></pre>

<p>其次在 <code>FRP</code> 客户端配置文件配置 <code>subdomain</code> 参数：</p>

<pre><code class="language-sh">$ vim frpc.ini
[web]
type = http
local_port = 80
subdomain = test
</code></pre>

<p>然后将泛域名 *.hi-linux.com 解析到 <code>FRP</code> 服务端所在服务器的公网 <code>IP</code> 地址。FRP 服务端 和 FRP 客户端都启动成功后，通过 <code>test.hi-linux.com</code> 就可以访问到内网的 <code>Web</code> 服务。</p>

<blockquote>
<ul>
<li>  同一个 <code>HTTP</code> 或 <code>HTTPS</code> 类型的代理中 <code>custom_domains</code> 和 <code>subdomain</code> 可以同时配置。</li>
<li>  需要注意的是如果 <code>FPR</code> 服务端配置了 <code>subdomain_host</code>，则 <code>custom_domains</code> 中不能是属于 <code>subdomain_host</code> 的子域名或者泛域名。</li>
</ul>
</blockquote>

<h5 id="toc_9"><a href="#%E4%BF%AE%E6%94%B9-host-header"></a>修改 Host Header</h5>

<p>通常情况下 <code>FRP</code> 不会修改转发的任何数据。但有一些后端服务会根据 <code>HTTP</code> 请求 <code>header</code> 中的 host 字段来展现不同的网站，例如 <code>Nginx</code> 的虚拟主机服务，启用 host-header 的修改功能可以动态修改 <code>HTTP</code> 请求中的 host 字段。</p>

<p>实现此功能只需要在 FRP 客户端配置文件中定义 <code>host_header_rewrite</code> 参数。</p>

<pre><code class="language-sh">$ vim frpc.ini
[web]
type = http
local_port = 80
custom_domains = test.hi-linux.com
host_header_rewrite = dev.hi-linux.com
</code></pre>

<p>原来 <code>HTTP</code> 请求中的 host 字段 <code>test.hi-linux.com</code> 转发到后端服务时会被替换为 <code>dev.hi-linux.com</code>。</p>

<blockquote>
<ul>
<li>  该功能仅限于 HTTP 类型的代理。</li>
</ul>
</blockquote>

<h5 id="toc_10"><a href="#url-%E8%B7%AF%E7%94%B1"></a>URL 路由</h5>

<p><code>FRP</code> 支持根据请求的 <code>URL</code> 路径路由转发到不同的后端服务。要实现这个功能可通过 <code>FRP</code> 客户端配置文件中的 <code>locations</code> 字段来指定。</p>

<pre><code class="language-sh">$ vim frpc.ini

[web01]
type = http
local_port = 80
custom_domains = web.hi-linux.com
locations = /

[web02]
type = http
local_port = 81
custom_domains = web.hi-linux.com
locations = /news,/about
</code></pre>

<p>按照上述的示例配置后，<code>web.hi-linux.com</code> 这个域名下所有以 /news 以及 /about 作为前缀的 <code>URL</code> 请求都会被转发到后端 web02 所在的后端服务，其余的请求会被转发到 web01 所在的后端服务。</p>

<blockquote>
<ul>
<li>  目前仅支持最大前缀匹配，之后会考虑支持正则匹配。</li>
</ul>
</blockquote>

<h4 id="toc_11"><a href="#%E9%80%9A%E8%BF%87-udp-%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%E6%9C%BA%E5%99%A8"></a>通过 UDP 访问内网机器</h4>

<p><code>DNS</code> 查询请求通常使用 <code>UDP</code> 协议，<code>FRP</code> 支持对内网 <code>UDP</code> 服务的穿透，配置方式和 <code>TCP</code> 基本一致。这里以转发到 Google 的 <code>DNS</code> 查询服务器 8.8.8.8 的 <code>UDP</code> 端口为例。</p>

<p>首先修改 FRP 客户端配置文件，并增加如下内容：</p>

<pre><code class="language-sh">$ vim frpc.ini
[dns]
type = udp
local_ip = 8.8.8.8
local_port = 53
remote_port = 6001
</code></pre>

<blockquote>
<ul>
<li>  要转发到内网 DNS 服务器只需把 <code>local_ip</code> 改成对应 IP 即可。</li>
</ul>
</blockquote>

<p>其次重新启动 <code>FRP</code> 客户端：</p>

<pre><code class="language-sh">$ ./frpc -c ./frpc.ini
2018/01/25 14:54:17 [I] [proxy_manager.go:284] proxy removed: []
2018/01/25 14:54:17 [I] [proxy_manager.go:294] proxy added: [ssh web dns]
2018/01/25 14:54:17 [I] [proxy_manager.go:317] visitor removed: []
2018/01/25 14:54:17 [I] [proxy_manager.go:326] visitor added: []
2018/01/25 14:54:17 [I] [control.go:240] [33e1de8a771112a6] login to server success, get run id [33e1de8a771112a6], server udp port [0]
2018/01/25 14:54:17 [I] [control.go:165] [33e1de8a771112a6] [ssh] start proxy success
2018/01/25 14:54:17 [I] [control.go:165] [33e1de8a771112a6] [web] start proxy success
2018/01/25 14:54:17 [I] [control.go:165] [33e1de8a771112a6] [dns] start proxy success
</code></pre>

<p>最后通过 <code>dig</code> 命令测试 <code>UDP</code> 包转发是否成功，预期会返回 <code>www.google.com</code> 域名的解析结果：</p>

<pre><code class="language-sh">$ dig @4.3.2.1 -p 6001 www.google.com
...

;; QUESTION SECTION:
;www.google.com.            IN  A

;; ANSWER SECTION:
www.google.com.     79  IN  A   69.63.184.30

...
</code></pre>

<h4 id="toc_12"><a href="#%E8%BD%AC%E5%8F%91-unix-%E5%9F%9F%E5%A5%97%E6%8E%A5%E5%AD%97"></a>转发 Unix 域套接字</h4>

<p>通过 <code>TCP</code> 端口访问内网的 <code>Unix</code> 域套接字，这里以和本地机器上的 Docker Daemon 通信为例。</p>

<p>首先修改 <code>FRP</code> 客户端配置文件，并增加如下内容：</p>

<pre><code class="language-sh">$ vim frpc.ini
[unix_domain_socket]
type = tcp
remote_port = 6002
plugin = unix_domain_socket
plugin_unix_path = /var/run/docker.sock
</code></pre>

<p>这里主要是使用 <code>plugin</code> 和 <code>plugin_unix_path</code> 两个参数启用了 <code>unix_domain_socket</code> 插件和配置对应的套接字路径。</p>

<p>其次重新启动 <code>FRP</code> 客户端：</p>

<pre><code class="language-sh">$ ./frpc -c ./frpc.ini

2018/01/25 15:09:33 [I] [proxy_manager.go:284] proxy removed: []
2018/01/25 15:09:33 [I] [proxy_manager.go:294] proxy added: [ssh web dns unix_domain_socket]
2018/01/25 15:09:33 [I] [proxy_manager.go:317] visitor removed: []
2018/01/25 15:09:33 [I] [proxy_manager.go:326] visitor added: []
2018/01/25 15:09:33 [I] [control.go:240] [f6424f0deb8b6ff7] login to server success, get run id [f6424f0deb8b6ff7], server udp port [0]
2018/01/25 15:09:33 [I] [control.go:165] [f6424f0deb8b6ff7] [ssh] start proxy success
2018/01/25 15:09:33 [I] [control.go:165] [f6424f0deb8b6ff7] [web] start proxy success
2018/01/25 15:09:33 [I] [control.go:165] [f6424f0deb8b6ff7] [dns] start proxy success
2018/01/25 15:09:33 [I] [control.go:165] [f6424f0deb8b6ff7] [unix_domain_socket] start proxy success
</code></pre>

<p>最后通过 <code>curl</code> 命令查看 <code>Docker</code> 版本信息进行测试：</p>

<pre><code class="language-sh">$ curl http://4.3.2.1:6002/version

{&quot;Platform&quot;:{&quot;Name&quot;:&quot;&quot;},&quot;Components&quot;:[{&quot;Name&quot;:&quot;Engine&quot;,&quot;Version&quot;:&quot;17.12.0-ce&quot;,&quot;Details&quot;:{&quot;ApiVersion&quot;:&quot;1.35&quot;,&quot;Arch&quot;:&quot;amd64&quot;,&quot;BuildTime&quot;:&quot;2017-12-27T20:12:29.000000000+00:00&quot;,&quot;Experimental&quot;:&quot;true&quot;,&quot;GitCommit&quot;:&quot;c97c6d6&quot;,&quot;GoVersion&quot;:&quot;go1.9.2&quot;,&quot;KernelVersion&quot;:&quot;4.9.60-linuxkit-aufs&quot;,&quot;MinAPIVersion&quot;:&quot;1.12&quot;,&quot;Os&quot;:&quot;linux&quot;}}],&quot;Version&quot;:&quot;17.12.0-ce&quot;,&quot;ApiVersion&quot;:&quot;1.35&quot;,&quot;MinAPIVersion&quot;:&quot;1.12&quot;,&quot;GitCommit&quot;:&quot;c97c6d6&quot;,&quot;GoVersion&quot;:&quot;go1.9.2&quot;,&quot;Os&quot;:&quot;linux&quot;,&quot;Arch&quot;:&quot;amd64&quot;,&quot;KernelVersion&quot;:&quot;4.9.60-linuxkit-aufs&quot;,&quot;Experimental&quot;:true,&quot;BuildTime&quot;:&quot;2017-12-27T20:12:29.000000000+00:00&quot;}
</code></pre>

<blockquote>
<ul>
<li>  <code>FRP</code> 从 1.5 版本开始支持客户端热加载配置文件，并不用每次都重启客户端程序。具体方法在后文 <code>FRP</code> 客户端热加载配置文件部分讲解。</li>
</ul>
</blockquote>

<h3 id="toc_13"><a href="#frp-%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6"></a>FRP 高级进阶</h3>

<h4 id="toc_14"><a href="#%E7%BB%99-frp-%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%A2%9E%E5%8A%A0%E4%B8%80%E4%B8%AA-dashboard"></a>给 FRP 服务端增加一个 Dashboard</h4>

<p>通过 <code>Dashboard</code> 可以方便的查看 <code>FRP</code> 的状态以及代理统计信息展示，要使用这个功能首先需要在 <code>FRP</code> 服务端配置文件中指定 <code>Dashboard</code> 服务使用的端口：</p>

<pre><code class="language-sh">$ vim frps.ini

[common]

# 指定 Dashboard 的监听的 IP 地址
dashboard_addr = 0.0.0.0

# 指定 Dashboard 的监听的端口
dashboard_port = 7500

# 指定访问 Dashboard 的用户名
dashboard_user = admin

# 指定访问 Dashboard 的端口
dashboard_pwd = admin
</code></pre>

<p>其次重新启动 FRP 服务端：</p>

<pre><code class="language-sh">$ ./frps -c ./frps.ini

2018/01/25 16:39:29 [I] [service.go:96] frps tcp listen on 0.0.0.0:7000
2018/01/25 16:39:29 [I] [service.go:125] http service listen on 0.0.0.0:8080
2018/01/25 16:39:29 [I] [service.go:164] Dashboard listen on 0.0.0.0:7500
2018/01/25 16:39:29 [I] [main.go:112] Start frps success
2018/01/25 16:39:29 [I] [main.go:114] PrivilegeMode is enabled, you should pay more attention to security issues
</code></pre>

<p>最后通过 <code>http://[server_addr]:7500</code> 访问 Dashboard 界面，用户名密码默认都为 admin。</p>

<p><img src="https://www.hi-linux.com/img/linux/frp1.png" alt=""/></p>

<p><img src="https://www.hi-linux.com/img/linux/frp2.png" alt=""/></p>

<h4 id="toc_15"><a href="#%E7%BB%99-frp-%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%8A%A0%E4%B8%8A%E8%BA%AB%E4%BB%BD%E9%AA%8C%E8%AF%81"></a>给 FRP 服务端加上身份验证</h4>

<p>默认情况下只要知道 <code>FRP</code> 服务端开放的端口，任意 <code>FRP</code> 客户端都可以随意在服务端上注册端口映射，这样对于在公网上的 <code>FRP</code> 服务来说显然不太安全。<code>FRP</code> 提供了身份验证机制来提高 <code>FRP</code> 服务端的安全性。要启用这一特性也很简单，只需在 <code>FRP</code> 服务端和 <code>FRP</code> 客户端的 common 配置中启用 <code>privilege_token</code> 参数就行。</p>

<pre><code class="language-sh">[common]
privilege_token = 12345678
</code></pre>

<p>启用这一特性后，只有 <code>FRP</code> 服务端和 <code>FRP</code> 客户端的 common 配置中的 <code>privilege_token</code> 参数一致身份验证才会通过，<code>FRP</code> 客户端才能成功在 <code>FRP</code> 服务端注册端口映射。否则就会注册失败，出现类似下面的错误：</p>

<pre><code class="language-sh">2018/01/25 17:29:27 [I] [proxy_manager.go:284] proxy removed: []
2018/01/25 17:29:27 [I] [proxy_manager.go:294] proxy added: [ssh web dns unix_domain_socket]
2018/01/25 17:29:27 [I] [proxy_manager.go:317] visitor removed: []
2018/01/25 17:29:27 [I] [proxy_manager.go:326] visitor added: []
2018/01/25 17:29:27 [E] [control.go:230] authorization failed
2018/01/25 17:29:27 [W] [control.go:109] login to server failed: authorization failed
authorization failed
</code></pre>

<blockquote>
<p>需要注意的是 <code>FRP</code> 客户端所在机器和 <code>FRP</code> 服务端所在机器的时间相差不能超过 15 分钟，因为时间戳会被用于加密验证中，防止报文被劫持后被其他人利用。这个超时时间可以在配置文件中通过 <code>authentication_timeout</code> 这个参数来修改，单位为秒，默认值为 900，即 15 分钟。如果修改为 0，则 <code>FRP</code> 服务端将不对身份验证报文的时间戳进行超时校验。</p>
</blockquote>

<h4 id="toc_16"><a href="#frp-%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%83%AD%E5%8A%A0%E8%BD%BD%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"></a>FRP 客户端热加载配置文件</h4>

<p>当修改了 <code>FRP</code> 客户端中的配置文件，从 0.15 版本开始可以通过 <code>frpc reload</code> 命令来动态加载配置文件，通常会在 10 秒内完成代理的更新。</p>

<p>启用此功能需要在 <code>FRP</code> 客户端配置文件中启用 admin 端口，用于提供 <code>API</code> 服务。配置如下：</p>

<pre><code class="language-sh">$ vim frpc.ini

[common]
admin_addr = 127.0.0.1
admin_port = 7400
</code></pre>

<p>重启 <code>FRP</code> 客户端，以后就可通过热加载方式进行 <code>FRP</code> 客户端配置变更了。</p>

<pre><code class="language-sh">$ ./frpc -c ./frpc.ini
2018/01/25 18:04:25 [I] [proxy_manager.go:326] visitor added: []
2018/01/25 18:04:25 [I] [control.go:240] [3653b9a878f8acc7] login to server success, get run id [3653b9a878f8acc7], server udp port [0]
2018/01/25 18:04:25 [I] [service.go:49] admin server listen on 127.0.0.1:7400
2018/01/25 18:04:25 [I] [control.go:165] [3653b9a878f8acc7] [ssh] start proxy success
2018/01/25 18:04:25 [I] [control.go:165] [3653b9a878f8acc7] [web] start proxy success
2018/01/25 18:04:25 [I] [control.go:165] [3653b9a878f8acc7] [dns] start proxy success
2018/01/25 18:04:25 [I] [control.go:165] [3653b9a878f8acc7] [unix_domain_socket] start proxy success

$ ./frpc reload -c ./frpc.ini
reload success
</code></pre>

<p>等待一段时间后客户端会根据新的配置文件创建、更新、删除代理。</p>

<blockquote>
<ul>
<li>  需要注意的是 [common] 中的参数除了 start 外目前无法被修改。</li>
</ul>
</blockquote>

<p>启用 <code>admin_addr</code> 后，还可以通过 <code>frpc status -c ./frpc.ini</code> 命令在 FRP 客户端很方便的查看当前代理状态信息。</p>

<pre><code class="language-sh">$ ./frpc status -c ./frpc.ini

Proxy Status...
TCP
Name                Status   LocalAddr     Plugin              RemoteAddr           Error
ssh                 running  127.0.0.1:22                      4.3.2.1:6000
unix_domain_socket  running                unix_domain_socket  4.3.2.1:6002

UDP
Name  Status   LocalAddr   Plugin  RemoteAddr           Error
dns   running  8.8.8.8:53          4.3.2.1:6001

HTTP
Name  Status   LocalAddr     Plugin  RemoteAddr              Error
web   running  127.0.0.1:80          mike.hi-linux.com:8080
</code></pre>

<h4 id="toc_17"><a href="#%E7%BB%99-frp-%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%A2%9E%E5%8A%A0%E7%AB%AF%E5%8F%A3%E7%99%BD%E5%90%8D%E5%8D%95"></a>给 FRP 服务端增加端口白名单</h4>

<p>为了防止 <code>FRP</code> 端口被滥用，<code>FRP</code> 提供了指定允许哪些端口被分配的功能。可通过 <code>FRP</code> 服务端的配置文件中 <code>privilege_allow_ports</code> 参数来指定：</p>

<pre><code class="language-sh">$ vim frps.ini

[common]
privilege_allow_ports = 2000-3000,3001,3003,4000-5000
</code></pre>

<blockquote>
<p><code>privilege_allow_ports</code> 可以配置允许使用的某个指定端口或者是一个范围内的所有端口，以 , 分隔，指定的范围以 - 分隔。</p>
</blockquote>

<p>当使用不允许的端口注册时，就会注册失败。出现类似以下错误：</p>

<pre><code class="language-sh">$ ./frpc status -c ./frpc.ini
Proxy Status...
TCP
Name                Status       LocalAddr     Plugin              RemoteAddr            Error
ssh                 start error  127.0.0.1:22                      4.3.2.1:60000  port not allowed
unix_domain_socket  start error                unix_domain_socket  4.3.2.1:60002  port not allowed
</code></pre>

<h4 id="toc_18"><a href="#%E5%90%AF%E7%94%A8-tcp-%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8"></a>启用 TCP 多路复用</h4>

<p>从 v0.10.0 版本开始，客户端和服务器端之间的连接支持多路复用，不再需要为每一个用户请求创建一个连接，使连接建立的延迟降低，并且避免了大量文件描述符的占用，使 <code>FRP</code> 可以承载更高的并发数。</p>

<p>该功能默认启用，如需关闭可以在 <code>FRP</code> 服务端配置文件和 <code>FRP</code> 客户端配置文件中配置，该配置项在服务端和客户端必须一致：</p>

<pre><code class="language-sh"># frps.ini 和 frpc.ini 中
[common]
tcp_mux = false
</code></pre>

<h4 id="toc_19"><a href="#frp-%E5%BA%95%E5%B1%82%E9%80%9A%E4%BF%A1%E5%90%AF%E7%94%A8-kcp-%E5%8D%8F%E8%AE%AE"></a>FRP 底层通信启用 KCP 协议</h4>

<p>FRP 从 v0.12.0 版本开始，底层通信协议支持选择 <code>KCP</code> 协议，在弱网络环境下传输效率会提升明显，但是会有一些额外的流量消耗。</p>

<p>要开启 <code>KCP</code> 协议支持，首先要在 <code>FRP</code> 服务端配置文件中启用 <code>KCP</code> 协议支持：</p>

<pre><code class="language-sh">$ vim frps.ini
[common]
bind_port = 7000
# 指定一个 UDP 端口用于接收客户端请求 KCP 绑定的是 UDP 端口，可以和 bind_port 一样
kcp_bind_port = 7000
</code></pre>

<p>其次是在 <code>FRP</code> 客户端配置文件指定需要使用的协议类型，目前只支持 <code>TCP</code> 和 <code>KCP</code>。其它代理配置不需要变更：</p>

<pre><code class="language-sh">$ vim  frpc.ini
[common]
server_addr = 4.3.2.1
# server_port 指定为 FRP 服务端里 kcp_bind_port 指定的端口
server_port = 7000
# 指定需要使用的协议类型，默认类型为 TCP
protocol = kcp
</code></pre>

<blockquote>
<ul>
<li>  需要注意开放相关机器上的 UDP 端口的访问权限。</li>
</ul>
</blockquote>

<h4 id="toc_20"><a href="#%E7%BB%99-frp-%E6%9C%8D%E5%8A%A1%E7%AB%AF%E9%85%8D%E7%BD%AE%E8%BF%9E%E6%8E%A5%E6%B1%A0"></a>给 FRP 服务端配置连接池</h4>

<p>默认情况下，当用户请求建立连接后，<code>FRP</code> 服务端才会请求 <code>FRP</code> 客户端主动与后端服务建立一个连接。</p>

<p>当为指定的 <code>FRP</code> 服务端启用连接池功能后，<code>FRP</code> 会预先和后端服务建立起指定数量的连接，每次接收到用户请求后，会从连接池中取出一个连接和用户连接关联起来，避免了等待与后端服务建立连接以及 <code>FRP</code> 客户端 和 <code>FRP</code> 服务端之间传递控制信息的时间。</p>

<p>首先需要在 <code>FRP</code> 服务端配置文件中设置每个代理可以创建的连接池上限，避免大量资源占用，客户端设置超过此配置后会被调整到当前值：</p>

<pre><code class="language-sh">$ vim frps.ini
[common]
max_pool_count = 5
</code></pre>

<p>其次在 <code>FRP</code> 客户端配置文件中为客户端启用连接池，指定预创建连接的数量：</p>

<pre><code class="language-sh">$ vim frpc.ini
[common]
pool_count = 1
</code></pre>

<blockquote>
<ul>
<li>  此功能比较适合有大量短连接请求时开启。</li>
</ul>
</blockquote>

<h4 id="toc_21"><a href="#%E5%8A%A0%E5%AF%86%E4%B8%8E%E5%8E%8B%E7%BC%A9"></a>加密与压缩</h4>

<p>如果公司内网防火墙对外网访问进行了流量识别与屏蔽，例如禁止了 <code>SSH</code> 协议等，可通过设置 use_encryption = true，将 <code>FRP</code> 客户端 与 <code>FRP</code> 服务端之间的通信内容加密传输，将会有效防止流量被拦截。</p>

<p>如果传输的报文长度较长，通过设置 use_compression = true 对传输内容进行压缩，可以有效减小 <code>FRP</code> 客户端 与 <code>FRP</code> 服务端之间的网络流量，来加快流量转发速度，但是会额外消耗一些 CPU 资源。</p>

<p>这两个功能默认是不开启的，需要在 <code>FRP</code> 客户端配置文件中通过配置来为指定的代理启用加密与压缩的功能，压缩算法使用的是 snappy。</p>

<pre><code class="language-sh">$ vim frpc.ini

[ssh]
type = tcp
local_port = 22
remote_port = 6000
use_encryption = true
use_compression = true
</code></pre>

<h4 id="toc_22"><a href="#%E9%80%9A%E8%BF%87-frp-%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BB%A3%E7%90%86%E5%85%B6%E5%AE%83%E5%86%85%E7%BD%91%E6%9C%BA%E5%99%A8%E8%AE%BF%E9%97%AE%E5%A4%96%E7%BD%91"></a>通过 FRP 客户端代理其它内网机器访问外网</h4>

<p><code>FRP</code> 客户端内置了 <code>http_proxy</code> 和 <code>socks5</code> 插件，通过这两个插件可以使其它内网机器通过 <code>FPR</code> 客户端的的网络访问互联网。</p>

<p>要启用此功能，首先需要在 <code>FRP</code> 客户端配置文件中启用相关插件，这里以 <code>http_proxy</code> 插件为例：</p>

<pre><code class="language-sh">$ vim frpc.ini

[common]
server_addr = 4.3.2.1
server_port = 7000

[http_proxy]
type = tcp
remote_port = 6000
plugin = http_proxy
</code></pre>

<p>其次将需要通过这个代理访问外网的内部机器的代理地址设置为 4.3.2.1:6000，这样就可以通过 FRP 客户端机器的网络访问互联网了。</p>

<blockquote>
<ul>
<li>  <code>http_proxy</code> 插件也支持认证机制，如果需要启用认证可通过配置参数 <code>plugin_http_user</code> 和 <code>plugin_http_passwd</code> 启用。</li>
<li>  如需启用 <code>Socks5</code> 代理，只需将 plugin 的值更换为 socks5 即可。</li>
</ul>
</blockquote>

<h4 id="toc_23"><a href="#%E9%80%9A%E8%BF%87%E4%BB%A3%E7%90%86%E8%BF%9E%E6%8E%A5-frp-%E6%9C%8D%E5%8A%A1%E7%AB%AF"></a>通过代理连接 FRP 服务端</h4>

<p>在只能通过代理访问外网的环境内，<code>FRP</code> 客户端支持通过 <code>HTTP_PROXY</code> 参数来配置代理和 <code>FRP</code> 服务端进行通信。要使用此功能可以通过设置系统环境变量 <code>HTTP_PROXY</code> 或者通过在 <code>FRP</code> 客户端的配置文件中设置 <code>http_proxy</code> 参数来使用此功能。</p>

<pre><code class="language-sh">$ vim frpc.ini

[common]
server_addr = 4.3.2.1
server_port = 7000
protocol = tcp
http_proxy = http://user:pwd@4.3.2.2:8080
</code></pre>

<blockquote>
<ul>
<li>  仅在 <code>protocol = tcp</code> 时生效，暂时不支持 kcp 协议。</li>
</ul>
</blockquote>

<h4 id="toc_24"><a href="#%E5%AE%89%E5%85%A8%E5%9C%B0%E6%9A%B4%E9%9C%B2%E5%86%85%E7%BD%91%E6%9C%8D%E5%8A%A1"></a>安全地暴露内网服务</h4>

<p>对于一些比较敏感的服务如果直接暴露于公网上将会存在安全隐患，<code>FRP</code> 也提供了一种安全的转发方式 <code>STCP</code>。使用 <code>STCP</code> (secret tcp) 类型的代理可以避免让任何人都能访问到穿透到公网的内网服务，要使用 <code>STCP</code> 模式访问者需要单独运行另外一个 <code>FRP</code> 客户端。</p>

<p>下面就以创建一个只有自己能访问到的 <code>SSH</code> 服务代理为例，<code>FRP</code> 服务端和其它的部署步骤相同，主要区别是在 <code>FRP</code> 客户端上。</p>

<p>首先配置 <code>FRP</code> 客户端，和常规 <code>TCP</code> 转发不同的是这里不需要指定远程端口。</p>

<pre><code class="language-sh">$ vim frpc.ini
[common]
server_addr = 4.3.2.1
server_port = 7000

[secret_ssh]
type = stcp
# 只有 sk 一致的用户才能访问到此服务
sk = abcdefg
local_ip = 127.0.0.1
local_port = 22
</code></pre>

<p>其次在要访问这个服务的机器上启动另外一个 <code>FRP</code> 客户端，配置如下：</p>

<pre><code class="language-sh">$ vim frpc.ini
[common]
server_addr = 4.3.2.1
server_port = 7000

[secret_ssh_visitor]
type = stcp
# STCP 的访问者
role = visitor
# 要访问的 STCP 代理的名字，和前面定义的相同。
server_name = secret_ssh
# 和前面定义的要一致
sk = abcdefg
# 绑定本地端口用于访问 ssh 服务
bind_addr = 127.0.0.1
bind_port = 6005
</code></pre>

<p>最后在本机启动一个 <code>FRP</code> 客户端，这样就可以通过本机 6005 端口对内网机器 <code>SSH</code> 服务进行访问，假设用户名为 mike：</p>

<pre><code class="language-sh">$ ./frpc -c ./frpc.ini
2018/01/26 15:03:24 [I] [proxy_manager.go:284] proxy removed: []
2018/01/26 15:03:24 [I] [proxy_manager.go:294] proxy added: []
2018/01/26 15:03:24 [I] [proxy_manager.go:317] visitor removed: []
2018/01/26 15:03:24 [I] [proxy_manager.go:326] visitor added: [secret_ssh_visitor]
2018/01/26 15:03:24 [I] [control.go:240] [60d2af2f68196537] login to server success, get run id [60d2af2f68196537], server udp port [0]
2018/01/26 15:03:24 [I] [proxy_manager.go:235] [60d2af2f68196537] try to start visitor [secret_ssh_visitor]
2018/01/26 15:03:24 [I] [proxy_manager.go:243] [secret_ssh_visitor] start visitor success

$ ssh -oPort=6005 mike@127.0.0.1
</code></pre>

<h4 id="toc_25"><a href="#%E7%82%B9%E5%AF%B9%E7%82%B9%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F"></a>点对点内网穿透</h4>

<p>在传输大量数据时如果都经过服务器中转的话，这样会对服务器端带宽压力比较大。<code>FRP</code> 提供了一种新的代理类型 <code>XTCP</code> 来解决这个问题，<code>XTCP</code> 模式下可以在传输大量数据时让流量不经过服务器中转。</p>

<p>使用方式同 <code>STCP</code> 类似，需要在传输数据的两端都部署上 <code>FRP</code> 客户端上用于建立直接的连接。</p>

<p>首先在 <code>FRP</code> 服务端配置上增加一个 <code>UDP</code> 端口用于支持该类型的客户端:</p>

<pre><code class="language-sh">$ vim frps.ini
bind_udp_port = 7001
</code></pre>

<p>其次配置 <code>FRP</code> 客户端，和常规 <code>TCP</code> 转发不同的是这里不需要指定远程端口。</p>

<pre><code class="language-sh">$ vim frpc.ini

[common]
server_addr = 4.3.2.1
server_port = 7000

[p2p_ssh]
type = xtcp
# 只有 sk 一致的用户才能访问到此服务
sk = abcdefg
local_ip = 127.0.0.1
local_port = 22
</code></pre>

<p>然后在要访问这个服务的机器上启动另外一个 <code>FRP</code> 客户端，配置如下：</p>

<pre><code class="language-sh">$ vim frpc.ini
[common]
server_addr = 4.3.2.1
server_port = 7000

[p2p_ssh_visitor]
type = xtcp
# XTCP 的访问者
role = visitor
# 要访问的 XTCP 代理的名字
server_name = p2p_ssh
sk = abcdefg
# 绑定本地端口用于访问 ssh 服务
bind_addr = 127.0.0.1
bind_port = 6006
</code></pre>

<p>最后在本机启动一个 FRP 客户端，这样就可以通过本机 6006 端口对内网机器 SSH 服务进行访问，假设用户名为 mike：</p>

<pre><code class="language-sh">$ ./frpc -c ./frpc.ini

2018/01/26 16:01:52 [I] [proxy_manager.go:326] visitor added: [p2p_ssh_visitor secret_ssh_visitor]
2018/01/26 16:01:52 [I] [control.go:240] [7c7e06878e11cc3c] login to server success, get run id [7c7e06878e11cc3c], server udp port [7001]
2018/01/26 16:01:52 [I] [proxy_manager.go:235] [7c7e06878e11cc3c] try to start visitor [p2p_ssh_visitor]
2018/01/26 16:01:52 [I] [proxy_manager.go:243] [p2p_ssh_visitor] start visitor success
2018/01/26 16:01:52 [I] [proxy_manager.go:235] [7c7e06878e11cc3c] try to start visitor [secret_ssh_visitor]
2018/01/26 16:01:52 [I] [proxy_manager.go:243] [secret_ssh_visitor] start visitor success

$ ssh -oPort=6006 mike@127.0.0.1
</code></pre>

<blockquote>
<ul>
<li>  目前 <code>XTCP</code> 模式还处于开发的初级阶段，并不能穿透所有类型的 <code>NAT</code> 设备，所以穿透成功率较低。穿透失败时可以尝试 <code>STCP</code> 的方式。</li>
</ul>
</blockquote>

<h3 id="toc_26"><a href="#frp-%E7%AE%A1%E7%90%86"></a>FRP 管理</h3>

<p><code>FRP</code> 的部署安装比较简单，项目官方也没有提供相应的管理脚本。不过好在开源项目总是有网友热心提供部署和管理脚本。如果你觉得手动部署太麻烦，还可以使用下面的一键安装脚本。</p>

<p>项目地址：<a href="https://github.com/clangcn/onekey-install-shell/">https://github.com/clangcn/onekey-install-shell/</a></p>

<h4 id="toc_27"><a href="#%E4%B8%8B%E8%BD%BD%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E8%84%9A%E6%9C%AC"></a>下载一键部署脚本</h4>

<pre><code class="language-sh">$ wget --no-check-certificate https://raw.githubusercontent.com/clangcn/onekey-install-shell/master/frps/install-frps.sh -O ./install-frps.sh
$ chmod 700 ./install-frps.sh
</code></pre>

<h4 id="toc_28"><a href="#%E5%AE%89%E8%A3%85-frp-%E6%9C%8D%E5%8A%A1%E7%AB%AF"></a>安装 FRP 服务端</h4>

<p>这个一键部署脚本比较好用，为了提高国内用户下载安装包速度还提供了阿里云节点的安装源。整个脚本使用起来也比较简单，对一些常用的 <code>FRP</code> 服务端配置参数都做了交互式选择让用户可以方便的根据自己实际情况进行选择。脚本比较贴心的一点是对默认的公网地址进行了检测，省去了手动输入的麻烦。</p>

<pre><code class="language-sh">$ ./install-frps.sh install

Please select frps download url:
[1].aliyun (default)
[2].github
Enter your choice (1, 2 or exit. default [aliyun]):
---------------------------------------
Your select: aliyun
---------------------------------------
Loading network version for frps, please wait...
frps Latest release file frp_0.15.1_linux_amd64.tar.gz
Loading You Server IP, please wait...
You Server IP:12.34.56.78
Please input your server setting:

Please input frps bind_port [1-65535](Default Server Port: 5443):7000
frps bind_port: 7000

Please input frps vhost_http_port [1-65535](Default vhost_http_port: 80):8080
frps vhost_http_port: 8080

Please input frps vhost_https_port [1-65535](Default vhost_https_port: 443):
frps vhost_https_port: 443

Please input frps dashboard_port [1-65535](Default dashboard_port: 6443):7500
frps dashboard_port: 7500

Please input dashboard_user (Default: admin):
frps dashboard_user: admin

Please input dashboard_pwd (Default: IY0p1bOg):admin
frps dashboard_pwd: admin

Please input privilege_token (Default: 9BqswPpd1R0TfGR5):mike
frps privilege_token: mike

Please input frps max_pool_count [1-200]
(Default max_pool_count: 50):
frps max_pool_count: 50

##### Please select log_level #####
1: info (default)
2: warn
3: error
4: debug
#####################################################
Enter your choice (1, 2, 3, 4 or exit. default [1]):
log_level: info

Please input frps log_max_days [1-30]
(Default log_max_days: 3 day):
frps log_max_days: 3

##### Please select log_file #####
1: enable (default)
2: disable
#####################################################
Enter your choice (1, 2 or exit. default [1]):
log_file: enable

##### Please select tcp_mux #####
1: enable (default)
2: disable
#####################################################
Enter your choice (1, 2 or exit. default [1]):
tcp_mux: true

##### Please select kcp support #####
1: enable (default)
2: disable
#####################################################
Enter your choice (1, 2 or exit. default [1]):
kcp support: true

============== Check your input ==============
You Server IP      : 12.34.56.78
Bind port          : 7000
kcp support        : true
vhost http port    : 8080
vhost https port   : 443
Dashboard port     : 7500
Dashboard user     : admin
Dashboard password : admin
Privilege token    : mike
tcp_mux            : true
Max Pool count     : 50
Log level          : info
Log max days       : 3
Log file           : enable
==============================================

Press any key to start...or Press Ctrl+c to cancel

frps install path:/usr/local/frps
config file for frps ... done
download frps ... done
download /etc/init.d/frps... done
setting frps boot... done

+--------------------------------------------------+
|        Manager for Frps, Written by Clang        |
+--------------------------------------------------+
| Intro: http://koolshare.cn/thread-65379-1-1.html |
+--------------------------------------------------+

Starting Frps(0.15.1)... done
Frps (pid 3325)is running.

+---------------------------------------------------------+
|        frps for Linux Server, Written by Clang          |
+---------------------------------------------------------+
|     A tool to auto-compile &amp; install frps on Linux      |
+---------------------------------------------------------+
|    Intro: http://koolshare.cn/thread-65379-1-1.html     |
+---------------------------------------------------------+

Congratulations, frps install completed!
==============================================
You Server IP      : 12.34.56.78
Bind port          : 7000
KCP support        : true
vhost http port    : 8080
vhost https port   : 443
Dashboard port     : 7500
Privilege token    : mike
tcp_mux            : true
Max Pool count     : 50
Log level          : info
Log max days       : 3
Log file           : enable
==============================================
frps Dashboard     : http://12.34.56.78:7500/
Dashboard user     : admin
Dashboard password : admin
==============================================
</code></pre>

<h4 id="toc_29"><a href="#%E9%85%8D%E7%BD%AE-frp-%E6%9C%8D%E5%8A%A1%E7%AB%AF"></a>配置 FRP 服务端</h4>

<pre><code class="language-sh">$ ./install-frps.sh config
</code></pre>

<h4 id="toc_30"><a href="#%E6%9B%B4%E6%96%B0-frp-%E6%9C%8D%E5%8A%A1%E7%AB%AF"></a>更新 FRP 服务端</h4>

<pre><code class="language-sh">$ ./install-frps.sh update
</code></pre>

<h4 id="toc_31"><a href="#%E5%8D%B8%E8%BD%BD-frp-%E6%9C%8D%E5%8A%A1%E7%AB%AF"></a>卸载 FRP 服务端</h4>

<pre><code class="language-sh">$ ./install-frps.sh uninstall
</code></pre>

<h4 id="toc_32"><a href="#frp-%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%97%A5%E5%B8%B8%E7%AE%A1%E7%90%86"></a>FRP 服务端日常管理</h4>

<p><code>FRP</code> 服务端安装完成后，一键部署脚本还提供了一个日常管理 <code>FRP</code> 服务端的管理脚本来进行日常的启动、重启、停止等操作，非常的方便。</p>

<pre><code class="language-sh">Usage: /etc/init.d/frps {start|stop|restart|status|config|version}
</code></pre>

<h3 id="toc_33"><a href="#%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3"></a>参考文档</h3>

<p><a href="http://www.google.com">http://www.google.com</a><br/>
<a href="https://github.com/fatedier/frp">https://github.com/fatedier/frp</a><br/>
<a href="http://koolshare.cn/thread-65379-1-1.html">http://koolshare.cn/thread-65379-1-1.html</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[初识 MQTT]]></title>
    <link href="http://panlw.github.io/15514073814858.html"/>
    <updated>2019-03-01T10:29:41+08:00</updated>
    <id>http://panlw.github.io/15514073814858.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://www.ibm.com/developerworks/cn/iot/iot-mqtt-why-good-for-iot/index.html">https://www.ibm.com/developerworks/cn/iot/iot-mqtt-why-good-for-iot/index.html</a></p>
</blockquote>

<p>为什么 MQTT 是最适合物联网的网络协议</p>

<p>物联网 (IoT) 设备必须连接互联网。通过连接到互联网，设备就能相互协作，以及与后端服务协同工作。互联网的基础网络协议是 TCP/IP。MQTT（消息队列遥测传输） 是基于 TCP/IP 协议栈而构建的，已成为 IoT 通信的标准。</p>

<p>MQTT 最初由 IBM 于上世纪 90 年代晚期发明和开发。它最初的用途是将石油管道上的传感器与卫星相链接。顾名思义，它是一种支持在各方之间异步通信的消息协议。异步消息协议在空间和时间上将消息发送者与接收者分离，因此可以在不可靠的网络环境中进行扩展。虽然叫做消息队列遥测传输，但它与消息队列毫无关系，而是使用了一个发布和订阅的模型。在 2014 年末，它正式成为了一种 OASIS 开放标准，而且在一些流行的编程语言中受到支持（通过使用多种开源实现）。</p>

<h2 id="toc_0">为何选择 MQTT</h2>

<p>MQTT 是一种轻量级的、灵活的网络协议，致力于为 IoT 开发人员实现适当的平衡：</p>

<ul>
<li>  这个轻量级协议可在严重受限的设备硬件和高延迟 / 带宽有限的网络上实现。</li>
<li>  它的灵活性使得为 IoT 设备和服务的多样化应用场景提供支持成为可能。</li>
</ul>

<p>为了了解为什么 MQTT 如此适合 IoT 开发人员，我们首先来分析一下为什么其他流行网络协议未在 IoT 中得到成功应用。</p>

<h2 id="toc_1">为什么不选择其他众多网络协议</h2>

<p>大多数开发人员已经熟悉 HTTP Web 服务。那么为什么不让 IoT 设备连接到 Web 服务？设备可采用 HTTP 请求的形式发送其数据，并采用 HTTP 响应的形式从系统接收更新。这种请求和响应模式存在一些严重的局限性：</p>

<ul>
<li>  HTTP 是一种同步协议。客户端需要等待服务器响应。Web 浏览器具有这样的要求，但它的代价是牺牲了可伸缩性。在 IoT 领域，大量设备以及很可能不可靠或高延迟的网络使得同步通信成为问题。异步消息协议更适合 IoT 应用程序。传感器发送读数，让网络确定将其传送到目标设备和服务的最佳路线和时间。</li>
<li>  HTTP 是单向的。客户端必须发起连接。在 IoT 应用程序中，设备或传感器通常是客户端，这意味着它们无法被动地接收来自网络的命令。</li>
<li>  HTTP 是一种 1-1 协议。客户端发出请求，服务器进行响应。将消息传送到网络上的所有设备上，不但很困难，而且成本很高，而这是 IoT 应用程序中的一种常见使用情况。</li>
<li>  HTTP 是一种有许多标头和规则的重量级协议。它不适合受限的网络。</li>
</ul>

<p>出于上述原因，大部分高性能、可扩展的系统都使用异步消息总线来进行内部数据交换，而不使用 Web 服务。事实上，企业中间件系统中使用的最流行的消息协议被称为 AMQP（高级消息排队协议）。但是，在高性能环境中，计算能力和网络延迟通常不是问题。AMQP 致力于在企业应用程序中实现可靠性和互操作性。它拥有庞大的特性集，但不适合资源受限的 IoT 应用程序。</p>

<p>除了 AMQP 之外，还有其他流行的消息协议。例如，XMPP（Extensible Messaging and Presence Protocol，可扩展消息和状态协议）是一种对等即时消息 (IM) 协议。它高度依赖于支持 IM 用例的特性，比如存在状态和介质连接。与 MQTT 相比，它在设备和网络上需要的资源都要多得多。</p>

<p>那么，MQTT 为什么如此轻量且灵活？MQTT 协议的一个关键特性是发布和订阅模型。与所有消息协议一样，它将数据的发布者与使用者分离。</p>

<h2 id="toc_2">发布和订阅模型</h2>

<p>MQTT 协议在网络中定义了两种实体类型：一个消息代理和一些客户端。代理是一个服务器，它从客户端接收所有消息，然后将这些消息路由到相关的目标客户端。客户端是能够与代理交互来发送和接收消息的任何事物。客户端可以是现场的 IoT 传感器，或者是数据中心内处理 IoT 数据的应用程序。</p>

<ol>
<li> 客户端连接到代理。它可以订阅代理中的任何消息 “主题”。此连接可以是简单的 TCP/IP 连接，也可以是用于发送敏感消息的加密 TLS 连接。</li>
<li> 客户端通过将消息和主题发送给代理，发布某个主题范围内的消息。</li>
<li> 代理然后将消息转发给所有订阅该主题的客户端。</li>
</ol>

<p>因为 MQTT 消息是按主题进行组织的，所以应用程序开发人员能灵活地指定某些客户端只能与某些消息交互。例如，传感器将在 “sensor_data” 主题范围内发布读数，并订阅 “config_change” 主题。将传感器数据保存到后端数据库中的数据处理应用程序会订阅 “sensor_data” 主题。管理控制台应用程序能接收系统管理员的命令来调整传感器的配置，比如灵敏度和采样频率，并将这些更改发布到 “config_change” 主题。（参阅图 1。）</p>

<h5 id="toc_3">图 1. IoT 传感器的 MQTT 发布和订阅模型</h5>

<p><img src="https://www.ibm.comimage1.png" alt=""/></p>

<p>同时，MQTT 是轻量级的。它有一个用来指定消息类型的简单标头，有一个基于文本的主题，还有一个任意的二进制有效负载。应用程序可对有效负载采用任何数据格式，比如 JSON、XML、加密二进制或 Base64，只要目标客户端能够解析该有效负载。</p>

<h2 id="toc_4">MQTT 开发入门</h2>

<p>开始进行 MQTT 开发的最简单工具是 Python mosquitto 模块，该模块包含在 <a href="http://www.eclipse.org/paho/">Eclipse Paho 项目</a> 中，提供了多种编程语言格式的 MQTT SDK 和库。它包含一个能在本地计算机上运行的 MQTT 代理，还包含使用消息与代理交互的命令行工具。可以从 <a href="https://mosquitto.org/">mosquitto 网站</a> 下载并安装 mosquitto 模块。</p>

<p>mosquitto 命令在本地计算机上运行 MQTT 代理。也可以使用 -d 选项在后台运行它。</p>

<p><code>$ mosquitto -d</code></p>

<p>接下来，在另一个终端窗口中，可以使用 mosquitto_sub 命令连接到本地代理并订阅一个主题。运行该命令后，它将等待从订阅的主题接收消息，并打印出所有消息。</p>

<p><code>$ mosquitto_sub -t &quot;dw/demo&quot;</code></p>

<p>在另一个终端窗口中，可以使用 mosquitto_pub 命令连接到本地代理，然后向一个主题发布一条消息。</p>

<p><code>$ mosquitto_pub -t &quot;dw/demo&quot; -m &quot;hello world!&quot;</code></p>

<p>现在，运行 mosquitto_sub 的终端会在屏幕上打印出 “hello world!”。您刚才使用 MQTT 代理发送并接收了一条消息！</p>

<p>当然，在生产系统中，不能使用本地计算机作为代理。相反，可以使用 IBM Bluemix Internet of Things Platform 服务，这是一种可靠的按需服务，功能与 MQTT 代理类似。要进一步了解这个 Bluemix 服务如何集成并使用 MQTT 作为与设备和应用程序通信的协议，请参阅 <a href="https://console.ng.bluemix.net/docs/services/IoT/reference/mqtt/index.html#ref-mqtt">该服务的文档</a>。）</p>

<p><a href="https://console.ng.bluemix.net/catalog/services/internet-of-things-platform">IBM Bluemix Internet of Things Platform 服务</a> 的工作原理如下。</p>

<ul>
<li>  从 Bluemix 控制台，可以在需要时创建 Internet of Things Platform 服务的实例。</li>
<li>  然后，可以添加能使用 MQTT 连接该服务实例的设备。每个设备有一个 ID 和名称。只有列出的设备能访问该服务，Watson IoT Platform 仪表板会报告这些设备上的流量和使用信息。</li>
<li>  对于每个设备客户端，Bluemix 会分配一个主机名、用户名和密码，用于连接到您的服务实例（MQTT 代理）。（在 Bluemix 上，用户名始终为 use-token-auth，密码始终是图 2 中显示的每个连接设备的令牌。）</li>
</ul>

<h5 id="toc_5">图 2. 在 IBM Bluemix 中创建 Internet of Things Platform 服务</h5>

<p><img src="https://www.ibm.comimage2.png" alt=""/></p>

<p>使用远程 MQTT 代理时，需要将代理的主机名和身份验证凭证传递给 mosquitto_sub 和 mosquitto_pub 命令。例如，下面的命令使用了 Bluemix 提供的用户名和密码，订阅我们的 Internet of Things Platform 服务上的 demo 主题：</p>

<p><code>$ mosquitto_sub -t &quot;demo&quot; -h host.iotp.mqtt.bluemix.com -u username -P password</code></p>

<p>有关使用 mosquitto 工具的更多选择，以及如何使用 mosquitto API 创建自己的 MQTT 客户端应用程序，请参阅 <a href="https://mosquitto.org/documentation/">mosquitto 网站</a> 上的文档。</p>

<p>有了必要的工具后，让我们来更深入地研究 MQTT 协议。</p>

<h2 id="toc_6">了解 MQTT 协议</h2>

<p>MQTT 是一种连接协议，它指定了如何组织数据字节并通过 TCP/IP 网络传输它们。但实际上，开发人员并不需要了解这个连接协议。我们只需要知道，每条消息有一个命令和数据有效负载。该命令定义消息类型（例如 CONNECT 消息或 SUBSCRIBE 消息）。所有 MQTT 库和工具都提供了直接处理这些消息的简单方法，并能自动填充一些必需的字段，比如消息和客户端 ID。</p>

<p>首先，客户端发送一条 CONNECT 消息来连接代理。CONNECT 消息要求建立从客户端到代理的连接。CONNECT 消息包含以下内容参数。</p>

<h5 id="toc_7">表 1. CONNECT 消息参数</h5>

<table>
<thead>
<tr>
<th><strong>参数</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>

<tbody>
<tr>
<td>cleanSession</td>
<td>此标志指定连接是否是持久性的。持久会话会将所有订阅和可能丢失的消息（具体取决于 QoS） 都存储在代理中。（请参阅 表 3 获取 QoS 的描述。）</td>
</tr>
<tr>
<td>username</td>
<td>代理的身份验证和授权凭证。</td>
</tr>
<tr>
<td>password</td>
<td>代理的身份验证和授权凭证。</td>
</tr>
<tr>
<td>lastWillTopic</td>
<td>连接意外中断时，代理会自动向某个主题发送一条 “last will” 消息。</td>
</tr>
<tr>
<td>lastWillQos</td>
<td>“last will” 消息的 QoS。（请参阅 表 3 来查看 QoS 的描述。）</td>
</tr>
<tr>
<td>lastWillMessage</td>
<td>“last will” 消息本身。</td>
</tr>
<tr>
<td>keepAlive</td>
<td>这是客户端通过 ping 代理来保持连接有效所需的时间间隔。</td>
</tr>
</tbody>
</table>

<p>客户端收到来自代理的一条 CONNACK 消息。CONNACK 消息包含以下内容参数。</p>

<h5 id="toc_8">表 2. CONNACK 消息参数</h5>

<table>
<thead>
<tr>
<th><strong>参数</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>

<tbody>
<tr>
<td>sessionPresent</td>
<td>此参数表明连接是否已有一个持久会话。也就是说，连接已订阅了主题，而且会接收丢失的消息。</td>
</tr>
<tr>
<td>returnCode</td>
<td>0 表示成功。其他值指出了失败的原因。</td>
</tr>
</tbody>
</table>

<p>建立连接后，客户端然后会向代理发送一条或多条 SUBSCRIBE 消息，表明它会从代理接收针对某些主题的消息。消息可以包含一个或多个重复的参数。如表 3。</p>

<h5 id="toc_9">表 3. SUBSCRIBE 消息参数</h5>

<table>
<thead>
<tr>
<th><strong>参数</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>

<tbody>
<tr>
<td>qos</td>
<td>qos（服务质量或 QoS）标志表明此主题范围内的消息传送到客户端所需的一致程度。</td>
</tr>
</tbody>
</table>

<ul>
<li>  值 0：不可靠，消息基本上仅传送一次，如果当时客户端不可用，则会丢失该消息。</li>
<li>  值 1：消息应传送至少 1 次。</li>
<li><p>值 2：消息仅传送一次。</p>

<p>|<br/>
| topic | 要订阅的主题。一个主题可以有多个级别，级别之间用斜杠字符分隔。例如，“dw/demo” 和 “ibm/bluemix/mqtt” 是有效的主题。 |</p></li>
</ul>

<p>客户端成功订阅某个主题后，代理会返回一条 SUBACK 消息，其中包含一个或多个 returnCode 参数。</p>

<h5 id="toc_10">表 4. SUBACK 消息参数</h5>

<table>
<thead>
<tr>
<th><strong>参数</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>

<tbody>
<tr>
<td>returnCode</td>
<td>SUBCRIBE 命令中的每个主题都有一个返回代码。返回值如下所示。</td>
</tr>
</tbody>
</table>

<ul>
<li>  值 0 - 2：成功达到相应的 QoS 级别。（参阅 表 3 进一步了解 QoS。）</li>
<li><p>值 128：失败。</p>

<p>|</p></li>
</ul>

<p>与 SUBSCRIBE 消息对应，客户端也可以通过 UNSUBSCRIBE 消息取消订阅一个或多个主题。</p>

<h5 id="toc_11">表 5. UNSUBSCRIBE 消息参数</h5>

<table>
<thead>
<tr>
<th><strong>参数</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>

<tbody>
<tr>
<td>topic</td>
<td>此参数可重复用于多个主题。</td>
</tr>
</tbody>
</table>

<p>客户端可向代理发送 PUBLISH 消息。该消息包含一个主题和数据有效负载。代理然后将消息转发给所有订阅该主题的客户端。</p>

<h5 id="toc_12">表 6. PUBLISH 消息参数</h5>

<table>
<thead>
<tr>
<th><strong>参数</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>

<tbody>
<tr>
<td>topicName</td>
<td>发布的消息的相关主题。</td>
</tr>
<tr>
<td>qos</td>
<td>消息传递的服务质量水平。（请参阅 表 3 来查看 QoS 的描述。）</td>
</tr>
<tr>
<td>retainFlag</td>
<td>此标志表明代理是否保留该消息作为针对此主题的最后一条已知消息。</td>
</tr>
<tr>
<td>payload</td>
<td>消息中的实际数据。它可以是文本字符串或二进制大对象数据。</td>
</tr>
</tbody>
</table>

<h2 id="toc_13">技巧和解决方法</h2>

<p>MQTT 的优势在于它的简单性。在可以使用的主题类型或消息有效负载上没有任何限制。这支持一些有趣的用例。例如，请考虑以下问题：</p>

<p><u>如何使用 MQTT 发送 1-1 消息？</u>双方可以协商使用一个特定于它们的主题。例如，主题名称可以包含两个客户端的 ID，以确保它的唯一性。</p>

<p><u>客户端如何传输它的存在状态？</u>系统可以为 “presence” 主题协商一个命名约定。例如，“presence/client-id” 主题可以拥有客户端的存在状态信息。当客户端建立连接时，将该消息被设置为 true，在断开连接时，该消息被设置为 false。客户端也可以将一条 last will 消息设置为 false，以便在连接丢失时设置该消息。代理可以保留该消息，让新客户端能够读取该主题并找到存在状态。</p>

<p><u>如何保护通信？</u>客户端与代理的连接可以采用加密 TLS 连接，以保护传输中的数据。此外，因为 MQTT 协议对有效负载数据格式没有任何限制，所以系统可以协商一种加密方法和密钥更新机制。在这之后，有效负载中的所有内容可以是实际 JSON 或 XML 消息的加密二进制数据。</p>

<h2 id="toc_14">结束语</h2>

<p>本文从技术角度介绍了 MQTT 协议。您了解了 MQTT 是什么，MQTT 为什么适合 IoT 应用程序，以及如何开始开发使用 MQTT 的应用程序。</p>

<h2 id="toc_15">参考资料</h2>

<ul>
<li><a href="http://mqtt.org/">MQTT 的官方网站</a></li>
<li><a href="http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html">MQTT v3.1.1 是一种官方 OASIS 标准</a></li>
<li><a href="http://www.eclipse.org/paho/">Eclipse Paho 项目包括流行的开源 MQTT 实现</a></li>
<li><a href="https://mosquitto.org/">mosquitto 项目是一个开源 Python MQTT 代理和客户端库</a></li>
<li><a href="https://console.ng.bluemix.net/docs/services/IoT/reference/mqtt/index.html#ref-mqtt">Internet of Things Platform 服务的 MQTT 文档</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用 lsof 代替 Mac OS X 中的 netstat 查看占用端口的程序]]></title>
    <link href="http://panlw.github.io/15514072490698.html"/>
    <updated>2019-03-01T10:27:29+08:00</updated>
    <id>http://panlw.github.io/15514072490698.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://tonydeng.github.io/2016/07/07/use-lsof-to-replace-netstat/">https://tonydeng.github.io/2016/07/07/use-lsof-to-replace-netstat/</a></p>
</blockquote>

<p>众所周知水果系统内核是有 BSD 血统的 <code>Darwin</code>，OS X 自带的很多 CLI 工具也是 BSD style 的，有一部分使用起来和 Linux 无异，有一部分可以通过 <code>brew</code> 安装 GNU 版本（如 <code>tar</code>），但是 OS X 的 <code>netstat</code> 不能查看使用端口的程序名让我一直很不爽，而且也没找到 GNU 版本，于是去搜了一下解决办法，stackoverflow 上的结论基本都是建议使用 <code>lsof</code> 代替 <code>netstat</code> 进行查看：</p>

<pre><code class="language-sh">sudo lsof -nP -iTCP:端口号 -sTCP:LISTEN
</code></pre>

<ul>
<li>  -n 表示不显示主机名</li>
<li>  -P 表示不显示端口俗称</li>
<li>  不加 sudo 只能查看以当前用户运行的程序</li>
</ul>

<p>另外，还可以通过管道来过滤想要的信息</p>

<pre><code class="language-sh">sudo lsof -nP -iTCP -sTCP:LISTEN | grep mysqld
</code></pre>

<p>基本效果如下：</p>

<p>查看当前所有监听的端口以及对应的<code>Command</code>和<code>PID</code></p>

<pre><code class="language-sh">➜  ~ lsof -nP -iTCP -sTCP:LISTENCOMMAND    PID     USER   FD   TYPE             DEVICE SIZE/OFF NODE NAMESSH\x20Pr 1553 tonydeng    8u  IPv4 0xee7327e39355d175      0t0  TCP 127.0.0.1:8087 (LISTEN)SSH\x20Pr 1553 tonydeng    9u  IPv6 0xee7327e38aad6e15      0t0  TCP [::1]:8087 (LISTEN)java      2978 tonydeng  166u  IPv6 0xee7327e38aad7e35      0t0  TCP *:62622 (LISTEN)node      3319 tonydeng   31u  IPv4 0xee7327e39f0f8745      0t0  TCP *:4000 (LISTEN)
</code></pre>

<p>查看指定端口对应的<code>Command</code>和<code>PID</code></p>

<pre><code class="language-sh">➜  ~ lsof -nP -iTCP:4000 -sTCP:LISTENCOMMAND  PID     USER   FD   TYPE             DEVICE SIZE/OFF NODE NAMEnode    3319 tonydeng   31u  IPv4 0xee7327e39f0f8745      0t0  TCP *:4000 (LISTEN)
</code></pre>

<p>PS ： 输出占用该端口的 PID</p>

<pre><code class="language-sh">lsof -nP -iTCP:4000 |grep LISTEN|awk &#39;{print $2;}&#39;
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RabbitMQ vs Kafka Part 1 - Two Different Takes on Messaging]]></title>
    <link href="http://panlw.github.io/15513682116421.html"/>
    <updated>2019-02-28T23:36:51+08:00</updated>
    <id>http://panlw.github.io/15513682116421.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://jack-vanlightly.com/blog/2017/12/4/rabbitmq-vs-kafka-part-1-messaging-topologies">https://jack-vanlightly.com/blog/2017/12/4/rabbitmq-vs-kafka-part-1-messaging-topologies</a></p>
</blockquote>

<p>In this part we&#39;ll explore what RabbitMQ and Apache Kafka are and their approach to messaging. Each technology has made very different decisions regarding every aspect of their design, each with strengths and weaknesses. We&#39;ll not come to any strong conclusions in this part, instead think of this as a primer on the technologies so we can dive deeper in subsequent parts of the series.</p>

<h1 id="toc_0">RabbitMQ</h1>

<p>RabbitMQ is a distributed message queue system. Distributed because it is usually run as a cluster of nodes where queues are spread across the nodes and optionally replicated for fault tolerance and high availability. It natively implements AMQP 0.9.1 and offers other protocols such as STOMP, MQTT and HTTP via plug-ins.</p>

<p>RabbitMQ takes both a classic and a novel take on messaging. Classic in the sense that it is oriented around message queues, and novel in its highly flexible routing capability. It is this routing capability that is its killer feature. Building a fast, scalable, reliable distributed messaging system is an achievement in itself, but the message routing functionality is what makes it truly stand out among the myriad of messaging technologies out there.</p>

<h3 id="toc_1">Exchanges and Queues</h3>

<p>The super simplified overview:</p>

<ul>
<li><p>Publishers send messages to exchanges</p></li>
<li><p>Exchanges route messages to queues and other exchanges</p></li>
<li><p>RabbitMQ sends acknowledgements to publishers on message receipt</p></li>
<li><p>Consumers maintain persistent TCP connections with RabbitMQ and declare which queue(s) they consume</p></li>
<li><p>RabbitMQ pushes messages to consumers</p></li>
<li><p>Consumers send acknowledgements of success/failure</p></li>
<li><p>Messages are removed from queues once consumed successfully</p></li>
</ul>

<p>Hidden in that list are a huge number of decisions that developers and admins should take to get the delivery guarantees they want, performance characterstics etc, all of which we&#39;ll cover in later sections of this series.</p>

<p>Let&#39;s take a look at a single publisher, exchange, queue and consumer:</p>

<p><div style="padding-bottom: 10.9063%; overflow: hidden;" class="image-block-wrapper lightbox  has-aspect-ratio" data-description="<p>Fig 1 - Single publisher and single consumer</p>&quot; data-lightbox-theme=&quot;dark&quot; id=&quot;yui_3_17_2_1_1551367744784_96&quot;&gt;<img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a26b52bc83025201d63f0dd/1512486216223/RabbitMQOnePubOneSub.png" alt="Fig 1 - Single publisher and single consumer"/><img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a26b52bc83025201d63f0dd/1512486216223/RabbitMQOnePubOneSub.png" alt=""/></p>

<p>Fig 1 - Single publisher and single consumer</p>

<p>What if you have multiple publishers of the same message? Also what if we have multiple consumers that each want to consume every message?</p>

<p><div style="padding-bottom: 35.6375%; overflow: hidden;" class="image-block-wrapper lightbox  has-aspect-ratio" data-description="<p>Fig 2 - Multiple publishers, multiple independent consumers</p>&quot; data-lightbox-theme=&quot;dark&quot; id=&quot;yui_3_17_2_1_1551367744784_124&quot;&gt;<img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a26b570652deaad92cb2e40/1512486266430/RabbitMQMultiplePubAndSub.png" alt="Fig 2 - Multiple publishers, multiple independent consumers"/><img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a26b570652deaad92cb2e40/1512486266430/RabbitMQMultiplePubAndSub.png" alt=""/></p>

<p>Fig 2 - Multiple publishers, multiple independent consumers</p>

<p>As you can see, the publishers send their messages to the same exchange, which route each message to three queues, each of which has a single consumer. With RabbitMQ, queues enable different consumers to consume each message. Contrast that to the diagram below:</p>

<p><div style="padding-bottom: 35.4839%; overflow: hidden;" class="image-block-wrapper lightbox  has-aspect-ratio" data-description="<p>Fig 3 - Multiple publishers, one queue with multiple competing consumers</p>&quot; data-lightbox-theme=&quot;dark&quot; id=&quot;yui_3_17_2_1_1551367744784_147&quot;&gt;<img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a26b5a2e4966bb15b1ae4f6/1512486317881/RabbitMQMultiplePubMultipleCompetingSub.png" alt="Fig 3 - Multiple publishers, one queue with multiple competing consumers"/><img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a26b5a2e4966bb15b1ae4f6/1512486317881/RabbitMQMultiplePubMultipleCompetingSub.png" alt=""/></p>

<p>Fig 3 - Multiple publishers, one queue with multiple competing consumers</p>

<p>In figure 3 we have three consumers all consuming from a single queue. These are competing consumers, that is they compete to consume the messages of a single queue. One would expect that on average, each consumer would consume one third of the messages of this queue. We use competing consumers to scale our message processing and with RabbitMQ it is very simple, just add or remove consumers on demand. No matter how many competing consumers you have, RabbitMQ will ensure that messages are delivered to only a single consumer.</p>

<p>We can combine figure 2 and 3 to have multiple sets competing consumers where each set consumes every message.</p>

<p><div style="padding-bottom: 61.6564%; overflow: hidden;" class="image-block-wrapper lightbox  has-aspect-ratio" data-description="<p>Fig 4 - Multiple publishers, multiple queues with competing consumers</p>&quot; data-lightbox-theme=&quot;dark&quot; id=&quot;yui_3_17_2_1_1551367744784_170&quot;&gt;<img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a26b5d2e4966bb15b1aece1/1512486360080/RabbitMQMultiplePubAndMultipleSetsOfSubs.png" alt="Fig 4 - Multiple publishers, multiple queues with competing consumers"/><img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a26b5d2e4966bb15b1aece1/1512486360080/RabbitMQMultiplePubAndMultipleSetsOfSubs.png" alt=""/></p>

<p>Fig 4 - Multiple publishers, multiple queues with competing consumers</p>

<p>The arrows between exchanges and queues are called bindings and we&#39;ll be taking a much closer look at those in Part 2 of this series.</p>

<h3 id="toc_2">GUARANTEES</h3>

<p>RabbitMQ offers &quot;_at most once delivery_&quot;and&quot;_at least once delivery_&quot;but not&quot;_exactly once delivery_&quot;guarantees. We&#39;ll take a deeper look at message delivery guarantees in <a href="https://jack-vanlightly.com/blog/2017/12/15/rabbitmq-vs-kafka-part-4-message-delivery-semantics-and-guarantees">Part 4</a> of the series.</p>

<p>Messages are <strong>delivered</strong> in order of their arrival to the queue (that is the definition of a queue after all). This does not guarantee the completion of message processing matches that exact same order when you have competing consumers. This is no fault of RabbitMQ but a fundamental reality of processing an ordered set of messages in parallel. This problem can be resolved by using the Consistent Hashing Exchange as you&#39;ll see in the next part on patterns and topologies.</p>

<h3 id="toc_3">PUSH AND CONSUMER PREFETCH</h3>

<p>RabbitMQ pushes messages to consumers in a stream. There is a Pull API but it has terrible performance as each message requires a request/response round-trip (note, I updated this paragraph due to a comment from Shiva Kumar).</p>

<p>Push-based systems can overwhelm consumers if messages arrive at the queue faster than the consumers can process them. So to avoid this each consumer can configure a prefetch limit (also known as a QoS limit).  This basically is the number of unacknowledged messages that a consumer can have at any one time. This acts as a safety cut-off switch for when the consumer starts to fall behind.</p>

<p>Why push and not pull? First of all it is great for low latency. Secondly, ideally when we have competing consumers of a single queue we want to distribute load evenly between them. If each consumer pulls messages then depending on how many they pull the distribution of work can get pretty uneven. The more uneven the distribution of messages the more latency and the further the loss of message order at processing time. For that reason RabbitMQ&#39;s Pull API only allows to pull one message at a time, but that seriously impacts performance. These factors make RabbitMQ lean towards a push mechanism. This is one of the scaling limitations of RabbitMQ. It is ameliorated by being able to group acknowledgements together.</p>

<h2 id="toc_4">Routing</h2>

<p>Exchanges are basically routers of messages to queues and/or other exchanges. In order for a message to travel from an exchange to a queue or other exchange, a binding is needed. Different exchanges require different bindings. There are four types of exchange and associated bindings:</p>

<ul>
<li><p><strong>Fanout</strong>. Routes to all queues and exchanges that have a binding to the exchange. The standard pub sub model.</p></li>
<li><p><strong>Direct</strong>. Routes messages based on a <strong>Routing Key</strong> that the message carries with it, set by the publisher. A routing key is a short string. Direct exchanges route messages to queues/exchanges that have a <strong>Binding Key</strong> that exactly matches the routing key.</p></li>
<li><p><strong>Topic</strong>. Routes messages based on a routing key, but allows wildcard matching.</p></li>
<li><p><strong>Header</strong>. RabbitMQ allows for custom headers to be added to messages. Header exchanges route messages according to those header values. Each binding includes exact match header values. Multiple values can be added to a binding with ANY or ALL values required to match.</p></li>
<li><p><strong>Consistent Hashing</strong>. This is an exchange that hashes either the routing key or a message header and routes to one queue only. This is useful when you need processing order guarantees with scaled out consumers.</p></li>
</ul>

<p><div style="padding-bottom: 27.4347%; overflow: hidden;" class="image-block-wrapper lightbox  has-aspect-ratio" data-description="<p>Fig 5. Topic exchange example</p>&quot; data-lightbox-theme=&quot;dark&quot; id=&quot;yui_3_17_2_1_1551367744784_193&quot;&gt;<img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a26d180ec212dbf79a04f16/1512493444266/RabbitMQTopicExample.png" alt="Fig 5\. Topic exchange example"/><img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a26d180ec212dbf79a04f16/1512493444266/RabbitMQTopicExample.png" alt=""/></p>

<p>Fig 5. Topic exchange example</p>

<p>We&#39;ll be looking more closely at routing in <a href="https://jack-vanlightly.com/blog/2017/12/5/rabbitmq-vs-kafka-part-2-rabbitmq-messaging-patterns-and-topologies">Part 2</a>, but above is an example of a Topic exchange. Publishers publish error logs with a Routing Key format of <u>LEVEL.AppName</u>.</p>

<ul>
<li><p>Queue 1 will receive all messages as it uses the multi-word # wildcard.</p></li>
<li><p>Queue 2 will receive any log level of the ECommerce.WebUI application. It uses the single-word * wildcard that covers the log level.</p></li>
<li><p>Queue 3 will see all ERROR level messages from any application. It uses the multi-word # wildcard to cover all applications.</p></li>
</ul>

<p>With four ways of routing messages, and allowing exchanges to route to other exchanges,  RabbitMQ offers a powerful and flexible set of messaging patterns. Next we&#39;ll touch on dead letter exchanges, ephemeral exchanges and queues, and you&#39;ll start seeing the power of RabbitMQ.</p>

<h2 id="toc_5">Dead Letter Exchanges</h2>

<p>We can configure queues to send messages to an exchange under the following conditions:</p>

<ul>
<li><p>Queue exceeds the configured number of messages.</p></li>
<li><p>Queue exceeds the configured number of bytes.</p></li>
<li><p>Message Time To Live (TTL) expired. The publisher can set the lifetime of the message and also the queue can have a message TTL. Which ever is shorter applies.</p></li>
</ul>

<p>We create a queue that has a binding to the dead letter exchange and these messages get stored there until action is taken. In a separate <a href="https://jack-vanlightly.com/blog/2017/6/11/improving-reliability-and-incident-response-via-a-message-lifecycle">post</a> I have described the topology I have implemented where all dead lettered messages go to a central clearing house where the support team can decide what actions to take.</p>

<p>Like with many RabbitMQ functionalities, dead letter exchanges give extra patterns that were not originally considered. We can use message TTLs and dead letter exchanges to implement delay queues and retry queues including exponential backoff. See my previous posts on this.</p>

<h2 id="toc_6">Ephemeral Exchanges and Queues</h2>

<p>Exchanges and queues can be dynamically created and given auto delete characteristics. After a certain time period they can self destruct. This allows for patterns such as ephermal reply queues for message based RPC.</p>

<h2 id="toc_7">Plug-Ins</h2>

<p>The first plug-in you will want to install is the Management Plug-In that provides an HTTP server, with web UI and REST API. It is really easy to install and gives you an easy to use UI to get you up and running. Scripting deployments via the REST API is really easy too.</p>

<p>Some other plug-ins include:</p>

<ul>
<li><p>Consistent Hashing Exchange, Sharding Exchange and more</p></li>
<li><p>protocols like STOMP and MQTT</p></li>
<li><p>web hooks</p></li>
<li><p>extra exchange types</p></li>
<li><p>SMTP integration</p></li>
</ul>

<p>There&#39;s a lot more to RabbitMQ but that is a good primer and gives you an idea of what RabbitMQ can do. Now we&#39;ll look at Kafka, which takes a completely different approach to messaging, and also has amazing features.</p>

<h1 id="toc_8">Apache Kafka</h1>

<p>Kafka is a distributed, replicated commit log. Kafka does not have the concept of a queue which might seem strange at first given that it is primary used as a messaging system. Queues have been synonymous with messaging systems for a long time. Let&#39;s break down&quot;distributed, replicated commit log&quot; a bit:</p>

<ul>
<li><p><strong>Distributed</strong> because Kafka is deployed as a cluster of nodes, for both fault tolerance and scale</p></li>
<li><p><strong>Replicated</strong> because messages are usually replicated across multiple nodes (servers).</p></li>
<li><p><strong>Commit Log</strong> because messages are stored in partitioned, append only logs which are called Topics. This concept of a log is the principal killer feature of Kafka.</p></li>
</ul>

<p>Understanding the log (Topic) and its partitions are the key to understanding Kafka. So how is a partitioned log different from a set of queues? Let&#39;s visualise it.</p>

<p><div style="padding-bottom: 35.8804%; overflow: hidden;" class="image-block-wrapper lightbox  has-aspect-ratio" data-description="<p>Fig 6 One producer, one partition, one consumer</p>&quot; data-lightbox-theme=&quot;dark&quot; id=&quot;yui_3_17_2_1_1551367744784_216&quot;&gt;<img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a26b62df9619ab1f7214c1a/1512486449380/KafkaOneProdOnePartOneCon.png" alt="Fig 6 One producer, one partition, one consumer"/><img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a26b62df9619ab1f7214c1a/1512486449380/KafkaOneProdOnePartOneCon.png" alt=""/></p>

<p>Fig 6 One producer, one partition, one consumer</p>

<p>Rather than put messages in a FIFO queue and track the status of that message in the queue like RabbitMQ does, Kafka just appends it to the log and that is that. The message stays put whether it is consumed once or a thousand times. It is removed according to the data retention policy (often a window time period). So how is a topic consumed? Each consumer tracks where it is in the log, it has a pointer to the last message consumed and this pointer is called the <strong>offset</strong>. Consumers maintain this offset via the client libraries and depending on the version of Kafka the offset is stored either in ZooKeeper or Kafka itself. ZooKeeper is a distributed consensus technology used by many distributed systems for things like leader election. Kafka relies on ZooKeeper for managing the state of the cluster.</p>

<p>What is amazing about this log model is that it instantly removes a lot of complexity around message delivery status and more importantly for consumers, it allows them to rewind and go back and consume messages from a previous offset. For example imagine you deploy a service that calculates invoices which consumes bookings placed by clients. The service has a bug and calculates all the invoices incorrectly for 24 hours. With RabbitMQ at best you would need to somehow republish those bookings and only to the invoice service. But with Kafka you simply move the offset for that consumer back 24 hours.</p>

<p>So let&#39;s see what it looks like with a topic that has a single partition and two consumers which each need to consume every message. From now on I have started to label the consumers because it will not be as clear (as the RabbitMQ diagrams) which are independent and which are competing consumers.</p>

<p><div style="padding-bottom: 42.926%; overflow: hidden;" class="image-block-wrapper lightbox  has-aspect-ratio" data-description="<p>Fig 7 One producer, one partition, two independent consumers</p>&quot; data-lightbox-theme=&quot;dark&quot; id=&quot;yui_3_17_2_1_1551367744784_239&quot;&gt;<img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a26b66e24a694f272e92ff0/1512486514718/KafkaOnePartTwoCon.png" alt="Fig 7 One producer, one partition, two independent consumers"/><img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a26b66e24a694f272e92ff0/1512486514718/KafkaOnePartTwoCon.png" alt=""/></p>

<p>Fig 7 One producer, one partition, two independent consumers</p>

<p>As you can see from the diagram, two independent consumers both consume from the same partition, but they are reading from different offsets. Perhaps the invoice service takes longer to process messages than the push notification service, or perhaps the invoice service was down for a while and catching up, or perhaps there was a bug and its offset had to be moved back a few hours.</p>

<p>Now let&#39;s say that the invoice service needs to be scaled out to three instances because it cannot keep up with the message velocity. With RabbitMQ we simply deploy two more invoice service apps which consume from the <u>bookings invoice service queue</u>. But Kafka does not support competing consumers on a single partition, Kafka&#39;s unit of parallelism is the partition itself. So if we need three invoice consumers we need at least three partitions. So now we have:</p>

<p><div style="padding-bottom: 91.1357%; overflow: hidden;" class="image-block-wrapper lightbox  has-aspect-ratio" data-description="<p>Fig 8 Three partitions and two sets of three consumers</p>&quot; data-lightbox-theme=&quot;dark&quot; id=&quot;yui_3_17_2_1_1551367744784_262&quot;&gt;<img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a26b69f419202a0af536a6f/1512486564349/KafkaMultiplePartitions.png" alt="Fig 8 Three partitions and two sets of three consumers"/><img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a26b69f419202a0af536a6f/1512486564349/KafkaMultiplePartitions.png" alt=""/></p>

<p>Fig 8 Three partitions and two sets of three consumers</p>

<p>So the implication is that you need at least as many partitions as the most scaled out consumer. Let&#39;s talk about partitions a bit.</p>

<h3 id="toc_9">PARTITIONS AND CONSUMER GROUPS</h3>

<p>Each partition is a separate data file which guarantees message ordering. That is important to remember: message ordering is only guaranteed within a single partition. That can introduce some tension later on between message ordering needs and performance needs as the unit of paralleism is also the partition. One partition cannot support competing consumers, so our invoice application can only have one instance consuming each partition.</p>

<p>Messages can be routed to partitions in a round robin manner or via a hashing function: _hash(message key) % number of partitions_. Using a hashing function has some benefits as we can design the message keys such that messages of the same entity, like a booking for example, always go to the same partition. This enables many patterns and message ordering guarantees.</p>

<p>Consumer Groups are like RabbitMQ&#39;s competing consumers. Each consumer in the group is an instance of the same application and will process a subset of all the messages in the topic. Whereas RabbitMQ&#39;s competing consumers all consume from the same queue, each consumer in a Consumer Group consumes from a different partition of the same topic. So in the examples above, the three instances of the invoice service all belong to the same Consumer Group.</p>

<p>At this point RabbitMQ looks a little more flexible with its guarantee of message order within a queue and its seamless ability to cope with changing numbers of competing consumers. With Kafka, how you partition your logs is important.</p>

<p>There is a subtle yet important advantage that Kafka had from the start that RabbitMQ added later on, regarding message order and parallelism. RabbitMQ maintains global order of the whole queue but offers no way for maintaining that order during the parallel processing of that queue. Kafka cannot offer global ordering of the topic, but it does offer ordering at the partition level. So if you only need ordering of related messages then Kafka offers <u>both ordered message delivery and ordered message processing</u>. Imagine you have messages that show the latest state of a client&#39;s booking, so you want to always process the messages of that booking sequentially (in temporal order). If you partition by the booking Id, then all messages of a given booking will all arrive at a single partition, where we have message ordering. So you can create a large number of partitions, making your processing highly parallelised and also get the guarantees you need for message ordering.</p>

<p>This capability exists in RabbitMQ also via the Consistent Hashing exchange which distributes messages over queues in the same way. Though Kafka enforces this ordered processing by the fact that only one consumer per consumer group can consume a single partition, and makes it easy as the coordinator node does all the work for you to ensure this rule is complied with. Whereas in RabbitMQ you could still have competing consumers consuming from one &quot;partitioned&quot; queue, and you must do the work to ensure that doesn&#39;t happen.</p>

<p>There&#39;s also a gotcha here, the moment you change the number of partitions, those messages for order Id 1000 now go to a different partition, so messages of order  Id 1000 exist in two partitions. Depending on how you process your messages this can introduce a headache. There exist scenarios now where the messages get processd out of order.</p>

<p>We&#39;ll be covering this subject in greater detail in the <a href="https://jack-vanlightly.com/blog/2017/12/15/rabbitmq-vs-kafka-part-4-message-delivery-semantics-and-guarantees">Part 4 Message Delivery Semantics and Guarantees</a> section of the series.</p>

<h3 id="toc_10">PUSH VS PULL</h3>

<p>RabbitMQ uses a push model and prevents overwhelming consumers via the consumer configured prefetch limit. This is great for low latency messaging and works well for RabbitMQ&#39;s queue based architecture. Kafka on the other hand uses a pull model where consumers request batches of messages from a given offset. To avoid tight loops when no messages exist beyond the current offset Kafka allows for long-polling.</p>

<p>A pull model makes sense for Kafka due to its partitions. As Kafka guarantees message order in a partition with no competing consumers, we can leverage the batching of messages for a more efficient message delivery that gives us higher throughput. This doesn&#39;t make so much sense for RabbitMQ as ideally we want to try to distribute messages one at a time as fast as possible to ensure that work is parallelised evenly and messages are processed close to the order in which they arrived in the queue. But with Kafka the partition is the unit of parallelism and message ordering so neither of those two factors are a concern for us.</p>

<h2 id="toc_11">Publish Subscribe</h2>

<p>Kafka supports basic pub sub with some extra patterns related to that fact it is a log and has partitions. The producers append messages to the end of the log partitions and the consumers could be positioned with their offset anywhere in the partition.</p>

<p><div style="padding-bottom: 42.926%; overflow: hidden;" class="image-block-wrapper lightbox  has-aspect-ratio" data-description="<p>Fig 9. Consumers with different offsets</p>&quot; data-lightbox-theme=&quot;dark&quot; id=&quot;yui_3_17_2_1_1551367744784_285&quot;&gt;<img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a25aff88165f5d59e22ac83/1512419334416/KafkaOnePartTwoCon.png" alt="Fig 9\. Consumers with different offsets"/><img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a25aff88165f5d59e22ac83/1512419334416/KafkaOnePartTwoCon.png" alt=""/></p>

<p>Fig 9. Consumers with different offsets</p>

<p>This style of diagram is not as easy to quickly interpret when there are multiple partitions and consumer groups, so for the remainder of the diagrams for Kafka I will use the following style:</p>

<p><div style="padding-bottom: 161.574%; overflow: hidden;" class="image-block-wrapper   has-aspect-ratio" data-description="<p>Fig 10. One producer, three partitions and one consumer group with three consumers</p>&quot; id=&quot;yui_3_17_2_1_1551367744784_308&quot;&gt;<img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a25b08ee2c48393e5890b93/1512419481863/KafkaPubSubSimple.png" alt="Fig 10\. One producer, three partitions and one consumer group with three consumers"/><img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a25b08ee2c48393e5890b93/1512419481863/KafkaPubSubSimple.png" alt=""/></p>

<p>Fig 10. One producer, three partitions and one consumer group with three consumers</p>

<p>We don&#39;t have to have the same number of consumers in our consumer group as there are partitions:</p>

<p><div style="padding-bottom: 118.79%; overflow: hidden;" class="image-block-wrapper lightbox  has-aspect-ratio" data-description="<p>Fig 11. Sone consumers read from more than one partition</p>&quot; data-lightbox-theme=&quot;dark&quot; id=&quot;yui_3_17_2_1_1551367744784_325&quot;&gt;<img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a25bd8324a6941e65e9d14a/1512422793776/KafkaMorePartitionsThanConsumers.png" alt="Fig 11\. Sone consumers read from more than one partition"/><img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a25bd8324a6941e65e9d14a/1512422793776/KafkaMorePartitionsThanConsumers.png" alt=""/></p>

<p>Fig 11. Sone consumers read from more than one partition</p>

<p>Consumers in one consumer group will coordinate the consumption of partitions, ensuring that one partition is not consumed by more than one consumer of the same consumer group.</p>

<p>Likewise, if we have more consumers than partitions, the extra consumer will remain idle, in reserve.</p>

<p><div style="padding-bottom: 121.678%; overflow: hidden;" class="image-block-wrapper lightbox  has-aspect-ratio" data-description="<p>Fig 12. One idle consumer</p>&quot; data-lightbox-theme=&quot;dark&quot; id=&quot;yui_3_17_2_1_1551367744784_348&quot;&gt;<img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a25be5b8165f530bb9de9e5/1512423024206/KafkaSpareConsumer.png" alt="Fig 12\. One idle consumer"/><img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a25be5b8165f530bb9de9e5/1512423024206/KafkaSpareConsumer.png" alt=""/></p>

<p>Fig 12. One idle consumer</p>

<p>After adding and removing consumers, the consumer group can become unbalanced. A rebalancing redistributes the consumers as evenly as possible across the partitions.</p>

<p><div style="padding-bottom: 40.5651%; overflow: hidden;" class="image-block-wrapper lightbox  has-aspect-ratio" data-description="<p>Fig 13. Addition of new consumers requires rebalancing</p>&quot; data-lightbox-theme=&quot;dark&quot; id=&quot;yui_3_17_2_1_1551367744784_371&quot;&gt;<img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a25c0b124a6941e65eab3a4/1512423611444/KafkaRebalancing.png" alt="Fig 13\. Addition of new consumers requires rebalancing"/><img src="https://static1.squarespace.com/static/56894e581c1210fead06f878/t/5a25c0b124a6941e65eab3a4/1512423611444/KafkaRebalancing.png" alt=""/></p>

<p>Fig 13. Addition of new consumers requires rebalancing</p>

<p>Rebalancing is automatically triggered after:</p>

<ul>
<li><p>a consumer joins a Consumer Group</p></li>
<li><p>a consumer leaves a Consumer Group (it shutsdown or is considered dead)</p></li>
<li><p>new partitions are added</p></li>
</ul>

<p>Rebalancing will cause a short period of extra latency while consumers stop reading batches of messages and get assigned to different partitions. Any in memory state that was maintained by the consumer may now be invalid. One of the patterns of consumption with Kafka is being able to direct all messages of a given entity, like a given booking, to the same partition and hence the same consumer. This is called data locality. Upon rebalancing any in memory data about that data will be useless unless the consumer gets assigned back to the same partition. Therefore consumers that maintain state will need to persist it externally.</p>

<h2 id="toc_12">Log Compaction</h2>

<p>The standard data retention policies are time and space based policies. Store up to the last week of messages or up to 50GB for example. But another type of data retention policy exists - Log Compaction. When a log is compacted, the result is that only the most recent message per message key is retained, the rest are removed.</p>

<p>Let&#39;s imagine we receive a message containing the current state of a user&#39;s booking. Every time a change is made to the booking a new event is generated with the current state of the booking. The topic may have a few messages for that one booking that represent the states of that booking since it was created. After the topic gets compacted only the most recent message related to that booking will be kept.</p>

<p>Depending on the volume of bookings and the size of each booking, you could theoretically store all bookings forever in the topic. By periodically compacting the topic we ensure we only store one message per booking.</p>

<p>Log compaction enables a few different patterns, which we will explore in <a href="https://jack-vanlightly.com/blog/2017/12/8/rabbitmq-vs-kafka-part-3-kafka-messaging-patterns">Part 3</a>.</p>

<h1 id="toc_13">More on Message Ordering</h1>

<p>We&#39;ve covered that scaling out and maintaining message ordering is possible with both RabbitMQ and Kafka, but Kafka makes it a lot easier. With RabbitMQ we must use the Consistent Hashing Exchange and manually implement the consumer group logic ourselves by using a distributed consensus service like ZooKeeper or Consul.</p>

<p>But RabbitMQ has one interesting capability that Kafka does not. It is not special to RabbitMQ itself but any publish-subscribe queue based messaging system. The capability is this: _Queue based messaging systems allow subscribers to order arbitrary groups of events._</p>

<p>Let&#39;s dive into that a little more. Different applications cannot share a queue because then they would compete to consume the messages. They need their own queue. This gives applications the freedom to configure their queue anyway they see fit. They can route multiple events types from multiple topics to their queue. This allows applications to maintain the ordering of related events. Which events it wants to combine can be configured differently for each application.</p>

<p>This is simply not possible with a log based messaging system like Kafka because logs are shared resources. Multiple applications read from the same log. So any grouping of related events into a single topic is a decision made at a wider system architecture level.</p>

<p>So there is no winner takes all here. RabbitMQ allows you to maintain relative ordering across arbitrary sets of events and Kafka provides a simple way of maintaining ordering at scale.</p>

<p><u>Update: I have built a library called Rebalanser that provides consumer group logic to RabbitMQ for .NET applications. Check out the</u> <a href="https://jack-vanlightly.com/blog/2018/7/22/creating-consumer-groups-in-rabbitmq-with-rebalanser-part-1">_post_</a> <u>on it and the</u> <a href="https://github.com/Vanlightly/Rebalanser/wiki">_GitHub repo_</a>_. If people show any interest then I&#39;d be up for making versions in other languages. Let me know._</p>

<h1 id="toc_14">Conclusions</h1>

<p>RabbitMQ offers a swiss army knife of messaging patterns due to the variety of functionality it offers. With its powerful routing, it can obviate the need for consumers to retrieve, deserialize and inspect every message when it only needs a subset. It is easy to work with, scaling up and down is done by simply adding and removing consumers. It&#39;s plug-in architecture allows it to support other protocols and add new features such as Consistent hashing exchange which is an important addition.</p>

<p>Kafka&#39;s distributed log with consumer offsets makes time travel possible. It&#39;s ability to route messages of the same key to the same consumer, in order, makes highly parallelised, ordered processing possible. Kafka&#39;s log compaction and data retention allow new patterns that RabbitMQ simply cannot deliver. Finally yes, Kafka can scale further than RabbitMQ, but most of us deal with a message volume that both can handle comfortably.</p>

<p>In the next part we&#39;ll take a closer look at messaging patterns and topologies with RabbitMQ.</p>

<ul>
<li><p><a href="https://jack-vanlightly.com/blog/2017/12/3/rabbitmq-vs-kafka-series-introduction">Series Introduction</a></p></li>
<li><p><a href="https://jack-vanlightly.com/blog/2017/12/4/rabbitmq-vs-kafka-part-1-messaging-topologies">Part 1 - Two different takes on messaging (high level design comparison)</a></p></li>
<li><p><a href="https://jack-vanlightly.com/blog/2017/12/5/rabbitmq-vs-kafka-part-2-rabbitmq-messaging-patterns-and-topologies">Part 2 - Messaging patterns and topologies with RabbitMQ</a></p></li>
<li><p><a href="https://jack-vanlightly.com/blog/2017/12/8/rabbitmq-vs-kafka-part-3-kafka-messaging-patterns">Part 3 - Messaging patterns and topologies with Kafka</a></p></li>
<li><p><a href="https://jack-vanlightly.com/blog/2017/12/15/rabbitmq-vs-kafka-part-4-message-delivery-semantics-and-guarantees">Part 4 - Message delivery semantics and guarantees</a></p></li>
<li><p><a href="https://jack-vanlightly.com/blog/2018/8/31/rabbitmq-vs-kafka-part-5-fault-tolerance-and-high-availability-with-rabbitmq">Part 5 - Fault tolerance and high availability with RabbitMQ</a></p></li>
<li><p><a href="https://jack-vanlightly.com/blog/2018/9/2/rabbitmq-vs-kafka-part-6-fault-tolerance-and-high-availability-with-kafka">Part 6 - Fault tolerance and high availability with Kafka</a></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kafka vs RabbitMQ]]></title>
    <link href="http://panlw.github.io/15513681283898.html"/>
    <updated>2019-02-28T23:35:28+08:00</updated>
    <id>http://panlw.github.io/15513681283898.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://www.bizety.com/2019/02/08/kafka-vs-rabbitmq/">https://www.bizety.com/2019/02/08/kafka-vs-rabbitmq/</a></p>
</blockquote>

<p>There are many messaging products in the market; but two of the most popular open source messaging technologies available today are <a href="https://www.rabbitmq.com/">RabbitMQ</a> and <a href="https://kafka.apache.org/intro">Apache Kafka</a>. Which software is right for you? Each has its own story, design framework, set of features, use cases in which it is particularly efficient, integration potential, and developer experience. We will focus on the features in this post, and touch on some of the other areas along the way.</p>

<p>At its simplest, Kafka is a message bus optimized for high-ingress data streams and replay while RabbitMQ is a mature, general purpose message broker that supports several standardized protocols, including AMQP.</p>

<p>A streaming platform has four main capabilities:</p>

<ul>
<li>  Publish and subscribe to streams of records, in a similar way to an enterprise messaging system or a message queue</li>
<li>  Score streams of records in a durable fault-tolerant manner</li>
<li>  Process streams of records as they happen</li>
<li>  Scale across large clusters of machines</li>
</ul>

<p>In the <a href="https://www.bizety.com/2017/06/05/open-source-stream-processing-flink-vs-spark-vs-storm-vs-kafka/">early days of data processing</a>, the main way to process and output data was batch-oriented data infrastructure. Today, however, in a world in which real-time analytics are necessary to keep up with network demands and functionality, stream processing has become essential.</p>

<p>The Publish/Subscribe pattern is a distributed interaction paradigm well suited to the deployment of scalable and loosely coupled systems.</p>

<h2 id="toc_0">Main Features of Pub/Sub Systems</h2>

<ul>
<li><p>Decoupling publishers and subscribers is probably the most fundamental aspect of pub/sub systems. Decoupling can be broken down into three separate dimensions:</p>

<ul>
<li>  <strong>Entity decoupling</strong> – publishers and consumers don’t need to be aware of one another;</li>
<li>  <strong>Time decoupling</strong> – publishers and consumers don’t need to actively participate in the interaction at the same time;</li>
<li>  <strong>Synchronization decoupling</strong> – the interaction between producer and consumer or the pub/sub infrastructure doesn’t need to synchronously block the producer or consumer execution threads as this will enable maximum usage of processor resources by producers and consumers both.</li>
</ul></li>
<li><p>Routing Logic (or subscription model), which determines if and where a packet that stems from a producer will end at a consumer. There are two kinds of routing logic:</p>

<ul>
<li>  <strong>Topic based</strong> – The publisher tags the message with a series of topics, which can be used efficiently to filter operations and determine which message goes to which consumer;</li>
<li>  <strong>Content based</strong> – Data and meta fields of the message can be used to set filtering terms</li>
</ul></li>
</ul>

<h2 id="toc_1">Founding Stories</h2>

<p>Apache Kafka <a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">began life at LinkedIn</a> as a way of making data ingestion to Hadoop from Apache Flume more straightforward. Ingesting and exporting from several data sources and destinations using tools like Flume involved writing separate data pipelines for every source and destination pairing. Kafka allowed LinkedIn to standardize the data pipelines and enabled the getting data out of each system just once and into each system just once, ultimately simplifying pipelines and operations. Kafka is now well integrated into the overall ecosystem of Apache Software Foundation projects. It is particularly well integrated into Apache Zookeeper, which provides the backbone for Kafka’s distributed partitions, and offers various clustering benefits for Kafka users.</p>

<p>RabbitMQ, meanwhile, was initially developed as a traditional message broker in order to implement a range of messaging protocols. At first, it was designed to implement <a href="http://www.amqp.org/product/overview">AMQP</a>, an open wire protocol for messaging that has powerful routing features. AMQP enabled cross-language flexibility for open source message brokers, thereby enabling the growth of non-Java applications, which needed distributed messaging. It was one of the first open source message brokers to achieve a strong level of features along with robust documentation, dev tools and client libraries. As a result, with over 35,000 production deployments, RabbitMQ is the most widely deployed open source message broker in the market.</p>

<h2 id="toc_2">Use Cases</h2>

<p>Kafka is used for two main classes of application:</p>

<ol>
<li> Building streaming data pipelines that operate in real-time in order to reliably get data from one system or application to another;</li>
<li> Building streaming applications that operate in real time-time in order to transform or react to the data streams.</li>
</ol>

<p>The way that Kafka describes itself is as a distributed streaming platform; however, it is well known mainly for being a durable storage repository with robust Hadoop/Spark support. Popular <a href="https://kafka.apache.org/uses">use cases</a> include:</p>

<ul>
<li>  Website Activity Tracking</li>
<li>  Metrics</li>
<li>  Log Aggregation</li>
<li>  Stream Processing</li>
<li>  Event Sourcing</li>
<li>  Commit Logs</li>
<li>  Messaging</li>
</ul>

<p>The <a href="https://dzone.com/articles/understanding-when-to-use-rabbitmq-or-apache-kafka">best types of messaging scenarios</a> in Kafka are:</p>

<ul>
<li>  Streaming from A to B without sophisticated routing with maximal throughput (100k/sec_), delivered at least once in partitioned order;</li>
<li>  Since Kafka is a durable message store, clients can receive a replay of events from the event stream on request (in traditional message brokers, the message is normally removed from the queue once it is delivered);</li>
<li>  Stream processing;</li>
<li>  Event processing.</li>
</ul>

<p>RabbitMQ, meanwhile, tends to be used in situations in which web servers need to respond quickly as opposed to being forced to perform resource-intensive procedures while the user waits for the result. RabbitMQ is also frequently used to distribute a message to several recipients for consumption or to balance loads between workers under high load (20k+/second). It also offers numerous features that extend beyond throughput, such as reliable delivery, routing, federation, security, management tools, HA, and others.</p>

<p>Scenarios that work best in RabbitMQ include:</p>

<ul>
<li>  If your application has to work with any combination of existing protocols, such as AMQP 0-9-1, STOMP, MQTT, AMQP 1.0.;</li>
<li>  If you need a granular type of consistency control/set of guarantees on a per-message basis;</li>
<li>  If your application demands variety in terms of publish/subscribe, point to point or request/reply messaging;</li>
<li>  For complex routing to consumers;</li>
<li>  To address similar use cases as Kafka above, but with downloaded software.</li>
</ul>

<p>RabbitMQ shines when integrating to existing IT infrastructure.</p>

<h2 id="toc_3">Design</h2>

<h3 id="toc_4">Apache Kafka</h3>

<ul>
<li>  Apache Kafka is designed for high volume publish-subscribe messages and streams;</li>
<li>  It is intended to be durable, fast and scalable;</li>
<li>  Kafka runs as a cluster on one or several servers that can span numerous data centers;</li>
<li>  The Kafka cluster stores record streams in categories that are known as topics;</li>
<li>  Each record comprises a key, a value, and a timestamp.</li>
</ul>

<h3 id="toc_5">RabbitMQ</h3>

<ul>
<li>  RabbitMQ communication can either be synchronous or asynchronous;</li>
<li>  <a href="https://www.youtube.com/watch?v=deG25y_r6OY">Publishers deliver messages to exchanges</a>, and consumers retrieve messages from queues;</li>
<li>  Decoupling a producer from a queue via an exchange means that producers are not overwhelmed by hard coded routing decisions;</li>
<li>  RabbitMQ also provides a series of distributed deployment scenarios, which can be configured for multi-node clusters to cluster federation without dependencies on external services (although some cluster formation plug-ins are capable of using AWS APIs, DNS, Consul, etc.)</li>
</ul>

<h2 id="toc_6">Languages and Libraries</h2>

<p>Apache Kafka only ships a Java client, however, the catalog of community <a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients">open source clients</a> and <a href="https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem">ecosystem projects</a> is growing. In addition, there is an adapter SDK, which lets you build your own system integration. Most of the configuration is performed programmatically or via properties files. There are many client libraries in Apache Kafka including Ruby, Python, Node.js and Java.</p>

<p>RabbitMQ, by contrast, supports a range of languages, including Java, Spring, .NET, PHP, Python, Ruby, JavaScript, Go, Elixir, Objective-C, Swift as official languages. It also supports various other <a href="https://www.rabbitmq.com/devtools.html">clients and devtools</a> via community plugins. Its client libraries are mature and well documented, and include Ruby, Python, Node.js, Clojure, Go, Java and C.</p>

<p>Since these two tools are so popular, most other software providers offer solutions that mean RabbitMQ and Kafka work well with or on their technology.</p>

<h2 id="toc_7">Security and Operations</h2>

<p>The Kafka 0.9 release added TLS and JAAS role-based access control, in addition to Kerberos/plain/scram auth, with the use of a CLI to manage security policy. This was an improvement on previous versions in which you could only lock down access at the network level, which made sharing and multi-tenancy tricky.</p>

<p>Kafka’s management CLI is made up of shell scripts, property files, and specifically formatted JSON files. Kafka Brokers, Producers, and Consumers emit metrics through Yammer/JMX, but they do not retain any history, which entails the use of a third party monitoring system.  Operations is able to manage partitions and topics through the use of these tools, in addition to checking the consumer offset position, and using the HA and FT capabilities that Apache Zookeeper offers for Kafka.</p>

<p>Security and operations are strengths of RabbitMQ. The plugin for RabbitMQ management offers an HTTP API, a browser-based UI for monitoring and management, in addition to CLI tools for operators. External tools such as CollectD, Datadog, or New Relic are necessary for longer-term monitoring data storage. RabbitMQ also offers APIs and tools for monitoring, auditing and other types of troubleshooting. In addition to offering TLS support, RabbitMQ ships with RBAC backed by a built-in data store, LDAP or external HTTPS-based providers and it supports authentication using the x509 certificate as opposed to username/password pairs. Additional authentication methods can be developed via the use of plugins.</p>

<h2 id="toc_8">Additional Features and Information</h2>

<ul>
<li>  Apache Kafka is open sourced via Apache License 2.0 whereas RabbiqMQ is open sourced via Mozilla Public License.</li>
<li>  Apache Kafka is written in Scala; RabbitMQ is written in Erlang.</li>
<li>  Both support high availability, but only RabbitMQ supports federated queues. This feature in RabbitMQ provides a way of load balancing a single queue over multiple nodes or clusters.</li>
<li>  Apache Kafka can support the performance of complex routing scenarios, but RabbitMQ does not.</li>
<li>  Apache Kafka is available via CloudKarafka; RabbitMQ is available from CloudAMQP.</li>
</ul>

<h2 id="toc_9">Conclusion</h2>

<p>Apache Kafka scales up to 100,000 msg/sec on a single server, so easily outbeats Kafka as well as all the other message brokers in terms of performance. It’s often a key driver for people in choosing to work with Kafka. However, while Kafka is well optimized to work with “fast” consumers, due to its partition-centric design, it is a little less successful at working with “slow” consumers. Its performance capability is also in part determined by a significant amount of responsibility on the developer writing the consumer code.</p>

<p>RabbitMQ supports a wide range of development platforms with ease of use and all the benefits of having a mature history behind it. It scales well at around 20,000 message/second on a single server, but it also scales well as more servers are added. If overall throughput is sufficient for requirements, this message broker works OK for “fast” consumers. “Slower” consumers, however, are the ones to really reap the benefits of RabbitMQ.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[3 Ways to add local jar to maven project]]></title>
    <link href="http://panlw.github.io/15512561553051.html"/>
    <updated>2019-02-27T16:29:15+08:00</updated>
    <id>http://panlw.github.io/15512561553051.html</id>
    <content type="html"><![CDATA[
<pre><code>June 2, 2016
</code></pre>

<blockquote>
<p>原文地址 <a href="http://roufid.com/3-ways-to-add-local-jar-to-maven-project/">http://roufid.com/3-ways-to-add-local-jar-to-maven-project/</a></p>
</blockquote>

<h2 id="toc_0"><strong>Introduction</strong></h2>

<p>You may need to add a custom JAR as a dependency to your Maven project. This tutorial shows 3 ways to do it:</p>

<ol>
<li> <a href="#manually">Install manually the JAR into your local Maven repository</a></li>
<li> <a href="#system">Adding the dependency as system scope</a></li>
<li> <a href="#different_local">Creating a different local Maven repository</a></li>
<li> <a href="#nexus">Using a Nexus repository manager</a></li>
</ol>

<h2 id="toc_1"><strong>1- Install manually the JAR into your local Maven repository</strong></h2>

<p>The first solution is to add manually the JAR into your local Maven repository by using the Maven goal <strong>_<a href="http://maven.apache.org/plugins/maven-install-plugin/install-file-mojo.html">install:install-file</a>_</strong>. The use of the plugin is very simple as below:</p>

<pre><code class="language-sh">mvn install:install-file -Dfile=&lt;path-to-file&gt;
</code></pre>

<p>Note that we didn’t specify _groupId_, <u>artifactId</u>, <u>version</u> and <u>packaging</u> of the JAR to install. Indeed, since the version 2.5 of <strong>_<a href="http://maven.apache.org/plugins/maven-install-plugin/">Maven-install-plugin</a>_</strong>, these information can be taken from an optionally specified <strong>_pomFile_</strong>.</p>

<p>These information can also be given in command line:</p>

<pre><code class="language-sh">mvn install:install-file -Dfile=&lt;path-to-file&gt; -DgroupId=&lt;group-id&gt; -DartifactId=&lt;artifact-id&gt; -Dversion=&lt;version&gt;
</code></pre>

<p>Where:</p>

<ul>
<li>  <strong>_<path-to-file>:_</strong> <u>Path to the JAR to install</u></li>
<li>  <u><strong><group-id>:</strong> Group id of the JAR to install</u></li>
<li>  <u><strong><artifact-id>:</strong> Artifact id of the JAR to install</u></li>
<li>  <u><strong><version>: </strong> Version of the JAR</u></li>
</ul>

<p>For example:</p>

<pre><code class="language-sh">mvn install:install-file –Dfile=C:\dev\app.jar -DgroupId=com.roufid.tutorials -DartifactId=example-app -Dversion=1.0
</code></pre>

<p>You can then add the dependency to your Maven project by adding those lines to your pom.xml file:</p>

<pre><code class="language-xml">&lt;dependency&gt;
    &lt;groupId&gt;com.roufid.tutorials&lt;/groupId&gt;
    &lt;artifactId&gt;example-app&lt;/artifactId&gt;
    &lt;version&gt;1.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>This solution can be very expensive. Why ? You have to consider that the day you change your local Maven repository you have to re-install the JAR. Or again, if there are many persons working on the project, each must install the JAR in his local repository. The portability of the project must be taken into account.</p>

<p>Another solution is to use the <strong>_maven-install-plugin_</strong> in your <strong>_pom.xml_</strong> which will install the jar during the Maven <strong>_“initialize”_</strong> phase. To do this, you must specify the location of the jar you want to install. The best way is to put the JAR in a folder created at the root of the project <u>(in the same directory as the pom.xml file)</u>.</p>

<p>Let’s consider that the jar is located under <strong>_<PROJECT_ROOT_FOLDER>/lib/app.jar._</strong> Below the configuration of maven-install-plugin:</p>

<pre><code class="language-xml">&lt;plugin&gt;
    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
    &lt;artifactId&gt;maven-install-plugin&lt;/artifactId&gt;
    &lt;version&gt;2.5&lt;/version&gt;
    &lt;executions&gt;
        &lt;execution&gt;
            &lt;phase&gt;initialize&lt;/phase&gt;
            &lt;goals&gt;
                &lt;goal&gt;install-file&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;configuration&gt;
                &lt;groupId&gt;com.roufid.tutorials&lt;/groupId&gt;
                &lt;artifactId&gt;example-app&lt;/artifactId&gt;
                &lt;version&gt;1.0&lt;/version&gt;
                &lt;packaging&gt;jar&lt;/packaging&gt;
                &lt;file&gt;${basedir}/lib/app.jar&lt;/file&gt;
            &lt;/configuration&gt;
        &lt;/execution&gt;
    &lt;/executions&gt;
&lt;/plugin&gt;
</code></pre>

<blockquote>
<p><strong>_${basedir}_</strong> <u>represents the directory containing pom.xml.</u></p>
</blockquote>

<p>You may encounter an error while adding the previous lines, add the following plugin to your project to allow the lifecycle mapping:</p>

<pre><code class="language-xml">&lt;pluginManagement&gt;
    &lt;plugins&gt;
        &lt;!--This plugin&#39;s configuration is used to store Eclipse m2e settings only. 
            It has no influence on the Maven build itself. --&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.eclipse.m2e&lt;/groupId&gt;
            &lt;artifactId&gt;lifecycle-mapping&lt;/artifactId&gt;
            &lt;version&gt;1.0.0&lt;/version&gt;
            &lt;configuration&gt;
                &lt;lifecycleMappingMetadata&gt;
                    &lt;pluginExecutions&gt;
                        &lt;pluginExecution&gt;
                            &lt;pluginExecutionFilter&gt;
                                &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;
                                &lt;artifactId&gt;aspectj-maven-plugin&lt;/artifactId&gt;
                                &lt;versionRange&gt;[1.0,)&lt;/versionRange&gt;
                                &lt;goals&gt;
                                    &lt;goal&gt;test-compile&lt;/goal&gt;
                                    &lt;goal&gt;compile&lt;/goal&gt;
                                &lt;/goals&gt;
                            &lt;/pluginExecutionFilter&gt;
                            &lt;action&gt;
                                &lt;execute /&gt;
                            &lt;/action&gt;
                        &lt;/pluginExecution&gt;
                        &lt;pluginExecution&gt;
                            &lt;pluginExecutionFilter&gt;
                                &lt;groupId&gt;
                                    org.apache.maven.plugins
                                &lt;/groupId&gt;
                                &lt;artifactId&gt;
                                    maven-install-plugin
                                &lt;/artifactId&gt;
                                &lt;versionRange&gt;
                                    [2.5,)
                                &lt;/versionRange&gt;
                                &lt;goals&gt;
                                    &lt;goal&gt;install-file&lt;/goal&gt;
                                &lt;/goals&gt;
                            &lt;/pluginExecutionFilter&gt;
                            &lt;action&gt;
                                &lt;execute&gt;
                                    &lt;runOnIncremental&gt;false&lt;/runOnIncremental&gt;
                                &lt;/execute&gt;
                            &lt;/action&gt;
                        &lt;/pluginExecution&gt;
                    &lt;/pluginExecutions&gt;
                &lt;/lifecycleMappingMetadata&gt;
            &lt;/configuration&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/pluginManagement&gt;
</code></pre>

<h2 id="toc_2"><strong>2- Adding directly the dependency as system scope</strong></h2>

<p>Another solution – dirty solution – is by adding the dependency as <strong>_system_</strong> scope and refer to it by its full path. Consider that the JAR is located in <strong>_<PROJECT_ROOT_FOLDER>/lib._</strong> Then add the dependency in your pom.xml file as following:</p>

<pre><code class="language-xml">&lt;dependency&gt;
    &lt;groupId&gt;com.roufid.tutorials&lt;/groupId&gt;
    &lt;artifactId&gt;example-app&lt;/artifactId&gt;
    &lt;version&gt;1.0&lt;/version&gt;
    &lt;scope&gt;system&lt;/scope&gt;
    &lt;systemPath&gt;${basedir}/lib/yourJar.jar&lt;/systemPath&gt;
&lt;/dependency&gt;
</code></pre>

<blockquote>
<p><strong>_${basedir}_</strong> <u>represents the directory containing pom.xml.</u></p>
</blockquote>

<h2 id="toc_3"><strong>3- Creating a different local Maven repository</strong></h2>

<p>The third solution is quite similar to the first one, the difference lies in the fact that the JARs will be installed in a different local Maven repository.</p>

<p>Let’s consider the new local Maven repository is named <strong>_“maven-repository”_</strong> and is located in <strong>_${basedir}_</strong> (the directory containing pom.xml). First you have to do is <strong>_deploying_</strong> the local JARs in the new local maven repository as below:</p>

<pre><code class="language-sh">mvn deploy:deploy-file -Dfile=&lt;path-to-file&gt; -DgroupId=&lt;group-id&gt; -DartifactId=&lt;artifact-id&gt; -Dversion=&lt;version&gt; -Dpackaging=jar -Durl=file:./maven-repository/ -DrepositoryId=maven-repository -DupdateReleaseInfo=true
</code></pre>

<p>Normally, the Maven <strong>_deploy:deploy-file_</strong> installs the artifact in a remote repository but in our case the repository is located in the local machine.</p>

<p>After installing the JARs your need to add the repository in your pom.xml file:</p>

<pre><code class="language-xml">&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;maven-repository&lt;/id&gt;
        &lt;url&gt;file:///${project.basedir}/maven-repository&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
</code></pre>

<p>Then you can add the dependency into your pom.xml</p>

<pre><code class="language-xml">&lt;dependency&gt;
    &lt;groupId&gt;com.roufid.tutorials&lt;/groupId&gt;
    &lt;artifactId&gt;example-app&lt;/artifactId&gt;
    &lt;version&gt;1.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<h2 id="toc_4"><strong>4- Using Nexus repository manager</strong></h2>

<p>The best solution is to use a Nexus Repository Manager which will contain all your JARs and you will use it as repository to download the dependency.</p>

<p><strong><a href="http://books.sonatype.com/nexus-book/reference/install.html">This book from the official Nexus site</a></strong> will show you how to install and use Nexus repository manager.</p>

<h2 id="toc_5"><strong>Refrences</strong></h2>

<ul>
<li>  <strong><a href="https://maven.apache.org/guides/mini/guide-3rd-party-jars-local.html">Maven : installing 3rd party JARs</a></strong></li>
<li>  <strong><a href="https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html">Maven lifecycle</a></strong></li>
<li>  <strong><a href="http://maven.apache.org/plugins/maven-deploy-plugin/deploy-file-mojo.html">Maven deploy:deploy-file goal</a></strong></li>
<li>  <strong><a href="http://maven.apache.org/plugins/maven-install-plugin/install-file-mojo.html">Maven install:install-file goal</a></strong></li>
<li>  <strong><a href="http://www.sonatype.org/nexus/">Nexus</a></strong></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Redisson 实现 Redis 分布式锁的N种姿势]]></title>
    <link href="http://panlw.github.io/15512418489211.html"/>
    <updated>2019-02-27T12:30:48+08:00</updated>
    <id>http://panlw.github.io/15512418489211.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://mp.weixin.qq.com/s/EcwPnD8jlZrBUzcADcRl6A">https://mp.weixin.qq.com/s/EcwPnD8jlZrBUzcADcRl6A</a></p>
</blockquote>

<p>来源：阿飞的博客</p>

<p>前几天发的一篇文章《<a href="https://mp.weixin.qq.com/s?__biz=MzU5ODUwNzY1Nw==&amp;mid=2247484155&amp;idx=1&amp;sn=0c73f45f2f641ba0bf4399f57170ac9b&amp;scene=21#wechat_redirect">Redlock：Redis 分布式锁最牛逼的实现</a>》，引起了一些同学的讨论，也有一些同学提出了一些疑问，这是好事儿。本文在讲解如何使用 Redisson 实现 Redis 普通分布式锁，以及 Redlock 算法分布式锁的几种方式的同时，也附带解答这些同学的一些疑问。</p>

<h2 id="toc_0">Redis 几种架构</h2>

<p>Redis 发展到现在，几种常见的部署架构有：</p>

<ol>
<li> 单机模式；</li>
<li> 主从模式；</li>
<li> 哨兵模式；</li>
<li> 集群模式；</li>
</ol>

<p>我们首先基于这些架构讲解 Redisson 普通分布式锁实现，需要注意的是，只有充分了解普通分布式锁是如何实现的，才能更好的了解 Redlock 分布式锁的实现，因为 <strong>Redlock 分布式锁的实现完全基于普通分布式锁</strong>。</p>

<h2 id="toc_1">普通分布式锁</h2>

<p>Redis 普通分布式锁原理这个大家基本上都了解，本文不打算再过多的介绍，上一篇文章<strong>《</strong><a href="http://mp.weixin.qq.com/s?__biz=MzAxODcyNjEzNQ==&amp;mid=2247486873&amp;idx=2&amp;sn=f2c8133777098f77a111a5d2b715e1d0&amp;chksm=9bd0a001aca7291768f959249209557d21781880a24a67896bcc20fcc3c0bcc287d3726638ea&amp;scene=21#wechat_redirect"><strong>Redlock：Redis 分布式锁的实现</strong></a><strong>》</strong>也讲的很细，并且也说到了几个重要的注意点。如果你对 Redis 普通的分布式锁还有一些疑问，可以再回顾一下这篇文章。</p>

<p>接下来直接 show you the code，毕竟 talk is cheap。</p>

<ul>
<li>  redisson 版本</li>
</ul>

<p>本次测试选择 redisson 2.14.1 版本。</p>

<h4 id="toc_2">单机模式</h4>

<p>源码如下：</p>

<pre><code class="language-java">// 构造redisson实现分布式锁必要的Config
Config config = new Config();
config.useSingleServer().setAddress(&quot;redis://172.29.1.180:5379&quot;).setPassword(&quot;a123456&quot;).setDatabase(0);
// 构造RedissonClient
RedissonClient redissonClient = Redisson.create(config);
// 设置锁定资源名称
RLock disLock = redissonClient.getLock(&quot;DISLOCK&quot;);
boolean isLock;
try {
    //尝试获取分布式锁
    isLock = disLock.tryLock(500, 15000, TimeUnit.MILLISECONDS);
    if (isLock) {
        //TODO if get lock success, do something;
        Thread.sleep(15000);
    }
} catch (Exception e) {
} finally {
    // 无论如何, 最后都要解锁
    disLock.unlock();
}
</code></pre>

<p>通过代码可知，经过 Redisson 的封装，实现 Redis 分布式锁非常方便，我们再看一下 Redis 中的 value 是啥，和前文分析一样，hash 结构，key 就是资源名称，field 就是 UUID+threadId，value 就是重入值，在分布式锁时，这个值为 1（Redisson 还可以实现重入锁，那么这个值就取决于重入次数了）：</p>

<pre><code class="language-sh">172.29.1.180:5379&gt; hgetall DISLOCK
1) &quot;01a6d806-d282-4715-9bec-f51b9aa98110:1&quot;
2) &quot;1&quot;
</code></pre>

<h4 id="toc_3">哨兵模式</h4>

<p>即 sentinel 模式，实现代码和单机模式几乎一样，唯一的不同就是 Config 的构造：</p>

<pre><code class="language-java">Config config = new Config();
config.useSentinelServers().addSentinelAddress(
        &quot;redis://172.29.3.245:26378&quot;,&quot;redis://172.29.3.245:26379&quot;, &quot;redis://172.29.3.245:26380&quot;)
        .setMasterName(&quot;mymaster&quot;)
        .setPassword(&quot;a123456&quot;).setDatabase(0);

</code></pre>

<h4 id="toc_4">集群模式</h4>

<p>集群模式构造 Config 如下：</p>

<pre><code class="language-java">Config config = new Config();
config.useClusterServers().addNodeAddress(
        &quot;redis://172.29.3.245:6375&quot;,&quot;redis://172.29.3.245:6376&quot;, &quot;redis://172.29.3.245:6377&quot;,
        &quot;redis://172.29.3.245:6378&quot;,&quot;redis://172.29.3.245:6379&quot;, &quot;redis://172.29.3.245:6380&quot;)
        .setPassword(&quot;a123456&quot;).setScanInterval(5000);
</code></pre>

<h4 id="toc_5">总结</h4>

<p>普通分布式实现非常简单，无论是那种架构，向 Redis 通过 EVAL 命令执行 LUA 脚本即可。</p>

<h2 id="toc_6">Redlock 分布式锁</h2>

<p>那么 Redlock 分布式锁如何实现呢？以单机模式 Redis 架构为例，直接看实现代码：</p>

<pre><code class="language-java">Config config1 = new Config();
config1.useSingleServer().setAddress(&quot;redis://172.29.1.180:5378&quot;)
        .setPassword(&quot;a123456&quot;).setDatabase(0);
RedissonClient redissonClient1 = Redisson.create(config1);

Config config2 = new Config();
config2.useSingleServer().setAddress(&quot;redis://172.29.1.180:5379&quot;)
        .setPassword(&quot;a123456&quot;).setDatabase(0);
RedissonClient redissonClient2 = Redisson.create(config2);

Config config3 = new Config();
config3.useSingleServer().setAddress(&quot;redis://172.29.1.180:5380&quot;)
        .setPassword(&quot;a123456&quot;).setDatabase(0);
RedissonClient redissonClient3 = Redisson.create(config3);

String resourceName = &quot;REDLOCK&quot;;
RLock lock1 = redissonClient1.getLock(resourceName);
RLock lock2 = redissonClient2.getLock(resourceName);
RLock lock3 = redissonClient3.getLock(resourceName);

RedissonRedLock redLock = new RedissonRedLock(lock1, lock2, lock3);
boolean isLock;
try {
    isLock = redLock.tryLock(500, 30000, TimeUnit.MILLISECONDS);
    System.out.println(&quot;isLock = &quot;+isLock);
    if (isLock) {
        //TODO if get lock success, do something;
        Thread.sleep(30000);
    }
} catch (Exception e) {
} finally {
    // 无论如何, 最后都要解锁
    System.out.println(&quot;&quot;);
    redLock.unlock();
}
</code></pre>

<p>最核心的变化就是<code>RedissonRedLock redLock = new RedissonRedLock(lock1, lock2, lock3);</code>，因为我这里是以三个节点为例。</p>

<p>那么如果是哨兵模式呢？需要搭建 3 个，或者 5 个 sentinel 模式集群（具体多少个，取决于你）。<br/>
那么如果是集群模式呢？需要搭建 3 个，或者 5 个 cluster 模式集群（具体多少个，取决于你）。</p>

<h2 id="toc_7">实现原理</h2>

<p>既然核心变化是使用了 <strong>RedissonRedLock</strong>，那么我们看一下它的源码有什么不同。这个类是 <strong>RedissonMultiLock</strong> 的子类，所以调用 tryLock 方法时，事实上调用了 <strong>RedissonMultiLock</strong> 的 tryLock 方法，精简源码如下：</p>

<pre><code class="language-java">public boolean tryLock(long waitTime, long leaseTime, TimeUnit unit) throws InterruptedException {
    // 实现要点之允许加锁失败节点限制（N-(N/2+1)）
    int failedLocksLimit = failedLocksLimit();
    List&lt;RLock&gt; acquiredLocks = new ArrayList&lt;RLock&gt;(locks.size());
    // 实现要点之遍历所有节点通过EVAL命令执行lua加锁
    for (ListIterator&lt;RLock&gt; iterator = locks.listIterator(); iterator.hasNext();) {
        RLock lock = iterator.next();
        boolean lockAcquired;
        try {
            // 对节点尝试加锁
            lockAcquired = lock.tryLock(awaitTime, newLeaseTime, TimeUnit.MILLISECONDS);
        } catch (RedisConnectionClosedException|RedisResponseTimeoutException e) {
            // 如果抛出这类异常，为了防止加锁成功，但是响应失败，需要解锁
            unlockInner(Arrays.asList(lock));
            lockAcquired = false;
        } catch (Exception e) {
            // 抛出异常表示获取锁失败
            lockAcquired = false;
        }

        if (lockAcquired) {
            // 成功获取锁集合
            acquiredLocks.add(lock);
        } else {
            // 如果达到了允许加锁失败节点限制，那么break，即此次Redlock加锁失败
            if (locks.size() - acquiredLocks.size() == failedLocksLimit()) {
                break;
            }               
        }
    }
    return true;
}
</code></pre>

<p>很明显，这段源码就是上一篇文章<strong>《</strong><a href="http://mp.weixin.qq.com/s?__biz=MzAxODcyNjEzNQ==&amp;mid=2247486873&amp;idx=2&amp;sn=f2c8133777098f77a111a5d2b715e1d0&amp;chksm=9bd0a001aca7291768f959249209557d21781880a24a67896bcc20fcc3c0bcc287d3726638ea&amp;scene=21#wechat_redirect"><strong>Redlock：Redis 分布式锁的实现</strong></a><strong>》</strong>提到的 Redlock 算法的完全实现。</p>

<p>以 sentinel 模式架构为例，如下图所示，有 sentinel-1，sentinel-2，sentinel-3 总计 3 个 sentinel 模式集群，如果要获取分布式锁，那么需要向这 3 个 sentinel 集群通过 EVAL 命令执行 LUA 脚本，需要 3/2+1=2，即至少 2 个 sentinel 集群响应成功，才算成功的以 Redlock 算法获取到分布式锁：</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/4o22OFcmzHk1pgWlVxe0MAziaMeUwZwzyHAsAbIrZpJRk0F9XfamODSUickaFtZoZiaYic56RicoQY2lCjJDYq4ARSQ/640?wx_fmt=png" alt=""/>Redlock 分布式锁</p>

<h2 id="toc_8">问题合集</h2>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/4o22OFcmzHk1pgWlVxe0MAziaMeUwZwzyvqB06oXq6Z5IcicsicbGLicuIiayRz9zBtwMEHFdWR6g3cRjF3Liabdt2Ug/640?wx_fmt=png" alt=""/>image.png</p>

<p>根据上面实现原理的分析，这位同学应该是对 Redlock 算法实现有一点点误解，假设我们用 5 个节点实现 Redlock 算法的分布式锁。那么<strong>要么是 5 个 redis 单实例，要么是 5 个 sentinel 集群，要么是 5 个 cluster 集群</strong>。而不是一个有 5 个主节点的 cluster 集群，然后向每个节点通过 EVAL 命令执行 LUA 脚本尝试获取分布式锁，如上图所示。</p>

<ul>
<li>  失效时间如何设置</li>
</ul>

<p>这个问题的场景是，假设设置失效时间 10 秒，如果由于某些原因导致 10 秒还没执行完任务，这时候锁自动失效，导致其他线程也会拿到分布式锁。</p>

<p>这确实是 Redis 分布式最大的问题，不管是普通分布式锁，还是 Redlock 算法分布式锁，都没有解决这个问题。也有一些文章提出了对失效时间续租，即延长失效时间，很明显这又提升了分布式锁的复杂度。另外就笔者了解，没有现成的框架有实现，如果有哪位知道，可以告诉我，万分感谢。</p>

<ul>
<li>  redis 分布式锁的高可用</li>
</ul>

<p>关于 Redis 分布式锁的安全性问题，在分布式系统专家 Martin Kleppmann 和 Redis 的作者 antirez 之间已经发生过一场争论。有兴趣的同学，搜索 &quot; <strong>基于 Redis 的分布式锁到底安全吗</strong> &quot; 就能得到你想要的答案，需要注意的是，有上下两篇（这应该就是传说中的神仙打架吧，哈）。</p>

<ul>
<li>  zookeeper or redis</li>
</ul>

<p>没有绝对的好坏，只有更适合自己的业务。就性能而言，redis 很明显优于 zookeeper；就分布式锁实现的健壮性而言，zookeeper 很明显优于 redis。如何选择，取决于你的业务！</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[面向 Kubernetes 编程： Kubernetes 是下一代操作系统]]></title>
    <link href="http://panlw.github.io/15511559357118.html"/>
    <updated>2019-02-26T12:38:55+08:00</updated>
    <id>http://panlw.github.io/15511559357118.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://raw.githubusercontent.com/answer1991/articles/master/Kubernetes-is-the-next-generation-os.md">https://raw.githubusercontent.com/answer1991/articles/master/Kubernetes-is-the-next-generation-os.md</a></p>
</blockquote>

<h2 id="toc_0">摘要</h2>

<p>此文章适合没有任何 Kubernetes/容器/Docker 经验的同学 — 在不久的将来，你不懂如何操作 Kubernetes 接口，就等于现在的你不懂最普通的 Linux 命令。此文章阅读耗时大概 15 分钟。</p>

<h2 id="toc_1">导言</h2>

<p>此文章着重介绍如何在入门阶段使用 Kubernetes，以及要面向 Kubernetes 编程带来的优势，不会介绍复杂的 Kubernetes 架构、实现。因此此文章适合没有任何 Kubernetes/容器/Docker 经验的同学，对 Kubernetes 有了解的同学也可以从此文章里面获取一些灵感，可以更加酷炫的玩转 Kubernetes。</p>

<p><strong>希望在阅读完此文章之后，你可以从 “我需要一个 Linux VM 做开发、测试和部署”，变成 “我需要一个 Kubernetes 做开发、测试和部署”。</strong></p>

<h2 id="toc_2">Kubernetes 是下一代操作系统</h2>

<p>Kubernetes 是这几年非常热门的一个词汇，大概所有的软件工程师都已经听说过这个词。</p>

<p>那么 Kubernetes 到底是什么呢？可能 Google 会告诉你很多，但是我想告诉你的是：Kubernetes 是下一代操作系统；一个 Kubernetes 集群是一个资源无限大(可扩容)的虚拟机。<strong>而且，Kubernetes 的接口是是声明式的，是天然面向分布式系统而设计的（下面会详细介绍）。</strong></p>

<p>说到这里，大家估计立刻就有疑问了。我想大概是这些：</p>

<hr/>

<p>Q: 那么，Linux、Windows 要被淘汰了？</p>

<p>A: 不会被淘汰，只是 Linux、Windows 是一个底层的单机操作系统。而我们这些普通的应用软件工程师将来都不会跟Linux 打交道了，都会使用 Kubernetes 这个更上层、同时功能也更强大的操作系统。</p>

<hr/>

<p>Q: 那么，我不学 Kubernetes 可以吗？</p>

<p>A: 不行！在未来不久的某一天，也许云厂商只卖 Kubernetes “虚拟机”了：阿里云不单独卖 ecs 了，亚马逊AWS，微软云，Google 云等各种云厂商都不卖 Linux 虚拟机了。如果你想买单机版的 Linux 虚拟机，他们都会一脸惊讶的问你，你买那么底层的、功能那么薄弱的计算机干什么？就像你现在从云厂商那里买不到一个还没有安装 Linux 的虚拟机一样。以后，云厂商交付的 “虚拟机” 必定是 “集群级别的虚拟机” ，而 “集群级别的虚拟机” 的操作系统就是 Kubernetes。</p>

<p><strong>在不久的将来，你不懂如何操作 Kubernetes 接口，就等于现在的你不懂最普通的 Linux 命令。</strong></p>

<hr/>

<p>Q: 那这样的话，我买不到 Linux 虚拟机，我连学习 Linux 的机会都没有了？</p>

<p>A: 当然不是，有了 Kubernetes，你可以在 1秒内自己搞一个任何 Linux 发行版本的 “单机虚拟机” 出来。</p>

<hr/>

<p>Q: Kubernetes 真的是一个操作系统？ Show me....</p>

<p>A:</p>

<table>
<thead>
<tr>
<th>功能/名词</th>
<th>单机 Linux</th>
<th>Kubernetes</th>
<th>说明</th>
</tr>
</thead>

<tbody>
<tr>
<td>Shell, CMD</td>
<td>sh, bash</td>
<td>kubectl</td>
<td>kubectl 是 Kubernetes 的 shell 工具，有了 kubectl 你就可以连接并管理 Kubernetes 这个超级虚拟机了。</td>
</tr>
<tr>
<td>用户，登录 Linux</td>
<td>User, Group, ssh 登录</td>
<td>kubeconfig 文件类似 Linux ssh 的 .key 文件，用户使用 kubeconfig 访问 Kubernetes 就自带了用户信息。Kubernetes 能根据用户限制权限，也能限制用户能使用的资源。kubectl 使用 kubeconfig 访问 Kubernetes 就好比使用 .ssh key 访问 Linux</td>
<td>Kubernetes 集群管理员(或者自动化的申请系统)为用户颁发 kubeconfig 文件。</td>
</tr>
<tr>
<td>进程</td>
<td>进程</td>
<td>Pod</td>
<td>Pod 就是 Kubernetes 这个 “超级虚拟机” 的进程。</td>
</tr>
<tr>
<td>管理进程</td>
<td>ps, kill</td>
<td>kubectl get pod, kubectl delete pod</td>
<td>发布、升级、管理 “进程”(或者说应用)</td>
</tr>
<tr>
<td>配置管理</td>
<td>登录各个 Linux VM，替换机器上的文件。</td>
<td>kubectl apply -f ./cm.yaml</td>
<td>使用 ConfigMap 管理应用的配置文件，一次提交，进程的每个实例自动生效新的配置。由于篇幅管理，使用 ConfigMap 配置应用（“进程”）启动参数不在此文章里面举例。</td>
</tr>
<tr>
<td>发布、管理、升级应用</td>
<td>在 Linux 上面发布一个应用，需要一顿疯狂的操作：先阅读如何发布、参数有什么、下载二进制包、搞定一些配置文件，然后运行应用。</td>
<td>kubectl apply -f ./my-app.yaml</td>
<td>my-app.yaml 可能是应用提供商提供的、面向 Kubernetes 发布应用的“菜单”文件(为什么叫“菜单”我后面会介绍)。只要提交这个“菜单”，应用就部署好了。Kubernetes 让一切简单，而且，它是分布式，是天然容灾的。只要向 Kubernetes 提交 Deployment 这样的“资源”即可，下文有介绍。</td>
</tr>
<tr>
<td>限制应用资源</td>
<td>一顿疯狂的操作，把应用进程的 Cgroup 限制好。</td>
<td>发布应用时已经做了</td>
<td>Kubernetes 让一切简单。</td>
</tr>
<tr>
<td>分布式应用发布</td>
<td>在各个 Linux 虚拟机上面发布好应用，然后把他们组网。</td>
<td>发布应用时已经做了</td>
<td>还是那句话，Kubernetes 让一切简单。</td>
</tr>
<tr>
<td>分布式应用容灾</td>
<td>搞个监控，监控我们各个 Linux 虚拟机上面的应用是不是不健康了。不健康了的话，我们起床，来一次“一顿操作猛如虎”的故障恢复操作。</td>
<td>/</td>
<td>天然容灾，安心睡你的觉。</td>
</tr>
<tr>
<td>数据持久化，故障时数据迁移</td>
<td>“一顿操作猛如虎”</td>
<td>用 PV（持久化存储卷），容灾把应用的一个应用实例从 “节点一” 切换到了 “节点二”，都不用做任何数据迁移。新的应用实例起来就能使用老数据。</td>
<td>还是那句话，Kubernetes 让一切简单。我都不用关心这个事情。（由于篇幅管理，下文的例子中也不会涉及 PV 的例子）</td>
</tr>
</tbody>
</table>

<p><strong>“一顿操作猛如虎” 听起来很酷，但是你在做一些没必要的事情，同时你做了这些事情并不讨好你的老板，可能在因为你的失误操作引起更大的故障和问题。</strong></p>

<p><strong>面向 Kubernetes 做最简单的操作，达到最佳的效果，才是更酷的事情。</strong></p>

<hr/>

<p>A: 行了行了，别说那么多了，我还是需要一个 Linux VM。</p>

<p>Q: 好的，我给您一个 Kubernetes，然后给你一个 基础 OS Pod “菜单”文件，然后您自己就可以创建任何一个 Linux 发行版、任何一个 Linux 版本的的 Linux VM了。在文章的最后会有介绍。</p>

<h1 id="toc_3">小试牛刀</h1>

<p>既然是“小试”，那么我们来尝试一个最简单的应用，一个 HTTP 服务： nignx。同时，我应该部署一个高可用、多副本(例子中为3副本)天然容灾的 nginx。</p>

<p>部署完成的结构图大概如下所示：</p>

<p><img src="https://github.com/answer1991/articles/raw/master/pics/kubernetes-is-the-next-generation-os/pic-1.png" alt="nginx-deploy"/> </p>

<h5 id="toc_4">没有 Kubernetes 之前的部署</h5>

<p>在没有 Kubernetes 之前，我们大概要做这么些操作才能交付这个 nginx 服务：</p>

<ol>
<li>到三个 Linux VM 上面，分别把三个 nginx 进程起好。这里可能还需要关心 nginx 进程怎么起、启动命令是啥、配置怎么配。</li>
<li>到负载均衡管理页面，申请一个负载均衡，把 3个 nignx 进程的 IP 填入。拿回负载均衡的 IP。</li>
<li>到 DNS 管理页面申请一个 DNS 记录，写入把拿到的负载均衡的 IP 写入 A 记录。</li>
<li>把这个 DNS 记录作为这个 nginx 服务的交付成果，交付给用户。</li>
</ol>

<h5 id="toc_5">有了 Kubernetes 的部署</h5>

<p>有了 Kubernetes 之后， 我们只需要写一个 nginx 如何部署的 “菜单”，然后提交这个“菜单”给 Kubernetes，我们就完成了部署。 “菜单” 是一个 yaml 文件(例子中文件名 nginx.yaml)，大概这个样子:</p>

<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app-name: my-nginx
  template:
    metadata:
      labels:
        app-name: my-nginx
    spec:
      containers:
        - name: nginx
          image: nginx
---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  selector:
    app-name: my-nginx
  type: ClusterIP
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 80
</code></pre>

<p>提交“菜单”到 Kubernetes 集群：</p>

<pre><code class="language-bash">$ kubectl apply -f ./nginx.yaml
</code></pre>

<p>访问刚部署的 HTTP 服务:</p>

<p><img src="https://github.com/answer1991/articles/raw/master/pics/kubernetes-is-the-next-generation-os/pic-2.png" alt="visit-nginx"/> </p>

<p>发生了什么？这里大概简单的介绍一下我们的“菜单”：</p>

<ol>
<li>我们向 Kubernetes 提交了一个叫 nginx 的 Deployment。Deployment 是 Kubernetes 里的一种副本保持 “资源声明”，我们在我们的 Deployment 声明了 需要<strong>3</strong>个副本(replica: 3)，副本的内容(template: ... )是用 nginx 镜像启动的Pod(Pod 即 Linux 里的进程，如上章节介绍的)。如果没有玩过 docker 的同学，可以把 nginx 镜像 认为是 nginx 二进制包，只不过它是 docker 镜像方式存在的，不在这里详细展开。我们的 nginx Pod 打上了 my-app=nginx 这样的 Label, Label 可以理解成 分类“标签”，别人(Service)来定位我们 Pod  需要用这样的 “标签” 来匹配。</li>
<li>我们还向 Kubernetes 提交了一个 Service。 提交一个 Service 就是向 Kubernetes 申请一个 负载均衡。Service 依靠 Label (selector: ...) 去找到它的后端真实进程(Pod)。</li>
<li>Service 会根据规则自动生成域名。规则不在这里详细展开介绍。</li>
<li>我们就能用 Service 自动生成的域名作为交付成果，交付给用户了！</li>
</ol>

<h5 id="toc_6">容灾</h5>

<p>Deployment 能自动副本保持，即我们的 nginx Pod 少了一个，Kubernetes 能自动帮我们补齐。</p>

<h5 id="toc_7">变更、发布、升级</h5>

<ol>
<li>如需要调整副本数目，我们只需要修改 Deployment.Spec.Replica 字段，再次 <code>kubectl apply -f ./nginx.yaml</code> 即可，副本调整完成。</li>
<li>如果我们需要升级镜像（nginx 二进制版本），同样的修改 PodSpec.Containers[*].Image 即可，然后 <code>kubectl apply -f ./nginx.yaml</code> </li>
<li>Deployment 支持滚动升级，在升级时可以一个个升级你的 Pod（滚动升级）。当然，我们线上可能有更加复杂的升级策略，蚂蚁金服提供的增强版 Kubernetes 提供比 Deployment 更加实用的升级策略，比如“灰度升级”， “分批升级” 等更加符合生产环境发布策略的升级方案。</li>
</ol>

<h1 id="toc_8">关于“菜单” 和 声明式系统</h1>

<p>Kubernetes 是声明式的系统。关于详细的介绍 “什么是声明式系统”，您可以去 Google 或者内网上面也有许多声明式系统的介绍。</p>

<p>我这边有个简单的比喻：假如你需要一桌菜（至于为什么是“菜”，是因为这个文章是我在做饭的时候构思的）。但是，这个放菜的 “桌子” 不太稳定(或者说有老鼠来偷吃菜品)，一直发生一些事故，就像我们的线上部署应用的环境一样，服务器可能故障。当你需要在这个桌子上面摆上一桌 “菜” 的时候，“菜”可能会坏掉。<strong>别担心，在声明式系统里，“厨师长”是个尽职的好同志，当你提交了一份“菜单”之后，我们的“厨师长”会一直保证你桌子上的菜一直会和你写的“菜单”里的菜一模一样。如果某道“菜”坏了，“厨师长”就帮你再做一份。</strong></p>

<p>在 Kubernetes 里面，有各种各样这样尽职的厨师长（有负责 Deployment 的厨师长，有负责 Service 的厨师长等等）。只要天没塌下来，你提交的“菜单”里的菜都会一直美美的在桌子上“迎客”。</p>

<p>那么，我们回过头来看我说的 “不久的某一天，云厂商只卖 Kubernetes 虚拟机了，而不单纯的卖 Linux VM”。你真的要买几个 Linux VM 自己去启动进程（命令式），然后自己去搭建一套声明式的系统去守护你的落在各个机器上的分布式应用进程？而不使用更高级、更好用的 Kubernetes 操作系统？</p>

<h1 id="toc_9">谈软件交付</h1>

<p>软件交付即把我们开发的一套应用程序部署到其它环境。</p>

<p><strong>软件交付几个重要关注的点：</strong></p>

<ol>
<li>如何快速的部署一套我们的 “全家桶” 应用到客户环境。为什么说“全家桶”，是因为我们不可能只交付一个 nginx 服务，我们肯定会交付一套非常复杂的应用或者中间件系统，比如交付一套 “支付宝系统” 到某个商业银行。</li>
<li>如何在客户现场做自动化容灾，降低驻场和支持成本，或者根本不驻场。</li>
<li>如何简单、可靠地升级后续的版本。</li>
<li>上述的几点，不管是 “部署”，“容灾”，“升级” 都需要关注客户现场的运行环境，我们有没有办法屏蔽这种运行环境差异。比如：我们带过去的可运行软件，在客户的 OS 上是不是能运行；客户现场的负载均衡方案都不一样，如果到了客户现场查看了他们的负载均衡方案之后再写一个负责均衡部署方案肯定大大降低了交付效率。</li>
<li>客户现场的环境不一样，必定带来配置文件不一样。让一个现场交付人员弄懂所有应用的配置参数，是不是一件让他很头疼的事情？</li>
</ol>

<p><strong>软件交付方案的历史：</strong></p>

<ol>
<li>交付 源代码： 这应该是最早的时代，客户现场的环境（操作系统、机型）都不同，需要带着代码到客户现场编译，然后运行软件。</li>
<li>交付 可运行文件：像 Java 提出的 &quot;Build once, run everywhere&quot; 概念，在这个时代，我们可以面向一个运行时虚拟机交付软件。或者我们都面向 Linux 交付，我们的使用 Go 编译的二进制，能顺利的部署到大多数的 Linux OS 上。但是这种方案也强依赖客户现场需要装上指定版本的 Java 虚拟机，或者 Linux 特定的版本(应用依赖 Linux 内核特性)。</li>
<li>交付 镜像：交付镜像，最大可能的屏蔽了底层 OS, Java 虚拟机的差异。在镜像里面，我们把自己需要的 OS 基础 和 Java 虚拟机也装上了。不再依赖客户现场的 OS 和 Java 虚拟机版本了。</li>
</ol>

<p>镜像最大可能的程度上把我们需要的运行时环境和我们的应用可执行文件打在一起，在各种环境下面都能完美地运行。那么，只有镜像就能快乐的交付软件了吗？在我眼里，镜像做的事情完全不够。原因无非也是这么些：</p>

<ol>
<li>怎么配置启动参数，要交付人员辛苦的读懂启动参数配置说明说吗？</li>
<li>怎么做分布式应用的组网、服务发现</li>
<li>怎么做容灾</li>
<li>怎么做部署的元数据录入： 今天我在客户现场把 A 应用部署到了 节点1 上面，我要把这个信息记在哪儿？</li>
</ol>

<p>你可能会告诉我，你说的这些我们的 PaaS 系统都能搞定。是的，你说的没错！但是当 PaaS 能用统一标准管理应用、屏蔽应用的细节，解决应用的组网和服务发现，监听每个应用的实例变化（自动化感知故障发生）然后自动恢复（副本保持），那么它就已经差不多是 Kubernetes 了。把上述的所有功能逻辑都整合在 PaaS，势必导致 PaaS 的臃肿，我们是不是可以面向 Kubernetes 这个 OS 去做一个轻量级的 PaaS？因为很多功能在 Kubernetes 已经有了，而且肯定比我们自己研发的 PaaS 要好用许多。我们的 PaaS 是不是可以向 Kubernetes 提交 Deployment，而不是自己亲自去做进程拉起、副本健康检查、副本保持等功能。</p>

<p><strong>面向 Kubernetes 交付软件</strong></p>

<p>正如我上文所说，可能有一天所有客户现场的 OS 都是 Kubernetes，那我们是不是可以像上文启动 nginx 服务一样，用这种 YAML “声明” 的方式去交付我们的软件？</p>

<p>当然可以！但是，当我们去交付一个 “全家桶” 服务的时候，我们会发现我们的 YAML 写了几千行，甚至上万行了。这个上万行的 YAML 谁来维护？就算是分开给各个子应用的 owner 维护，是不是也可能会发生牵一发而动全身。有没有更加简单的方式？</p>

<p>当然有。我们再回来谈 “菜单” 和做菜。我是一个吃货，但是我很懒。当我点菜的时候，非常希望点一个 “套餐”，而不希望一个个的去点每个菜，更不希望弄懂菜是怎么做出来的。我想要点一个 “满汉全席”(复杂的应用)，可我不想清楚的弄懂套餐里面单独有什么菜、每个菜的配方是什么。一个“满汉全席”(复杂的应用)里面，可能有“山珍”(数据库)和“海味”(Web服务)。我只想告诉我们的“大厨”， 我要 “满汉全席” ，然后我们的 “大厨” 就心领神会的知道 “满汉全席” 里面有什么，然后把 “满汉全席” 给做出来。那么这个 “大厨” 要亲自做这里的每道菜吗？也不必，因为我们的 “大厨” 也可能是个 “懒人”，当需要一桌 “满汉全席” 的时候，他只会告诉负责 “山珍” 的 “大厨” ，要一桌 “山珍”（数据库），然后告诉负责 “海味” 的 “大厨”，要一桌 “海味”（Web服务）。当我们的大厨发现 “山珍“ 和 “海味” 都准备好的时候，他会告我 “满汉全席” 准备好了。</p>

<p>是不是发现和交付软件很像？为什么我们不交付一个 “大厨” 出去？到客户现场负责交付的人员，只要告诉 “大厨” 我们需要一个 “满汉全席” 这种套餐级别的声明就行了。并不是说套餐没有参数，只是套餐的参数能做到对用户屏蔽不必要的细节：比如我们的 “满汉全席” 就只要一个参数，只要让用户填他的 “满汉全席” 需要支持 “多少QPS”。</p>

<h1 id="toc_10">面向 Kubernetes 编程：使用 Operator 交付软件</h1>

<p>上面提到的使用“交付大厨”的方式去交付软件看起来很美好。那么，如何实现呢？也就是我们要怎么培养出属于我们自己的“厨师”。</p>

<p>其实我们在 “小试牛刀” 的章节已经介绍了一位 “厨师” 了： 负责 “Deployment” 的厨师。只是，他的工作比较通用化、没有什么业务含义。它只是负责守护用户在 “菜单” 里面描述的 “进程”(Pod) 数量，至于怎么起 “进程” 都是用户传入的。</p>

<p>那么，我们是不是可以根据这个 “厨师” 的模仿出有业务含义的 “厨师”？比如，业界有一位比较出名的一个 “厨师”，是负责 etcd 集群的。如果我需要一个副本数是3的 etcd 集群，只要向 Kubernetes 提交如下的一个 “菜单”：</p>

<pre><code class="language-yaml">apiVersion: &quot;etcd.database.coreos.com/v1beta2&quot;
kind: &quot;EtcdCluster&quot;
metadata:
  name: &quot;example-etcd-cluster&quot;
spec:
  size: 3
</code></pre>

<p>“etcd厨师长” 就会根据这个 “菜单” 做出一个副本数是3（Spec.Size=3）的 etcd 集群给你。用户不需要知道 3 副本的 etcd 集群里每个副本参数是什么样的。</p>

<p>“etcd厨师长” 真实的名字叫 etcd-operator。顾名思义，operator 就是“厨师长”，“xxx-operator”就是 “xxx应用-厨师长”。在不久的将来，我觉得我们也会有 “xx-db-operator”，“xx-web-operator”，我们也用这种简洁明了的声明方式，快速得到一个 db 实例， 或者一个 “xx-web” 应用。</p>

<p>回到怎么培养厨师长的话题，首先我们来介绍一下名词:</p>

<ol>
<li>CRD (CustomResourceDefinitions)：定义“满汉全席”这样的全家桶。Deployment 是 Kubernetes 官方的资源定义，Kubernetes 同时向开发者提供了 “自定义资源定义”。开发者可以向 Kubernetes 集群提交有 “满汉全席” 的定义，那么当用户提交一桌 “满汉全席” 时，Kubernetes 就明白用户的请求了，也就是说 Kubernetes 知道用户所说的 “满汉全席” 是什么了。</li>
<li>CR (Custom Resources)：一个 CRD 实例，即一桌 “满汉全席”，也就是类似上文一样的 YAML 声明。</li>
<li>Custom Controller： 我们的“厨师长”。当 Controller 发现用户提交了 一桌 “满汉全席”，那么他就开始做菜了。当然它并不是完全亲自做每道菜，正如我上文所说。我们的 “厨师长” 可以依赖另一位 “厨师长”，比如 “db 厨师长” 可以依赖 “Deployment 厨师长”，当用户需要一个 db 实例的时候，“db 厨师长” 只需要负责再向 Kubernetes 提交一个 Deployment 声明即可。注意：“厨师长” 之间的交互也是靠 “CR” 或者 Kubernetes 官方定义的资源（如 Deployment、Pod）。“厨师长” 一般也是通过 Deployment 的方式部署在 Kubernetes 集群内（保证了 “厨师长” 自身的稳定性），除非像 “Deployment 厨师长” 这种 Kubernetes 核心 “厨师长”的稳定性 由 Kubernetes 服务提供商保证。</li>
<li>Operator: Kubernetes 里的 Operator，即 CRD + Custom Controller。</li>
</ol>

<p><strong>一个简单的图将他们串起来：</strong></p>

<p><img src="https://github.com/answer1991/articles/raw/master/pics/kubernetes-is-the-next-generation-os/pic-3.png" alt="my-app-operator"/>  </p>

<p>上图展示了我们的 My-App-Operator 的 “厨师长” 的关系图。当我们需要一个 &quot;my-app&quot; 应用实例时，我们只要告诉我们的 “厨师长” 是需要多少副本数的实例。我们的 “厨师长”  自动将需求转化成 Deployment，其它的功能就完全依靠了 “Deployment 厨师长”。</p>

<h5 id="toc_11">如何面向 Kubernetes 写一个 Operator</h5>

<p>首先我们来看一下 “厨师长” (Operator) 需要关注一些什么事情，以及它做了什么事情：</p>

<ol>
<li>多久要开始做菜（Observe）：即 “厨师长” (Operator/Controller) 需要知道多久他要开始工作了。当用户提交了 “菜单”(CR)，“厨师长” 要开始做菜。或者，因为 “桌子” 上少了一个预期中的 “菜”(Pod 因为故障少了一个)，“厨师长” 也要开始做菜了。</li>
<li>做什么菜（Analyze）： “厨师长” 首先要看看桌子上的菜，再对比一下用户的 “菜单”，得出还缺少什么菜，那么就做什么菜。</li>
<li>开始做菜（Action）：得出做什么菜之后，那么后面的事情就简单了。通知其他 “厨师长” 做菜（提交一个其他的CR），或者自己亲手做个菜（提交一个 Pod）。</li>
</ol>

<p>这3件事情，其实就是 Controller 模式的核心三件事：</p>

<p><img src="https://github.com/answer1991/articles/raw/master/pics/kubernetes-is-the-next-generation-os/pic-4.png" alt="operator-pattern"/> </p>

<p>那么用 Kubernetes 写一个 Operator 需要多久？</p>

<p>可能从 “0” 到可以把 Operator 运行起来只需要 10分钟吧。因为 Kubernetes 的 Kube-Apiserver 提供天然的 watch 接口，我们可以去关注我们在意的资源(我们的 CR，我们的 Pod 等)，这样我们的 “厨师” 就能很自然的得到通知该干活了。然后 “厨师” 就开始做出分析，到最后再向 Kube-Apiserver 提交我们想要的资源（Deployment，或者其它的 CR）。我们都面向 Kube-Apiserver 做编程， 所有的“厨师”都 向 Kube-Apiserver 提交、修改、Watch资源作为统一的交互协议，一切都会变得简单许多。</p>

<p>最后，再加上 Operator 的脚手架帮我们生成基础代码（初始化 Kubernetes 客户端，建立 Watch 等），我们开发者只需要关心的怎么 Analyze 和 Action 即可。 Operator 的脚手架社区常见的有 kube-builder 和 coreos 提供的 operator-framework 等。</p>

<p>我们用伪代码来写一下上文画的 My-App-Operator 核心逻辑 （其它都脚手架做好了，甚至如何 build，Operator 本身它自己如何部署的“菜单” YAML 都是脚手架生成好了）：</p>

<pre><code class="language-go">// Reconcile 即我们 Operator 核心代码逻辑
// Reconcile 何时触发，也是 Operator 生成好了
func Reconcile(crName string) error {
    // 获取 CR (用户提交的“菜单”)
    cr := client.getCR(crName)
    // 计算出这个 CR 期望的 Deployment (用户提交的“菜单”应该在桌子上有什么菜)
    desireDeployment := getDesireDeployment(client, cr)
    // 目前集群里面实际的 Deployment (实际上桌子上有什么菜)
    deployment := client.GetDeployment(crName)
    
    // 如果期望和实际的不太一样，把实际的更新一下就行了。
    if diff(desireDeployment, deployment) {
        return client.UpdateDeployment(desireDeployment);
    }
    
     // 如果期望和实际都一样，什么事情都不做了。
    return nil
}
</code></pre>

<h5 id="toc_12">面向 Kubernetes 变成 和 Operator  的优势总结</h5>

<ol>
<li>统一的信息获取源和统一的接口： Kube-Apiserver 就像是一个大的信息流转中心。所有的组件（“厨师长”）都通过这个中心上传他负责的资源（CR，Deployment，Pod都是 Kubernetes 的资源）的信息，同时，他也通过这个接口，去感知其它资源状态的变化，如果这些变化是他感兴趣的，那么他就开始工作（“厨师长” 开始工作）。</li>
<li>构建在 Kubernetes 核心组件以及 社区通用的 Operator 之上：站在巨人的肩膀上能让我们的工作更加的减负同时达到更加理想的效果。上文中，我们的 Operator 可能在依赖 Deployment 之后，他负责的 “菜”（Pod）就自带副本保持功能。同时，假如我们的应用（DB，Web）要依赖社区的 MySQL 数据库，那么我们的应用 Operator（Web + DB） 可以通过社区的 MySQL-Operator 提供的 CR 快速建出 MySQL 实例，然后再使用 Deployment 建出 Web。</li>
</ol>

<h5 id="toc_13">优秀的社区 Operator</h5>

<ol>
<li>优秀的社区 Operator (awesome-operators): <a href="https://github.com/operator-framework/awesome-operators">https://github.com/operator-framework/awesome-operators</a></li>
</ol>

<h2 id="toc_14">FaaS</h2>

<p>用 Operator 交付软件，目前看起来是最酷的一种交付软件方式。</p>

<p>但是在当今云原生技术快速发展的时代，可能在不久的将来，Operator 模式可能也会被淘汰。因为 Operator 也需要开发者关注一些部署的细节，让开发者真正只关注在自己的业务逻辑，“业务代码” 变成 “服务” 完全对开发者透明，可能需要比 Kubernetes 更上层的框架 - FaaS框架。</p>

<p>FaaS 全称是 <a href="https://en.wikipedia.org/wiki/Function_as_a_service">Function as a service</a> 。用户只要写自己的业务函数，向 Kubernetes 提交业务函数，FaaS 框架将业务函数变成 Deployment，变成 Pod，变成 Service。但是 FaaS 目前还在发展阶段，并不像 Kubernetes 已经变成事实标准，这里不再详细讨论。</p>

<h2 id="toc_15">落地</h2>

<p>说了那么多，其实我的初衷是希望每个开发者都从 Linux VM 转向 Kubernetes &quot;VM&quot;。但是转变发生在每个人身上，应该是有各种困难。我能想到的一些最基本的困难大概列在下面，同时欢迎跟我交流你的一些困惑。</p>

<h5 id="toc_16">代码变成镜像</h5>

<p>大家都知道，Kubernetes 只允许以 Pod 的方式运行“进程”。在 FaaS 没成熟之前，如何把我们的代码变成一个镜像是一个比较头疼的事情。可能应用的开发同学并不想自己去理解 docker，怎么去打镜像。</p>

<p>别担心！Spring 框架或者其扩展的脚手架应该已经可以在工程里自动添加 Dockerfile 文件，使用脚手架之后，用户只要执行 <code>make image</code> 这样的命令，就能构建出镜像了。</p>

<h5 id="toc_17">别说了，我还是想要一个 Linux VM</h5>

<p>向 Kubernetes 提交下面这样的一个 YAML 文件，你就能得到一个 ubuntu VM：</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-vm-1
spec:
  containers:
  - name: vm
    image: ubuntu
</code></pre>

<p>同时，告诉你一个更酷炫的玩法：自己定制一个属于你自己的 Linux 发行版！在原有的 OS 镜像基础上，加上你的 Shell 工具脚本、写一串向爱人表白的话、搞个开机 Logo，都很简单！做一个属于你自己的 Linux 镜像，那么在世界的任何地方，你都能起动一个经过你定制的 Linux VM。</p>

<h5 id="toc_18">哪里去获取一个 Kubernetes</h5>

<p>首先，试试 mini-kube，或者立刻向 阿里云买一个！</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[开始使用 ECC 证书]]></title>
    <link href="http://panlw.github.io/15511555476861.html"/>
    <updated>2019-02-26T12:32:27+08:00</updated>
    <id>http://panlw.github.io/15511555476861.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://imququ.com/post/ecc-certificate.html">https://imququ.com/post/ecc-certificate.html</a></p>
</blockquote>

<p><strong>文章目录</strong></p>

<ul>
<li>  <a href="#toc-0">简单介绍</a></li>
<li>  <a href="#toc-1">如何申请</a></li>
<li>  <a href="#toc-2">如何使用</a></li>
</ul>

<p>提醒：本文最后更新于 912 天前，文中所描述的信息可能已发生改变，请谨慎使用。</p>

<p>我之前的文章多次提到 ECC 证书，但一直没有专门介绍 ECC 证书的文章，今天补上。本文包含三部分内容：1）简单介绍 ECC 证书是什么；2）介绍如何申请 ECC 证书；3）以 Nginx 为例介绍如何使用 ECC 证书。顺便说下，本站已经用上了 ECC 证书。要查看本站主要支持哪些技术特性，<a href="https://imququ.com/post/readme.html">可以点这里</a>。</p>

<h3 id="toc_0">简单介绍</h3>

<p>HTTPS 通过 TLS 层和证书机制提供了内容加密、身份认证和数据完整性三大功能，可以有效防止数据被监听或篡改，还能抵御 MITM（中间人）攻击。TLS 在实施加密过程中，需要用到非对称密钥交换和对称内容加密两大算法。</p>

<p>对称内容加密强度非常高，加解密速度也很快，只是无法安全地生成和保管密钥。在 TLS 协议中，应用数据都是经过对称加密后传输的，传输中所使用的对称密钥，则是在握手阶段通过非对称密钥交换而来。常见的 AES-GCM、ChaCha20-Poly1305，都是对称加密算法。</p>

<p>非对称密钥交换能在不安全的数据通道中，产生只有通信双方才知道的对称加密密钥。目前最常用的密钥交换算法有 RSA 和 ECDHE：RSA 历史悠久，支持度好，但不支持 PFS（Perfect Forward Secrecy）；而 ECDHE 是使用了 ECC（椭圆曲线）的 DH（Diffie-Hellman）算法，计算速度快，支持 PFS。要了解更多 RSA 和 ECDHE 密钥交换的细节，可以阅读 Cloudflare 的<a href="https://blog.cloudflare.com/keyless-ssl-the-nitty-gritty-technical-details/">这篇文章</a>。</p>

<p>只有非对称密钥交换，依然无法抵御 MITM 攻击，还得引入身份认证机制。对于大部分 HTTPS 网站来说，服务端一般通过 HTTP 应用层的帐号体系就能完成客户端身份认证；而浏览器想要验证服务端身份，需要用到服务端提供的证书。</p>

<p>浏览器会在两个步骤中用到证书：1）证书合法性校验。确保证书由合法 CA 签署，且适用于当前网站；2）使用证书提供的非对称加密公钥，完成密钥交换和服务端认证。</p>

<p>证书合法性校验的原理，简单总结如下：</p>

<ul>
<li>  根据版本号、序列号、签名算法标识、发行者名称、有效期、证书主体名、证书主体公钥信息、发行商唯一标识、主体唯一标识、扩展等信息，生成 TBSCertificate（To Be Signed Certificate）信息；</li>
<li>  签发数字签名：使用 HASH 函数对 TBSCertificate 计算得到消息摘要，再用 CA 的私钥进行加密，得到签名；</li>
<li>  校验数字签名：使用相同的 HASH 函数对 TBSCertificate 计算得到消息摘要，与使用 CA 公钥解密签名得到内容相比较；</li>
</ul>

<p>可以看到校验证书需要同时用到签名和非对称加密算法：目前必须使用 SHA-2 做为证书签名函数（没有打 XP SP3 补丁的 IE6 不支持）；目前一般使用 RSA 算法对 TBSCertificate 进行非对称加密。可以通过 openssl 工具来查看证书签名算法：</p>

<pre>**BASH**`$ openssl x509 -in chained.pem -noout -text | grep 'Signature Algorithm'

Signature Algorithm: sha256WithRSAEncryption` </pre>

<p>大部分 CA 都有证书链，浏览器对于收到的多级证书，需要从站点证书开始逐级验证，直至出现操作系统或浏览器内置的受信任 CA 根证书。</p>

<p>浏览器还需要校验当前访问的域名是否存在于证书 TBSCertificate 的 <code>Common Name</code> 或 <code>Subject Alternative Name</code> 字段之中。</p>

<p>在 RSA 密钥交换中，浏览器使用证书提供的 RSA 公钥加密相关信息，如果服务端能解密，意味着服务端拥有证书对应的私钥，同时也能算出对称加密所需密钥。密钥交换和服务端认证合并在一起。</p>

<p>在 ECDHE 密钥交换中，服务端使用证书私钥对相关信息进行签名，如果浏览器能用证书公钥验证签名，就说明服务端确实拥有对应私钥，从而完成了服务端认证。密钥交换和服务端认证是完全分开的。</p>

<p>可用于 ECDHE 数字签名的算法主要有 RSA 和 ECDSA，也就是目前密钥交换 + 签名有三种主流选择：</p>

<ul>
<li>  RSA 密钥交换（无需签名）；</li>
<li>  ECDHE 密钥交换、RSA 签名；</li>
<li>  ECDHE 密钥交换、ECDSA 签名；</li>
</ul>

<p>以下是 Chrome 中这三种密钥交换方式的截图（截图来自于早期 Chrome，新版 Chrome 查看位置有了变化）：</p>

<p><img src="https://st.imququ.com/i/webp/static/uploads/2016/03/key_exchange.png.webp" alt=""/></p>

<p>内置 ECDSA 公钥的证书一般被称之为 ECC 证书，内置 RSA 公钥的证书就是 RSA 证书。由于 256 位 ECC Key 在安全性上等同于 3072 位 RSA Key，加上 ECC 运算速度更快，ECDHE 密钥交换 + ECDSA 数字签名无疑是最好的选择。由于同等安全条件下，ECC 算法所需的 Key 更短，所以 ECC 证书文件体积比 RSA 证书要小一些。以下是本站的对比，可以看到左侧的 ECC 证书要小 1/3：</p>

<p><img src="https://st.imququ.com/i/webp/static/uploads/2016/08/ecc-certificate-vs-rsa-certificate.png.webp" alt=""/></p>

<p>RSA 证书可以用于 RSA 密钥交换（RSA 非对称加密）或 ECDHE 密钥交换（RSA 非对称签名）；而 ECC 证书只能用于 ECDHE 密钥交换（ECDSA 非对称签名）。</p>

<p>并不是所有浏览器都支持 ECDHE 密钥交换，也就是说 ECC 证书的兼容性要差一些。例如在 Windows XP 中，使用 ECC 证书的网站只有 Firefox 能访问（Firefox 的 TLS 自己实现，不依赖操作系统）；Android 平台中，也需要 Android 4+ 才支持 ECC 证书。</p>

<p>好消息是，Nginx 1.11.0 开始提供了对 RSA/ECC 双证书的支持。它的实现原理是：分析在 TLS 握手中双方协商得到的 Cipher Suite，如果支持 ECDSA 就返回 ECC 证书，否则返回 RSA 证书。</p>

<p>也就是说，配合最新的 Nginx，我们可以使用 ECC 证书为现代浏览器提供更好的体验，同时老旧浏览器依然会得到 RSA 证书，从而保证了兼容性。这一次，鱼与熊掌可以兼得。</p>

<h3 id="toc_1">如何申请</h3>

<p>如果你的 CA 支持签发 ECC 证书，使用以下命令生成 CSR（Certificate Signing Request，证书签名请求）文件并提交给提供商，就可以获得 ECC 证书：</p>

<pre><code>openssl ecparam -genkey -name secp256r1 | openssl ec -out ecc.key
openssl req -new -key ecc.key -out ecc.csr

</code></pre>

<p>以上命令中可供选择的算法有 secp256r1 和 secp384r1，secp521r1 已被 Chrome 和 Firefox 废弃。</p>

<p>我目前在用的 Let&#39;s Encrypt，也支持签发 ECC 证书。我使用了 <a href="https://github.com/Neilpang/acme.sh">acme.sh</a> 这个小巧的工具来签发证书，指定 <code>-k ec-256</code> 就可以将证书类型改为 ECC：</p>

<pre><code>&quot;/root/.acme.sh&quot;/acme.sh --issue --dns dns_cx -d imququ.com -d www.imququ.com -k ec-256

</code></pre>

<p>目前 Let&#39;s Encrypt 只提供 RSA 中间证书，官方预计会在 2017 年 3 月底提供 ECC 中间证书（<a href="https://letsencrypt.org/upcoming-features/">via</a>）。</p>

<h3 id="toc_2">如何使用</h3>

<p>有了 RSA/ECC 双证书之后，还需要安装 Nginx 1.11.x。这部分内容我之前详细写过，<a href="https://imququ.com/post/my-nginx-conf.html">请点击查看</a>。</p>

<p>一切准备妥当后，将证书配置改为双份即可：</p>

<pre>**NGINX**`ssl_certificate     example.com.rsa.crt;
ssl_certificate_key example.com.rsa.key;

ssl_certificate     example.com.ecdsa.crt;
ssl_certificate_key example.com.ecdsa.key;` </pre>

<p>问题来了！本站使用 Cloudflare 提供的 Cipher Suites 配置，在 Nginx 中配置了双证书并重启，用 Chrome 测试发现仍然没有采用 ECC 证书。这是为什么呢？</p>

<pre>**NGINX**`# https://github.com/cloudflare/sslconfig/blob/master/conf
ssl_ciphers                 EECDH+CHACHA20:EECDH+CHACHA20-draft:EECDH+AES128:RSA+AES128:EECDH+AES256:RSA+AES256:EECDH+3DES:RSA+3DES:!MD5;
ssl_prefer_server_ciphers   on;` </pre>

<p>研究发现，Chrome 与服务端协商到的 Cipher Suites 是 <code>ECDHE-RSA-AES128-GCM-SHA256</code>，来自于 <code>ssl_ciphers</code> 配置中的 <code>EECDH+AES128</code> 这部分。我们通过 openssl 工具看一下 <code>EECDH+AES128</code> 具体包含哪些 Cipher Suites：</p>

<pre>**BASH**`openssl ciphers -V 'EECDH+AES128' | column -t

0xC0,0x2F  -  ECDHE-RSA-AES128-GCM-SHA256    TLSv1.2  Kx=ECDH  Au=RSA    Enc=AESGCM(128)  Mac=AEAD
0xC0,0x2B  -  ECDHE-ECDSA-AES128-GCM-SHA256  TLSv1.2  Kx=ECDH  Au=ECDSA  Enc=AESGCM(128)  Mac=AEAD
0xC0,0x27  -  ECDHE-RSA-AES128-SHA256        TLSv1.2  Kx=ECDH  Au=RSA    Enc=AES(128)     Mac=SHA256
0xC0,0x23  -  ECDHE-ECDSA-AES128-SHA256      TLSv1.2  Kx=ECDH  Au=ECDSA  Enc=AES(128)     Mac=SHA256
0xC0,0x13  -  ECDHE-RSA-AES128-SHA           SSLv3    Kx=ECDH  Au=RSA    Enc=AES(128)     Mac=SHA1
0xC0,0x09  -  ECDHE-ECDSA-AES128-SHA         SSLv3    Kx=ECDH  Au=ECDSA  Enc=AES(128)     Mac=SHA1` </pre>

<p>可以看到，使用 RSA 做为签名认证算法（Au=RSA）的加密套件排到了前面，导致 Nginx 作出了错误判断。</p>

<p>知道原因就好办了，将这段配置改为 <code>EECDH+ECDSA+AES128:EECDH+aRSA+AES128</code>，再看一下：</p>

<pre>**BASH**`openssl ciphers -V 'EECDH+ECDSA+AES128:EECDH+aRSA+AES128' | column -t

0xC0,0x2B  -  ECDHE-ECDSA-AES128-GCM-SHA256  TLSv1.2  Kx=ECDH  Au=ECDSA  Enc=AESGCM(128)  Mac=AEAD
0xC0,0x23  -  ECDHE-ECDSA-AES128-SHA256      TLSv1.2  Kx=ECDH  Au=ECDSA  Enc=AES(128)     Mac=SHA256
0xC0,0x09  -  ECDHE-ECDSA-AES128-SHA         SSLv3    Kx=ECDH  Au=ECDSA  Enc=AES(128)     Mac=SHA1
0xC0,0x2F  -  ECDHE-RSA-AES128-GCM-SHA256    TLSv1.2  Kx=ECDH  Au=RSA    Enc=AESGCM(128)  Mac=AEAD
0xC0,0x27  -  ECDHE-RSA-AES128-SHA256        TLSv1.2  Kx=ECDH  Au=RSA    Enc=AES(128)     Mac=SHA256
0xC0,0x13  -  ECDHE-RSA-AES128-SHA           SSLv3    Kx=ECDH  Au=RSA    Enc=AES(128)     Mac=SHA1` </pre>

<p>这下就没问题了。</p>

<p>并不是所有加密套件都需要把 ECDSA 和 aRSA 分开写，例如 <code>EECDH+CHACHA20</code> 就不需要，ECDSA 默认就在前面：</p>

<pre>**BASH**`openssl ciphers -V 'EECDH+CHACHA20' | column -t

0xCC,0xA9  -  ECDHE-ECDSA-CHACHA20-POLY1305  TLSv1.2  Kx=ECDH  Au=ECDSA  Enc=ChaCha20-Poly1305  Mac=AEAD
0xCC,0xA8  -  ECDHE-RSA-CHACHA20-POLY1305    TLSv1.2  Kx=ECDH  Au=RSA    Enc=ChaCha20-Poly1305  Mac=AEAD` </pre>

<p>最终，我的 Cipher Suites 配置如下，供参考：</p>

<pre><code>ssl_ciphers                EECDH+CHACHA20:EECDH+CHACHA20-draft:EECDH+ECDSA+AES128:EECDH+aRSA+AES128:RSA+AES128:EECDH+ECDSA+AES256:EECDH+aRSA+AES256:RSA+AES256:EECDH+ECDSA+3DES:EECDH+aRSA+3DES:RSA+3DES:!MD5;

</code></pre>

<p>本文链接：<a href="https://imququ.com/post/ecc-certificate.html" title="Permalink to 开始使用 ECC 证书">https://imququ.com/post/ecc-certificate.html</a>，<a href="https://imququ.com/post/ecc-certificate.html#comments">参与评论 »</a></p>

<p>--<acronym title="End of File">EOF</acronym>--</p>

<p>发表于 2016-08-27 23:10:18 ，并被添加「 <a href="/search.html?s=tag%3ACertificate">Certificate</a> 、 <a href="/search.html?s=tag%3AHTTPS">HTTPS</a> 」标签 。<a href="https://imququ.com/post/ecc-certificate.md">查看本文 Markdown 版本 »</a></p>

<p>本站使用「<a href="http://creativecommons.org/licenses/by/4.0/deed.zh">署名 4.0 国际</a>」创作共享协议，<a href="/post/about.html#toc-1">相关说明 »</a></p>

<p>提醒：本文最后更新于 912 天前，文中所描述的信息可能已发生改变，请谨慎使用。</p>

<h3 id="toc_3">专题「Web 服务器」的其他文章 <a href="/series.html#toc-13" title="更多">»</a></h3>

<ul>
<li>  <a href="/post/enable-tls-1-3.html">本博客开始支持 TLS 1.3</a> (Aug 06, 2017)</li>
<li>  <a href="/post/troubleshooting-https.html">HTTPS 常见部署问题及解决方案</a> (Dec 12, 2016)</li>
<li>  <a href="/post/use-verynginx.html">开始使用 VeryNginx</a> (Dec 10, 2016)</li>
<li>  <a href="/post/moving-to-https-asap.html">为什么我们应该尽快升级到 HTTPS？</a> (May 16, 2016)</li>
<li>  <a href="/post/my-nginx-conf.html">本博客 Nginx 配置之完整篇</a> (Mar 21, 2016)</li>
<li>  <a href="/post/why-can-not-turn-on-ocsp-stapling.html">从无法开启 OCSP Stapling 说起</a> (Mar 13, 2016)</li>
<li>  <a href="/post/certificate-transparency.html">Certificate Transparency 那些事</a> (Feb 03, 2016)</li>
<li>  <a href="/post/letsencrypt-certificate.html">Let&#39;s Encrypt，免费好用的 HTTPS 证书</a> (Dec 25, 2015)</li>
<li>  <a href="/post/why-nginx-disable-gzip-in-http10.html">从 Nginx 默认不压缩 HTTP/1.0 说起</a> (Dec 15, 2015)</li>
<li>  <a href="/post/optimize-tls-handshake.html">TLS 握手优化详解</a> (Nov 08, 2015)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Understanding When to use RabbitMQ or Apache Kafka]]></title>
    <link href="http://panlw.github.io/15510600639169.html"/>
    <updated>2019-02-25T10:01:03+08:00</updated>
    <id>http://panlw.github.io/15510600639169.html</id>
    <content type="html"><![CDATA[
<pre><code>APRIL 26, 2017 PIETER HUMPHREY
</code></pre>

<blockquote>
<p><a href="https://content.pivotal.io/blog/understanding-when-to-use-rabbitmq-or-apache-kafka">https://content.pivotal.io/blog/understanding-when-to-use-rabbitmq-or-apache-kafka</a></p>
</blockquote>

<p>How do humans make decisions? In everyday life, emotion is often the circuit-breaking factor in pulling the trigger on a complex or overwhelming decision.  But for experts making complex decisions that have long term consequences, it can’t be pure impulse.  High performers typically use the circuit breaker of “instinct”, “gut feel” or other emotions only once their expert, unconscious mind has absorbed all the facts required to make a decision.</p>

<p>Today there are dozens of messaging technologies, countless ESBs, and nearly 100 iPaaS vendors in market.   Naturally, this leads to questions about how to choose the right messaging technology for your needs - particularly for those already invested in a particular choice.  Do we switch wholesale?  Just use the right tool for the right job? Have we correctly framed the job at hand for the business need? Given that, what is the right tool for me?  Worse, an exhaustive market analysis might never finish, but due diligence is critical given the average lifespan of integration code.</p>

<p>This post endeavors give the unconscious, expert mind some even handed treatment to consider, starting with the most modern, popular choices today: RabbitMQ and Apache Kafka.  Each has it’s own origin story, design intent, uses cases where it shines, integration capabilities and developer experience. Origins are revealing about the overall design intent for any piece of software, and make good starting point.  However it’s important to note that in this article, my aim is to compare the two around the overlapping use case of message broker, less the “event store / event sourcing” use case, where Kafka excels today.</p>

<h2 id="toc_0">Origins</h2>

<p>RabbitMQ is a “traditional” message broker that implements variety of messaging protocols. It was one of the first open source message brokers to achieve a reasonable level of features, client libraries, dev tools, and quality documentation. RabbitMQ was originally developed to implement AMQP, an open wire protocol for messaging with powerful routing features. While Java has messaging standards like JMS, it’s not helpful for non-Java applications that need distributed messaging which is severely limiting to any integration scenario, microservice or monolithic. With the advent of AMQP, cross-language flexibility became real for open source message brokers.</p>

<p>Apache Kafka is developed in Scala and started out at LinkedIn as a way to connect different internal systems. At the time, LinkedIn was moving to a more distributed architecture and needed to reimagine capabilities like data integration and realtime stream processing, breaking away from previously monolithic approaches to these problems. Kafka is well adopted today within the Apache Software Foundation ecosystem of products and is particularly useful in event-driven architecture. </p>

<h2 id="toc_1">Architecture and Design</h2>

<p>RabbitMQ is designed as a general purpose message broker, employing several variations of point to point, request/reply and pub-sub communication styles patterns.  It uses a smart broker / dumb consumer model, focused on consistent delivery of messages to consumers that consume at a roughly similar pace as the broker keeps track of consumer state.  It is mature, performs well when configured correctly, is well supported (client libraries Java, .NET, node.js, Ruby, PHP and many more languages) and has dozens of plugins available that extend it to more use cases and integration scenarios.</p>

<p>Figure 1 - Simplified overall RabbitMQ architecture. Source: <a href="http://kth.diva-portal.org/smash/get/diva2:813137/FULLTEXT01.pdf">http://kth.diva-portal.org/smash/get/diva2:813137/FULLTEXT01.pdf</a> </p>

<p>Communication in RabbitMQ can be either synchronous or asynchronous as needed.  Publishers send messages to exchanges, and consumers retrieve messages from queues. Decoupling producers from queues via exchanges ensures that producers aren&#39;t burdened with hardcoded routing decisions.  RabbitMQ also offers a number of distributed deployment scenarios (and does require all nodes be able to resolve hostnames). It can be setup for multi-node clusters to cluster federation and does not have dependencies on external services (but some cluster formation plugins can use AWS APIs, DNS, Consul, etcd).  </p>

<p>Apache Kafka is designed for high volume publish-subscribe messages and streams, meant to be durable, fast, and scalable. At its essence, Kafka provides a durable message store, similar to a log, run in a server cluster, that stores streams of records in categories called topics.</p>

<p>Figure 2 - Global Apache Kafka architecture (with 1 topic, 1 partition, replication factor 4). Source: <a href="http://kth.diva-portal.org/smash/get/diva2:813137/FULLTEXT01.pdf">http://kth.diva-portal.org/smash/get/diva2:813137/FULLTEXT01.pdf</a> </p>

<p>Every message consists of a key, a value, and a timestamp.  Nearly the opposite of RabbitMQ, Kafka employs a dumb broker and uses smart consumers to read its buffer.  Kafka does not attempt to track which messages were read by each consumer and only retain unread messages; rather, Kafka retains all messages for a set amount of time, and consumers are responsible to track their location in each log (consumer state). Consequently, with the right developer talent creating the consumer code, Kafka can support a large number of consumers and retain large amounts of data with very little overhead.   As the diagram above shows, Kafka does require external services to run - in this case Apache Zookeeper, which is often regarded as non-trivial to understand, setup and operate.    </p>

<h2 id="toc_2">Requirements and Use Cases</h2>

<p>Many developers begin exploring messaging when they realize they have to connect lots of things together, and other integration patterns such as shared databases are not feasible or too dangerous.</p>

<p>Apache Kafka includes the broker itself, which is actually the best known and the most popular part of it, and has been designed and prominently marketed towards stream processing scenarios. In addition to that, Apache Kafka has recently added Kafka Streams which positions itself as an alternative to streaming platforms such as Apache Spark, Apache Flink, Apache Beam/Google Cloud Data Flow and Spring Cloud Data Flow. The documentation does a good job of discussing popular use cases like Website Activity Tracking, Metrics, Log Aggregation, Stream Processing, Event Sourcing and Commit logs. One of those use cases it describes is messaging, which can generate some confusion.  So let’s unpack that a bit and get some clarity on which messaging scenarios are best for Kafka for, like:</p>

<p>Stream from A to B without complex routing, with maximal throughput (100k/sec+), delivered in partitioned order at least once.<br/>
When your application needs access to stream history, delivered in partitioned order at least once.  Kafka is a durable message store and clients can get a “replay” of the event stream on demand, as opposed to more traditional message brokers where once a message has been delivered, it is removed from the queue.<br/>
Stream Processing<br/>
Event Sourcing<br/>
RabbitMQ is a general purpose messaging solution, often used to allow web servers to respond to requests quickly instead of being forced to perform resource-heavy procedures while the user waits for the result. It’s also good for distributing a message to multiple recipients for consumption or for balancing loads between workers under high load (20k+/sec).  When your requirements extend beyond throughput, RabbitMQ has a lot to offer: features for reliable delivery, routing, federation, HA, security, management tools and other features.  Let’s examine some scenarios best for RabbitMQ, like:</p>

<p>Your application needs to work with any combination of existing protocols like AMQP 0-9-1, STOMP, MQTT, AMQP 1.0.<br/>
You need a finer-grained consistency control/guarantees on a per-message basis (dead letter queues, etc.) However, Kafka has recently added better support for transactions. <br/>
Your application needs variety in point to point, request / reply, and publish/subscribe messaging<br/>
Complex routing to consumers, integrate multiple services/apps with non-trivial routing logic<br/>
RabbitMQ can also effectively address several of Kafka’s strong uses cases above, but with the help of additional software. RabbitMQ is often used with Apache Cassandra when application needs access to stream history, or with the LevelDB plugin for applications that need an “infinite” queue, but neither feature ships with RabbitMQ itself.</p>

<p>For a deeper dive on microservice - specific use cases with Kafka and RabbitMQ, head over to the Pivotal blog and read this short post by Fred Melo.</p>

<h2 id="toc_3">Developer Experience</h2>

<p>RabbitMQ officially supports Java, Spring, .NET, PHP, Python, Ruby, JavaScript, Go, Elixir, Objective-C, Swift - with many other clients and devtools via community plugins. The RabbitMQ client libraries are mature and well documented.</p>

<p>Apache Kafka has made strides in this area, and while it only ships a Java client, there is a growing catalog of community open source clients, ecosystem projects, and well as an adapter SDK allowing you to build your own system integration.  Much of the configuration is done via .properties files or programmatically.</p>

<p>The popularity of these two options has a strong influence on many other software providers who  make sure that RabbitMQ and Kafka work well with or on their technology.</p>

<p>As for developer experience...it&#39;s worth mentioning  the support that we provide in Spring Kafka, Spring Cloud Stream, etc. </p>

<h2 id="toc_4">Security and Operations</h2>

<p>Both are strengths of RabbitMQ.  RabbitMQ management plugin provides an HTTP API, a browser-based UI for management and monitoring, plus CLI tools for operators. External tools like CollectD, Datadog, or New Relic are required for longer term monitoring data storage. RabbitMQ also provides API and tools for monitoring, audit and application troubleshooting.  Besides support for TLS, RabbitMQ ships with RBAC backed by a built-in data store, LDAP or external HTTPS-based providers and supports authentication using x509 certificate instead of username/password pairs. Additional authentication methods can be fairly straightforwardly developed with plugins.</p>

<p>These domains pose a challenge for Apache Kafka.  On the security front, the recent Kafka 0.9 release added TLS, JAAS role based access control and kerberos/plain/scram auth, using a CLI to manage security policy.  This made a substantial improvement on earlier versions where you could only lock down access at the network level, which didn’t work well for sharing or multi-tenancy.</p>

<p>Kafka uses a management CLI comprised of shell scripts, property files and specifically formatted JSON files. Kafka Brokers, Producers and Consumers emit metrics via Yammer/JMX but do not maintain any history, which pragmatically means using a 3rd party monitoring system.  Using these tools, operations is able manage partitions and topics, check consumer offset position, and use the HA and FT capabilities that Apache Zookeeper provides for Kafka. While many view the requirement for Zookeeper with a high degree of skepticism, it does confer clustering benefits for Kafka users.</p>

<p>For example, a 3-node Kafka cluster the system is functional even after 2 failures.  However if you want to support as many failures in Zookeeper you need an additional 5 Zookeeper nodes as Zookeeper is a quorum based system and can only tolerate N/2+1 failures. These obviously should not be co-located with the Kafka nodes - so to stand up a 3 node Kafka system you need ~ 8 servers.  Operators must take the properties of the ZK cluster into account when reasoning about the availability of any Kafka system, both in terms of resource consumption and design.</p>

<h2 id="toc_5">Performance</h2>

<p>Kafka shines here by design: 100k/sec performance is often a key driver for people choosing Apache Kafka.  </p>

<p>Of course, message per second rates are tricky to state and quantify since they depend on so much including your environment and hardware, the nature of your workload, which delivery guarantees are used (e.g. persistent is costly, mirroring even more so), etc.</p>

<p>20K messages per second is easy to push through a single Rabbit queue, indeed rather more than that isn&#39;t hard, with not much demanded in the way of guarantees. The queue is backed by a single Erlang lightweight thread that gets cooperatively scheduled on a pool of native OS threads - so it  becomes a natural choke point or bottleneck as a single queue is never going to do more work than it can get CPU cycles to work in.</p>

<p>Increasing the messages per second often comes down to properly exploiting the parallelism available in one&#39;s environment by doing such things as breaking traffic across multiple queues via clever routing (so that different queues can be running concurrently). When RabbitMQ achieved 1 million message per second , this use case basically came down entirely to doing that judiciously - but was achieved using lot of resources, around 30 RabbitMQ nodes.  Most RabbitMQ users enjoy excellent performance with clusters made up of anywhere from three to seven RabbitMQ nodes.</p>

<h2 id="toc_6">Making the call</h2>

<p>Absorb some research on a few of the other top options on the market. If you want to go deeper with the most popular options, a master’s thesis from Nicolas Nannoni inspired this article and it features a side-by-side comparison table in Section 4.4 (page 39) but is a bit dated at this point. If you feel like plunking down $15.00 USD, this ACM report is also excellent and more recent.</p>

<p>While researching, loop back with the stakeholders and the business as often as possible. Understanding the business use case is the single largest factor in making the right choice for your situation.  Then, if you are pop psychology fan, your best bet is sleep on it, let it percolate, and let your instincts take over.  You got this.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Difference between stream processing and message processing]]></title>
    <link href="http://panlw.github.io/15510591024767.html"/>
    <updated>2019-02-25T09:45:02+08:00</updated>
    <id>http://panlw.github.io/15510591024767.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://stackoverflow.com/questions/41744506/difference-between-stream-processing-and-message-processing">https://stackoverflow.com/questions/41744506/difference-between-stream-processing-and-message-processing</a></p>
</blockquote>

<p>In traditional message processing, you apply simple computations on the messages -- in most cases individually per message.</p>

<p>In stream processing, you apply complex operations on multiple input streams and multiple records (ie, messages) at the same time (like aggregations and joins).</p>

<p>Furthermore, traditional messaging system cannot go &quot;back in time&quot; -- ie, the automatically delete messages after they got delivered to all subscribed consumers. In contrast, Kafka keeps the messages as it uses a pull based model (ie, consumer pull data out of Kafka) for a configurable amount of time. This allows consumers to &quot;rewind&quot; and consume messages multiple times -- or if you add a new consumer, it can read the complete history. This makes stream processing possible, because it allows for more complex applications. Furthermore, stream processing is not necessarily about real-time processing -- it&#39;s about processing infinite input stream (in contrast to batch processing that is applied to finite inputs).</p>

<p>And Kafka offers Kafka Connect and Streams API -- so it is a stream processing platform and not just a messaging/pub-sub system (even if it uses this in it&#39;s core).</p>

<h1 id="toc_0">What are the differences between Apache Kafka and RabbitMQ?</h1>

<pre><code>2017/5/30
</code></pre>

<blockquote>
<p><a href="https://www.quora.com/What-are-the-differences-between-Apache-Kafka-and-RabbitMQ">https://www.quora.com/What-are-the-differences-between-Apache-Kafka-and-RabbitMQ</a></p>
</blockquote>

<p>Kafka is a general purpose message broker, like RabbItMQ, with similar distributed deployment goals, but with very different assumptions on message model semantics. I would be skeptical of the &quot;AMQP is more mature&quot; argument and look at the facts of how either solution solves your problem.</p>

<p>TL;DR,</p>

<p>a) Use Kafka if you have a fire hose of events (20k+/sec per producer) you need delivered in partitioned order &#39;at least once&#39; with a mix of online and batch consumers, but most importantly <strong>_you’re OK with your consumers managing the state of your “cursor” on the Kafka topic._</strong></p>

<p>Kafka’s <strong>main superpower is that it is less like a</strong> <strong>_queue system_</strong> <strong>and more like a</strong> <strong>_circular buffer_</strong>that scales as much as your disk on your cluster, and thus allows you to be able to re-read messages.</p>

<p>b) Use Rabbit if you have messages (20k+/sec per queue) that need to be routed in complex ways to consumers, you want per-message delivery guarantees, you need one or more features of protocols like AMQP 0.9.1, 1.0, MQTT, or STOMP, and <strong>_you want the broker to manage that state of which consumer has been delivered which message_</strong>.</p>

<p>RabbitMQ’s main superpowers are that it’s a <strong>_scalable, high performance queue system_</strong> with <strong>well-defined consistency rules</strong>, and <strong>ability to create interesting exchange toplogies</strong>.</p>

<p><strong>Neither offers &quot;filter/processing&quot; capabilities</strong> - if you need that, consider using a <u>data flow or stream processing framework</u> - there are many: <strong>Apache Beam (which is an abstraction on top of Google Dataflow, Flink, Spark, or Apex), Storm, NiFi,</strong> direct use of <strong>Apex, Flink, or Spark</strong> or <strong>Spring Cloud Data Flow</strong> on top of one of these solutions to add computation, filtering, querying, on your streams. You may also want to use something like <strong>Apache</strong> <strong>Cassandra</strong> or <strong>Geode</strong> or <strong>Ignite</strong> as your queryable stream cache.</p>

<p>Kafka traditionally hasn’t offered transactional semantics in its writes, though this is changing in 0.11.</p>

<p>Pivotal has recently published a <a href="https://content.pivotal.io/blog/understanding-when-to-use-rabbitmq-or-apache-kafka">reasonably fair post on when to use RabbitMQ or Kafka</a>, which I provided some input into. Pivotal is the owner of RabbitMQ but is also a fan of using the right tool for the job, and encouraging open source innovation … and thus is a fan of Kafka!</p>

<p>Details:</p>

<p>Firstly, on RabbitMQ vs. Kafka. They are both excellent solutions, RabbitMQ being more mature, but both have very different design philosophies. Fundamentally, I&#39;d say <strong>RabbitMQ is broker-centric</strong>, focused around delivery guarantees between producers and consumers, with transient preferred over durable messages. Whereas Kafka is <strong>producer-centric</strong>, based around partitioning a fire hose of event data into durable message brokers with cursors, supporting batch consumers that may be offline, or online consumers that want messages at low latency.</p>

<p>RabbitMQ uses the broker itself to maintain state of what&#39;s consumed (via message acknowledgements) - it uses Erlang&#39;s Mnesia to maintain delivery state around the broker cluster. Kafka doesn&#39;t have message acknowledgements, it assumes the consumer tracks of what&#39;s been consumed so far. Kafka brokers use Zookeeper to reliably maintain their state across a cluster.</p>

<p>RabbitMQ presumes that consumers are mostly online, and any messages &quot;in wait&quot; (persistent or not) are held opaquely (i.e. no cursor). Kafka was based from the beginning around both online and batch consumers, and also has producer message batching - it&#39;s designed for holding and distributing large volumes of messages.</p>

<p>RabbitMQ provides rich routing capabilities with AMQP 0.9.1&#39;s exchange, binding and queuing model. Kafka has a very simple routing approach - in AMQP parlance it uses topic exchanges only.</p>

<p>Both solutions run as distributed clusters, but RabbitMQ&#39;s philosophy is to make the cluster transparent, as if it were a virtual broker. Kafka makes partitions explicit, by forcing the producer to know it is partitioning a topic&#39;s messages across several nodes., this has the benefit of <strong>preserving ordered delivery</strong> within a partition.</p>

<p>RabbitMQ <strong>ensures queued messages are stored in published order</strong> even in the face of requeues or channel closure. One can setup a similar topology &amp; order delivery to Kafka using the <a href="https://github.com/rabbitmq/rabbitmq-consistent-hash-exchange">consistent hash</a> exchange or <a href="https://github.com/rabbitmq/rabbitmq-sharding">sharding plugin</a>., or even more interesting topologies.</p>

<p>Put another way, Kafka <strong>presumes that producers generate a massive stream of events on their own timetable</strong> - there&#39;s no room for throttling producers because consumers are slow, since the data is too massive. The whole job of Kafka is to provide the&quot;shock absorber&quot; between the flood of events and those who want to consume them in their own way -- some online, others offline - only batch consuming on an hourly or even daily basis.</p>

<p>Performance-wise, <strong>both are excellent performers</strong>, but have major architectural differences. RabbitMQ has demonstrated setups of over a <a href="https://content.pivotal.io/blog/rabbitmq-hits-one-million-messages-per-second-on-google-compute-engine">million messages/sec</a>, Kafka has demonstrated setups of <a href="http://events.linuxfoundation.org/sites/events/files/slides/Kafka%20At%20Scale.pdf">several million messages/sec</a>. The primary architectural difference is that RabbitMQ handles its messages largely in-memory and thus uses a large cluster in these benchmarks (30+ nodes), whereas Kafka proudly leverages the powers of sequential disk I/O and requires less hardware (<a href="https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines">this benchmark</a> uses 3x 6 core / 32 GB RAM nodes).</p>

<p>This older paper indicates Kafka handled 500,000 messages published per second and 22,000 messages consumed per second on a 2-node cluster with 6-disk RAID 10.<br/>
<a href="http://research.microsoft.com/en-us/um/people/srikanth/netdb11/netdb11papers/netdb11-final12.pdf">http://research.microsoft.com/en...</a></p>

<p>Now, a word on AMQP. Frankly, it seems <strong>the standard was a mess but has stabilized</strong>. Officially there is a 1.0 specification standardized by OASIS . In practice it is a forked standard, with 0.9.1 being broadly deployed in production, and a smaller number of users of 1.0.</p>

<p>AMQP has lost some of its sheen and momentum, but it has already succeeded in its goal of helping to break the hold TIBCO had on high performance, low latency messaging through 2007 or so. Now there are many options.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Netty 的资源泄露探测机制]]></title>
    <link href="http://panlw.github.io/15503695043576.html"/>
    <updated>2019-02-17T10:11:44+08:00</updated>
    <id>http://panlw.github.io/15503695043576.html</id>
    <content type="html"><![CDATA[
<pre><code>2017/11/11
</code></pre>

<blockquote>
<p>原文地址 <a href="https://ylgrgyq.github.io/2017/11/11/netty-resource-leack-detector/">https://ylgrgyq.github.io/2017/11/11/netty-resource-leack-detector/</a></p>
</blockquote>

<p>这里资源泄露主要是指某个对象占用有某些资源，比如连接、内存等，在这个对象被 GC 之前，必须主动执行一个方法，如 close、release 之类的，将其占用的资源释放出来，该对象才能被安全的 GC。否则就会出现资源泄露，比如连接没有关闭，内存没有释放。随着服务的运行，泄露的资源由于无法被释放，整个服务占用的资源就会越来越多，最终让服务无法分配新资源，导致服务异常。</p>

<p>Netty 中有个 ResourceLeakDetector，能对占用资源的对象进行监控，如果对象被 GC 之前没有主动释放资源，则 ResourceLeakDetector 会发现这个泄露，并会以打印日志的方式告知给开发者。ResourceLeakDetector 可以保护任何一个可能出现泄露的资源，不过在 Netty 中 ResourceLeakDetector 最主要的使用场所还是去保护、记录 Netty 使用的各种 ByteBuf。无论是 Pooled 还是 Unpooled，无论是 Direct 还是 Heap，所有的 ByteBuf 都要被 ResourceLeakDetector 记录起来，从而在开发者出现忘记为 ByteBuf 调用 release 的时候，通过日志告知开发者有泄露，要求开发者来排查问题。如果出现泄露，就可能会出现比如 Pooled ByteBuf 对象没有放入 Pool 中就被 GC，或者 Direct ByteBuf 没有执行释放内存的方法就被 GC 的情况。因为 Netty 大量使用 ByteBuf，如果 ByteBuf 出现泄露，则服务很容易出现 OOM。</p>

<h2 id="toc_0">老版本的 ResourceLeakDetector 的使用</h2>

<p>在大概 2016 年 12 月的时候，Netty 对 ResourceLeakDetector 做了改动，修复了一个隐藏很久的 Bug。对于这个稍后再说，新老版本在使用上差别不大，所以我们还是先看老版本的 ResourceLeakDetector，先看看它是怎么使用的，之后再说说这个 Bug 是怎么回事，怎么被修复的。</p>

<p>首先，你需要有个待保护的 Resource，这个 Resource 可以有各种功能，但肯定要有个 close 或者 release 方法，需要在不再使用这个 Resource 的时候记得执行一下，释放资源。忘记释放就会造成资源泄露。</p>

<pre><code class="language-java">interface Resource {
  boolean close();
}
</code></pre>

<p>为了提醒我们要记得释放资源，我们用 ResourceLeakDetector 协助我们做检查。比如我们有个资源 DefaultResource:</p>

<pre><code class="language-java">class DefaultResource implements Resource {
  static final ResourceLeakDetector&lt;Resource&gt; leakDetector = new ResourceLeakDetector&lt;Resource&gt;(Resource.class);
  public boolean close() {
    // .... 有一堆释放资源的工作
  }
}
</code></pre>

<p>我们需要在申请出资源之后，将资源交给 DefaultResource 内的 leakDetector 做登记：</p>

<pre><code class="language-java">// 比如我们就这么拿到一个资源
DefaultResource rsc = ResourceFactory.get();
// 交给 leakDetector 做登记
ResourceLeak leak = DefaultResource.leakDetector.open(rsc)
</code></pre>

<p>相当于每申请一个资源后，就将资源交给 leakDetector 做登记，并且在释放资源的时候，不但要执行 rsc.close() 清理资源，还要执行 leak.close() 将 leakDetector 登记的记录销毁，所以最好是不直接使用 DefaultResource 而是对其用 Decorator 包装一下，从而将这两步 close 操作封装为一步操作，用户使用 Decorator 去执行 close 时就将 leak 和资源都 close，从而完成资源清理和 leakDetector 的注销。</p>

<pre><code class="language-java">class LeakAwareResource implements Resource {
  Resource rsc;
  ResourceLeak leak;
  LeakAwareResource(Resource rsc, ResourceLeak leak){
    this.rsc = rsc;
    this.leak = leak
  }
  @Override
  public boolean close() {
    boolean closed = this.rsc.close()
    if (closed){
      // 需要 rsc 释放完了之后再 close leak
      this.leak.close();
    }
    return closed;
  }
}
</code></pre>

<p>用户使用的时候直接使用 LeakAwareResource 即可，释放资源时执行它的 close 方法，将资源释放，也将 leakDetector 登记的记录清理。忘记执行 LeakAwareResource 的 close 的话，该对象在被 GC 的时候，leakDetector 会有一定概率能发现这个泄露并打印日志。一定概率是因为 ResourceLeakDetector 记录资源泄露是有开销的，所以是抽样的做记录，不是所有泄露都能被抓住，但基本上如果有泄露使用久了总能被发现。</p>

<h2 id="toc_1">老版本的 ResourceLeakDetector 实现</h2>

<p>ResourceLeakDetector 使用虽然是有一些额外工作需要做，但总体来看不算太麻烦，再看看其实现方法。上面 <code>DefaultResource.leakDetector.open(rsc)</code> 返回的 ResourceLeak 是一个 PhantomReference，它指向被检查的资源 rsc，也就是一个 DefaultResource 对象。如下图：</p>

<p><img src="media/15503695043576/15503753321677.jpg" alt=""/></p>

<p>PhantomReference 不影响 DefaultResource 的 GC，DefaultResource 被 GC 后，指向它的 PhantomReference (即 ResourceLeak) 会被塞到一个指定队列里，消费这个队列就能拿到 PhantomReference 。如果按照上面 LeakAwareResource 的实现，DefaultResource 总是和 PhantomReference 一起创建，并且 DefaultResource close 时也会执行与其绑定的 PhantomReference 的 close。就能在这个 PhantomReference (ResourceLeak) 内记录有没有执行过 close 这个事情，等 DefaultResource 被 GC 后，从队列中依次读取所有 PhantomReference，并判断里面 close 标识是否置位，发现没置位的 close 就说明该 PhantomReference 曾经指向的 DefaultResource 没有执行 close 就被 GC 了，就存在泄漏，需要打日志报警。</p>

<p>从 PhantomReference 拿不到 DefaultResource 状态，但是在 DefaultResource 被 GC 后，队列内取到 PhantomReference 内部状态是全的。为了在 DefaultResource 出现泄露时候报警打日志，可以在 PhantomReference 里记录所有操作 DefaultResource 的 Stack，从而在有泄露的时候将这些 Stack 打印出来，以追踪问题。</p>

<p>打印堆栈开销很大，并且这个 Leak Detector 是即使服务工作正常，最好也开着以捕获意外的泄露，所以上面提到了 Netty 内部以采样的方式为 ByteBuf 设置 Detector，并提供了很多 Detection 级别，默认是 SIMPLE，还有 ADVANCE，PARANOID，级别越高采样比率越大，记录 Stack 的开销也越大。调整方法是调整 <code>io.netty.leakDetection.level</code> 这个参数，或者老版本 Netty 是这个 <code>-Dio.netty.leakDetectionLevel</code> 。这个参数一般没有泄露不会调整。</p>

<p>每次操作 Resource 留下的 Stack 记录叫做 record，除了采样比率之外，Netty 还能调整 Record 数量。比如一个 ByteBuf 操作次数比较多，默认的最大 record 数量 4 个不够，就需要调整 record 数，从而排查问题。调整的参数是 <code>-Dio.netty.leakDetection.maxRecords</code> 。这个参数比 detection level 更少调整，只有在发现有泄露，并且提示说 record 数量不足，不足以定位问题的时候才需要调整，正常情况下完全不用动。</p>

<h3 id="toc_2">保证 PhantomReference 不会提前被 GC</h3>

<p>上面提到，DefaultReference 被 GC 后，Phantom Reference 队列能取到指向它的 PhantomReference，并且 PhantomReference 如果内部有其它字段、状态的话此时都是能读取到的。但这个保证是需要有前提的，前提就是有 GC Root 能指向这个 PhantomReference，不能出现 DefaultReference 还未被 GC 或者 DefaultResource 被 GC 还未来得及处理 Phantom Reference 队列时，PhantomReference 自己就因为没有 GC Root 指向而被 GC 掉。</p>

<p>所以，ReferenceDetector 内是有个 static 的链表，每次执行 <code>DefaultResource.leakDetector.open(rsc)</code> 构造出新的 PhantomReference 的时候，会将这个 Reference 塞到这个 static 的链表当中，只有调用 leak.close() 时，才会将 PhantomReference 从链表上摘下，才能被 GC。从而保证了如果出现泄露忘记执行 LeakAwareResource 的 close，则 PhantomReference 不会在其指向的 DefaultResource 被 GC 前 GC。从而实现 ReferenceDetector 的功能。</p>

<h2 id="toc_3">老版本 ResourceLeakDetector 的 Bug</h2>

<p>这个 Bug 藏的非常隐晦，所以很多很多年都没有被发现。Bug 的现象是即使记得调用了 LeakAwareResource 的 close，释放了 Resource，但 Netty 的 ReferenceDetector 还是会错误报出发现内存泄露。也就是说 DefaultReference 执行了 close，但在 <code>leak.close()</code> 执行之前，DefaultResource 就被 GC 了，且指向它的 PhantomReference 被从队列中拿了出来，发现该 PhantomReference (其实是 ResourceLeak) 内的 closed 标识未置位，从而错误报出有内存泄露。</p>

<p>来看看之前 LeakAwareResource 的实现。LeakAwareResource 是 Netty 内 SimpleLeakAwareByteBuf 的简化版本，和 Netty 的实现逻辑是一样的。</p>

<pre><code class="language-java">class LeakAwareResource implements Resource {
  // ..... 和前面一样
  @Override
  public boolean close() {
    boolean closed = this.rsc.close()
    if (closed){
      // 需要 rsc 释放完了之后再 close leak
      this.leak.close();
    }
    return closed;
  }
}
</code></pre>

<p>看上去只要 <code>this.rsc.close()</code> 成功，则一定会执行 this.leak.close()。但是由于 JIT/GC 的存在，在 <code>this.rsc.close()</code> 执行后，JVM 会推算出代码不再需要 this.rsc 所指的对象了，所以在执行 <code>this.leak.close()</code> 之前就将 <code>this.rsc</code> 指向的对象 DefaultResource 直接 GC 掉，此时如果有另外的线程在执行处理 PhantomReference 队列的逻辑，就会从队列中拿到指向刚被 GC 掉的 DefaultResource 的 PhantomReference。此时 <code>this.leak.close()</code> 还未执行，所以会报出内存泄露，而实际上 <code>this.leak.close()</code>终究是会被执行的。并且 DefaultReference 也已经成功执行过 <code>close()</code> 方法。</p>

<p>这种 Bug 感觉就是传说中的 Heisenbug，如果你手工调试，或者在执行 <code>this.leak.close()</code>之前检查 this.rsc 的状态，你一定无法发现这个 Bug，因为会影响 JIT/GC 的工作。你的探测会导致程序行为变化，使 Bug 隐藏起来，不探测的时候 Bug 又会出现。想眼睁睁的看着 Bug 一步一步的复现是不可能的。这个 Bug 修复之后 Netty 也增加了测试，测试也只是调整线程数，调整 ReferenceDetector 探测级别，让一堆线程不断的重复构造 Resource 又释放，看看是否出现误报。</p>

<h2 id="toc_4">新版本的 ResourceLeakDetector</h2>

<p>该怎么修上面那个 Bug 呢？修复就是要保证 <code>this.rsc.close()</code> 执行完，还要骗过 JVM 让它误认为你还需要 <code>this.rsc</code> 指向的对象，不要将其 GC 掉。需要再说明一下，我这里所说的老版本代码基于 v4.0.28，新版本代码基于 v4.1.9。在 v4.1.9 后，ResourceLeakDetector 又做了很多性能上的优化，只是这里为了不引入太多东西看着复杂，所以不再说后来优化的事情了，在最下边参考里有列出性能优化的 PR，有兴趣的可以看看。</p>

<p>老版本 ResourceLeakDetector 内 ResourceLeak 的 close 实现大致如下：</p>

<pre><code class="language-java">public boolean close() {
  if (markFreed()) {
    synchronized (link) {
      removeThisReferenceFromLink(link)
    }
    return true;
  }
  return false
}
</code></pre>

<p>即先标记 ResourceLeak 为 free，再将其从 ResourceLeakDetector 内 static 的链表上移除。移除后 ResourceLeak 这个 PhantomReference 就能被安全 GC 了。</p>

<p>新版本 ResourceLeakDetector 的实现大致是：</p>

<pre><code class="language-java">public boolean close() {
  allLeaks.remove(this, LeakEntry.INSTANCE);
}
public boolean close(T trackedObject) {
  close() &amp;&amp; trackedObject != null;
}
</code></pre>

<p>一方面 ResourceLeakDetector 内存放 PhantomReference 的不再是链表，而是一个 ConcurrentHashMap。并发时清理 PhantomReference 效率有所提升。</p>

<p>再一个就是对使用者来说，都要使用带参数的 close，即需要将被保护的 Resource 传入 close。LeakAwareResource 需要改为：</p>

<pre><code class="language-java">class LeakAwareResource implements Resource {
  // .... 与之前一致
  @Override
  public boolean close() {
    boolean closed = this.rsc.close()
    if (closed){
      // 只改了这里
      this.leak.close(this.rsc);
    }
    return closed;
  }
}
</code></pre>

<p>从而保证在执行 <code>boolean closed = this.rsc.close()</code> 后，<code>this.rsc</code> 不能被清理，因为 <code>this.leak.close(this.rsc)</code> 还要用到 <code>this.rsc</code>。并且在 ResourceLeak 内， 执行完 <code>allLeaks.remove(this, LeakEntry.INSTANCE);</code> 后因为还要检查 <code>trackedObject != null</code> ，所以在 ResourceLeak 从 ResourceLeakDetector 的 static 的 ConcurrentHashMap 移除之前，被保护的 Resource 也就是 trackedObject 不能被 GC 掉。</p>

<p>是不是有点感觉不可靠？因为这个修复最关键的点就是 <code>close() &amp;&amp; trackedObject != null;</code> 即 <code>close()</code> 之后通过再检查一下 trackedObject 是不是 null 来保证 trackedObject 不会在 <code>close()</code> 执行之前被 GC。有没有可能 JVM 一开始就判断 trackedObject != null 并将结果记录下来，再去执行 close() 从而又会出现 close() 执行之前就能判断出 trackedObject 再也用不上了，从而再次出现之前的 Bug。</p>

<p>个人理解是因为 <code>close() &amp;&amp; trackedObject != null;</code> 这里用了 &amp;&amp;，JVM 不确定 &amp;&amp; 后面的计算到底有多复杂，按照短路执行原则，应该先执行 <code>close()</code> 从而根据 <code>close()</code> 执行结果判断要不要再执行 <code>trackedObject != null</code>。因为有短路执行机制存在，有可能执行完 <code>close()</code> 不用再执行 <code>trackedObject != null</code>，省一次操作，所以 JVM 会先执行 <code>close()</code> 再执行 <code>trackedObject != null</code>。</p>

<p>但是 <code>trackedObject != null</code> 毕竟是个非常简单的语句，不知道未来 JVM 有没有可能发现这个语句很简单而直接先执行了，好处是能将 trackedObject 提前 GC 掉。</p>

<h2 id="toc_5">参考</h2>

<ul>
<li>修复 ResourceLeakDetector 问题的 PR：
<a href="https://github.com/netty/netty/pull/6087">Fix false-positives when using ResourceLeakDetector. by normanmaurer · Pull Request #6087 · netty/netty · GitHub</a></li>
<li>对 ResourceLeakDetector 性能优化的 PR，虽然本文没有说跟它相关的事情，但是确实挺值得看看的:
<a href="https://github.com/netty/netty/pull/7217">Reduce performance overhead of ResourceLeakDetector by normanmaurer · Pull Request #7217 · netty/netty · GitHub</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Project Loom: Fibers and Continuations for the Java Virtual Machine]]></title>
    <link href="http://panlw.github.io/15496160495808.html"/>
    <updated>2019-02-08T16:54:09+08:00</updated>
    <id>http://panlw.github.io/15496160495808.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://cr.openjdk.java.net/%7Erpressler/loom/Loom-Proposal.html">https://cr.openjdk.java.net/~rpressler/loom/Loom-Proposal.html</a></p>

<p>参考: <a href="https://qconsf.com/sf2017/system/files/presentation-slides/servlet_vs_reactive_choosing_the_right_stack_qcon_sf.pdf">https://qconsf.com/sf2017/system/files/presentation-slides/servlet_vs_reactive_choosing_the_right_stack_qcon_sf.pdf</a></p>
</blockquote>

<h2 id="toc_0"><a></a>Overview</h2>

<p>Project Loom&#39;s mission is to make it easier to write, debug, profile and maintain concurrent applications meeting today&#39;s requirements. Threads, provided by Java from its first day, are a natural and convenient concurrency construct (putting aside the separate question of communication among threads) which is being supplanted by less convenient abstractions because their current implementation as OS kernel threads is insufficient for meeting modern demands, and wasteful in computing resources that are particularly valuable in the cloud. Project Loom will introduce fibers as lightweight, efficient threads managed by the Java Virtual Machine, that let developers use the same simple abstraction but with better performance and lower footprint. We want to make concurrency simple(r) again! A fiber is made of two components — a continuation and a scheduler. As Java already has an excellent scheduler in the form of <code>ForkJoinPool</code>, fibers will be implemented by adding continuations to the JVM.</p>

<h2 id="toc_1"><a></a>Motivation</h2>

<p>Many applications written for the Java Virtual Machine are concurrent — meaning, programs like servers and databases, that are required to serve many requests, occurring concurrently and competing for computational resources. Project Loom is intended to significantly reduce the difficulty of writing efficient concurrent applications, or, more precisely, to eliminate the tradeoff between simplicity and efficiency in writing concurrent programs.</p>

<p>One of Java&#39;s most important contributions when it was first released, over twenty years ago, was the easy access to threads and synchronization primitives. Java threads (either used directly, or indirectly through, for example, Java servlets processing HTTP requests) provided a relatively simple abstraction for writing concurrent applications. These days, however, one of the main difficulties in writing concurrent programs that meet today&#39;s requirements is that the software unit of concurrency offered by the runtime — the thread — cannot match the scale of the domain&#39;s unit of concurrency, be it a user, a transaction or even a single operation. Even if the unit of application concurrency is coarse — say, a session, represented by single socket connection — a server can handle upward of a million concurrent open sockets, yet the Java runtime, which uses the operating system&#39;s threads for its implementation of Java threads, cannot efficiently handle more than a few thousand. A mismatch in several orders of magnitude has a big impact.</p>

<p>Programmers are forced to choose between modeling a unit of domain concurrency directly as a thread and lose considerably in the scale of concurrency a single server can support, or use other constructs to implement concurrency on a finer-grained level than threads (tasks), and support concurrency by writing <u>asynchronous</u> code that does not block the thread running it.</p>

<p>Recent years have seen the introduction of many asynchronous APIs to the Java ecosystem, from asynchronous NIO in the JDK, asynchronous servlets, and many asynchronous third-party libraries. Those APIs were created not because they are easier to write and to understand, for they are actually harder; not because they are easier to debug or profile — they are harder (they don&#39;t even produce meaningful stacktraces); not because they compose better than synchronous APIs — they compose less elegantly; not because they fit better with the rest of the language or integrate well with existing code — they are a much worse fit, but just because the implementation of the software unit of concurrency in Java — the thread — is insufficient from a footprint and performance perspective. This is a sad case of a good and natural abstraction being abandoned in favor of a less natural one, which is overall worse in many respects, merely because of the runtime performance characteristics of the abstraction.</p>

<p>While there are some advantages to using kernel threads as the implementation of Java threads — most notably because all native code is supported by kernel threads, and so Java code running in a thread can call native APIs — the disadvantages mentioned above are too great to ignore, and result either in hard-to-write, expensive-to-maintain code, or in a significant waste of computing resources, that is especially costly when code runs in the cloud. Indeed, some languages and language runtimes successfully provide a lightweight thread implementation, most famous are Erlang and Go, and the feature is both very useful and popular.</p>

<p>The main goal of this project is to add a lightweight thread construct, which we call fibers, managed by the Java runtime, which would be optionally used alongside the existing heavyweight, OS-provided, implementation of threads. Fibers are much more lightweight than kernel threads in terms of memory footprint, and the overhead of task-switching among them is close to zero. Millions of fibers can be spawned in a single JVM instance, and programmers need not hesitate to issue synchronous, blocking calls, as blocking will be virtually free. In addition to making concurrent applications simpler and/or more scalable, this will make life easier for library authors, as there will no longer be a need to provide both synchronous and asynchronous APIs for a different simplicity/performance tradeoff. Simplicity will come with no tradeoff.</p>

<p>As we will see, a thread is not an atomic construct, but a composition of two concerns — a scheduler and a <u>continuation</u>. It is our current intention to separate the two concerns, and implement Java fibers on top of those two building blocks, and, although fibers are the main motivation for this project, to also add continuations as a user facing abstraction, as continuations have other uses, too (e.g. <a href="https://wiki.python.org/moin/Generators">Python&#39;s generators</a>).</p>

<h2 id="toc_2"><a></a>Goals and Scope</h2>

<p>Fibers can provide a low-level primitive upon which interesting programming paradigms can be implemented, like channels, actors and dataflow, but while those uses will be taken into account, it is <u>not</u> the goal of this project to design any of those higher level constructs, nor to suggest new programming styles or recommended patterns for the exchange information among fibers (e.g. shared memory vs. message passing). As the issue of limiting memory access for threads is the subject of other OpenJDK projects, and as this issue applies to any implementation of the thread abstraction, be it heavyweight or lightweight, this project will probably intersect with others.</p>

<p>It <u>is</u> the goal of this project to add a lightweight thread construct — fibers — to the Java platform. What user-facing form this construct may take will be discussed below. The goal is to allow <u>most</u> Java code (meaning, code in Java class files, not necessarily written in the Java programming language) to run inside fibers unmodified, or with minimal modifications. It is <u>not</u> a requirement of this project to allow native code called from Java code to run in fibers, although this <u>may</u> be possible in some circumstances. It is also <u>not</u> the goal of this project to ensure that <u>every</u> piece of code would enjoy performance benefits when run in fibers; in fact, some code that is less appropriate for lightweight threads may suffer in performance when run in fibers.</p>

<p>It <u>is</u> the goal of this project to add a public <u>delimited continuation</u> (or <u>coroutine</u>) construct to the Java platform. However, this goal is secondary to fibers (which require continuations, as explained later, but those continuations need not necessarily be exposed as a public API).</p>

<p>It <u>is</u> the goal of this project to experiment with various <u>schedulers</u> for fibers, but it is <u>not</u> the intention of this project to conduct any serious research in scheduler design, largely because we think that <code>ForkJoinPool</code> can serve as a very good fiber scheduler.</p>

<p>As adding the ability to manipulate call stacks to the JVM will undoubtedly be required, it is <u>also</u> the goal of this project to add an even lighter-weight construct that will allow unwinding the stack to some point and then invoke a method with given arguments (basically, a generalization of efficient tail-calls). We will call that feature <u>unwind-and-invoke</u>, or UAI. It is <u>not</u> the goal of this project to add an automatic tail-call optimization to the JVM.</p>

<p>This project will likely involve different components of the Java platform, with features believed to be divided thus:</p>

<ul>
<li>  Continuations and UAI will be done in the JVM and exposed as very thin Java APIs.</li>
<li>  Fibers will be mostly implemented in Java in the JDK libraries, but may require some support in the JVM.</li>
<li>  JDK libraries making use of native code that blocks threads would need to be adapted to be able to run in fibers. In particular this implies changing the <code>java.io</code> classes.</li>
<li>  JDK libraries that make use of low-level thread synchronization (and in particular the <code>LockSupport</code> class), such as <code>java.util.concurrent</code> will need to be adapted to support fibers, but the amount of work required will depend on the fiber API, and in any event, expected to be small (as fibers expose a very similar API to threads).</li>
<li>  Debuggers, profilers and other serviceability services would need to be aware of fibers to provide a good user experience. This means that JFR and JVMTI would need to accommodate fibers, and relevant platform MBeans may be added.</li>
<li>  At this point we do not foresee a need for a change in the Java language.</li>
</ul>

<p>It is early days for this project, and so everything — including its scope — is subject to change.</p>

<h2 id="toc_3"><a></a>Terminology</h2>

<p>As kernel threads and lightweight threads are just different implementations of the same abstraction, some confusion over terminology is bound to ensue. This document will adopt the following convention, and every correspondence in the project should follow suit:</p>

<ul>
<li>  The word <u>thread</u> will refer to the abstraction only (which will be explored shortly) and never to a particular implementation, so <u>thread</u> may refer either to any implementation of the abstraction, whether done by the OS or by the runtime.</li>
<li>  When a particular implementation is referred, the terms <u>heavyweight thread</u>, <u>kernel threads</u> and <u>OS thread</u> can be used interchangeable to mean the implementation of thread provided by the operating system kernel. The terms <u>lightweight thread</u>, <u>user-mode thread</u>, and <u>fiber</u> can be used interchangeably to mean an implementation of threads provided by the language runtime — the JVM and JDK libraries in the case of the Java platform. Those words <u>do not</u> (at least in these early stages, when the API design is unclear) refer to specific Java classes.</li>
<li>  The capitalized words <code>Thread</code> and <code>Fiber</code> would refer to particular Java classes, and will be used mostly when discussing the design of the API rather than of the implementation.</li>
</ul>

<h2 id="toc_4"><a></a>What Threads Are</h2>

<p>A <u>thread</u> is a sequence of computer instructions executed sequentially. As we are dealing with operations that may involve not just calculations but also IO, timed pauses, and synchronization — in general, instructions that cause the stream of computation to wait for some event external to it — a thread, then, has the ability to <u>suspend</u> itself, and to <u>automatically resume</u> when the event it waits for occurs. While a thread waits, it should vacate the CPU core, and allow another to run.</p>

<p>These capabilities are provided by two different concerns. A <u>continuation</u> is a sequence of instructions that execute sequentially, and may suspend itself (a more thorough treatment of continuations is given later, in the section <a href="#header-n135">Continuations</a>). A <u>scheduler</u> assigns continuations to CPU cores, replacing a paused one with another that&#39;s ready to run, and ensuring that a continuation that is ready to resume will eventually be assigned to a CPU core. A thread, then, requires two constructs: a continuation and a scheduler, although the two may not necessarily be separately exposed as APIs.</p>

<p>Again, threads — at least in this context — are a fundamental abstraction, and do not imply any programming paradigm. In particular, they refer only to the abstraction allowing programmers to write sequences of code that can run and pause, and not to any mechanism of sharing information among threads, such as shared memory or passing messages.</p>

<p>As there are two separate concerns, we can pick different implementations for each. Currently, the thread construct offered by the Java platform is the <code>Thread</code> class, which is implemented by a kernel thread; it relies on the OS for the implementation of both the continuation and the scheduler.</p>

<p>A continuation construct exposed by the Java platform can be combined with existing Java schedulers — such as <code>ForkJoinPool</code>, <code>ThreadPoolExecutor</code> or third-party ones — or with ones especially optimized for this purpose, to implement fibers.</p>

<p>It is also possible to split the implementation of these two building-blocks of threads between the runtime and the OS. For example, modifications to the Linux kernel done at Google (<a href="https://www.youtube.com/watch?v=KXuZi9aeGTw">video</a>, <a href="http://www.linuxplumbersconf.org/2013/ocw/system/presentations/1653/original/LPC%20-%20User%20Threading.pdf">slides</a>), allow user-mode code to take over scheduling kernel threads, thus essentially relying on the OS just for the implementation of continuations, while having libraries handle the scheduling. This has the benefits offered by user-mode scheduling while still allowing native code to run on this thread implementation, but it still suffers from the drawbacks of relatively high footprint and not resizable stacks, and isn&#39;t available yet. Splitting the implementation the other way — scheduling by the OS and continuations by the runtime — seems to have no benefit at all, as it combines the worst of both worlds.</p>

<p>But why would user-mode threads be in any way better than kernel threads, and why do they deserve the appealing designation of <u>lightweight</u>? It is, again, convenient to separately consider both components, the continuation and the scheduler.</p>

<p>In order to suspend a computation, a continuation is required to store an entire call-stack context, or simply put, store the stack. To support native languages, the memory storing the stack must be contiguous and remain at the same memory address. While virtual memory does offer some flexibility, there are still limitations on just how lightweight and flexible such kernel continuations (i.e. stacks) can be. Ideally, we would like stacks to grow and shrink depending on usage. As a language runtime implementation of threads is not required to support arbitrary native code, we can gain more flexibility over how to store continuations, which allows us to reduce footprint.</p>

<p>The much bigger problem with the OS implementation of threads is the scheduler. For one, the OS scheduler runs in kernel mode, and so every time a thread blocks and control returned to the scheduler, a non-cheap user/kernel switch must occur. For another, OS schedulers are designed to be general-purpose and schedule many different kinds of program threads. But a thread running a video encoder behaves very differently from one serving requests coming over the network, and the same scheduling algorithm will not be optimal for both. Threads handling transactions on servers tend to present certain behavior patterns that present a challenge to a general-purpose OS scheduler. For example, it is a common pattern for a transaction-serving thread <code>A</code> to perform some action on the request, and then pass data on to another thread, <code>B</code>, for further processing. This requires some synchronization of a handoff between the two threads that can involve either a lock or a message queue, but the pattern is the same: <code>A</code> operates on some data <code>x</code>, hands it over to <code>B</code>, wakes <code>B</code> up and then blocks until it is handed another request from the network or another thread. This pattern is so common that we can assume that <code>A</code> will block shortly after unblocking <code>B</code>, and so scheduling <code>B</code> on the same core as <code>A</code> will be beneficial, as <code>x</code> is already in the core&#39;s cache; in addition, adding <code>B</code> to a core-local queue doesn&#39;t require any costly contended synchronization. Indeed, a work-stealing scheduler like <code>ForkJoinPool</code> makes this precise assumption, as it adds tasks scheduled by running task into a local queue. The OS kernel, however, cannot make such an assumption. As far as it knows, thread <code>A</code> may want to continue running for a long while after waking up <code>B</code>, and so it would schedule the recently unblocked <code>B</code> to a different core, thus both requiring some synchronization, and causing a cache-fault as soon as <code>B</code> accesses <code>x</code>.</p>

<h2 id="toc_5"><a></a>Fibers</h2>

<p>Fibers are, then, what we call Java&#39;s planned user-mode threads. This section will list the requirements of fibers and explore some design questions and options. It is not meant to be exhaustive, but merely present an outline of the design space and provide a sense of the challenges involved.</p>

<p>In terms of basic capabilities, fibers must run an arbitrary piece of Java code, concurrently with other threads (lightweight or heavyweight), and allow the user to await their termination, namely, join them. Obviously, there must be mechanisms for suspending and resuming fibers, similar to <code>LockSupport</code>&#39;s <code>park</code>/<code>unpark</code>. We would also want to obtain a fiber&#39;s stack trace for monitoring/debugging as well as its state (suspended/running) etc.. In short, because a fiber is a thread, it will have a very similar API to that of heavyweight threads, represented by the <code>Thread</code> class. With respect to the Java memory model, fibers will behave exactly like the current implementation of <code>Thread</code>. While fibers will be implemented using JVM-managed continuations, we may also want to make them compatible with OS continuations, like Google&#39;s user-scheduled kernel threads.</p>

<p>There are a few capabilities unique to fibers: we want a fiber to be scheduled by a pluggable scheduler (either fixed at the fiber&#39;s construction, or changeable when it is paused, e.g. with an <code>unpark</code> method that takes a scheduler as a parameter), and we&#39;d like fibers to be serializable (discussed in a separate section).</p>

<p>In general, the fiber API will be nearly identical to that of <code>Thread</code> as the abstraction is the same, and we&#39;d also like to run code that so far has run in kernel threads to run in fibers with little or no modification. This immediately suggests two design options:</p>

<ol>
<li> Represent fibers as a <code>Fiber</code> class, and factor out the common API for <code>Fiber</code> and <code>Thread</code> into a common super-type, provisionally called <code>Strand</code>. Thread-implementation-agnostic code would be programmed against <code>Strand</code>, so that <code>Strand.currentStrand</code> would return a fiber if the code is running in a fiber, and <code>Strand.sleep</code> would suspend the fiber if the code is running in a fiber.</li>
<li> Use the same <code>Thread</code> class for both kinds of threads — user-mode and kernel-mode — and choose an implementation as a dynamic property set in a constructor or a setter called prior to invoking <code>start</code>.</li>
</ol>

<p>A separate <code>Fiber</code> class might allow us more flexibility to deviate from <code>Thread</code>, but would also present some challenges. Because a user-mode scheduler does not have direct access to CPU cores, assigning a fiber to a core is done by running it in some worker kernel thread, and so every fiber has an underlying kernel thread, at least while it is scheduled to a CPU core, although the identity of underlying kernel thread is not fixed, and may change if the scheduler decides to schedule the same fiber to a different worker kernel thread. If the scheduler is written in Java — as we want — every fiber even has an underlying <code>Thread</code> instance. If fibers are represented by the <code>Fiber</code> class, the underlying <code>Thread</code> instance would be accessible to code running in a fiber (e.g. with <code>Thread.currentThread</code> or <code>Thread.sleep</code>), which seems inadvisable.</p>

<p>If fibers are represented by the same <code>Thread</code> class, a fiber&#39;s underlying kernel thread would be inaccessible to user code, which seems reasonable but has a number of implications. For one, it would require more work in the JVM, which makes heavy use of the <code>Thread</code> class, and would need to be aware of a possible fiber implementation. For another, it would limit our design flexibility. It also creates some circularity when writing schedulers, that need to <u>implement</u> threads (fibers) by assigning them to threads (kernel threads). This means that we would need to expose the fiber&#39;s (represented by <code>Thread</code>) continuation for use by the scheduler.</p>

<p>Because fibers are scheduled by Java schedulers, they need not be GC roots, as at any given time a fiber is either runnable, in which case a reference to it is held by its scheduler, or blocked, in which case a reference to it is held by the object on which it is blocked (e.g. a lock or an IO queue), so that it can be unblocked.</p>

<p>Another relatively major design decision concerns thread locals. Currently, thread-local data is represented by the (<code>Inheritable</code>)<code>ThreadLocal</code> class(es). How do we treat thread-locals in fibers? Crucially, <code>ThreadLocal</code>s have two very different uses. One is associating data with a thread context. Fibers will probably need this capability, too. Another is to reduce contention in concurrent data structures with striping. That use abuses <code>ThreadLocal</code> as an approximation of a processor-local (more precisely, a CPU-core-local) construct. With fibers, the two different uses would need to be clearly separated, as now a thread-local over possibly millions of threads (fibers) is not a good approximation of processor-local data at all. This requirement for a more explicit treatment of thread-as-context vs. thread-as-an-approximation-of-processor is not limited to the actual <code>ThreadLocal</code> class, but to any class that maps <code>Thread</code> instances to data for the purpose of striping. If fibers are represented by <code>Thread</code>s, then some changes would need to be made to such striped data structures. In any event, it is expected that the addition of fibers would necessitate adding an explicit API for accessing processor identity, whether precisely or approximately.</p>

<p>An important feature of kernel threads is timeslice-based preemption (which will be called forceful, or forced preemption here, for brevity). A kernel thread that computes for a while without blocking on IO or synchronization will be forcefully-preempted after some time. While at first glance this seems to be an important design and implementation issue for fibers — and, indeed, we may decide to support it; JVM safepoints should make it easy — not only is it not important, but having this feature doesn&#39;t make much of a difference at all (so it is best to forgo it). The reason is as follows: unlike kernel threads, the number of fibers may be very large (hundreds of thousands or even millions). If <u>many</u> fibers require so much CPU time that they need to <u>often</u> be forcefully preempted then as the number of threads exceeds the number of cores by several orders of magnitude, the application is under-provisioned by orders of magnitude, and no scheduling policy will help. If <u>many</u> fibers need to run long computations <u>infrequently</u>, then a good scheduler will work around this by assigning fibers to available cores (i.e. worker kernel threads). If a <u>few</u> fibers need to run long computations <u>frequently</u>, then it is better to run that code in heavyweight threads; while different thread implementations provide the same abstraction, there are times where one implementation is better than the other, and it is not necessary for our fibers to be preferable to kernel threads in every circumstance.</p>

<p>A real implementation challenge, however, may be how to reconcile fibers with internal JVM code that blocks kernel threads. Examples include hidden code, like loading classes from disk to user-facing functionality, such as <code>synchronized</code> and <code>Object.wait</code>. As the fiber scheduler multiplexes many fibers onto a small set of worker kernel threads, blocking a kernel thread may take out of commission a significant portion of the scheduler&#39;s available resources, and should therefore be avoided.</p>

<p>On one extreme, each of these cases will need to be made fiber-friendly, i.e., block only the fiber rather than the underlying kernel thread if triggered by a fiber; on the other extreme, all cases may continue to block the underlying kernel thread. In between, we may make some constructs fiber-blocking while leaving others kernel-thread-blocking. There is good reason to believe that many of these cases can be left unchanged, i.e. kernel-thread-blocking. For example, class loading occurs frequently only during startup and only very infrequently afterwards, and, as explained above, the fiber scheduler can easily schedule around such blocking. Many uses of <u>synchronized</u> only protect memory access and block for extremely short durations — so short that the issue can be ignored altogether. We may even decide to leave <u>synchronized</u> unchanged, and encourage those who surround IO access with <u>synchronized</u> and block frequently in this way, to change their code to make use of the <code>j.u.c</code> constructs (which will be fiber-friendly) if they want to run the code in fibers. Similarly, for the use of <code>Object.wait</code>, which isn&#39;t common in modern code, anyway (or so we believe at this point), which uses <code>j.u.c</code>.</p>

<p>In any event, a fiber that blocks its underlying kernel thread will trigger some system event that can be monitored with JFR/MBeans.</p>

<p>While fibers encourage the use of ordinary, simple and natural synchronous blocking code, it is very easy to adapt existing asynchronous APIs, turning them into fiber-blocking ones. Suppose that a library exposes this asynchronous API for some long-running operation, <code>foo</code>, which returns a <code>String</code>:</p>

<pre><code class="language-java">interface AsyncFoo {
   public void asyncFoo(FooCompletion callback);
}
</code></pre>

<p>where the callback, or completion handler <code>FooCompletion</code> is defined like so:</p>

<pre><code class="language-java">interface FooCompletion {
  void success(String result);
  void failure(FooException exception);
}
</code></pre>

<p>We will provide an async-to-fiber-blocking construct that may look something like this:</p>

<pre><code class="language-java">abstract class _AsyncToBlocking&lt;T, E extends Throwable&gt; {
    private _Fiber f;
    private T result;
    private E exception;
  
    protected void _complete(T result) {
        this.result = result;
        unpark f
    }
  
    protected void _fail(E exception) { 
        this.exception = exception;
        unpark f
    }
  
    public T run() throws E { 
        this.f = current fiber
        register();
        park
        if (exception != null)
           throw exception;
        return result;
    }
  
    public T run(_timeout) throws E, TimeoutException { ... }
  
    abstract void register();
}
</code></pre>

<p>We can then create a blocking version of the API by first defining the following class:</p>

<pre><code class="language-java">abstract class AsyncFooToBlocking extends _AsyncToBlocking&lt;String, FooException&gt; 
     implements FooCompletion {
  @Override
  public void success(String result) {
    _complete(result);
  }
  @Override
  public void failure(FooException exception) {
    _fail(exception);
  }
}
</code></pre>

<p>which we then use to wrap the asynchronous API with as synchronous version:</p>

<pre><code class="language-java">class SyncFoo {
    AsyncFoo foo = get instance;
  
    String syncFoo() throws FooException {
        new AsyncFooToBlocking() {
          @Override protected void register() { foo.asyncFoo(this); }
        }.run();
    }
}
</code></pre>

<p>We can include such ready integrations for common asynchronous classes, such as <code>CompletableFuture</code>.</p>

<h2 id="toc_6"><a></a>Continuations</h2>

<p>The motivation for adding continuations to the Java platform is for the implementation of fibers, but continuations have some other interesting uses, and so it is a secondary goal of this project to provide continuations as a public API. The utility of those other uses is, however, expected to be much lower than that of fibers. In fact, continuations don&#39;t add expressivity on top of that of fibers (i.e., continuations can be implemented on top of fibers).</p>

<p>In this document and everywhere in Project Loom, the word <u>continuation</u> will mean a <u>delimited continuation</u> (also sometimes called a <u>coroutine</u><sup><a href="#dfref-footnote-1">1</a></sup>). Here we will think of delimited continuations as sequential code that may suspend (itself) and resume (be resumed by a caller). Some may be more familiar with the point of view that sees continuations as objects (usually subroutines) representing &quot;the rest&quot; or &quot;the future&quot; of a computation. The two describe the very same thing: a suspended continuation, is an object that, when resumed or &quot;invoked&quot;, carries out the rest of some computation.</p>

<p>A delimited continuation is a sequential sub-program with an entry point (like a thread), which we&#39;ll call simply the <u>entry point</u> (in Scheme, this is the <u>reset point</u>), which may suspend or yield execution at some point, which we&#39;ll call the <u>suspension point</u> or the <u>yield point</u> (the <u>shift</u> point in Scheme). When a delimited continuation suspends, control is passed outside of the continuation, and when it is resumed, control returns to the last yield point, with the execution context up to the entry point intact. There are many ways to present delimited continuations, but to Java programmers, the following rough pseudocode would explain it best:</p>

<pre><code class="language-java">foo() { // (2)
  ... 
  bar()
  ...
}
​
bar() {
  ...
  suspend // (3)
  ... // (5)
}
​
main() {
  c = continuation(foo) // (0)
  c.continue() // (1)
  c.continue() // (4)
}
</code></pre>

<p>A continuation is created (0), whose entry point is <code>foo</code>; it is then invoked (1) which passes control to the entry point of the continuation (2), which then executes until the next suspension point (3) inside the <code>bar</code> subroutine, at which point the invocation (1) returns. When the continuation is invoked again (4), control returns to the line following the yield point (5).</p>

<p>The continuations discussed here are &quot;stackful&quot;, as the continuation may block at any nested depth of the call stack (in our example, inside the function <code>bar</code> which is called by <code>foo</code>, which is the entry point). In contrast, stackless continuations may only suspend in the same subroutine as the entry point. Also, the continuations discussed here are non-reentrant, meaning that any invocation of the continuation may change the &quot;current&quot; suspension point. In other words, the continuation object is stateful.</p>

<p>The main technical mission in implementing continuations — and indeed, of this entire project — is adding to HotSpot the ability to capture, store and resume callstacks not as part of kernel threads. JNI stack frames will likely <u>not</u> be supported.</p>

<p>As continuations serve as the basis for fibers, if continuations are exposed as a public API, we will need to support nested continuations, meaning code running inside a continuation must be able to suspend not only the continuation itself, but an enclosing one (e.g., suspend the enclosing fiber). For example, a common use for continuations is in the implementation of generators. A generator exposes an iterator, and the code running inside the generator produces another value for the iterator every time it yields. It should therefore be possible to write code like this:</p>

<pre><code class="language-java">new _Fiber(() -&gt; {
  for (Object x : new _Generator(() -&gt; {
      produce 1
      fiber sleep 100ms
      produce 2
      fiber sleep 100ms
      produce 3
  })) {
      System.out.println(&quot;Next: &quot; + x);
  }
})
</code></pre>

<p>In the literature, nested continuations that allow such behavior are sometimes call &quot;delimited continuations with multiple named prompts&quot;, but we&#39;ll call them <u>scoped continuations</u>. See <a href="http://blog.paralleluniverse.co/2015/08/07/scoped-continuations/">this blog post</a> for a discussion of the theoretical expressivity of scoped continuations (to those interested, continuations are a &quot;general effect&quot;, and can be used to implement any effect — e.g. assignment — even in a pure language that has no other side-effect; this is why, in some sense, continuations are the fundamental abstraction of imperative programming).</p>

<p>Code running inside a continuation is not expected to have a reference to the continuation, and the scopes normally have some fixed names (so suspending scope <code>A</code> would suspend the innermost enclosing continuation of scope <code>A</code>). However, the yield point provides a mechanism to pass information from the code to the continuation instance and back. When a continuation suspends, no <code>try/finally</code> blocks enclosing the yield point are triggered (i.e., code running in a continuation cannot detect that it is in the process of suspending).</p>

<p>As one of the reasons for implementing continuations as an independent construct of fibers (whether or not they are exposed as a public API) is a clear separation of concerns. Continuations, therefore, are not thread-safe and none of their operations creates cross-thread happens-before relations. Establishing the memory visibility guarantees necessary for migrating continuations from one kernel thread to another is the responsibility of the fiber implementation.</p>

<p>A rough outline of a possible API is presented below. Continuations are a very low-level primitive that will only be used by library authors to build higher-level constructs (just as <code>java.util.Stream</code> implementations leverage <code>Spliterator</code>). It is expected that classes making use of contiuations will have a private instance of the continuation class, or even, more likely, of a subclass of it, and that the continuation instance will not be directly exposed to consumers of the construct.</p>

<pre><code class="language-java">class _Continuation {
    public _Continuation(_Scope scope, Runnable target) 
    public boolean run()
    public static _Continuation suspend(_Scope scope, Consumer&lt;_Continuation&gt; ccc)
    
    public ? getStackTrace()
}
</code></pre>

<p>The <code>run</code> method returns <code>true</code> when the continuation terminates, and false if it suspends. The <code>suspend</code> method allows passing information from the yield point to the continuation (using the <code>ccc</code> callback that can inject information into the continuation instance it is given), and back from the continuation to the suspension point (using the return value, which is the continuation instance itself, from which information can be queried).</p>

<p>To demonstrate how easily fibers can be implemented in terms of continuations, here is a partial, simplistic implementation of a <code>_Fiber</code> class representing a fiber. As you&#39;ll note, most of the code maintains the fiber&#39;s state, to ensure it doesn&#39;t get scheduled more than once concurrently:</p>

<pre><code class="language-java">class _Fiber {
    private final _Continuation cont;
    private final Executor scheduler;
    private volatile State state;
    private final Runnable task;
​
    private enum State { NEW, LEASED, RUNNABLE, PAUSED, DONE; }
  
    public _Fiber(Runnable target, Executor scheduler) {
        this.scheduler = scheduler;
        this.cont = new _Continuation(_FIBER_SCOPE, target);
      
        this.state = State.NEW;
        this.task = () -&gt; {
              while (!cont.run()) {
                  if (park0())
                     return; // parking; otherwise, had lease -- continue
              }
              state = State.DONE;
        };
    }
  
    public void start() {
        if (!casState(State.NEW, State.RUNNABLE))
            throw new IllegalStateException();
        scheduler.execute(task);
    }
  
    public static void park() {
        _Continuation.suspend(_FIBER_SCOPE, null);
    }
  
    private boolean park0() {
        State st, nst;
        do {
            st = state;
            switch (st) {
              case LEASED:   nst = State.RUNNABLE; break;
              case RUNNABLE: nst = State.PAUSED;   break;
              default:       throw new IllegalStateException();
            }
        } while (!casState(st, nst));
        return nst == State.PAUSED;
    }
  
    public void unpark() {
        State st, nst;
        do {
            State st = state;
            switch (st) {
              case LEASED: 
              case RUNNABLE: nst = State.LEASED;   break;
              case PAUSED:   nst = State.RUNNABLE; break;
              default:       throw new IllegalStateException();
            }
        } while (!casState(st, nst));
        if (nst == State.RUNNABLE)
            scheduler.execute(task);
    }
  
    private boolean casState(State oldState, State newState) { ... }  
}
</code></pre>

<h2 id="toc_7"><a></a>Schedulers</h2>

<p>As mentioned above, work-stealing schedulers like <code>ForkJoinPools</code> are particularly well-suited to scheduling threads that tend to block often and communicate over IO or with other threads. Fibers, however, will have pluggable schedulers, and users will be able to write their own ones (the SPI for a scheduler can be as simple as that of <code>Executor</code>). Based on prior experience, it is expected that <code>ForkJoinPool</code> in asynchronous mode can serve as an excellent default fiber scheduler for most uses, but we may want to explore one or two simpler designs, as well, such as a pinned-scheduler, that always schedules a given fiber to a specific kernel thread (which is assumed to be pinned to a processor).</p>

<h2 id="toc_8"><a></a>Unwind-and-Invoke</h2>

<p>Unlike continuations, the contents of the unwound stack frames is not preserved, and there is no need in any object reifying this construct.</p>

<p>TBD</p>

<h2 id="toc_9"><a></a>Additional Challenges</h2>

<p>While the main motivation for this goal is to make concurrency easier/more scalable, a thread implemented by the Java runtime and over which the runtime has more control, has other benefits. For example, such a thread could be paused and serialized on one machine and then deserialized and resumed on another. This is useful in distributed systems where code could benefit from being relocated closer to the data it accesses, or in a cloud platform offering <a href="https://en.wikipedia.org/wiki/Function_as_a_Service">function-as-a-service</a>, where the machine instance running user code could be terminated while that code awaits some external event, and later resumed on another instance, possibly on a different physical machine, thus making better use of available resources and reducing costs for both host and client. A fiber would then have methods like <code>parkAndSerialize</code>, and <code>deserializeAndUnpark</code>.</p>

<p>As we want fibers to be serializable, continuations should be serializable as well. If they are serializable, we might as well make them cloneable, as the ability to clone continuations actually adds expressivity (as it allows going back to a previous suspension point). It is, however, a very serious challenge to make continuation cloning useful enough for such uses, as Java code stores a lot of information off-stack, and to be useful, cloning would need to be &quot;deep&quot; in some customizable way.</p>

<h2 id="toc_10"><a></a>Other Approaches</h2>

<p>An alternative solution to that of fibers to concurrency&#39;s simplicity vs. performance issue is known as async/await, and has been adopted by C# and Node.js, and will likely be adopted by standard JavaScript. Continuations and fibers dominate async/await in the sense that async/await is easily implemented with continuations (in fact, it can be implemented with a weak form of delimited continuations known as stackless continuations, that don&#39;t capture an entire call-stack but only the local context of a single subroutine), but not vice-versa.</p>

<p>While implementing async/await is easier than full-blown continuations and fibers, that solution falls far too short of addressing the problem. While async/await makes code simpler and gives it the appearance of normal, sequential code, like asynchronous code it still requires significant changes to existing code, explicit support in libraries, and does not interoperate well with synchronous code. In other words, it does not solve what&#39;s known as the <a href="http://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/">&quot;colored function&quot; problem</a>.</p>

<hr/>

<p>1 Whether we&#39;ll call it continuation or coroutine going forward is TBD — there is a difference in meaning, but the nomenclature does not seem to be fully standardized, and continuation seems to be used as the more general term.<a>↩</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What are the differences between Bazel and Gradle?]]></title>
    <link href="http://panlw.github.io/15491638602626.html"/>
    <updated>2019-02-03T11:17:40+08:00</updated>
    <id>http://panlw.github.io/15491638602626.html</id>
    <content type="html"><![CDATA[
<pre><code>kristina 2018/3/31
</code></pre>

<blockquote>
<p><a href="https://stackoverflow.com/questions/29245787/what-are-the-differences-between-bazel-and-gradle">https://stackoverflow.com/questions/29245787/what-are-the-differences-between-bazel-and-gradle</a></p>

<p>Disclaimer: I work on Bazel and I&#39;m not intimately familiar with Gradle. However, one of my coworkers wrote up a comparison of the two systems, which I will paraphrase here:</p>
</blockquote>

<p>Bazel and Gradle emphasize different aspects of the build experience. To some extent, their priorities are incompatible - Gradle’s desire for flexibility and non-obtrusiveness limits the restrictions it can place on build structure, while Bazel&#39;s desire for reliability and performance necessarily enforces non-negotiable restrictions.</p>

<p>Gradle does value the same principles that Bazel does, i.e. the Gradle team pays great attention to performance (incremental builds, parallelized configuration and execution, the Gradle daemon), correctness (content-based “up-to-date” checking), and reproducibility (rich support for declarative syntax, dependency versioning, explicitly declared dependencies). And Bazel respects the need for flexible project layouts.</p>

<p>The nuance is that Gradle wants to promote good practice while Bazel wants to require it. Gradle aims for a middle ground between the Ant experience (freedom to define your own project structure with incoherent results) and the Maven experience (enforced best practices with no room for varying project needs). Bazel believes that flexible project support is possible without sacrificing the strong guarantees that enable its powerful workflows.</p>

<p>Neither philosophy is more “correct” - whichever tool best suits a project depends on that particular project’s values.</p>

<h2 id="toc_0">Gradle Overview</h2>

<p>Gradle is a highly flexible system that makes it easy for users to construct complete, reliable build flows with minimal constraints on how they organize their projects. It does this by supplying powerful building blocks (e.g. automatic dependency tracking and retrieval, tightly integrated plugin support) with a generic, Turing-complete, scripting interface that can combine these blocks however the users wants.</p>

<h3 id="toc_1">Gradle emphasizes the following features:</h3>

<p>Easy migration from other systems. Gradle easily accommodates any project organization to easily implement arbitrary workflow structures. It natively understands Ant tasks, and natively integrates with Maven and Ivy repositories.<br/>
Highly extensible scripting model. Users implement all build logic by writing Groovy scripts. A “build” is simply a dependency-sequenced execution of generic tasks, which are essentially open-ended, overridable, extensible method definitions.<br/>
Rich dependency management. Versioned dependencies can be declared and automatically staged from external code repositories, local filesystems, and other Gradle projects. Build outputs can likewise be auto-published to repositories and other locations.<br/>
Tightly integrated plugin system. Plugins are simply bundles of tasks organized to facilitate a desired workflow. Many of Gradle’s &quot;core&quot; features are actually implemented through plugins (e.g. Java, Android). Plugins interact (at their discretion) tightly with build script logic. Plugins enjoy deep access to Gradle’s core data structures.</p>

<h2 id="toc_2">Bazel Overview</h2>

<p>Bazel evolved out of the need to build internal Google projects reliably and efficiently. Because Google’s development environment is unusually large and complex, Bazel offers unusually strong guarantees about the integrity of its builds and unusually low performance overhead in achieving them.</p>

<p>This provides a foundation for powerful development workflows built around reproducible builds, where a “build” becomes an abstract entity that can be referenced, repeated, passed around to different machines, and passed to arbitrary programs and services such that every instance is known to be exactly the same.</p>

<h3 id="toc_3">Bazel emphasizes the following features:</h3>

<p>Correctness. Bazel builds are designed to always produce correct output, period. If two users invoke the same build at the same commit with the same Bazel flags on different machines, they will see identical results. Incremental builds are as reliably correct as clean builds, rendering the latter essentially unnecessary.<br/>
Performance. Builds are designed to execute as fast as intrinsically possible given the resources available to them. Tasks are as parallelizable as their dependency chains allow. Unnecessary work is never executed (i.e. “up-to-date” tasks are always skipped). Work can naturally be farmed out to remote executors to overcome local machine limits.<br/>
Reproducibility. Any instance of a build can be faithfully reproduced in any environment. For example, if a bug report says version X of software Y fails in production environment Z, a developer can faithfully recreate it on their own machine with confidence that they’re debugging the same thing.</p>

]]></content>
  </entry>
  
</feed>
