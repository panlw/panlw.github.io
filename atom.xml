<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Junkman]]></title>
  <link href="http://panlw.github.io/atom.xml" rel="self"/>
  <link href="http://panlw.github.io/"/>
  <updated>2018-05-28T00:54:25+08:00</updated>
  <id>http://panlw.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[蚂蚁通信框架实践]]></title>
    <link href="http://panlw.github.io/15277835170976.html"/>
    <updated>2018-06-01T00:18:37+08:00</updated>
    <id>http://panlw.github.io/15277835170976.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzUzMzU5Mjc1Nw==&amp;mid=2247483812&amp;idx=1&amp;sn=580d9003b01b3c5dab821c04a97b77cb&amp;chksm=faa0ee7ecdd767684e66a7844270ca1d0cc94b05719b39068d161e2e56560d00167efd753859&amp;mpshare=1&amp;scene=23&amp;srcid=0531OaPJuQOHngeGUjgDXvfG%23rd">原文地址</a></p>
</blockquote>

<pre><code>**原创声明**：本文系作者原创，谢绝个人、媒体、公众号或网站未经授权转载，违者追究其法律责任。
</code></pre>

<h2 id="toc_0"><strong>前  言</strong></h2>

<p>互联网领域的通信技术，有各式各样的通信协议可以选择，比如基于 TCP/IP 协议簇的 HTTP(1/2)、SPDY 协议、WebSocket、Google 基于 UDP 的 QUIC 协议等。这些协议，都有完整的报文格式与字段定义，对安全，序列化机制，数据压缩机制，CRC 校验机制等各种通信细节都有较好的设计。能够高效、稳定、且安全地运行在公网环境。</p>

<p>而对于私网环境，比如一个公司的 IDC 内部，如果所有应用的节点间，全部通过标准协议来通信，会有很多问题：比如研发效率方面的影响，我们的研发框架，需要做大量业务数据转化成标准协议的工作；再比如升级兼容性，标准协议的字段众多，版本各异，兼容性也得不到保障；除此还有无用字段的传输，也会造成资源浪费，功能定制也可能不那么灵活。而解决这些问题，比较常见的做法就是自己来设计协议，可以自己来定义字段，制定升级方式，可插拔可开关的特性需求等，我们把这样的协议叫做私有通信协议。</p>

<p>在蚂蚁金服的分布式技术体系下，我们大量的技术产品（非网关类产品），都需要在内网，进行节点间通信。高吞吐、高并发的通信，数量众多的连接管理（C10K 问题），便捷的升级机制，兼容性保障，灵活的线程池模型运用，细致的异常处理与日志埋点等，这些功能都需要在通信协议和实现框架上做文章。本文主要从如下几个方面来对蚂蚁通信框架实践之路进行介绍：</p>

<ol>
<li> 私有通信协议设计</li>
<li> 基础通信功能设计要点分析</li>
<li> 私有通信协议设计举例</li>
<li> 蚂蚁自研通信框架 Bolt</li>
</ol>

<h2 id="toc_1"><strong>私有通信协议设计</strong></h2>

<p>我们的分布式架构，所需要的内部通信模块，采用了私有协议来设计和研发。当然私有协议，也是有很多弊端的，比如在通用性上、公网传输的能力上相比标准协议会有劣势。然而，我们为了最大程度的提升性能，降低成本，提高灵活性与效率，最佳选择还是高度定制化的私有协议：</p>

<ul>
<li>  可以有效地利用协议里的各个字段</li>
<li>  灵活满足各种通信功能需求：比如 CRC 校验，Server Fail-Fast 机制，自定义序列化器</li>
<li>  最大程度满足性能需求：IO 模型与线程模型的灵活运用</li>
</ul>

<p>比如一个典型的 Request-Response 通信场景：</p>

<ol>
<li> 在一个通信节点上，如何把一个请求对象，序列化成字节流，通过怎样的网络传输方式，传递到另一个节点</li>
<li> 在对端的通信节点上，需要高效的读取字节流，并反序列化成原始的请求对象，然后根据请求内容，做一些逻辑处理。处理完成后，响应返回。</li>
<li> 同时，此时要考虑，如何充分利用网络 IO、CPU 以及内存，来保证吞吐和处理效率的最优。</li>
</ol>

<p>文章后面的内容，比较清晰地介绍了这个通信场景的设计与实现方案。</p>

<p><img src="media/15277835170976/15277842914150.jpg" alt=""/><br/>
图1 - 私有协议与必要的功能模块</p>

<p>首先协议设计上，我们需要考虑的几个关键问题：</p>

<p><strong>Protocol</strong></p>

<ul>
<li>  协议应该包括哪些必要字段与主要业务负载字段：协议里设计的每个字段都应该被使用到，避免无效字段；</li>
<li>  需要考虑通信功能特性的支持：比如CRC校验，安全校验，数据压缩机制等；</li>
<li>  需要考虑协议的可扩展性：充分评估现有业务的需求，设计一个通用，扩展性高的协议，避免经常对协议进行修改；</li>
<li>  需要考虑协议的升级机制：毕竟是私有协议，没有长期的验证，字段新增或者修改，是有可能发生的，因此升级机制是必须考虑的；</li>
</ul>

<p><strong>Encoder 与 Decoder</strong></p>

<ul>
<li>  协议相关的编解码方式：私有协议需要有核心的encode与decode过程，并且针对业务负载能支持不同的序列化与反序列化机制。这部分，不同的私有协议，由于字段的差异，核心encode和decode过程是不一样的，因此需要分开考虑</li>
</ul>

<p><strong>Heartbeat</strong></p>

<ul>
<li>  协议相关的心跳触发与处理：不同的协议对心跳的需求，处理逻辑也可能是不同的。因此心跳的触发逻辑，心跳的处理逻辑，也都需要单独考虑。</li>
</ul>

<p><strong>Command 与 Command Handler</strong></p>

<ul>
<li>  可扩展的命令与命令处理器管理
<img src="media/15277835170976/15277843020616.jpg" alt=""/>
图2 - 通信命令设计举例</li>
<li>  负载命令：一般传输的业务的具体数据，比如带着请求参数，响应结果的命令；</li>
<li>  控制命令：一些功能管理命令，心跳命令等，它们通常完成一些复杂的分布式跨节点的协调功能，以此来保证负载命令通信过程的稳定，是必不可少的一部分。</li>
<li>  协议的通信过程，会有各种命令定义，逻辑上，我们把传输业务具体负载的请求对象，叫做负载命令（Payload Command），另一种叫做控制命令（Control Command），比如一些功能管理命令，或者心跳命令。</li>
<li>  定义了通信命令，我们还需要定义命令处理器，用来编写各个命令对应的业务处理逻辑。同时，我们需要保存命令与命令处理器的映射关系，以便在处理阶段，走到正确的处理器。</li>
</ul>

<p>有了私有协议的设计要点，我们接下来分两部分来介绍下实现：基础通信模块与私有协议设计举例。</p>

<p>首先是基础通信功能模块的实现，这部分沉淀了我们的一些优化和最佳实践，可以被不同的私有协议复用。</p>

<h2 id="toc_2"><strong>基础通信功能设计要点分析</strong></h2>

<p>蚂蚁的中间件产品，主要是 Java 语言开发，如果通信产品直接用原生的 Java NIO 接口开发，工作量相当庞大。通常我们会选择一些基础网络编程框架，而在基础网络通信框架上，我们也经历了自研（比如伯岩的 Gecko）、基于 Apache Mina 实现。最终，由于 Netty 在网络编程领域的出色表现，我们逐步切换到了 Netty 上。</p>

<p>Netty 在 2008 年就发布了<code>3.0.0</code> 版本，到现在已经经历了 10 年多的发展。而且从 <code>4.x</code> 之后的版本，把无锁化的设计理念放在第一位，然后针对内存分配，高效的 Queue 队列，高吞吐的超时机制等，做了各种细节优化。同时 Netty 的核心 Committer 与社区非常活跃，如果发现了缺陷能够及时得到修复。所有这些，使得 Netty 性能非常的出色和稳定，成为当下 Java 领域最优秀的网络通信组件。接下来主要介绍我们对 Netty 的学习经验，内部使用上的一些最佳实践。</p>

<h3 id="toc_3">1. 网络 IO 模型与线程模型</h3>

<p><img src="media/15277835170976/15277843394656.jpg" alt=""/><br/>
图3 - Netty与Reactor</p>

<p>如果你对 Java 网络 IO 这个话题感兴趣的话，肯定看过 Doug Lea 的《Scalable IO in Java》，在这个 PPT 里详细介绍了如何使用 Java NIO 的技术来实现 Douglas C. Schmidt 发表的 Reactor 论文里所描述的 IO 模型。针对这个高效的通信模型，Netty 做了非常友好的支持：</p>

<ul>
<li><p><strong>Reactor模型</strong></p>

<ul>
<li><p>我们只需要在初始化 <code>ServerBootstrap</code> 时，提供两个不同的 <code>EventLoopGroup</code> 实例，就实现了 Reactor 的主从模型。我们通常把处理建连事件的线程，叫做 BossGroup，对应 <code>ServerBootstrap</code> 构造方法里的 <code>parentGroup</code> 参数，即我们常说的 Acceptor 线程；处理已创建好的 <code>channel</code>  相关连 IO 事件的线程，叫做 WorkerGroup，对应 <code>ServerBootstrap</code> 构造方法里的 <code>childGroup</code> 参数，即我们常说的 IO 线程。</p></li>
<li><p>最佳实践：通常 <code>bossGroup</code> 只需要设置为 <code>1</code> 即可，因为 <code>ServerSocketChannel</code> 在初始化阶段，只会注册到某一个 <code>eventLoop</code> 上，而这个 <code>eventLoop</code> 只会有一个线程在运行，所以没有必要设置为多线程（什么时候需要多线程呢，可以参考 Norman Maurer 在 StackOverflow 上的这个回答）；而 IO 线程，为了充分利用 CPU，同时考虑减少线上下文切换的开销，通常设置为 CPU 核数的两倍，这也是 Netty 提供的默认值。</p></li>
</ul></li>
<li><p><strong>串行化设计理念</strong></p>

<ul>
<li>  Netty 从 <code>4.x</code> 的版本之后，所推崇的设计理念是串行化处理一个 <code>Channel</code> 所对应的所有 IO 事件和异步任务，单线程处理来规避并发问题。Netty 里的 <code>Channel</code> 在创建后，会通过 <code>EventLoopGroup</code> 注册到某一个 <code>EventLoop</code> 上，之后该 <code>Channel</code> 所有读写事件，以及经由 <code>ChannelPipeline</code> 里各个 <code>Handler</code> 的处理，都是在这一个线程里。一个 <code>Channel</code> 只会注册到一个 <code>EventLoop</code> 上，而一个 <code>EventLoop</code> 可以注册多个 <code>Channel</code> 。所以我们在使用时，也需要尽可能避免使用带锁的实现，能无锁化就无锁。</li>
<li>  最佳实践：<code>Channel</code> 的实现是线程安全的，因此我们通常在运行时，会保存一个 <code>Channel</code> 的引用，同时为了保持 Netty 的无锁化理念，也应该尽可能避免使用带锁的实现，尤其是在 <code>Handler</code> 里的处理逻辑。举个例子：这里会有一个比较特殊的容易死锁的场景，比如在业务线程提交异步任务前需要先抢占某个锁，<code>Handler</code> 里某个异步任务的处理也需要获取同一把锁。如果某一个时刻业务线程先拿到锁 lock1，同时 <code>Handler</code> 里由于事件机制触发了一个异步任务 A，并在业务线程提交异步任务之前，提交到了 <code>EventLoop</code> 的队列里。之后，业务线程提交任务 B，等待 B 执行完成后才能释放锁 lock1；而任务 A 在队列里排在 B 之前，先被执行，执行过程需要获取锁 lock1 才能完成。这样死锁就发生了，与常见的资源竞争不同，而是任务执行权导致的死锁。要规避这类问题，最好的办法就是不要加锁；如果实在需要用锁，需要格外注意 Netty 的线程模型与任务处理机制。</li>
</ul></li>
<li><p><strong>业务处理</strong></p>

<ul>
<li>  IO 密集型的轻计算业务：此时线程的上下文切换消耗，会比 IO 线程的占用消耗更为突出，所以我们通常会建议在 IO 线程来处理请求；</li>
<li>  CPU 密集型的计算业务：比如需要做远程调用，操作 DB 的业务，此时 IO 线程的占用远远超过线程上下文切换的消耗，所以我们就会建议在单独的业务线程池里来处理请求，以此来释放 IO 线程的占用。该模式，也是我们蚂蚁微服务，消息通信等最常使用的模型。该模式在后面的 RPC 协议实现举例部分会详细介绍。</li>
<li>  如文章开头所描述的场景，我们需要合理设计，来将硬件的 IO 能力，CPU 计算能力与内存结合起来，发挥最佳的效果。针对不同的业务类型，我们会选择不同的处理方式</li>
<li>  最佳实践：“Never block the event loop, reduce context-swtiching”，引自Netty committer Norman Maurer，另外阿里 HSF 的作者毕玄也有类似的总结。</li>
</ul></li>
<li><p><strong>其他实践建议</strong></p>

<ul>
<li>  最小化线程池，能复用 <code>EventLoopGroup</code> 的地方尽量复用。比如蚂蚁因为历史原因，有过两版 RPC 协议，在两个协议升级过渡期间，我们会复用 Acceptor 线程与 IO 线程在同一个端口处理不同协议的请求；除此，针对多应用合并部署的场景，我们也会复用 IO 线程防止一个进程开过多的 IO 线程。</li>
<li>  对于无状态的 <code>ChannelHandler</code> ，设置成共享模式。比如我们的事件处理器，RPC 处理器都可以设置为共享，减少不同的 <code>Channel</code> 对应的 <code>ChannelPipeline</code> 里生成的对象个数。</li>
<li>  正确使用 <code>ChannelHandlerContext</code> 的 <code>ctx.write()</code> 与 <code>ctx.channel().write()</code> 方法。前者是从当前 <code>Handler</code> 的下一个 <code>Handler</code> 开始处理，而后者会从 tail 开始处理。大多情况下使用 <code>ctx.write()</code> 即可。</li>
<li>  在使用 <code>Channel</code> 写数据之前，建议使用 <code>isWritable()</code> 方法来判断一下当前 <code>ChannelOutboundBuffer</code> 里的写缓存水位，防止 OOM 发生。不过实践下来，正常的通信过程不太会 OOM，但当网络环境不好，同时传输报文很大时，确实会出现限流的情况。</li>
</ul></li>
</ul>

<h3 id="toc_4">2. 连接管理</h3>

<p>为了提高通信效率，我们需要考虑复用连接，减少 TCP 三次握手的次数，因此需要有连接管理的机制。而在业务的通信场景中，我们还识别到一些不得不走硬负载（比如 LVS VIP）的场景，此时如果只建立单链接，可能会出现负载不均衡的问题，此时需要建立多个连接，来缓解负载不均的问题。我们需要设计一个针对某个连接地址（IP 与 Port 唯一确定的地址）建立特定数目连接的实现，同时保存在一个连接池里。该连接池设计了一个通用的 <code>PoolKey</code>不限定 Key 的类型。</p>

<p>需要注意的是，这里的建连过程，有一个并发问题要解，比如客户端在高并发的调用建连接口时，如何保证建立的连接刚好是所设定的个数呢？为了配合 Netty 的无锁理念，我们也采用一个无锁化的建连过程来实现，利用 <code>ConcurrentHashMap</code> 的 <code>putIfAbsent</code> 接口：</p>

<p><img src="media/15277835170976/15277843573310.jpg" alt=""/><br/>
代码1 - 无锁建连代码</p>

<p>除此，我们的连接管理，还要具备定时断连功能，自动重连功能，自定义连接选择算法功能来适用不同的连接场景。</p>

<ul>
<li>  最佳实践：在 Netty 的 4.0.28.Final#3218 里，提供了一种 <code>ChannelPool</code> 的接口类与默认实现，其中 <code>FixedChannelPool</code> 与我们实现的连接池做的事情一样。而 Netty 采用了更巧妙的方式来规避并发问题，即在初始化 <code>FixedChannelPool</code> 时，就将其关联到某一个 <code>eventLoop</code> 上，后续的建连动作，采用经典的 <code>inEventLoop()</code> 方法来判断，如果不在 <code>eventLoop</code> 线程，则入队等待下次调度。如此规避了并发问题。这个功能，我们目前还没有实践过，后续计划采用这个官方实现重构一版。</li>
</ul>

<h3 id="toc_5">3. 基础通信模型</h3>

<p><img src="media/15277835170976/15277843662971.jpg" alt=""/><br/>
图4 - 几种通信模型</p>

<p>如图所示，我们实现了多种通信接口 <code>oneway</code> ，<code>sync</code> ，<code>future</code> ，<code>callback</code> 。图中都是ping/pong模式的通信，蓝色部分表示线程正在执行任务</p>

<ul>
<li>  可以看到 <code>oneway</code> 不关心响应，请求线程不会被阻塞，但使用时需要注意控制调用节奏，防止压垮接收方；</li>
<li>  <code>sync</code> 调用会阻塞请求线程，待响应返回后才能进行下一个请求。这是最常用的一种通信模型；</li>
<li>  <code>future</code> 调用，在调用过程不会阻塞线程，但获取结果的过程会阻塞线程；</li>
<li>  <code>callback</code> 是真正的异步调用，永远不会阻塞线程，结果处理是在异步线程里执行。</li>
</ul>

<h3 id="toc_6">4. 超时控制</h3>

<p><img src="media/15277835170976/15277843806586.jpg" alt=""/><br/>
图5 - 超时控制模型</p>

<p>除了 <code>oneway</code> 模式，其他三种通信模型都需要进行超时控制，我们同样采用 Netty 里针对超时机制，所设计的高效方案 <code>HashedWheelTimer</code> 。如图所示，其原理是首先在发起调用前，我们会新增一个超时任务 <code>timeoutTask</code> 到 <code>MpscQueue</code> （Netty 实现的一种高效的无锁队列）里，然后在循环里，会不断的遍历 Queue 里的这些超时任务（每次最多10万），针对每个任务，会根据其设置的超时时间，来计算该任务所属于的 <code>bucket</code> 位置与剩余轮数 <code>remainingRounds</code> ，然后加入到对应 <code>bucket</code> 的链表结构里。随着 <code>tick++</code> 的进行，时间在不断的增长，每 <code>tick</code> 8 次，就是 1 个时间轮 <code>round</code>。当对应超时任务的<code>remainingRounds</code>减到 <code>0</code> 时，就是触发这个超时任务的时候，此时再执行其 <code>run()</code> 方法，做超时逻辑处理。</p>

<ul>
<li>  最佳实践：通常一个进程使用一个<code>HashedWheelTimer</code>实例，采用单例模型即可。</li>
</ul>

<h3 id="toc_7">5. 批量解包与批量提交</h3>

<p><img src="media/15277835170976/15277843927772.jpg" alt=""/><br/>
图6 - 批量解包与批量提交</p>

<p>Netty 提供了一个方便的解码工具类 <code>ByteToMessageDecoder</code> ，如图上半部分所示，这个类具备 <code>accumulate</code> 批量解包能力，可以尽可能的从 <code>socket</code> 里读取字节，然后同步调用 <code>decode</code> 方法，解码出业务对象，并组成一个 <code>List</code> 。最后再循环遍历该 <code>List</code> ，依次提交到 <code>ChannelPipeline</code> 进行处理。此处我们做了一个细小的改动，如图下半部分所示，即将提交的内容从单个 <code>command</code> ，改为整个 <code>List</code> 一起提交，如此能减少 <code>pipeline</code> 的执行次数，同时提升吞吐量。这个模式在低并发场景，并没有什么优势，而在高并发场景下对提升吞吐量有不小的性能提升。</p>

<ul>
<li>  最佳实践：<code>ByteToMessageDecoder</code>  因为内部的实现有成员变量，不是无状态的，所以一定不能被设置为 <code>@Sharable</code></li>
</ul>

<h3 id="toc_8">6. 其他有用的功能</h3>

<ul>
<li>  <strong>事件触发与监听机制</strong>

<ul>
<li>  Netty 的 <code>ChannelHandler</code> 完美实现了拦截器模式。在 <code>ChannelHandler</code> 里 <code>hook</code> 了各个IO事件与IO操作的方法，我们可以方便的覆写这些方法，来加一些自定义的逻辑。比如为了把建连，断连事件触发给上层业务，方便做一些准备或者优雅关闭的处理，我们实现一个继承了 <code>ChannelInBoundHandler</code> 与 <code>ChannelOutboundHandler</code> 的处理器，覆盖这些事件所对应的建连与断连方法，然后设计一套业务的 <code>event</code> 感知逻辑即可。</li>
</ul></li>
<li>  <strong>双工通信</strong>

<ul>
<li>  我们知道 TCP 是可以提供全双工的通信能力的。因此，当客户端与服务端建立连接后，我们是可以由服务端发起通信请求，客户端来处理的。而为了支持这个功能，我们只需要把可以复用的 <code>inboundHandler</code> 与 <code>outboundHandler</code> 在 客户端的 <code>Bootstrap</code> 与服务端的 <code>ServerBootstrap</code> 里都注册一遍即可</li>
</ul></li>
</ul>

<p>有了私有协议的设计要点，与基础通信模块的实现，我们来看一个私有协议设计的举例，一种典型的 RPC 特征的通信实现。</p>

<h2 id="toc_9"><strong>私有通信协议举例</strong></h2>

<h3 id="toc_10">1. 通信协议的设计</h3>

<p><img src="media/15277835170976/15277844025910.jpg" alt=""/><br/>
图7 - 协议字段举例</p>

<ul>
<li>  <code>ProtocolCode</code> ：如果一个端口，需要处理多种协议的请求，那么这个字段是必须的。因为需要根据 <code>ProtocolCode</code> 来进入不同的核心编解码器。比如在支付宝，因为曾经使用过基于mina开发的通信框架，当时设计了一版协议。因此，我们在设计新版协议时，需要预留该字段，来适配不同的协议类型。该字段可以在想换协议的时候，方便的进行更换。</li>
<li>  <code>ProtocolVersion</code> ：确定了某一种通信协议后，我们还需要考虑协议的微小调整需求，因此需要增加一个 <code>version</code> 的字段，方便在协议上追加新的字段</li>
</ul>

<p><img src="media/15277835170976/15277844148635.jpg" alt=""/><br/>
图8 - 协议号与版本号的关系</p>

<ul>
<li>  <code>RequestType</code> ：请求类型， 比如<code>request</code> <code>response</code> <code>oneway</code></li>
<li>  <code>CommandCode</code> ：请求命令类型，比如 <code>request</code> 可以分为：负载请求，或者心跳请求。<code>oneway</code> 之所以需要单独设置，是因为在处理响应时，需要做特殊判断，来控制响应是否回传。</li>
<li>  <code>CommandVersion</code> ：请求命令版本号。该字段用来区分请求命令的不同版本。如果修改 <code>Command</code> 版本，不修改协议，那么就是纯粹代码重构的需求；除此情况，<code>Command</code> 的版本升级，往往会同步做协议的升级。</li>
<li>  <code>RequestId</code> ：请求 ID，该字段主要用于异步请求时，保留请求存根使用，便于响应回来时触发回调。另外，在日志打印与问题调试时，也需要该字段。</li>
<li>  <code>Codec</code> ：序列化器。该字段用于保存在做业务的序列化时，使用的是哪种序列化器。通信框架不限定序列化方式，可以方便的扩展。</li>
<li>  <code>Switch</code> ：协议开关，用于一些协议级别的开关控制，比如 CRC 校验，安全校验等。</li>
<li>  <code>Timeout</code> ：超时字段，客户端发起请求时，所设置的超时时间。该字段非常有用，在后面会详细讲解用法。</li>
<li>  <code>ResponseStatus</code> ：响应码。从字段精简的角度，我们不可能每次响应都带上完整的异常栈给客户端排查问题，因此，我们会定义一些响应码，通过编号进行网络传输，方便客户端定位问题。</li>
<li>  <code>ClassLen</code> ：业务请求类名长度</li>
<li>  <code>HeaderLen</code> ：业务请求头长度</li>
<li>  <code>ContentLen</code> ：业务请求体长度</li>
<li>  <code>ClassName</code> ：业务请求类名。需要注意类名传输的时候，务必指定字符集，不要依赖系统的默认字符集。曾经线上的机器，因为运维误操作，默认的字符集被修改，导致字符的传输出现编解码问题。而我们的通信框架指定了默认字符集，因此躲过一劫。</li>
<li>  <code>HeaderContent</code> ：业务请求头</li>
<li>  <code>BodyContent</code> ：业务请求体</li>
<li>  <code>CRC32</code> ：CRC校验码，这也是通信场景里必不可少的一部分，而我们金融业务属性的特征，这个显得尤为重要。</li>
</ul>

<h3 id="toc_11">2. 灵活的反序列化时机控制</h3>

<p>从上面的协议介绍，可以看到协议的基本字段所占用空间是比较小的，目前只有24个字节。协议上的主要负载就是 <code>ClassName</code>  ，<code>HeaderContent</code> ， <code>BodyContent</code> 这三部分。这三部分的序列化和反序列化是整个请求响应里最耗时的部分。在请求发送阶段，在调用 Netty 的写接口之前，会在业务线程先做好序列化，这里没有什么疑问。而在请求接收阶段，反序列化的时机就需要考虑一下了。结合上面提到的最佳实践的网络 IO 模型，请求接收阶段，我们有 IO 线程，业务线程两种线程池。为了最大程度的配合业务特性，保证整体吞吐我们设计了精细的开关来控制反序列化时机：</p>

<p><img src="media/15277835170976/15277844266936.jpg" alt=""/><br/>
图9 - 反序列化与业务处理时序图</p>

<p><img src="media/15277835170976/15277844352308.jpg" alt=""/><br/>
表格1 - 反序列化场景具体介绍</p>

<h3 id="toc_12">3. Server Fail-Fast 机制</h3>

<p><img src="media/15277835170976/15277844481916.jpg" alt=""/><br/>
图10 - Server Fail-Fast机制</p>

<p>在协议里，留意到我们有timeout这个字段，这个是把客户端发起调用时，所设置的超时时间通过协议传到了 Server 端。有了这个，我们就可以实现 Fail-Fast 快速失败的机制。比如当客户端设置超时时间 1s，当请求到达 Server 开始计时 <code>arriveTimeStamp</code> ，到任务被线程调度到开始处理时，记录 <code>startToProcessTimestamp</code> ，二者的差值即请求反序列化与线程池排队的时延，如果这个时间间隔已经超过了 1s，那么请求就没有必要被处理了。这个机制，在服务端出现处理抖动时，对于快速恢复会很有用。</p>

<ul>
<li>  最佳实践：不要依赖跨系统的时钟，因为时钟可能会不一致，跨系统就会出现误差，因此是从请求到达 Server 的那一刻，在 Server 的进程里开始计时。</li>
</ul>

<h3 id="toc_13">4. 用户请求处理器(UserProcessor)</h3>

<p>在通用设计部分，我们提到了命令处理器。而为了方便开发者使用，我们还提供了一个用户请求处理器，即在 RPC 的命令处理器中，再增加一层映射关系，保存的是 业务传输对象的 <code>className</code> 与 <code>UserProcessor</code> 的对应关系。此时服务端只需要简单注册一个 <code>className</code> 对应的processor，并提供一个独立的 <code>executor</code> ，就可以实现在业务线程处理请求了。</p>

<p><img src="media/15277835170976/15277844585373.jpg" alt=""/><br/>
图11 - 命令处理器与用户请求处理器的关系</p>

<p>除此，我们还设计了一个 <code>RemotingContext</code> 用于保存请求处理阶段的一些通信层的关键辅助类或者信息，方便通信框架开发者使用；同时还提供了一个 <code>BizContext</code> ，有选择把通信层的信息暴露给框架使用者，方便框架使用者使用。有了用户请求处理器，以及上下文的传递机制，我们就可以方便的把通信层处理逻辑与业务处理逻辑联动起来，比如一些开关的控制，字段的传递等定制功能：</p>

<ul>
<li>  请求超时处理开关：用于开关 Server Fail-Fast 机制。</li>
<li>  IO 线程业务处理开关：用户可以选择在 IO 线程处理业务请求；或者在业务线程来处理。</li>
<li>  线程池选择器 <code>ExecutorSelector</code> ：用户可以提供多个业务线程池，使用 <code>ExecutorSelector</code> 来实现选择逻辑</li>
<li>  泛化调用的支持：序列化请求与反序列化响应阶段，针对泛化调用，使用特殊的序列化器。而是否开启该功能，需要依赖上下文来传递一些标识。</li>
</ul>

<h3 id="toc_14">5. 其他实现细节</h3>

<ul>
<li><p><strong>可扩展的序列化机制</strong><br/>
针对业务对象里的 <code>HeaderContent</code> 与 <code>BodyContent</code> ，我们提供了用户自定义逻辑：用户可以结合自身的请求内容做定制的序列化和反序列化动作；如果用户没有自定义，那么会默认使用 Bolt 框架当前集成的序列化器，比如 Hessian（默认使用）、FastJson 等。</p></li>
<li><p><strong>埋点与异常处理</strong><br/>
为了精细化请求处理过程，我们会记录请求发送阶段的建连耗时，客户端超时时间，请求到达时间，线程调度等待时间等，然后通过上下文传递机制，连通业务与通信层；同时还会细化各个异常场景，比如请求超时异常，服务端线程池繁忙，序列化异常（请求与响应），反序列化异常（请求与响应）等。有了这些就能方便进行问题排查和快速定位。</p></li>
<li><p><strong>日志打印</strong><br/>
<img src="media/15277835170976/15277844755617.jpg" alt=""/><br/>
图12 - 日志模板</p>

<p>作为通信框架，必要的日志打印也是很重要的。比如可以打印建连与断连的日志，便于排查连接问题；一些关键的异常场景也可以打印出来，方便定位问题；还可以打印一些关键字，来表示程序 BUG，便于框架开发者定位和分析。而打印日志的方式，我们选择依赖日志门面 <code>SLF4J</code> ，然后提供不同的日志实现所需要的配置文件。运行时，根据业务所依赖的日志实现（比如 <code>log4j</code> ， <code>log4j2</code> ， <code>logback</code> 来动态加载日志配置）。同时默认使用异步 <code>logger</code> 来打印日志。</p></li>
</ul>

<h2 id="toc_15"><strong>蚂蚁通信框架-BOLT</strong></h2>

<ul>
<li><p>为了让 Java 程序员，花更多的时间在一些 Productive 的事情上，而不是纠结底层 NIO 的实现，处理难以调试的网络问题，Netty 应运而生</p></li>
<li><p>为了让中间件的开发者，花更多的时间在中间件的特性实现上，而不是重复地一遍遍制造通信框架的轮子，Bolt 应运而生。</p></li>
</ul>

<p>Bolt 即为本文所描述的方法论的一个实践实现，名字取自迪士尼动画，闪电狗。定位是一个基于 Netty 最佳实践过的，通用、高效、稳定的通信框架。我们希望能把这些年，在 RPC，MSG 在网络通信上碰到的问题与解决方案沉淀到这个基础组件里，不断的优化和完善它。让更多的需要网络通信的场景能够统一受益。目前已经运用在了蚂蚁中间件的微服务，消息中心，分布式事务，分布式开关，配置中心等众多产品上。</p>

<p>除了 Bolt 提供的高效通信能力外，还可以方便的进行协议适配的工作。比如蚂蚁内部之前使用的 RPC 协议是 Tr 协议，是基于 Apache Mina 开发的老版本通信框架，由于年久失修，同时性能逐步落伍，我们重新设计了 Bolt 协议，精简以及新增了一些协议字段，同时切换到了 Netty 上。在新老 RPC 协议的切换期间，我们利用 Bolt 进行了协议适配，开发了 BoltTrAdaptor，最大程度的复用基础通信能力，仅仅把协议相关的部分单独实现，以此来保证新老协议调用的兼容性。</p>

<p>针对蚂蚁内部的新老通信框架，我们进行了细致的压测，如下图所示。我们的压测环境是，4 核10G 的虚拟机，千兆网卡，请求与响应包大小 1024 字节，分别压测了四种场景。由压测结果能看出 Bolt -&gt; Bolt 的场景，整体吞吐量最大，平均RT最小，同时对比了 IO ，CPU 使用率等情况，资源整体利用率上也提升很多。</p>

<p><img src="media/15277835170976/15277844880461.jpg" alt=""/><br/>
图13 - 压测TPS数据</p>

<p><img src="media/15277835170976/15277844973704.jpg" alt=""/><br/>
图14 - 压测平均RT数据</p>

<p>Bolt 在实验室里的极限性能压测，采用的是 32 核物理机，万兆网卡的环境，请求和响应 100 字节负载，服务端收到请求后马上返回响应，瓶颈基本就是业务线程池所使用的 <code>ArrayBlockingQueue</code> <code>LinkedBlockingQueue</code> 的性能瓶颈，压力到了十多万，就会出现较大幅度的毛刺和抖动。纯粹为了压测场景，改成使用 <code>SynchronousQueue</code> 后，毛刺减少了很多，基本能稳定在 30W TPS 的处理能力。</p>

<h2 id="toc_16"><strong>写在最后</strong></h2>

<p>近期我们也在准备开源蚂蚁 Bolt 通信框架，主要是吸取 Netty 的开源精神，回馈社区，与社区共建与完善。如果你也有制造通信框架轮子的需求，或者想适配内部的自有或者开源通信协议（比如 Dubbo 等），可以试一下蚂蚁 Bolt 通信框架，敬请期待！我们有很多想法还在实验室里酝酿，还没有落地到生产环境使用。非常欢迎一起来探讨网络通信问题，参与共建。</p>

<p>最后附上蚂蚁中间件的招聘链接，通信是分布式架构体系的基础设施，欢迎有志之士加盟，打造高效、稳定的通信技术。点击左下角的【阅读原文】获取蚂蚁中间件通信组的招聘信息。</p>

<h2 id="toc_17"><strong>参考</strong></h2>

<ol>
<li> Scalable IO in Java, Slides, by Doug Lea, <a href="http://gee.cs.oswego.edu/dl/cpjslides/nio.pdf">http://gee.cs.oswego.edu/dl/cpjslides/nio.pdf</a></li>
<li> Reactor, Thesis, by Douglas C. Schmidt, <a href="http://www.dre.vanderbilt.edu/%7Eschmidt/PDF/reactor-siemens.pdf">http://www.dre.vanderbilt.edu/~schmidt/PDF/reactor-siemens.pdf</a></li>
<li> Hashed and Hierarchical Timing Wheels, Thesis, by George Varghese and Anthony Lauck, <a href="http://www.cs.columbia.edu/%7Enahum/w6998/papers/ton97-timing-wheels.pdf">http://www.cs.columbia.edu/~nahum/w6998/papers/ton97-timing-wheels.pdf</a></li>
<li> Netty Best Practices, Slides, by Norman Maurer, <a href="http://normanmaurer.me/presentations/2014-facebook-eng-netty/slides.html">http://normanmaurer.me/presentations/2014-facebook-eng-netty/slides.html</a></li>
<li> NSF-RPC的优化过程，博客文章，来自毕玄，<a href="http://bluedavy.me/?p=384">http://bluedavy.me/?p=384</a></li>
<li> Netty 源码分析系列 ，博客文章，来自永顺，<a href="https://segmentfault.com/a/1190000007282628">https://segmentfault.com/a/1190000007282628</a></li>
<li> 《Netty权威指南》，书籍，来自李林锋</li>
<li> 《Netty实战》，书籍，来自Norman Maurer等著，何品翻译</li>
</ol>

<h3 id="toc_18"><strong>附文中提到的一些链接地址信息</strong></h3>

<ol>
<li> Gecko: <a href="https://github.com/killme2008/gecko">https://github.com/killme2008/gecko</a></li>
<li> Mina: <a href="http://mina.apache.org/">http://mina.apache.org/</a></li>
<li> Netty: <a href="http://netty.io/">http://netty.io/</a></li>
<li> Stackoverflow - Do we need more than a single thread for boss group?：<a href="https://stackoverflow.com/questions/22280916/do-we-need-more-than-a-single-thread-for-boss-group">https://stackoverflow.com/questions/22280916/do-we-need-more-than-a-single-thread-for-boss-group</a></li>
<li> [#3218] Add ChannelPool abstraction and implementations：<a href="https://github.com/netty/netty/pull/3607">https://github.com/netty/netty/pull/3607</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[揭秘JDBC超时机制]]></title>
    <link href="http://panlw.github.io/15277833297034.html"/>
    <updated>2018-06-01T00:15:29+08:00</updated>
    <id>http://panlw.github.io/15277833297034.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzAxNjM2MTk0Ng==&amp;mid=2247484354&amp;idx=1&amp;sn=6179c839cb309b03421f39b5b65a2374&amp;chksm=9bf4b377ac833a61acd0563fd3a010a18ba7dfd749a4dac933097e153ba99aa322828510e55c&amp;mpshare=1&amp;scene=23&amp;srcid=0531q9DrnbcTctSQuYmLHhtJ%23rd">原文地址</a></p>
</blockquote>

<p>恰当的JDBC超时设置能够有效地减少服务失效的时间。本文将对数据库的各种超时设置及其设置方法做介绍。</p>

<h3 id="toc_0">真实案例：应用服务器在遭到DDos攻击后无法响应</h3>

<p>在遭到DDos攻击后，整个服务都垮掉了。由于第四层交换机不堪重负，网络变得无法连接，从而导致业务系统也无法正常运转。安全组很快屏蔽了所有的DDos攻击，并恢复了网络，但业务系统却还是无法工作。 通过分析系统的thread dump发现，业务系统停在了JDBC API的调用上。20分钟后，系统仍处于WAITING状态，无法响应。30分钟后，系统抛出异常，服务恢复正常。</p>

<blockquote>
<p>为什么我们明明将query timeout设置成了3秒，系统却持续了30分钟的WAITING状态？为什么30分钟后系统又恢复正常了？</p>
</blockquote>

<p>当你对理解了JDBC的超时设置后，就能找到问题的答案。</p>

<h3 id="toc_1">为什么我们要了解JDBC</h3>

<p>当遇到性能问题或系统出错时，业务系统和数据库通常是我们最关心的两个部分。在公司里，这两个部分是交由两个不同的部门来负责的，因此各个部门都会集中精力地在自身领域内寻找问题，这样的话，在业务系统和数据库之间的部分就会成为一个盲区。对于Java应用而言，这个盲区就是DBCP数据库连接池和JDBC，本文将集中介绍JDBC。</p>

<h3 id="toc_2">什么是JDBC</h3>

<p>JDBC是Java应用中用来连接关系型数据库的标准API。Sun公司一共定义了4种类型的JDBC，我们主要使用的是第4种，该类型的Driver完全由Java代码实现，通过使用socket与数据库进行通信。</p>

<p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt=""/></p>

<p>图1 JDBC Type 4.</p>

<p>第4种类型的JDBC通过socket对字节流进行处理，因此也会有一些基本网络操作，类似于<code>HttpClient</code>这种用于网络操作的代码库。当在网络操作中遇到问题的时候，将会消耗大量的cpu资源，并且失去响应超时。如果你之前用过HttpClient，那么你一定遇到过未设置timeout造成的错误。同样，第4种类型的JDBC，若没有合理地设置socket timeout，也会有相同的错误——连接被阻塞。</p>

<p>接下来，就让我们来学习一下如何正确地设置socket timeout，以及需要考虑的问题。</p>

<p>应用与数据库间的timeout层级 </p>

<p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt=""/></p>

<p>图2 Timeout Class.</p>

<p>上图展示了简化后应用与数据库间的timeout层级。（译者注：WAS/BLOC是作者公司的具体应用名称，无需深究） 高级别的timeout依赖于低级别的timeout，只有当低级别的timeout无误时，高级别的timeout才能确保正常。例如，当socket timeout出现问题时，高级别的statement timeout和transaction timeout都将失效。</p>

<p>我们收到的很多评论中提到：</p>

<blockquote>
<p>即使设置了statement timeout，当网络出错时，应用也无法从错误中恢复。</p>
</blockquote>

<p><strong>statement timeout无法处理网络连接失败时的超时，它能做的仅仅是限制statement的操作时间</strong>。网络连接失败时的timeout必须交由JDBC来处理。</p>

<p>JDBC的socket timeout会受到操作系统socket timeout设置的影响，这就解释了为什么在之前的案例中，JDBC连接会在网络出错后阻塞30分钟，然后又奇迹般恢复，即使我们并没有对JDBC的socket timeout进行设置。</p>

<p>DBCP连接池位于图2的左侧，你会发现timeout层级与DBCP是相互独立的。DBCP负责的是数据库连接的创建和管理，并不干涉timeout的处理。当连接在DBCP中创建，或是DBCP发送校验query检查连接有效性的时候，socket timeout将会影响这些过程，但并不直接对应用造成影响。</p>

<p>当在应用中调用DBCP的<code>getConnection()</code>方法时，你可以设置获取数据库连接的超时时间，但是这和JDBC的timeout毫不相关。</p>

<p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt=""/></p>

<p>图3 Timeout for Each Levels.</p>

<h3 id="toc_3">什么是Transaction Timeout？</h3>

<p><strong>transaction timeout</strong>一般存在于框架（Spring, EJB）或应用级。</p>

<p>transaction timeout或许是个相对陌生的概念，简单地说，transaction timeout就是“<code>Statement Timeout * N（需要执行的statement数量） + @（垃圾回收等其他时间）</code>”。transaction timeout用来限制执行statement的总时长。</p>

<p>例如，假设执行一个statement需要0.1秒，那么执行少量statement不会有什么问题，但若是要执行100,000个statement则需要10,000秒（约7个小时）。这时，transaction timeout就派上用场了。EJB CMT (Container Managed Transaction)就是一种典型的实现，它提供了多种方法供开发者选择。但我们并不使用EJB，Spring的transaction timeout设置会更常用一些。在Spring中，你可以使用下面展示的XML或是在源码中使用<code>@Transactional</code>注解来进行设置。</p>

<pre><code>&lt;tx:attributes&gt;  
        &lt;tx:method name=“…” timeout=“3″/&gt;  
&lt;/tx:attributes&gt;
</code></pre>

<p>Spring提供的transaction timeout配置非常简单，它会记录每个事务的开始时间和消耗时间，当特定的事件发生时就会对消耗时间做校验，当超出timeout值时将抛出异常。</p>

<p>Spring中，数据库连接被保存在ThreadLocal里，这被称为<code>事务同步（Transaction Synchronization）</code>，与此同时，事务的开始时间和消耗时间也被保存下来。当使用这种代理连接创建statement时，就会校验事务的消耗时间。</p>

<p>EJB CMT的实现方式与之类似，其结构本身也十分简单。 当你选用的容器或框架并不支持transaction timeout这一特性，你可以考虑自己来实现。transaction timeout并没有标准的API。Lucy框架的1.5和1.6版本都不支持transaction timeout，但是你可以通过使用Spring的Transaction Manager来达到与之同样的效果。 假设某个事务中包含5个statement，每个statement的执行时间是200ms，其他业务逻辑的执行时间是100ms，那么transaction timeout至少应该设置为1,100ms（200 * 5 + 100）。</p>

<p>什么是Statement Timeout？</p>

<p>statement timeout用来限制statement的执行时长，timeout的值通过调用JDBC的java.sql.Statement.setQueryTimeout(int timeout) API进行设置。不过现在开发者已经很少直接在代码中设置，而多是通过框架来进行设置。 以iBatis为例，statement timeout的默认值可以通过sql-map-config.xml中的defaultStatementTimeout 属性进行设置。同时，你还可以设置sqlmap中select，insert，update标签的timeout属性，从而对不同sql语句的超时时间进行独立的配置。 如果你使用的是Lucy1.5或1.6版本，通过设置queryTimeout属性可以在datasource层面对statement timeout进行设置。 statement timeout的具体值需要依据应用本身的特性而定，并没有可供推荐的配置。</p>

<p>JDBC的statement timeout处理过程<br/>
不同的关系型数据库，以及不同的JDBC驱动，其statement timeout处理过程会有所不同。其中，Oracle和MS SQLServer的处理相类似，MySQL和CUBRID类似。</p>

<p><strong>Oracle JDBC Statement的QueryTimeout处理过程</strong></p>

<ol>
<li><p>通过调用Connection的createStatement()方法创建statement</p></li>
<li><p>调用Statement的executeQuery()方法</p></li>
<li><p>statement通过自身connection将query发送给Oracle数据库</p></li>
<li><p>statement在OracleTimeoutPollingThread（每个classloader一个）上进行注册</p></li>
<li><p>达到超时时间</p></li>
<li><p>OracleTimeoutPollingThread调用OracleStatement的cancel()方法</p></li>
<li><p>通过connection向正在执行的query发送cancel消息</p></li>
</ol>

<p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt=""/></p>

<p>图4 Query Timeout Execution Process for Oracle JDBC Statement.</p>

<p><strong>JTDS (MS SQLServer) Statement的QueryTimeout处理过程</strong></p>

<ol>
<li><p>通过调用Connection的createStatement()方法创建statement</p></li>
<li><p>调用Statement的executeQuery()方法</p></li>
<li><p>statement通过自身connection将query发送给MS SqlServer数据库</p></li>
<li><p>statement在TimerThread上进行注册</p></li>
<li><p>达到超时时间</p></li>
<li><p>TimerThread调用JtdsStatement实例中的<code>TsdCore.cancel()</code>方法</p></li>
<li><p>通过ConnectionJDBC向正在执行的query发送cancel消息</p></li>
</ol>

<p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt=""/></p>

<p>图5 QueryTimeout Execution Process for JTDS (MS SQLServer) Statement.</p>

<p><strong>MySQL JDBC Statement的QueryTimeout处理过程（5.0.8）</strong></p>

<ol>
<li><p>通过调用<code>Connection.createStatement()</code>方法创建statement</p></li>
<li><p>调用<code>Statement.executeQuery()</code>方法</p></li>
<li><p>statement通过自身connection将query发送给MySQL数据库</p></li>
<li><p>statement创建一个新的timeout-execution线程用于超时处理</p></li>
<li><p>5.1版本后改为每个connection分配一个timeout-execution线程</p></li>
<li><p>向timeout-execution线程进行注册</p></li>
<li><p>达到超时时间</p></li>
<li><p>timeout-execution线程创建一个和statement配置相同的connection</p></li>
<li><p>使用新创建的connection向超时query发送cancel query（<code>KILL QUERY</code> “connectionId”）</p></li>
</ol>

<p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt=""/></p>

<p>图6 QueryTimeout Execution Process for MySQL JDBC Statement (5.0.8).</p>

<p><strong>CUBRID JDBC Statement的QueryTimeout处理过程</strong></p>

<ol>
<li><p>通过调用Connection的createStatement()方法创建statement</p></li>
<li><p>调用Statement的executeQuery()方法</p></li>
<li><p>statement通过自身connection将query发送给CUBRID数据库</p></li>
<li><p>statement创建一个新的timeout-execution线程用于超时处理</p></li>
<li><p>5.1版本后改为每个connection分配一个timeout-execution线程 6. 向timeout-execution线程进行注册</p></li>
<li><p>达到超时时间</p></li>
<li><p>TimerThread调用JtdsStatement实例中的TsdCore.cancel()方法</p></li>
<li><p>timeout-execution线程创建一个和statement配置相同的connection</p></li>
<li><p>使用新创建的connection向超时query发送cancel消息</p></li>
</ol>

<p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt=""/></p>

<p>图7 QueryTimeout Execution Process for CUBRID JDBC Statement.</p>

<p>什么是JDBC的socket timeout？</p>

<p>第4种类型的JDBC使用socket与数据库连接，数据库并不对应用与数据库间的连接超时进行处理。 JDBC的socket timeout在数据库被突然停掉或是发生网络错误（由于设备故障等原因）时十分重要。由于TCP/IP的结构原因，socket没有办法探测到网络错误，因此应用也无法主动发现数据库连接断开。如果没有设置socket timeout的话，应用在数据库返回结果前会无期限地等下去，这种连接被称为dead connection。 为了避免dead connections，socket必须要有超时配置。socket timeout可以通过JDBC设置，socket timeout能够避免应用在发生网络错误时产生无休止等待的情况，缩短服务失效的时间。</p>

<p>不推荐使用socket timeout来限制statement的执行时长，因此socket timeout的值必须要高于statement timeout，否则，socket timeout将会先生效，这样statement timeout就变得毫无意义，也无法生效。</p>

<p>下面展示了socket timeout的两个设置项，不同的JDBC驱动其配置方式会有所不同。</p>

<ul>
<li><p>socket连接时的timeout：通过Socket.connect(SocketAddress endpoint, int timeout)设置</p></li>
<li><p>socket读写时的timeout：通过Socket.setSoTimeout(int timeout)设置</p></li>
</ul>

<p>通过查看CUBRID，MySQL，MS SQL Server (JTDS)和Oracle的JDBC驱动源码，我们发现所有的驱动内部都是使用上面的2个API来设置socket timeout的。</p>

<p>下面是不同驱动的socket timeout配置方式。</p>

<p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt=""/></p>

<ul>
<li><p><code>connectTimeout</code>和<code>socketTimeout</code>的默认值为0时，timeout不生效。</p></li>
<li><p>除了调用DBCP的API以外，还可以通过properties属性进行配置。</p></li>
</ul>

<p>通过properties属性进行配置时，需要传入key为“connectionProperties”的键值对，value的格式为“[propertyName=property;]*”。下面是iBatis中的properties配置。<br/>
Xml代码</p>

<pre><code>&lt;transactionManager type=“JDBC”&gt;  
  &lt;dataSource type=“com.nhncorp.lucy.db.DbcpDSFactory”&gt;  
     ….  
     &lt;property name=“connectionProperties” value=“oracle.net.CONNECT_TIMEOUT=6000;oracle.jdbc.ReadTimeout=6000″/&gt;   
  &lt;/dataSource&gt;  
&lt;/transactionManager&gt;
</code></pre>

<p>操作系统的socket timeout配置</p>

<p>如果不设置socket timeout或connect timeout，应用多数情况下是无法发现网络错误的。因此，当网络错误发生后，在连接重新连接成功或成功接收到数据之前，应用会无限制地等下去。但是，通过本文开篇处的实际案例我们发现，30分钟后应用的连接问题奇迹般的解决了，这是因为操作系统同样能够对socket timeout进行配置。公司的Linux服务器将socket timeout设置为了30分钟，从而会在操作系统的层面对网络连接做校验，因此即使JDBC的socket timeout设置为0，由网络错误造成的数据库连接问题的持续时间也不会超过30分钟。</p>

<p>通常，应用会在调用Socket.read()时由于网络问题被阻塞住，而很少在调用Socket.write()时进入waiting状态，这取决于网络构成和错误类型。当Socket.write()被调用时，数据被写入到操作系统内核的缓冲区，控制权立即回到应用手上。因此，一旦数据被写入内核缓冲区，Socket.write()调用就必然会成功。但是，如果系统内核缓冲区由于某种网络错误而满了的话，Socket.write()也会进入waiting状态。这种情况下，操作系统会尝试重新发包，当达到重试的时间限制时，将产生系统错误。在我们公司，重新发包的超时时间被设置为15分钟。</p>

<p>至此，我已经对JDBC的内部操作做了讲解，希望能够让大家学会如何正确的配置超时时间，从而减少错误的发生。<br/>
最后，我将列出一些常见的问题。</p>

<h3 id="toc_4">FAQ</h3>

<blockquote>
<p><strong>Q1. 我已经使用Statement.setQueryTimeout()方法设置了查询超时，但在网络出错时并没有产生作用。</strong><br/>
➔ 查询超时仅在socket timeout生效的前提下才有效，它并不能用来解决外部的网络错误，要解决这种问题，必须设置JDBC的socket timeout。</p>

<p><strong>Q2.  transaction timeout，statement timeout和socket timeout和DBCP的配置有什么关系？</strong> <br/>
➔ 当通过DBCP获取数据库连接时，除了DBCP获取连接时的waitTimeout配置以外，其他配置对JDBC没有什么影响。</p>

<p><strong>Q3. 如果设置了JDBC的socket timeout，那DBCP连接池中处于IDLE状态的连接是否也会在达到超时时间后被关闭？</strong><br/>
➔ 不会。socket的设置只会在产生数据读写时生效，而不会对DBCP中的IDLE连接产生影响。当DBCP中发生新连接创建，老的IDLE连接被移除，或是连接有效性校验的时候，socket设置会对其产生一定的影响，但除非发生网络问题，否则影响很小。</p>

<p><strong>Q4. socket timeout应该设置为多少？ </strong><br/>
➔ 就像我在正文中提的那样，socket timeout必须高于statement timeout，但并没有什么推荐值。在发生网络错误的时候，socket timeout将会生效，但是再小心的配置也无法避免网络错误的发生，只是在网络错误发生后缩短服务失效的时间（如果网络恢复正常的话）。</p>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[【原创】分布式之redis复习精讲]]></title>
    <link href="http://panlw.github.io/15277832551515.html"/>
    <updated>2018-06-01T00:14:15+08:00</updated>
    <id>http://panlw.github.io/15277832551515.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="http://www.cnblogs.com/rjzheng/p/9096228.html">http://www.cnblogs.com/rjzheng/p/9096228.html</a></p>
</blockquote>

<h2 id="toc_0">引言</h2>

<h3 id="toc_1">为什么写这篇文章?</h3>

<p>博主的<a href="http://www.cnblogs.com/rjzheng/p/8994962.html">《分布式之消息队列复习精讲》</a>得到了大家的好评，内心诚惶诚恐，想着再出一篇关于复习精讲的文章。但是还是要说明一下，复习精讲的文章偏面试准备，真正在开发过程中，还是脚踏实地，一步一个脚印，不要投机取巧。<br/>
考虑到绝大部分写业务的程序员，在实际开发中使用 redis 的时候，只会 setvalue 和 getvalue 两个操作，对 redis 整体缺乏一个认知。又恰逢博主某个同事下周要去培训 redis，所以博主斗胆以 redis 为题材，对 redis 常见问题做一个总结，希望能够弥补大家的知识盲点。</p>

<h3 id="toc_2">复习要点?</h3>

<p>本文围绕以下几点进行阐述<br/>
1、为什么使用 redis<br/>
2、使用 redis 有什么缺点<br/>
3、单线程的 redis 为什么这么快<br/>
4、redis 的数据类型，以及每种数据类型的使用场景<br/>
5、redis 的过期策略以及内存淘汰机制<br/>
6、redis 和数据库双写一致性问题<br/>
7、如何应对缓存穿透和缓存雪崩问题<br/>
8、如何解决 redis 的并发竞争问题</p>

<h2 id="toc_3">正文</h2>

<h3 id="toc_4">1、为什么使用 redis</h3>

<p><strong>分析</strong>: 博主觉得在项目中使用 redis，主要是从两个角度去考虑: <strong>性能</strong>和<strong>并发</strong>。当然，redis 还具备可以做分布式锁等其他功能，但是如果只是为了分布式锁这些其他功能，完全还有其他中间件 (如 zookpeer 等) 代替，并不是非要使用 redis。因此，这个问题主要从性能和并发两个角度去答。<br/>
<strong>回答</strong>: 如下所示，分为两点<br/>
<strong>（一）性能</strong><br/>
如下图所示，我们在碰到需要执行耗时特别久，且结果不频繁变动的 SQL，就特别适合将运行结果放入缓存。这样，后面的请求就去缓存中读取，使得请求能够<strong>迅速响应</strong>。<br/>
<img src="https://images.cnblogs.com/cnblogs_com/rjzheng/1202350/o_redis1.png" alt=""/><br/>
<strong>题外话：</strong>忽然想聊一下这个<strong>迅速响应</strong>的标准。其实根据交互效果的不同，这个响应时间没有固定标准。不过曾经有人这么告诉我:&quot; 在理想状态下，我们的页面跳转需要在<strong>瞬间</strong>解决，对于页内操作则需要在<strong>刹那</strong>间解决。另外，超过<strong>一弹指</strong>的耗时操作要有进度提示，并且可以随时中止或取消，这样才能给用户最好的体验。&quot;<br/>
那么<strong>瞬间、刹那、一弹指</strong>具体是多少时间呢？<br/>
根据《摩诃僧祗律》记载</p>

<pre><code>一刹那者为一念，二十念为一瞬，二十瞬为一弹指，二十弹指为一罗预，二十罗预为一须臾，一日一夜有三十须臾。
</code></pre>

<p>那么，经过周密的计算，一<strong>瞬间</strong>为 0.36 秒, 一<strong>刹那</strong>有 0.018 秒. 一<strong>弹指</strong>长达 7.2 秒。<br/>
<strong>（二）并发</strong><br/>
如下图所示，在大并发的情况下，所有的请求直接访问数据库，数据库会出现连接异常。这个时候，就需要使用 redis 做一个缓冲操作，让请求先访问到 redis，而不是直接访问数据库。<br/>
<img src="https://images.cnblogs.com/cnblogs_com/rjzheng/1202350/o_redis2.png" alt=""/></p>

<h3 id="toc_5">2、使用 redis 有什么缺点</h3>

<p><strong>分析</strong>: 大家用 redis 这么久，这个问题是必须要了解的，基本上使用 redis 都会碰到一些问题，常见的也就几个。<br/>
<strong>回答</strong>: 主要是四个问题<br/>
(一) 缓存和数据库双写一致性问题<br/>
(二) 缓存雪崩问题<br/>
(三) 缓存击穿问题<br/>
(四) 缓存的并发竞争问题<br/>
这四个问题，我个人是觉得在项目中，比较常遇见的，具体解决方案，后文给出。</p>

<h3 id="toc_6">3、单线程的 redis 为什么这么快</h3>

<p><strong>分析</strong>: 这个问题其实是对 redis 内部机制的一个考察。其实根据博主的面试经验，很多人其实都不知道 redis 是单线程工作模型。所以，这个问题还是应该要复习一下的。<br/>
<strong>回答</strong>: 主要是以下三点<br/>
(一) 纯内存操作<br/>
(二) 单线程操作，避免了频繁的上下文切换<br/>
(三) 采用了非阻塞 <strong>I/O 多路复用机制</strong></p>

<p><strong>题外话：</strong>我们现在要仔细的说一说 I/O 多路复用机制，因为这个说法实在是太通俗了，通俗到一般人都不懂是什么意思。博主打一个比方：小曲在 S 城开了一家快递店，负责同城快送服务。小曲因为资金限制，雇佣了<strong>一批</strong>快递员，然后小曲发现资金不够了，只够买<strong>一辆</strong>车送快递。<br/>
<strong>经营方式一</strong><br/>
客户每送来一份快递，小曲就让一个快递员盯着，然后快递员开车去送快递。慢慢的小曲就发现了这种经营方式存在下述问题</p>

<ul>
<li>  几十个快递员基本上时间都花在了抢车上了，大部分快递员都处在闲置状态，谁抢到了车，谁就能去送快递</li>
<li>  随着快递的增多，快递员也越来越多，小曲发现快递店里越来越挤，没办法雇佣新的快递员了</li>
<li>  快递员之间的协调很花时间</li>
</ul>

<p>综合上述缺点，小曲痛定思痛，提出了下面的经营方式<br/>
<strong>经营方式二</strong><br/>
小曲只雇佣一个快递员。然后呢，客户送来的快递，小曲按<strong>送达地点</strong>标注好，然后<strong>依次</strong>放在一个地方。最后，那个快递员<strong>依次</strong>的去取快递，一次拿一个，然后开着车去送快递，送好了就回来拿下一个快递。</p>

<p><strong>对比</strong><br/>
上述两种经营方式对比，是不是明显觉得第二种，效率更高，更好呢。在上述比喻中:</p>

<ul>
<li>  每个快递员 ------------------&gt; 每个线程</li>
<li>  每个快递 --------------------&gt; 每个 socket(I/O 流)</li>
<li>  快递的送达地点 --------------&gt;socket 的不同状态</li>
<li>  客户送快递请求 --------------&gt; 来自客户端的请求</li>
<li>  小曲的经营方式 --------------&gt; 服务端运行的代码</li>
<li>  一辆车 ----------------------&gt;CPU 的核数</li>
</ul>

<p>于是我们有如下结论<br/>
1、经营方式一就是传统的并发模型，每个 I/O 流 (快递) 都有一个新的线程 (快递员) 管理。<br/>
2、经营方式二就是 I/O 多路复用。只有单个线程 (一个快递员)，通过跟踪每个 I/O 流的状态 (每个快递的送达地点)，来管理多个 I/O 流。</p>

<p>下面类比到真实的 redis 线程模型，如图所示<br/>
<img src="https://images.cnblogs.com/cnblogs_com/rjzheng/1202350/o_redis3.png" alt=""/><br/>
参照上图，简单来说，就是。我们的 redis-client 在操作的时候，会产生具有不同事件类型的 socket。在服务端，有一段 I/0 多路复用程序，将其置入队列之中。然后，文件事件分派器，依次去队列中取，转发到不同的事件处理器中。<br/>
需要说明的是，这个 I/O 多路复用机制，redis 还提供了 select、epoll、evport、kqueue 等多路复用函数库，大家可以自行去了解。</p>

<h3 id="toc_7">4、redis 的数据类型，以及每种数据类型的使用场景</h3>

<p><strong>分析</strong>：是不是觉得这个问题很基础，其实我也这么觉得。然而根据面试经验发现，至少百分八十的人答不上这个问题。建议，在项目中用到后，再类比记忆，体会更深，不要硬记。基本上，一个合格的程序员，五种类型都会用到。<br/>
<strong>回答</strong>：一共五种<br/>
(一)String<br/>
这个其实没啥好说的，最常规的 set/get 操作，value 可以是 String 也可以是数字。一般做<strong>一些复杂的计数功能的缓存。</strong><br/>
(二)hash<br/>
这里 value 存放的是结构化的对象，比较方便的就是操作其中的某个字段。博主在做<strong>单点登录</strong>的时候，就是用这种数据结构存储用户信息，以 cookieId 作为 key，设置 30 分钟为缓存过期时间，能很好的模拟出类似 session 的效果。<br/>
(三)list<br/>
使用 List 的数据结构，可以<strong>做简单的消息队列的功能</strong>。另外还有一个就是，可以利用 lrange 命令，<strong>做基于 redis 的分页功能</strong>，性能极佳，用户体验好。<br/>
(四)set<br/>
因为 set 堆放的是一堆不重复值的集合。所以可以做<strong>全局去重的功能</strong>。为什么不用 JVM 自带的 Set 进行去重？因为我们的系统一般都是集群部署，使用 JVM 自带的 Set，比较麻烦，难道为了一个做一个全局去重，再起一个公共服务，太麻烦了。<br/>
另外，就是利用交集、并集、差集等操作，可以<strong>计算共同喜好，全部的喜好，自己独有的喜好等功能</strong>。<br/>
(五)sorted set<br/>
sorted set 多了一个权重参数 score, 集合中的元素能够按 score 进行排列。可以做<strong>排行榜应用，取 TOP N 操作</strong>。另外，参照另一篇<a href="https://www.cnblogs.com/rjzheng/p/8972725.html">《分布式之延时任务方案解析》</a>，该文指出了 sorted set 可以用来做<strong>延时任务</strong>。最后一个应用就是可以做<strong>范围查找</strong>。</p>

<h3 id="toc_8">5、redis 的过期策略以及内存淘汰机制</h3>

<p><strong>分析</strong>: 这个问题其实相当重要，到底 redis 有没用到家，这个问题就可以看出来。比如你 redis 只能存 5G 数据，可是你写了 10G，那会删 5G 的数据。怎么删的，这个问题思考过么？还有，你的数据已经设置了过期时间，但是时间到了，内存占用率还是比较高，有思考过原因么?<br/>
<strong>回答</strong>:<br/>
redis 采用的是定期删除 + 惰性删除策略。<br/>
<strong>为什么不用定时删除策略?</strong><br/>
定时删除, 用一个定时器来负责监视 key, 过期则自动删除。虽然内存及时释放，但是十分消耗 CPU 资源。在大并发请求下，CPU 要将时间应用在处理请求，而不是删除 key, 因此没有采用这一策略.<br/>
<strong>定期删除 + 惰性删除是如何工作的呢?</strong><br/>
定期删除，redis 默认每个 100ms 检查，是否有过期的 key, 有过期 key 则删除。需要说明的是，redis 不是每个 100ms 将所有的 key 检查一次，而是随机抽取进行检查 (如果每隔 100ms, 全部 key 进行检查，redis 岂不是卡死)。因此，如果只采用定期删除策略，会导致很多 key 到时间没有删除。<br/>
于是，惰性删除派上用场。也就是说在你获取某个 key 的时候，redis 会检查一下，这个 key 如果设置了过期时间那么是否过期了？如果过期了此时就会删除。<br/>
<strong>采用定期删除 + 惰性删除就没其他问题了么?</strong><br/>
不是的，如果定期删除没删除 key。然后你也没即时去请求 key，也就是说惰性删除也没生效。这样，redis 的内存会越来越高。那么就应该采用<strong>内存淘汰机制</strong>。<br/>
在 redis.conf 中有一行配置</p>

<pre><code># maxmemory-policy volatile-lru
</code></pre>

<p>该配置就是配内存淘汰策略的 (什么，你没配过？好好反省一下自己)<br/>
1）noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。<strong>应该没人用吧。</strong><br/>
2）allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key。<strong>推荐使用，目前项目在用这种。</strong><br/>
3）allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个 key。<strong>应该也没人用吧，你不删最少使用 Key, 去随机删。</strong><br/>
4）volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key。<strong>这种情况一般是把 redis 既当缓存，又做持久化存储的时候才用。不推荐</strong><br/>
5）volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 key。<strong>依然不推荐</strong><br/>
6）volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除。<strong>不推荐</strong><br/>
ps：如果没有设置 expire 的 key, 不满足先决条件 (prerequisites); 那么 volatile-lru, volatile-random 和 volatile-ttl 策略的行为, 和 noeviction(不删除) 基本上一致。</p>

<h3 id="toc_9">6、redis 和数据库双写一致性问题</h3>

<p><strong>分析</strong>: 一致性问题是分布式常见问题，还可以再分为最终一致性和强一致性。数据库和缓存双写，就必然会存在不一致的问题。答这个问题，先明白一个前提。就是<strong>如果对数据有强一致性要求，不能放缓存。</strong>我们所做的一切，只能保证最终一致性。另外，我们所做的方案其实从根本上来说，只能说<strong>降低不一致发生的概率</strong>，无法完全避免。因此，有强一致性要求的数据，不能放缓存。<br/>
<strong>回答</strong>:<a href="https://www.cnblogs.com/rjzheng/p/9041659.html">《分布式之数据库和缓存双写一致性方案解析》</a>给出了详细的分析，在这里简单的说一说。首先，采取正确更新策略，先更新数据库，再删缓存。其次，因为可能存在删除缓存失败的问题，提供一个补偿措施即可，例如利用消息队列。</p>

<h3 id="toc_10">7、如何应对缓存穿透和缓存雪崩问题</h3>

<p><strong>分析</strong>: 这两个问题，说句实在话，一般中小型传统软件企业，很难碰到这个问题。如果有大并发的项目，流量有几百万左右。这两个问题一定要深刻考虑。<br/>
<strong>回答</strong>: 如下所示<br/>
<strong>缓存穿透</strong>，即黑客故意去请求缓存中不存在的数据，导致所有的请求都怼到数据库上，从而数据库连接异常。<br/>
<strong>解决方案</strong>:<br/>
(一) 利用互斥锁，缓存失效的时候，先去获得锁，得到锁了，再去请求数据库。没得到锁，则休眠一段时间重试<br/>
(二) 采用异步更新策略，无论 key 是否取到值，都直接返回。value 值中维护一个缓存失效时间，缓存如果过期，异步起一个线程去读数据库，更新缓存。需要做<strong>缓存预热</strong> (项目启动前，先加载缓存) 操作。<br/>
(三) 提供一个能迅速判断请求是否有效的拦截机制，比如，利用布隆过滤器，内部维护一系列合法有效的 key。迅速判断出，请求所携带的 Key 是否合法有效。如果不合法，则直接返回。<br/>
<strong>缓存雪崩</strong>，即缓存同一时间大面积的失效，这个时候又来了一波请求，结果请求都怼到数据库上，从而导致数据库连接异常。<br/>
<strong>解决方案</strong>:<br/>
(一) 给缓存的失效时间，加上一个随机值，避免集体失效。<br/>
(二) 使用互斥锁，但是该方案吞吐量明显下降了。<br/>
(三) 双缓存。我们有两个缓存，缓存 A 和缓存 B。缓存 A 的失效时间为 20 分钟，缓存 B 不设失效时间。自己做缓存预热操作。然后细分以下几个小点</p>

<ul>
<li>  I 从缓存 A 读数据库，有则直接返回</li>
<li>  II A 没有数据，直接从 B 读数据，直接返回，并且异步启动一个更新线程。</li>
<li>  III 更新线程同时更新缓存 A 和缓存 B。</li>
</ul>

<h3 id="toc_11">8、如何解决 redis 的并发竞争 key 问题</h3>

<p><strong>分析</strong>: 这个问题大致就是，同时有多个子系统去 set 一个 key。这个时候要注意什么呢？大家思考过么。需要说明一下，博主提前百度了一下，发现答案基本都是推荐用 redis 事务机制。博主<strong>不推荐使用 redis 的事务机制。</strong>因为我们的生产环境，基本都是 redis 集群环境，做了数据分片操作。你一个事务中有涉及到多个 key 操作的时候，这多个 key 不一定都存储在同一个 redis-server 上。因此，<strong>redis 的事务机制，十分鸡肋。</strong><br/>
<strong>回答:</strong> 如下所示<br/>
(1) 如果对这个 key 操作，<strong>不要求顺序</strong><br/>
这种情况下，准备一个分布式锁，大家去抢锁，抢到锁就做 set 操作即可，比较简单。<br/>
(2) 如果对这个 key 操作，<strong>要求顺序</strong><br/>
假设有一个 key1, 系统 A 需要将 key1 设置为 valueA, 系统 B 需要将 key1 设置为 valueB, 系统 C 需要将 key1 设置为 valueC.<br/>
期望按照 key1 的 value 值按照 valueA--&gt;valueB--&gt;valueC 的顺序变化。这种时候我们在数据写入数据库的时候，需要保存一个时间戳。假设时间戳如下</p>

<pre><code>系统A key 1 {valueA  3:00}
系统B key 1 {valueB  3:05}
系统C key 1 {valueC  3:10}
</code></pre>

<p>那么，假设这会系统 B 先抢到锁，将 key1 设置为 {valueB 3:05}。接下来系统 A 抢到锁，发现自己的 valueA 的时间戳早于缓存中的时间戳，那就不做 set 操作了。以此类推。</p>

<p>其他方法，比如利用队列，将 set 方法变成串行访问也可以。总之，灵活变通。</p>

<h2 id="toc_12">总结</h2>

<p>本文对 redis 的常见问题做了一个总结。大部分是博主自己在工作中遇到，以及以前面试别人的时候，爱问的一些问题。另外，<strong>不推荐大家临时抱佛脚</strong>，真正碰到一些有经验的工程师，其实几下就能把你问懵。最后，希望大家有所收获吧。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[【RPC 专栏】深入理解 RPC 之动态代理篇]]></title>
    <link href="http://panlw.github.io/15277824520436.html"/>
    <updated>2018-06-01T00:00:52+08:00</updated>
    <id>http://panlw.github.io/15277824520436.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="https://www.cnkirito.moe/rpc-dynamic-proxy/">https://www.cnkirito.moe/rpc-dynamic-proxy/</a></p>
</blockquote>

<p>提到 JAVA 中的动态代理，大多数人都不会对 JDK 动态代理感到陌生，Proxy，InvocationHandler 等类都是 J2SE 中的基础概念。动态代理发生在服务调用方/客户端，RPC 框架需要解决的一个问题是：像调用本地接口一样调用远程的接口。于是如何组装数据报文，经过网络传输发送至服务提供方，屏蔽远程接口调用的细节，便是动态代理需要做的工作了。RPC 框架中的代理层往往是单独的一层，以方便替换代理方式（如 motan 代理层位于<code>com.weibo.api.motan.proxy</code> ，dubbo代理层位于 <code>com.alibaba.dubbo.common.bytecode</code> ）。</p>

<p>实现动态代理的方案有下列几种：</p>

<ul>
<li>  jdk 动态代理</li>
<li>  cglib 动态代理</li>
<li>  javassist 动态代理</li>
<li>  ASM 字节码</li>
<li>  javassist 字节码</li>
</ul>

<p>其中 cglib 底层实现依赖于 ASM，javassist 自成一派。由于 ASM 和 javassist 需要程序员直接操作字节码，导致使用门槛相对较高，但实际上他们的应用是非常广泛的，如 Hibernate 底层使用了 javassist（默认）和 cglib，Spring 使用了 cglib 和 jdk 动态代理。</p>

<p>RPC 框架无论选择何种代理技术，所需要完成的任务其实是固定的，不外乎‘整理报文’，‘确认网络位置’，‘序列化’,’网络传输’，‘反序列化’，’返回结果’…</p>

<h2 id="toc_0"><a href="#%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B%E7%9A%84%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0" title="技术选型的影响因素"></a>技术选型的影响因素</h2>

<p>框架中使用何种动态代理技术，影响因素也不少。</p>

<h3 id="toc_1"><a href="#%E6%80%A7%E8%83%BD" title="性能"></a>性能</h3>

<p>从早期 dubbo 的作者梁飞的博客 <a href="http://javatar.iteye.com/blog/814426">http://javatar.iteye.com/blog/814426</a> 中可以得知 dubbo 选择使用 javassist 作为动态代理方案主要考虑的因素是<strong>性能</strong>。</p>

<p>从其博客的测试结果来看 javassist &gt; cglib &gt; jdk 。但实际上他的测试过程稍微有点瑕疵：在 cglib 和 jdk 代理对象调用时，走的是反射调用，而在 javassist 生成的代理对象调用时，走的是直接调用（可以先阅读下梁飞大大的博客）。这意味着 cglib 和 jdk 慢的原因并不是由动态代理产生的，而是由反射调用产生的（顺带一提，很多人认为 jdk 动态代理的原理是反射，其实它的底层也是使用的字节码技术）。而最终我的测试结果，结论如下： javassist ≈ cglib &gt; jdk 。javassist 和 cglib 的效率基本持平 ，而他们两者的执行效率基本可以达到 jdk 动态代理的2倍（这取决于测试的机器以及 jdk 的版本，jdk1.8 相较于 jdk1.6 动态代理技术有了质的提升，所以并不是传闻中的那样：cglib 比 jdk 快 10倍）。文末会给出我的测试代码。</p>

<h3 id="toc_2"><a href="#%E4%BE%9D%E8%B5%96" title="依赖"></a>依赖</h3>

<blockquote>
<p>motan默认的实现是jdk动态代理，代理方案支持SPI扩展，可以自行扩展其他实现方式。</p>

<p>使用jdk做为默认，主要是减少core包依赖，性能不是唯一考虑因素。另外使用字节码方式javaassist性能比较优秀，动态代理模式下jdk性能也不会差多少。</p>

<p>– <strong>rayzhang0603</strong>(motan贡献者)</p>
</blockquote>

<p>motan 选择使用 jdk 动态代理，原因主要有两个：减少 motan-core 的依赖，方便。至于扩展性，dubbo 并没有预留出动态代理的扩展接口，而是写死了 bytecode ，这点上 motan 做的较好。</p>

<h3 id="toc_3"><a href="#%E6%98%93%E7%94%A8%E6%80%A7" title="易用性"></a>易用性</h3>

<p>从 dubbo 和 motan 的源码中便可以直观的看出两者的差距了，dubbo 为了使用 javassist 技术花费不少的精力，而 motan 使用 jdk 动态代理只用了一个类。dubbo 的设计者为了追求极致的性能而做出的工作是值得肯定的，motan 也预留了扩展机制，两者各有千秋。</p>

<h2 id="toc_4"><a href="#%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97" title="动态代理入门指南"></a>动态代理入门指南</h2>

<p>为了方便对比几种动态代理技术，先准备一个统一接口。</p>

<pre><code class="language-java">public interface BookApi {
    void sell();
}
</code></pre>

<h3 id="toc_5"><a href="#JDK%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86" title="JDK动态代理"></a>JDK动态代理</h3>

<pre><code class="language-java">private static BookApi createJdkDynamicProxy(final BookApi delegate) {
        BookApi jdkProxy = (BookApi) Proxy.newProxyInstance(ClassLoader.getSystemClassLoader(),
                new Class[]{BookApi.class}, new JdkHandler(delegate));
        return jdkProxy;
}

private static class JdkHandler implements InvocationHandler {

        final Object delegate;

        JdkHandler(Object delegate) {
            this.delegate = delegate;
        }

        @Override
        public Object invoke(Object object, Method method, Object[] objects)
                throws Throwable {
            //添加代理逻辑&lt;1&gt;
            if(method.getName().equals(&quot;sell&quot;)){
                System.out.print(&quot;&quot;);
            }
            return null;
//            return method.invoke(delegate, objects);
        }
</code></pre>

<p><1> 在真正的 RPC 调用中 ，需要填充‘整理报文’，‘确认网络位置’，‘序列化’,’网络传输’，‘反序列化’，’返回结果’等逻辑。</p>

<h3 id="toc_6"><a href="#Cglib%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86" title="Cglib动态代理"></a>Cglib动态代理</h3>

<pre><code class="language-java">private static BookApi createCglibDynamicProxy(final BookApi delegate) throws Exception {
        Enhancer enhancer = new Enhancer();
        enhancer.setCallback(new CglibInterceptor(delegate));
        enhancer.setInterfaces(new Class[]{BookApi.class});
        BookApi cglibProxy = (BookApi) enhancer.create();
        return cglibProxy;
    }

    private static class CglibInterceptor implements MethodInterceptor {

        final Object delegate;

        CglibInterceptor(Object delegate) {
            this.delegate = delegate;
        }

        @Override
        public Object intercept(Object object, Method method, Object[] objects,
                                MethodProxy methodProxy) throws Throwable {
            //添加代理逻辑
            if(method.getName().equals(&quot;sell&quot;)) {
                System.out.print(&quot;&quot;);
            }
            return null;
//            return methodProxy.invoke(delegate, objects);
        }
    }
</code></pre>

<p>和 JDK 动态代理的操作步骤没有太大的区别，只不过是替换了 cglib 的API而已。</p>

<p>需要引入 cglib 依赖：</p>

<pre><code class="language-xml">&lt;dependency&gt;
    &lt;groupId&gt;cglib&lt;/groupId&gt;
    &lt;artifactId&gt;cglib&lt;/artifactId&gt;
    &lt;version&gt;3.2.5&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<h3 id="toc_7"><a href="#Javassist%E5%AD%97%E8%8A%82%E7%A0%81" title="Javassist字节码"></a>Javassist字节码</h3>

<p>到了 javassist，稍微有点不同了。因为它是通过直接操作字节码来生成代理对象。</p>

<pre><code class="language-java">private static BookApi createJavassistBytecodeDynamicProxy() throws Exception {
    ClassPool mPool = new ClassPool(true);
    CtClass mCtc = mPool.makeClass(BookApi.class.getName() + &quot;JavaassistProxy&quot;);
    mCtc.addInterface(mPool.get(BookApi.class.getName()));
    mCtc.addConstructor(CtNewConstructor.defaultConstructor(mCtc));
    mCtc.addMethod(CtNewMethod.make(
            &quot;public void sell() { System.out.print(\&quot;\&quot;) ; }&quot;, mCtc));
    Class&lt;?&gt; pc = mCtc.toClass();
    BookApi bytecodeProxy = (BookApi) pc.newInstance();
    return bytecodeProxy;
}
</code></pre>

<p>需要引入 javassist 依赖：</p>

<pre><code class="language-xml">&lt;dependency&gt;
    &lt;groupId&gt;org.javassist&lt;/groupId&gt;
    &lt;artifactId&gt;javassist&lt;/artifactId&gt;
    &lt;version&gt;3.21.0-GA&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<h2 id="toc_8"><a href="#%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E6%B5%8B%E8%AF%95" title="动态代理测试"></a>动态代理测试</h2>

<p>测试环境：window i5 8g jdk1.8 cglib3.2.5 javassist3.21.0-GA</p>

<p>动态代理其实分成了两步：代理对象的创建，代理对象的调用。坊间流传的动态代理性能对比主要指的是后者；前者一般不被大家考虑，如果远程Refer的对象是单例的，其只会被创建一次，而如果是原型模式，多例对象的创建其实也是性能损耗的一个考虑因素（只不过远没有调用占比大）。</p>

<blockquote>
<p>Create JDK Proxy: 21 ms</p>

<p>Create CGLIB Proxy: 342 ms</p>

<p>Create Javassist Bytecode Proxy: 419 ms</p>
</blockquote>

<p>可能出乎大家的意料，JDK 创建动态代理的速度比后两者要快10倍左右。</p>

<p>下面是调用速度的测试：</p>

<blockquote>
<p>case 1:</p>

<p>JDK Proxy invoke cost 1912 ms</p>

<p>CGLIB Proxy invoke cost 1015 ms</p>

<p>JavassistBytecode Proxy invoke cost 1280 ms</p>

<p>case 2:</p>

<p>JDK Proxy invoke cost 1747 ms</p>

<p>CGLIB Proxy invoke cost 1234 ms</p>

<p>JavassistBytecode Proxy invoke cost 1175 ms</p>

<p>case 3:</p>

<p>JDK Proxy invoke cost 2616 ms</p>

<p>CGLIB Proxy invoke cost 1373 ms</p>

<p>JavassistBytecode Proxy invoke cost 1335 ms</p>
</blockquote>

<p>Jdk 的执行速度一定会慢于 Cglib 和 Javassist，但最慢也就2倍，并没有达到数量级的差距；Cglib 和 Javassist不相上下，差距不大（测试中偶尔发现Cglib实行速度会比平时慢10倍，不清楚是什么原因）</p>

<p>所以出于易用性和性能，私以为使用 Cglib 是一个很好的选择（性能和 Javassist 持平，易用性和 Jdk 持平）。</p>

<h2 id="toc_9"><a href="#%E5%8F%8D%E5%B0%84%E8%B0%83%E7%94%A8" title="反射调用"></a>反射调用</h2>

<p>既然提到了动态代理和 cglib ，顺带提一下反射调用如何加速的问题。RPC 框架中在 Provider 服务端需要根据客户端传递来的 className + method + param 来找到容器中的实际方法执行反射调用。除了反射调用外，还可以使用 Cglib 来加速。</p>

<h3 id="toc_10"><a href="#JDK%E5%8F%8D%E5%B0%84%E8%B0%83%E7%94%A8" title="JDK反射调用"></a>JDK反射调用</h3>

<pre><code class="language-java">Method method = serviceClass.getMethod(methodName, new Class[]{});
method.invoke(delegate, new Object[]{});
</code></pre>

<h3 id="toc_11"><a href="#Cglib%E8%B0%83%E7%94%A8" title="Cglib调用"></a>Cglib调用</h3>

<pre><code class="language-java">FastClass serviceFastClass = FastClass.create(serviceClass);
FastMethod serviceFastMethod = serviceFastClass.getMethod(methodName, new Class[]{});
serviceFastMethod.invoke(delegate, new Object[]{});
</code></pre>

<p>但实测效果发现 Cglib 并不一定比 JDK 反射执行速度快，还会跟具体的方法实现有关(大雾)。</p>

<h2 id="toc_12"><a href="#%E6%B5%8B%E8%AF%95%E4%BB%A3%E7%A0%81" title="测试代码"></a>测试代码</h2>

<pre><code class="language-java">public class Main {

    public static void main(String[] args) throws Exception {

        BookApi delegate = new BookApiImpl();
        long time = System.currentTimeMillis();
        BookApi jdkProxy = createJdkDynamicProxy(delegate);
        time = System.currentTimeMillis() - time;
        System.out.println(&quot;Create JDK Proxy: &quot; + time + &quot; ms&quot;);

        time = System.currentTimeMillis();
        BookApi cglibProxy = createCglibDynamicProxy(delegate);
        time = System.currentTimeMillis() - time;
        System.out.println(&quot;Create CGLIB Proxy: &quot; + time + &quot; ms&quot;);

        time = System.currentTimeMillis();
        BookApi javassistBytecodeProxy = createJavassistBytecodeDynamicProxy();
        time = System.currentTimeMillis() - time;
        System.out.println(&quot;Create JavassistBytecode Proxy: &quot; + time + &quot; ms&quot;);

        for (int i = 0; i &lt; 10; i++) {
            jdkProxy.sell();//warm
        }
        long start = System.currentTimeMillis();
        for (int i = 0; i &lt; 10000000; i++) {
            jdkProxy.sell();
        }
        System.out.println(&quot;JDK Proxy invoke cost &quot; + (System.currentTimeMillis() - start) + &quot; ms&quot;);

        for (int i = 0; i &lt; 10; i++) {
            cglibProxy.sell();//warm
        }
        start = System.currentTimeMillis();
        for (int i = 0; i &lt; 10000000; i++) {
            cglibProxy.sell();
        }
        System.out.println(&quot;CGLIB Proxy invoke cost &quot; + (System.currentTimeMillis() - start) + &quot; ms&quot;);

        for (int i = 0; i &lt; 10; i++) {
            javassistBytecodeProxy.sell();//warm
        }
        start = System.currentTimeMillis();
        for (int i = 0; i &lt; 10000000; i++) {
            javassistBytecodeProxy.sell();
        }
        System.out.println(&quot;JavassistBytecode Proxy invoke cost &quot; + (System.currentTimeMillis() - start) + &quot; ms&quot;);

        Class&lt;?&gt; serviceClass = delegate.getClass();
        String methodName = &quot;sell&quot;;
        for (int i = 0; i &lt; 10; i++) {
            cglibProxy.sell();//warm
        }
        // 执行反射调用
        for (int i = 0; i &lt; 10; i++) {//warm
            Method method = serviceClass.getMethod(methodName, new Class[]{});
            method.invoke(delegate, new Object[]{});
        }
        start = System.currentTimeMillis();
        for (int i = 0; i &lt; 10000000; i++) {
            Method method = serviceClass.getMethod(methodName, new Class[]{});
            method.invoke(delegate, new Object[]{});
        }
        System.out.println(&quot;反射 invoke cost &quot; + (System.currentTimeMillis() - start) + &quot; ms&quot;);

        // 使用 CGLib 执行反射调用
        for (int i = 0; i &lt; 10; i++) {//warm
            FastClass serviceFastClass = FastClass.create(serviceClass);
            FastMethod serviceFastMethod = serviceFastClass.getMethod(methodName, new Class[]{});
            serviceFastMethod.invoke(delegate, new Object[]{});
        }
        start = System.currentTimeMillis();
        for (int i = 0; i &lt; 10000000; i++) {
            FastClass serviceFastClass = FastClass.create(serviceClass);
            FastMethod serviceFastMethod = serviceFastClass.getMethod(methodName, new Class[]{});
            serviceFastMethod.invoke(delegate, new Object[]{});
        }
        System.out.println(&quot;CGLIB invoke cost &quot; + (System.currentTimeMillis() - start) + &quot; ms&quot;);

    }

    private static BookApi createJdkDynamicProxy(final BookApi delegate) {
        BookApi jdkProxy = (BookApi) Proxy.newProxyInstance(ClassLoader.getSystemClassLoader(),
                new Class[]{BookApi.class}, new JdkHandler(delegate));
        return jdkProxy;
    }

    private static class JdkHandler implements InvocationHandler {

        final Object delegate;

        JdkHandler(Object delegate) {
            this.delegate = delegate;
        }

        @Override
        public Object invoke(Object object, Method method, Object[] objects)
                throws Throwable {
            //添加代理逻辑
            if(method.getName().equals(&quot;sell&quot;)){
                System.out.print(&quot;&quot;);
            }
            return null;
//            return method.invoke(delegate, objects);
        }
    }

    private static BookApi createCglibDynamicProxy(final BookApi delegate) throws Exception {
        Enhancer enhancer = new Enhancer();
        enhancer.setCallback(new CglibInterceptor(delegate));
        enhancer.setInterfaces(new Class[]{BookApi.class});
        BookApi cglibProxy = (BookApi) enhancer.create();
        return cglibProxy;
    }

    private static class CglibInterceptor implements MethodInterceptor {

        final Object delegate;

        CglibInterceptor(Object delegate) {
            this.delegate = delegate;
        }

        @Override
        public Object intercept(Object object, Method method, Object[] objects,
                                MethodProxy methodProxy) throws Throwable {
            //添加代理逻辑
            if(method.getName().equals(&quot;sell&quot;)) {
                System.out.print(&quot;&quot;);
            }
            return null;
//            return methodProxy.invoke(delegate, objects);
        }
    }

    private static BookApi createJavassistBytecodeDynamicProxy() throws Exception {
        ClassPool mPool = new ClassPool(true);
        CtClass mCtc = mPool.makeClass(BookApi.class.getName() + &quot;JavaassistProxy&quot;);
        mCtc.addInterface(mPool.get(BookApi.class.getName()));
        mCtc.addConstructor(CtNewConstructor.defaultConstructor(mCtc));
        mCtc.addMethod(CtNewMethod.make(
                &quot;public void sell() { System.out.print(\&quot;\&quot;) ; }&quot;, mCtc));
        Class&lt;?&gt; pc = mCtc.toClass();
        BookApi bytecodeProxy = (BookApi) pc.newInstance();
        return bytecodeProxy;
    }

}
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Aspect-Oriented Programming in Spring Boot Part 2: Spring JDK Proxies vs CGLIB vs AspectJ]]></title>
    <link href="http://panlw.github.io/15277821532847.html"/>
    <updated>2018-05-31T23:55:53+08:00</updated>
    <id>http://panlw.github.io/15277821532847.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>Justin Wilson May 9, 2016<br/>
<a href="https://www.credera.com/blog/technology-insights/open-source-technology-insights/aspect-oriented-programming-in-spring-boot-part-2-spring-jdk-proxies-vs-cglib-vs-aspectj/">https://www.credera.com/blog/technology-insights/open-source-technology-insights/aspect-oriented-programming-in-spring-boot-part-2-spring-jdk-proxies-vs-cglib-vs-aspectj/</a></p>
</blockquote>

<p>This is the second entry in a three-part series on aspect-oriented programming (AOP) with Spring Boot. This entry covers the differences between using AOP with Spring JDK proxies, CGLIB proxes, and AspectJ load-time weaving (LTW). The <a href="https://www.credera.com/blog/technology-insights/open-source-technology-insights/aspect-oriented-programming-in-spring-boot-part-1-making-your-own-hystrix-aspect/">first entry</a> covered how to write your own aspect, and the <a href="https://www.credera.com/blog/technology-insights/open-source-technology-insights/aspect-oriented-programming-in-spring-boot-part-3-setting-up-aspectj-load-time-weaving/">last entry</a> will contain detailed AspectJ LTW setup information.</p>

<p><strong>How Can You Call Code Without Writing Its Invocation?</strong></p>

<p>The Spring framework supports two main methods of AOP—Spring proxies and AspectJ. Both methods allow code snippets called aspects to be injected or “woven” around existing code by targeting specific join points. However, even though both AOP styles can use the same code for their aspects, the way in which they weave those aspects into existing code is completely different.</p>

<p><strong>Spring Proxies: JDK and CGLIB Styles</strong></p>

<p>Every Spring bean (heretofore referred to as just a “bean”) can be considered a managed component. Whenever you declare a bean in XML or use @Component, @Service, or @Repository on a class targeted by Spring’s annotation component scanning (which is enabled by default for Spring Boot), that class is instantiated and managed as a singleton bean by the Spring framework. (It’s technically possible to use different scoping mechanisms to avoid singletons, but those won’t be covered here; see <a href="https://www.credera.com/blog/technology-insights/java/dependency-injection-part-3-spring/">my blog series on dependency injection</a> for more information.) Spring can inject references to that bean into other beans that request it using the @Autowired annotation (or @Resource or @Inject annotations) or beans that use “ref” in their XML definition. However, Spring doesn’t actually provide a literal reference to the original bean when it’s injected—it wraps the bean in a proxy class to give Spring a chance to weave in AOP code if needed.</p>

<p>There are two ways Spring can make proxies:</p>

<p>1. By default, Spring will try to use <a href="http://www.ibm.com/developerworks/java/library/j-jtp08305/index.html">JDK dynamic proxy libraries</a> to create a new instance of the injected bean’s interface which will act as a delegate to that bean. This behavior is demonstrated by the <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-proxy/src/test/java/org/jdw/blog/ProxyHystrixAspectTest.java#L36">unit tests</a> that use injected beans with interfaces in the <a href="https://github.com/jwilsoncredera/spring-aop-blog/tree/master/spring-aop-proxy">spring-aop-proxy sample project</a>.</p>

<p><a href="https://www.credera.com/wp-content/uploads/2016/04/Picture2-4.png"><img src="https://www.credera.com/wp-content/uploads/2016/04/xPicture2-4.png.pagespeed.ic.dhHNYptsoi.webp" alt="Picture2"/></a></p>

<p>2. If CGLIB is available on the classpath (which comes by default with Spring Boot) and there is no interface to implement a proxy with, CGLIB will be used to make a new object that extends the target bean’s class and acts as a delegate to that original bean. This behavior is also demonstrated by a <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-proxy/src/test/java/org/jdw/blog/ProxyHystrixAspectTest.java#L65">unit test</a> that uses an injected bean without an interface in the <a href="https://github.com/jwilsoncredera/spring-aop-blog/tree/master/spring-aop-proxy">spring-aop-proxy sample project</a>.</p>

<p><a href="https://www.credera.com/wp-content/uploads/2016/04/Picture3-3.png"><img src="https://www.credera.com/wp-content/uploads/2016/04/xPicture3-3.png.pagespeed.ic.W2zdwkeosT.webp" alt="Picture3"/></a></p>

<p>By default, Spring Boot will use JDK proxies if the bean has an interface and CGLIB proxies if the bean does not. If you try to inject a bean using its concrete class and that bean has an interface, Spring will fail to find the bean with a NoSuchBeanDefinitionException because it will fail to create a JDK proxy using the concrete class. In the first example above, that would be using this:</p>

<p>| </p>

<pre><code class="language-java">@Autowired
private ImplForInterface bean;
</code></pre>

<p>Instead of this:</p>

<pre><code class="language-java">@Autowired
private Interface bean;
</code></pre>

<p>To get around this limitation and be able to inject concrete classes that have interfaces, you have to tell Spring to always use CGLIB for all beans. This will disable Spring’s use of JDK proxies entirely, making Spring always extend concrete classes even if an interface is injected. To enable this in Spring Boot, just add the following Java property, as seen in the <a href="https://github.com/jwilsoncredera/spring-aop-blog/tree/master/spring-aop-proxy-cglib">spring-aop-proxy-cglib</a> sample project’s <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-proxy-cglib/src/main/resources/application.properties">application.properties</a> file:</p>

<p>spring.aop.proxy-target-class=true</p>

<p><strong>Spring Proxies: Annotations on Interfaces</strong></p>

<p>Consider an aspect that’s used to weave code around annotated methods (such as the <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-common/src/main/java/org/jdw/blog/common/aspect/HystrixAspect.java">HystrixAspect</a> that targets <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-common/src/main/java/org/jdw/blog/common/annotation/HystrixWrapper.java">@HystrixWrapper</a> annotated methods from part one in this series).</p>

<ul>
<li>  When a Spring JDK proxy is used, the join point annotation should be present on both the interface’s method and the concrete class’s method for the aspect to trigger correctly.

<ul>
<li>  The <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-proxy/src/test/java/org/jdw/blog/ProxyHystrixAspectTest.java#L72">second to last test in the spring-aop-proxy test class</a> proves that both the interface and the concrete class require the join point annotation when a JDK proxy is used.</li>
</ul></li>
<li>  When a CGLIB proxy is used, the concrete implementation must have the annotation on its method for the aspect to trigger.

<ul>
<li>  The <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-proxy-cglib/src/test/java/org/jdw/blog/ProxyCglibHystrixAspectTest.java#L96">second to last test in the spring-aop-proxy-cglib test class</a> proves that the interface’s join point annotations are ignored when CGLIB is used.</li>
</ul></li>
</ul>

<p>The <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-proxy/src/test/java/org/jdw/blog/ProxyHystrixAspectTest.java#L48">JDK proxy unit test that expects an <strong>InaccessablePointcutAnnotationException</strong></a> is of particular interest. It demonstrates that if you’re using a JDK proxy and the join point annotation is present on the concrete implementation of a bean and not on its interface, the aspect will still trigger but it won’t be able to find the annotation that was used to trigger it. This prevents us from finding the Hystrix command group key that should come from the wrapped method’s annotation, as shown in the bold parts of the example below. (<a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-common/src/main/java/org/jdw/blog/common/annotation/HystrixWrapper.java">This</a> is the GitHub link for the @HystrixWrapper annotation, and <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-common/src/main/java/org/jdw/blog/common/aspect/HystrixAspect.java">this</a> is the link for the HystrixAspect.)</p>

<pre><code class="language-java">@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.METHOD)
public @interface HystrixWrapper {
  public String commandGroupKey();
}
@Aspect
@Component
public class HystrixAspect {

  @Around(&quot;within(org.jdw.blog..*) &amp;amp;&amp;amp; &quot;  &quot;@annotation(org.jdw.blog.common.annotation.HystrixWrapper)&quot;)
  public Object around(final ProceedingJoinPoint joinPoint) {

    Method method = ((MethodSignature) joinPoint.getSignature()).getMethod();
    HystrixWrapper annotation = method.getAnnotation(HystrixWrapper.class);
    if (annotation == null) {      // Can occur when not using CCGLIB-style &#39;subclass&#39; proxies,
      // when using an interface that has a concrete class
      // that has the annotation.
      throw new InaccessablePointcutAnnotationException();    }

    // Hystrix invocation code goes here, see GitHub for details. 
    // [https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-common/src/main/java/org/jdw/blog/common/aspect/HystrixAdvice.java](https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-common/src/main/java/org/jdw/blog/common/aspect/HystrixAdvice.java)
  }
}
</code></pre>

<p><strong>I recommend always using CGLIB to avoid this edge case.</strong> Doing so makes all Spring proxy behavior consistent since JDK proxies will never be used, it allows the injection of concrete implementations of beans which can be useful for unit tests, and it negates the need to duplicate join point annotations on interfaces and concrete classes (which could easily get out of sync). As a reminder, to always use CGLIB, just set the “spring.aop.proxy-target-class” property to true.</p>

<p>Before using CGLIB, ensure your codebase always uses pre-existing AOP annotations (such as @Transactional) on concrete classes instead of only on interfaces. Interface-only AOP annotations will be ignored when CGLIB is enabled. Changing when @Transactional aspects are triggered could lead to items not being saved to the database, or poor performance due to transactional boundary shifting.</p>

<p><strong>Spring Proxies: Annotations in Concrete Classes</strong></p>

<p>Both JDK- and CGLIB-style Spring proxies have one glaring weakness—AOP can only be invoked by calling a method on an injected bean. Consider a bean with these two methods (all of the classes in <a href="https://github.com/jwilsoncredera/spring-aop-blog/tree/master/spring-aop-common/src/main/java/org/jdw/blog/common/executable">this package</a> qualify):</p>

<pre><code class="language-java">@HystrixWrapper(commandGroupKey = &quot;blog&quot;)
public long hystrixWrappedGetCurrentThreadId() {
  return getCurrentThreadId();
}

public long nestedHystrixWrappedGetCurrentThreadId() {
  return hystrixWrappedGetCurrentThreadId();
}
</code></pre>

<p>The second method cannot trigger the HystrixAspect when calling the first method because it invokes the first method directly without going through a Spring proxy—the bean is just calling one of its own methods. Spring doesn’t have any opportunities to inject the aspect code. This is demonstrated by every other test in both the <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-proxy/src/test/java/org/jdw/blog/ProxyHystrixAspectTest.java#L42">spring-aop-proxy test class</a> and <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-proxy-cglib/src/test/java/org/jdw/blog/ProxyCglibHystrixAspectTest.java#L41">spring-aop-proxy-cglib test class</a> (all the tests that start with “testNestedHystrixWrappedMethod”).</p>

<p>However, when AspectJ is used instead of Spring proxies, the HystrixAspect <u>does</u> trigger when a bean calls its own join point annotated method. This is shown by every other test in the <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-aspectj-ltw/src/test/java/org/jdw/blog/AspectJHystrixAspectTest.java#L41">spring-aop-aspectj-ltw test class</a> (which also begin with “testNestedHystrixWrappedMethod”)—every scenario that fails to invoke the Hystrix aspect for Spring proxies succeeds for AspectJ (except for <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-aspectj-ltw/src/test/java/org/jdw/blog/AspectJHystrixAspectTest.java#L101">one specific scenario</a> which will be covered later). How can this be?</p>

<p><strong>AspectJ</strong></p>

<p>AspectJ extends the Java compiler to weave aspect code directly into join points. It doesn’t require Spring to run. Instead, you need to use <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-aspectj-ltw/README.md">JVM arguments</a> to enable a compiler extension along with an <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-aspectj-ltw/src/main/resources/META-INF/aop.xml">aop.xml file</a> and <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-aspectj-ltw/src/main/java/org/jdw/blog/config/AspectJConfig.java">another configuration file</a> which will be covered in part three of this series.</p>

<p>This has two important ramifications. First, if you’re using AspectJ, you don’t need to inject Spring beans to invoke AOP functionality. Technically you don’t even need Spring beans for AOP at all when AspectJ is enabled—you could manually create a new object and invoke AOP code on it right away if it had join points your aspects could target.</p>

<p>Second, if an object calls its own methods it can trigger AOP code, unlike AOP using Spring proxies. Here’s a visual example of how this AspectJ behavior would look if the @HystrixAspect used @Before instead of @Around to inject code in front of its join point targets instead of completely wrapping them (because it’s easier to depict):</p>

<p><a href="https://www.credera.com/wp-content/uploads/2016/04/Picture5-2.png"><img src="https://www.credera.com/wp-content/uploads/2016/04/xPicture5-2.png.pagespeed.ic.PXtdJ6_MY_.webp" alt="Picture5"/></a></p>

<p>The fact that beans can trigger their own AOP code means you need to be very careful when enabling AspectJ on existing Spring projects, because this can change when database transactions are started by @Transactional annotations. For example, if you had a bean with two methods and one didn’t have an @Transactional annotation and it called another one that did, then that invocation would start a database transaction with AspectJ enabled but not with default Spring proxy-based AOP.</p>

<pre><code class="language-java">public void notAnnotated() {
  annotated();
}

@Transactional
public void annotated() {
  // With AspectJ, a direct call from notAnnotated will enable the transaction.
  // Without it, this method will only get a transaction if it was called from a Spring proxy.
}
</code></pre>

<p>Luckily, there is a way around this issue—AspectJ and Spring proxies can co-exist. The <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-aspectj-ltw/src/main/resources/META-INF/aop.xml">aop.xml file</a> lets you decide which aspects will be managed by AspectJ and which packages are included or excluded from AspectJ code weaving. If you exclude classes that use @Transactional from AspectJ weaving, their behavior will default to the same Spring proxy behavior they were using before. Stay tuned for part three of this series, which covers how to configure an aop.xml file.</p>

<p><strong>What About That Unit Test Where AspectJ Didn’t Work?</strong></p>

<p>As noted earlier, every other test in the <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-aspectj-ltw/src/test/java/org/jdw/blog/AspectJHystrixAspectTest.java#L41">spring-aop-aspectj unit test</a> (which each begin with “testNestedHystrixWrappedMethod”) demonstrates how beans can trigger the HystrixAspect when calling their own @HystrixWrapper join point annotated methods without going through Spring beans… except for the <a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-aspectj-ltw/src/test/java/org/jdw/blog/AspectJHystrixAspectTest.java#L102">last test</a>.</p>

<p>That last test invokes the “nestedGetCurrentThreadId” method on a Spring CGLIB proxy of a class similar to the example below (<a href="https://github.com/jwilsoncredera/spring-aop-blog/blob/master/spring-aop-common/src/main/java/org/jdw/blog/common/executable/NonAnnotatedImplForInterfaceWithAnnotation.java#L22">GitHub link for the original class</a>). The class’s interface annotates “getCurrentThreadId” with @HystrixWrapper, but the concrete class itself does not. Thus, when “getCurrentThreadId” is invoked by “nestedGetCurrentThreadId” within the CGLIB proxy, AspectJ looks at the concrete class instead of the interface and can’t find the @HystrixWrapper join point. Therefore, the HystrixAspect is not woven.</p>

<pre><code class="language-java">@Override
public long getCurrentThreadId () {
  return Thread.currentThread().getId();
}

@Override
public long nestedGetCurrentThreadId () {
  return getCurrentThreadId();
}
</code></pre>

<p>The takeaway from this scenario is that you should always put join point annotations on concrete classes when using AspectJ, just like you should when using Spring CGLIB proxies for AOP.</p>

<p><strong>Next Steps</strong></p>

<p>Stay tuned for the third and final entry in this blog series, which will cover how to configure AspectJ for Spring Boot. It contains a critical piece of JVM configuration information that isn’t widely found in other tutorials (including Spring’s own documentation), so be sure to catch the next entry by following @CrederaOpen on Twitter or connecting with us on LinkedIn.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[定时任务框架Quartz详解-基础篇]]></title>
    <link href="http://panlw.github.io/15277817499101.html"/>
    <updated>2018-05-31T23:49:09+08:00</updated>
    <id>http://panlw.github.io/15277817499101.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原创： 蜗牛  Java架构师之路  前天<br/>
<a href="https://mp.weixin.qq.com/s?__biz=MzI3NjU2ODA5Mg==&amp;mid=2247484030&amp;idx=1&amp;sn=13559ded2625e103a9722b7223927683&amp;chksm=eb72c30ddc054a1b07b1983fc9ad1ba4d9ec362e38b86fd9fd6a49f73a5ee1472b198d507fc9&amp;mpshare=1&amp;scene=23&amp;srcid=0529GJVxZA61CVVHbErVxkwQ%23rd">原文地址</a></p>
</blockquote>

<h3 id="toc_0">概述</h3>

<p>Quartz 是 OpenSymphony 开源组织的一个开源项目，定时任务框架，纯 Java 语言实现，最新版本为 2.3.0。<br/>
Quartz 中用到的设计模式：</p>

<ul>
<li>  Builder 模式</li>
<li>  Factory 模式</li>
<li>  组件模式</li>
<li>  链式模式</li>
</ul>

<h3 id="toc_1">Quartz 组成部分</h3>

<ul>
<li>  调度器：scheduler</li>
<li>  任务：JobDetail</li>
<li>  触发器：Trigger, 包括 SimpleTrigger 和 CronTrigger</li>
</ul>

<h3 id="toc_2">第一个 Quartz 程序</h3>

<p>实现每隔 1 秒打印一个 Hello World</p>

<h4 id="toc_3">1. 创建 Maven 项目，添加依赖：</h4>

<pre><code class="language-xml">&lt;!-- https://mvnrepository.com/artifact/org.quartz-scheduler/quartz --&gt;
&lt;dependency&gt;
  &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt;
  &lt;artifactId&gt;quartz&lt;/artifactId&gt;
  &lt;version&gt;2.3.0&lt;/version&gt;
&lt;/dependency&gt;  
</code></pre>

<h4 id="toc_4">2. 创建 HelloWorldJob 类</h4>

<pre><code class="language-java">package quartz;
import org.quartz.Job;
import org.quartz.JobExecutionContext;
import org.quartz.JobExecutionException;
import java.text.SimpleDateFormat;
import java.util.Date;
/**
 * created by Java-Road
 * created in 2018/5/26
 */
public class HelloWorldJob implements Job {
    @Override
    public void execute(JobExecutionContext jobExecutionContext) throws JobExecutionException {
        String strTime = new SimpleDateFormat(&quot;HH-mm-ss&quot;).format(new Date());
        System.out.println( strTime + &quot;:Hello World！&quot;);
    }
}
</code></pre>

<h4 id="toc_5">3. 创建 MyScheduler 类</h4>

<pre><code class="language-java">package quartz;
import org.quartz.*;
import org.quartz.impl.StdSchedulerFactory;
/**
 * created by Java-Road
 * created in 2018/5/26
 */
public class MyScheduler {
    public static void main(String[] args) throws SchedulerException {
        //创建调度器Schedule
        SchedulerFactory schedulerFactory = new StdSchedulerFactory();
        Scheduler scheduler = schedulerFactory.getScheduler();
        //创建JobDetail实例，并与HelloWordlJob类绑定
        JobDetail jobDetail = JobBuilder.newJob(HelloWorldJob.class).withIdentity(&quot;job1&quot;, &quot;jobGroup1&quot;)
                .build();
        //创建触发器Trigger实例(立即执行，每隔1S执行一次)
        Trigger trigger = TriggerBuilder.newTrigger()
                .withIdentity(&quot;trigger1&quot;, &quot;triggerGroup1&quot;)
                .startNow()
                .withSchedule(SimpleScheduleBuilder.simpleSchedule().withIntervalInSeconds(1).repeatForever())
                .build();
        //开始执行
        scheduler.scheduleJob(jobDetail, trigger);
        scheduler.start();
    }
}
</code></pre>

<p></section></p>

<h3 id="toc_6">Quartz 中几个重要的对象</h3>

<h2 id="toc_7">1.Job 和 JobDetail</h2>

<p>Job 是 Quartz 中的一个接口，接口下只有 execute 方法，在这个方法中编写业务逻辑。</p>

<p>该接口的源码：</p>

<pre><code class="language-java">package org.quartz;  
public interface Job {  
    void execute(JobExecutionContext var1) throws JobExecutionException;  
}  
</code></pre>

<p>每次调度执行 Job 时，调用 execute 方法前会创建一个新的 Job 实例，执行完后，关联的 Job 对象实例会被释放，随后 jvm 执行 GC。</p>

<p>JobDetail 是用来绑定 Job，为 Job 实例提供了许多属性，以及 JobDataMap 成员变量属性。调度器 scheduler 通过 JobDetail 对象来添加 Job 实例。</p>

<p>属性：</p>

<ul>
<li>  name</li>
<li>  group</li>
<li>  jobClass</li>
<li>  jobDataMap</li>
</ul>

<h2 id="toc_8">2.JobExecutionContext</h2>

<p>当调度器 Scheduler 调用一个 Job 时，就会将 JobExecutionContext 传递给 Job 的 execute() 方法，Job 能通过 JobExecutionContext 对象访问到 Quartz 运行时的环境以及 Job 本身的详细数据信息。</p>

<p>代码演示：</p>

<pre><code class="language-java">public class HelloWorldJob implements Job {
    @Override
    public void execute(JobExecutionContext jobExecutionContext) throws JobExecutionException {
        String strTime = new SimpleDateFormat(&quot;HH-mm-ss&quot;).format(new Date());
        System.out.println( strTime + &quot;:Hello World！&quot;);
        System.out.println(&quot;JobDetail&#39;name:&quot; + jobExecutionContext.getJobDetail().getKey().getName());
        System.out.println(&quot;JobDetail&#39;group:&quot; + jobExecutionContext.getJobDetail().getKey().getGroup());
        System.out.println(&quot;JobDetail&#39;class:&quot; + jobExecutionContext.getJobDetail().getClass());
    }
}
</code></pre>

<h2 id="toc_9">3.JobDataMap</h2>

<p>任务调度时可以通过 JobExecutionContext 获取 JobDataMap，可以装在任何可序列化的数据对象，JobDataMap 实现了 JDK 的 Map 接口，可以以 Key-Value 的形式存储数据。</p>

<p>实战：JobDetail 和 Trigger 传递数据，HelloWorldJob 类 execute 三种方式获取数据。</p>

<pre><code class="language-java">//创建JobDetail实例，并与HelloWordlJob类绑定
JobDetail jobDetail = JobBuilder.newJob(HelloWorldJob.class).withIdentity(&quot;job1&quot;, &quot;jobGroup1&quot;)
        .usingJobData(&quot;key1&quot;,&quot;this is jobDetail&quot;)
        .build();
//创建触发器Trigger实例(立即执行，每隔1S执行一次)
Trigger trigger = TriggerBuilder.newTrigger()
        .withIdentity(&quot;trigger1&quot;, &quot;triggerGroup1&quot;)
        .usingJobData(&quot;key2&quot;, &quot;this is trigger&quot;)
        .startNow()
        .withSchedule(SimpleScheduleBuilder.simpleSchedule().withIntervalInSeconds(1).repeatForever())
        .build();
</code></pre>

<p>HelloWorldJob 获取数据：</p>

<pre><code class="language-java">public class HelloWorldJob implements Job {
    private String key1;
    private String key2;
    public String getKey1() {
        return key1;
    }
    public void setKey1(String key1) {
        this.key1 = key1;
    }
    public String getKey2() {
        return key2;
    }
    public void setKey2(String key2) {
        this.key2 = key2;
    }
    @Override
    public void execute(JobExecutionContext jobExecutionContext) throws JobExecutionException {
        String strTime = new SimpleDateFormat(&quot;HH-mm-ss&quot;).format(new Date());
        System.out.println( strTime + &quot;:Hello World！&quot;);
        //获取DataMap数据方法一
        System.out.println(&quot;JobDetail JobDataMap:&quot; + jobExecutionContext.getJobDetail().getJobDataMap().get(&quot;key1&quot;));
        System.out.println(&quot;Trigger JobDataMap:&quot; + jobExecutionContext.getTrigger().getJobDataMap().get(&quot;key2&quot;));
        //获取DataMap数据方法二
        System.out.println(&quot;JobDataMap:&quot; + jobExecutionContext.getMergedJobDataMap().get(&quot;key1&quot;));
        System.out.println(&quot;JobDataMap:&quot; + jobExecutionContext.getMergedJobDataMap().get(&quot;key2&quot;));
        //获取数据方法三
        System.out.println(&quot;通过成员变量获取&quot; + key1);
        System.out.println(&quot;通过成员变量获取&quot; + key2);
    }
}
</code></pre>

<h2 id="toc_10">4.Trigger</h2>

<p>Trigger 是 Quartz 中的触发器，任务执行时会通知调度器 Scheduler 何时触发，几个重要的属性。</p>

<ol>
<li> Jobkey：表示 job 实例的标识</li>
<li> StartTime：表示触发器首次被触发的时间 (Java.util.Date)。</li>
<li> EndTime：表示触发器结束触发的时间 (Java.util.Date)</li>
</ol>

<p>实战：实现 5S 后执行，10S 后结束，期间每隔 1S 执行一次定时任务</p>

<p>代码演示：</p>

<p>MyScheduler 类</p>

<pre><code class="language-java">public class MyScheduler {
    public static void main(String[] args) throws SchedulerException {
        //创建调度器Schedule
        SchedulerFactory schedulerFactory = new StdSchedulerFactory();
        Scheduler scheduler = schedulerFactory.getScheduler();
        //创建JobDetail实例，并与HelloWordlJob类绑定
        JobDetail jobDetail = JobBuilder.newJob(HelloWorldJob.class).withIdentity(&quot;job1&quot;, &quot;jobGroup1&quot;)
                .build();
        //创建触发器Trigger实例(5S后执行，10S后结束)
            //开始时间(5S后)
        Date date1 = new Date();
        date1.setTime(date1.getTime() + 5000);
            //结束时间(10S后)
        Date date2 = new Date();
        date2.setTime(date2.getTime() + 10000);
        Trigger trigger = TriggerBuilder.newTrigger()
                .withIdentity(&quot;trigger1&quot;, &quot;triggerGroup1&quot;)
                .startAt(date1)
                .endAt(date2)
                .withSchedule(SimpleScheduleBuilder.simpleSchedule().withIntervalInSeconds(1).repeatForever())
                .build();
        //开始执行
        scheduler.scheduleJob(jobDetail, trigger);
        scheduler.start();
    }
}
</code></pre>

<h2 id="toc_11">5.SimpleTrigger</h2>

<p>SimpleTrigger 可以实现在一个指定时间段内执行一次作业任务或一个时间段内多次执行作业任务。</p>

<p>实战：5S 后开始执行，间隔时间为 1S，第一次执行后连续执行 3 次</p>

<pre><code class="language-java">package quartz2;
import org.quartz.*;
import org.quartz.impl.StdSchedulerFactory;
import java.util.Date;
/**
 * created by Java-Road
 * created in 2018/5/27
 */
public class MyScheduler2 {
    public static void main(String[] args) throws SchedulerException {
        //创建调度器Schedule
        SchedulerFactory schedulerFactory = new StdSchedulerFactory();
        Scheduler scheduler = schedulerFactory.getScheduler();
        //创建JobDetail实例，并与HelloWordlJob类绑定
        JobDetail jobDetail = JobBuilder.newJob(HelloWorldJob.class).withIdentity(&quot;job1&quot;, &quot;jobGroup1&quot;)
                .build();
        //创建触发器Trigger实例(5S后执行,一直执行)
        //开始时间(5S后)
        Date date1 = new Date();
        date1.setTime(date1.getTime() + 5000);
        SimpleTrigger trigger = (SimpleTrigger) TriggerBuilder.newTrigger()
                .withIdentity(&quot;trigger1&quot;, &quot;triggerGroup1&quot;)
                .startAt(date1)
                .withSchedule(SimpleScheduleBuilder.simpleSchedule().withIntervalInSeconds(1)
                            .withRepeatCount(3))
                .build();
        //开始执行
        scheduler.scheduleJob(jobDetail, trigger);
        scheduler.start();
    }
}
</code></pre>

<h2 id="toc_12">6.CronTrigger</h2>

<p>CronTrigger 功能非常强大，是基于日历的作业调度，而 SimpleTrigger 是精准指定间隔，所以相比 SimpleTrigger，CroTrigger 更加常用。CroTrigger 是基于 Cron 表达式的，先了解下 Cron 表达式：</p>

<p>由 7 个子表达式组成字符串的，格式如下：</p>

<p>[秒] [分] [小时] [日] [月] [周] [年]</p>

<p>Cron 表达式的语法就不多说了，因为我也记不住只能度娘，给大家提供个在线生成 Cron 表达式的工具：<a href="http://cron.qqe2.com/">http://cron.qqe2.com/</a> ，方便实用。</p>

<p>实战：实现每周一到周五上午 10:30 执行定时任务</p>

<pre><code class="language-java">package quartz2;
import org.quartz.*;
import org.quartz.impl.StdSchedulerFactory;
import java.util.Date;
/**
 * created by Java-Road
 * created in 2018/5/27
 */
public class MyScheduler3 {
    public static void main(String[] args) throws SchedulerException {
        //创建调度器Schedule
        SchedulerFactory schedulerFactory = new StdSchedulerFactory();
        Scheduler scheduler = schedulerFactory.getScheduler();
        //创建JobDetail实例，并与HelloWordlJob类绑定
        JobDetail jobDetail = JobBuilder.newJob(HelloWorldJob.class).withIdentity(&quot;job1&quot;, &quot;jobGroup1&quot;)
                .build();
        //创建触发器CronTrigger实例(每周一到周五10:30执行任务)
        CronTrigger trigger = (CronTrigger) TriggerBuilder.newTrigger()
                .withIdentity(&quot;trigger1&quot;, &quot;triggerGroup1&quot;)
                .startNow()
                .withSchedule(CronScheduleBuilder.cronSchedule(&quot;* 30 10 ? * 1/5 *&quot;))
                .build();
        //开始执行
        scheduler.scheduleJob(jobDetail, trigger);
        scheduler.start();
    }
}
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[有货移动端DevOps-自建APM系统]]></title>
    <link href="http://panlw.github.io/15277814705334.html"/>
    <updated>2018-05-31T23:44:30+08:00</updated>
    <id>http://panlw.github.io/15277814705334.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原创：曹镏  高效开发运维 2018/05/28</p>

<p><a href="https://mp.weixin.qq.com/s?__biz=MzIzNjUxMzk2NQ==&amp;mid=2247489376&amp;idx=1&amp;sn=2c06cd726d03317ab3b2134e393d101e&amp;chksm=e8d7e8a2dfa061b4b8f4b95a710e7538becf501951ca451c63473e219d9e610c230549ceb709&amp;mpshare=1&amp;scene=23&amp;srcid=0529xDVbCCeY0HgyVSeq5kjF%23rd">原文地址</a></p>
</blockquote>

<pre><code>本文概述了有货 App 团队的同学，在资源有限的情况下，自己动手，借助多种数据工具，通过简洁的架构，为有货 App 开发一套多维实时的监控系统，从而更好地完成自己的工作，也给其他团队的小伙伴在自建 APM 的道路上提供一个参考，为移动端的 DevOps 打开一些思路。
</code></pre>

<p>得益于智能手机的发展，当下我们对用户的体验追求已经到了几乎极致的程度。主流的监控 OneAPM，New Relic… 由于面向广大用户，强调数据共性，很难通过 UDID 或者 UID 来定位一些比较难以复现或局部的问题。同时由于数据保密性等原因，对事件上报的信息量也是非常的有限。随着用户规模的增长，特殊问题，局部问题覆盖的用户越来越来，自建监控就一个必然的选择。</p>

<p>需求是迫切的，资源是有限的。为了尽快的上线监控系统。有货 iOS 和 Android 的小伙伴们本着 “流自己的汗，吃自己的饭，自己的事情自己干” 的原则，不强调客观条件，自己撸出了一套 App 的 APM 监控系统。</p>

<h2 id="toc_0">设计</h2>

<p>我们把问题处理的流程大致梳理为 “持续监控”、“发现问题” 、“定位问题”、“修复问题”。这往复的四个步骤，其中“监控”，“发现” 和“定位”都依赖于系统的采集和上报。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/FE4VibF0SjfMusg7SyM0ljwABGInyksDablCiaxacwcMhpe7mshHNYgG6nx8TaOGTPnjoPibnP1wcibNY1QOSvibbKg/640?wx_fmt=png" alt=""/></p>

<p>通过对需求的分析，我们把数据以事件的形式上报，上报后数据根据场景分为两类：</p>

<ol>
<li> 指标类数据</li>
<li> 日志类数据</li>
</ol>

<h3 id="toc_1">1. 指标类数据</h3>

<p>指标类数据主要是对用户上报的信息根据我们需要的维度做聚合，便于我们掌握 app 的整体情况，比如用户整体的网络错误，网络延时。从整体的角度观察我们 app 的健康程度。 这个过程概括为 “发现问题”。</p>

<h3 id="toc_2">2. 日志类数据</h3>

<p>日志类数据则记录完整的上报数据，用于我们对问题的诊断。 这个过程概括为 “定位问题”。<br/>
围绕这两类数据的采集和分析，我们启动了第一阶段的项目。</p>

<h2 id="toc_3">第一阶段</h2>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/FE4VibF0SjfMusg7SyM0ljwABGInyksDaGjaAY4ibHZrcU93rkaS4Mrb4DSFRibVMMfXia9e5Pbyfcru4WHIBLaACw/640?wx_fmt=png" alt=""/></p>

<p>App 将事件数据上报至 OpenResty，在 OpenResty 中对请求进行简单的处理和封装，拆分成指标和日志，然后将处理完成的数据分别写入 InfluxDB 和 MongoDB 中。再通过 Grafana 实现数据可视化。通过一个 Web 页面来查询存储在 MongoDB 中的日志。</p>

<p>App 上报的数据转换为有意义的指标, 这个过程是靠 “聚合” 来实现的。这套系统中 “聚合” 则是借助 InfluxDB 完成的。</p>

<h3 id="toc_4">InfluxDB</h3>

<p>在 InfluxDB 中，我们先将所有数据写入不同类型的事实表中，然后通过多个 Continuous Query 对数据进行聚合处理，把大量的明细数据聚合成一定时间周期的聚合数据（主要的周期为 30 秒或 1 分钟），再通过 Retention Policy 来控制不同类型表的数据有效期，达到资源最高效利用的目的。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/FE4VibF0SjfMusg7SyM0ljwABGInyksDaPyFpDrdMAVBjicTsq0VNibcEaDz40Cia8A6RgHOB9hNpSMpGafRfl1LZA/640?wx_fmt=png" alt=""/></p>

<p>数据可视化部分则用 Grafana 来支撑的，我们的 Main Dashboard 通过 20 张的图表来展示整体的健康状况，同时在这 20 张图表背后，还有数个更加详细的图表支撑微观方面数据。</p>

<p>比如有张图表绘制的是当前全网用户设备发生的网络错误率。 通过这个指标和 24 小时前的对比，我们能够直观了解整体的网络健康状况。当该指标超过某个阈值或相比昨天同时段对比有明显上升，则触发告警。实现 “发现问题”。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/FE4VibF0SjfMusg7SyM0ljwABGInyksDaoHzy1BDC7ichtoHqVhdAB7UFB2CgYrGYSiadsNGM8VruT7cwzic8CePoQ/640?wx_fmt=png" alt=""/></p>

<p>当出现告警后，通过网络相关的 dashboard 查看是由哪个接口发生的，什么类型的错误。再根据错误类型去日志平台查询日志。完成 “定位问题”。</p>

<h3 id="toc_5">MongoDB</h3>

<p>对于日志数据我们直接将上报的 json 内容逐条拆分后写入 Mongodb。直接通过 web 页面从 Mongodb 查询对应时间的 App 日志。</p>

<p>这套系统的优势体现在架构非常的轻盈，成本很低。只需要简单的 OpenResty 的开发以及对 InfluxDB 做一些连续查询。在日志量不大的情况下，完全可以满足基本的移动端性能监控以及异常监测的需求。</p>

<p>在运行了一段时间后，我们有了新的需求。</p>

<p>中国有 23 个省，5 个自治区，4 个直辖市，以及香港、澳门 2 个特别行政区。电信、联通、移动三大运营商。网络制式从 2G、3G、4G 到 WiFi，还有各种各样的移动设备。这些条件构成了数量难以想象的场景。当仅仅在个别场景下发生问题，或是部分场景下发生问题的时候。我们 “发现问题” 的即时性会变差，“定位问题”所用的时间也越发的变长。</p>

<p>为了缩短 “定位问题” 的时间。提高发现问题和解决问题的效率。多维分析能力是我们所渴望的，如果在原有的系统上做多维分析，存在如下问题：</p>

<ol>
<li><p>InfluxDB 缺乏合理免费的集群方案，我们在建立了自己的 APM 以后，客户端同学对数据和监测的需求突飞猛进，数据量每天以几何级数增长，再对其进行多维分析，单机的 InfluxDB 很难满足性能要求。</p></li>
<li><p>MongoDB 在数据量很大的场景下，直接查询速度很慢。由于我们日志数据的多样性，在对多个维度建索引后，整体索引效率很低。</p></li>
</ol>

<p><section class="" style="font-family: Avenir, -apple-system-font, 微软雅黑, sans-serif;line-height: 1.1;color: rgb(63, 63, 63);font-size: 20px;text-align: center;white-space: normal;background-color: rgb(255, 255, 255);">第二阶段</section></p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/FE4VibF0SjfMusg7SyM0ljwABGInyksDaFpHbicibZsT8FMpAf9ibF97iadUDnfFBjvvWcW3CU2gcGP4oTmYk6qVVlw/640?wx_fmt=png" alt=""/></p>

<p>为了满足大量（这里暂时不成为海量）数据的多维实时查询的需求，我们对架构中数据库和数据摄入过程进行了升级，InfluxDB 变更为 Druid， MongoDB 变更为 Greenplum， 数据也不再由 OpenResty 直接插入，而是通过了 Kafka 和 log 文件把写入过程解耦，减少数据写入次数，提升处理数据的量级。</p>

<h3 id="toc_6">Druid</h3>

<p>Druid 是在海量时序数据上面提供实时分析查询的开源 OLAP 数据存储。</p>

<p>Druid 本身对于 join 这样的操作很不友好，所以我们没有考虑用一些模型（星型或者雪花型），而是采用大的事实表，其中包含了一次事实关联的所有维度，看着很臃肿，但是查询的性能让人满意。在随时变更维度的情况下，Druid 实时查询依然维持在秒级。这不得不让人刮目相看。</p>

<p>同时丰富的查询组件和聚合函数，让复杂的查询得以简单的实现。所以在查询性能、便捷性、数据量等方面的综合考量下，我们选择了 Druid 作为 InfluxDB 的升级方案。</p>

<p>不同于 InfluxDB，我们不再通过连续查询来做数据聚合，Druid 自身在摄入的过程中可以对指定的 Dimension 进行 roll-up 操作。极大的压缩了数据量。我们的数据经过 roll-up，每分钟从 25 万条压缩到了 8 万多条，效果比较显著。不过压缩数据是以牺牲原始数据为代价的，预聚合仅保留指定的 Dimension 及 Aggregations 结果，所以请妥善设置 Dimension。 Dimension 会发生 SQL 中 Group By 的效果，参数维度多势必降低聚合效果。尤其需要的注意 Dimension 的选择，如果一不小心选择了高基字段作为 Dimension，那么压缩效率会低的惊人。我们曾经因为数据处理的失误，将一个 Dimension 的 Value 数从 2000 变成了 200W, 直接导致当天的 segments 占用的空间比以往大 22 倍。通过摄入周期的调整，还可以做到更大的压缩比，这个取决 queryGranularity 字段，它决定了压缩后的数据最小时间粒度，目前我们使用的是分钟。</p>

<p>在使用 Grafana 接入 Druid 的时候还是遇到一些小坑，由于 Grafana 官方集成的 Druid 插件实在是功能太有限，比如对 Time Range 的限制，我们不得不在源码的基础上做了部分调整。</p>

<p><section class="" style="font-family: Avenir, -apple-system-font, 微软雅黑, sans-serif;line-height: 1.1;color: rgb(63, 63, 63);font-size: 16px;text-align: center;white-space: normal;background-color: rgb(255, 255, 255);">Kafka</section></p>

<p>Druid 两种数据摄入源:</p>

<ol>
<li> 流式数据</li>
<li> 静态文件数据</li>
</ol>

<p>这两种数据源又都有两种方式摄入，分别为 pull 方式和 push 方式。我们采用了 real time index（push 的一种）方式，启动实时 task 来从 kafka 中摄取数据。</p>

<h3 id="toc_7">Greenplum</h3>

<p>Greenplum 相比我们原先使用的 MongoDB, 支持列式存储, 并行查询效率更高。在不建索引的情况下，50 列千万级别的数据在 1，2 秒能查完，完全满足我们对日志查询的需求。</p>

<p>起初我们使用 Http 的方式批量插入 Greenplum，但是当写入速度超过 6 千条每秒时，查询速度直线下降。</p>

<p>明显写入不是 Greenplum 的强项，为了解决这个问题，我们是把日志以文件的形式写入 NFS 中, 由 Greenplum 定时摄入。</p>

<p>同时为了保证性能和简化写入，还对 OpenResty 的写入进行了优化，利用 nginx 的 subrequest 写 access.log 的功能来实现日志写入，并且整个过程封装到协程中执行。调整后每分钟能够轻松写入 50 万条。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/FE4VibF0SjfMusg7SyM0ljwABGInyksDaHan3wopRd9Jujiay2MExXXoq7zpGiaRwhmHJjdh0NwWnAclbFzqB1VJg/640?wx_fmt=png" alt=""/></p>

<h2 id="toc_8">总结</h2>

<p>这套系统对个性问题，共性问题的快速甄别起到了关键性的作用，也极大的提升了定位问题的速度。在针对某些维度或特征做针对性的优化的时候，效果也更容易监控。</p>

<p>通过不同类型事件的采集，我们还实现了页面响应时间（多维）、http 请求分阶段响应时间（多维）、页面卡顿分析、页面加载性能监控等多方位的监控系统，为我们的优化工作指明了方向。对于 iOS 或者 Android 开发的同学，这样一套系统既能满足需求，也易于开发，相比传统 ELK 日志分析在实时性和简易性都提升很多。</p>

<p>如果技术团队的资源比较紧张，移动端的同学自己动手实践下 DevOps 也是不错的选择。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[电商平台备战促销季的运维秘诀——高可用服务层]]></title>
    <link href="http://panlw.github.io/15274398936380.html"/>
    <updated>2018-05-28T00:51:33+08:00</updated>
    <id>http://panlw.github.io/15274398936380.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>曹林华 纯洁的微笑 2018/5/26</p>

<p><a href="https://mp.weixin.qq.com/s/5vVXBXkd-Ilh7zk5G6Wxcg">原文地址</a></p>

<p>高可用设计是互联网系统架构的基础之一，以天猫双十二交易数据为例，支付宝峰值支付次数超过 8 万笔。大家设想一下，如果这个时候系统出现不可用的情况，那后果将不可想象。<br/>
而解决这个问题的根本就是服务层的高可用。</p>
</blockquote>

<h2 id="toc_0">什么是服务层</h2>

<p>众所周知，服务层主要用来处理网站业务逻辑的，是大型业务网站的核心。比如下面三个业务系统就是典型的服务层，提供基础服务功能的聚合</p>

<ul>
<li><p>用户中心：主要负责用户注册、登录、获取用户用户信息功能</p></li>
<li><p>交易中心：主要包括正向订单生成、逆向订单、查询、金额计算等功能</p></li>
<li><p>支付中心：主要包括订单支付、收银台、对账等功能</p></li>
</ul>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlialgfhicgwy6oEeNHT1HKDxoPpnY0JKoKLZsfKd0Z7zxcfiaSzf0DdSqA/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<h2 id="toc_1">整体架构</h2>

<p>业务发展初期主要以业务为导向，一般采用 「ALL IN ONE」的架构方式来开发产品，这个阶段用一句话概括就是 「糙猛快」。当发展起来之后就会遇到下面这些问题</p>

<ul>
<li><p>文件大：一个代码文件出现超过 2000 行以上</p></li>
<li><p>耦合性严重：不相关业务都直接堆积在 Serivce 层中</p></li>
<li><p>维护代价高：人员离职后，根本没有人了解里面的业务逻辑</p></li>
<li><p>牵一发动全身：改动少量业务逻辑，需要重新把所有依赖包打包并发布</p></li>
</ul>

<p>遇到这些问题，主要还是通过「拆」来解决</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlR3f0gpEglmktfDdT7JZsPNts8y6PwIDvpoVzYzXjriaLYicTz9sib2IYQ/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<p>具体拆的方式，主要根据业务领域划分单元，进行垂直拆分。拆分开来的好处很明显，主要有以下这些：</p>

<ul>
<li><p>每个业务一个独立的业务模块</p></li>
<li><p>业务间完全解耦</p></li>
<li><p>业务间互不影响</p></li>
<li><p>业务模块独立</p></li>
<li><p>单独开发、上线、运维</p></li>
<li><p>效率高</p></li>
</ul>

<h2 id="toc_2">无状态设计</h2>

<p>对于业务逻辑服务层，一般会设计成无状态化的服务，无状态化也就是服务模块只处理业务逻辑，而无需关心业务请求的上下文信息。所以无状态化的服务器之间是相互平等且独立的。</p>

<p>只有服务变为无状态的时候，故障转移才会变的很轻松。通常故障转移就是在某一个应用服务器不能服务用户请求的时候，通过负责均衡的方式，转移用户请求到其他应用服务器上来进行业务逻辑处理</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlrVLkBKpJZja8eqMrcYwM3Mh3TVhgibEniaQickmJw3W7IiccBmMFm5sZAw/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<h2 id="toc_3">超时设置</h2>

<p>一般网站服务都会有主调服务和被调服务之分。超时设置就是主调服务在调用被调服务的时候，设置一个超时等待时间 Timeout。主调服务发现超时后，就进入超时处理流程。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlBhI38T15JZ8gV0fNRfWcjqFibk2VjNPedQAwdy5DOETk1TV43qic5aVA/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<ol>
<li><p>主调服务 A 调用被调服务 B 时，设置超时等待时间为 3 秒，可能由于 B 服务宕机、网络情况不好或程序 BUG 之类，导致 B 服务不能及时响应 A 服务的调用。</p></li>
<li><p>此时 A 服务在等待 3 秒后，将触发超时逻辑而不再关心 B 服务的回复情况。</p></li>
<li><p>A 服务的超时逻辑可以依据情况而定，比如可以采取重试，对另一个对等的 B 服务去请求，或直接放弃结束这个请求调用。</p></li>
</ol>

<p>超时设置的好处在于当某个服务不可用时，不至于整个系统发生雪崩反应。</p>

<h2 id="toc_4">异步调用</h2>

<p>一般请求调用分为同步与异步两种。同步请求就像打电话，需要实时响应，而异步请求就像发送邮件一样，不需要马上回复。</p>

<p>这两种调用各有优劣，主要看面对哪种业务场景。比如在面对并发性能要求比较高的场景，异步调用就比同步调用有比较大的优势，这就好比一个人不能同时打多个电话，但是可以发送很多邮件。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlrMaqR6L2RGN4ZNKoX60pk0ckuAypBfThcu227jxF2rbN8XENfgbLTg/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<blockquote>
<p>那我们什么时候该采用异步调用？</p>
</blockquote>

<p>其实主要看业务场景，如果业务允许延迟处理，那就采用异步的方式处理</p>

<blockquote>
<p>那我们该怎么实现异步调用呢？</p>
</blockquote>

<p>通常采用队列的方式来实现业务上的延迟处理，比如像订单中心调用配送中心，这种场景下面，业务是能接受延迟处理的。</p>

<p>那消息队列主要有哪些功能呢？</p>

<ul>
<li><p>异步处理 - 增加吞吐量</p></li>
<li><p>削峰填谷 - 提高系统稳定性</p></li>
<li><p>系统解耦 - 业务边界隔离</p></li>
<li><p>数据同步 - 最终一致性保证</p></li>
</ul>

<p>那到底有多少种队列呢？其实主要看处理业务的范围大小</p>

<ul>
<li><p>应用内部 - 采用线程池，比如 Java ThreadPool 中 BlockingQueue 来做任务级别的缓冲与处理</p></li>
<li><p>应用外部 - 比如 RabbitMQ 、ActiveMQ 就是做应用级别的队列，方便进行业务边界隔离与提高吞吐量</p></li>
</ul>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlvaSzVuYpLsyDzfC4AJF2X5Ww7L1iaaQceaRwoSjwhL05oJdibvVMQIrw/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<p>同时，技术上来讲，消息队列一般分为两种模型：Pull VS Push</p>

<ul>
<li><p>Pull 模型：消费者主动请求消息队列，获取队列中的消息。</p></li>
<li><p>Push 模型：消息队列主动推送消息到消费者</p></li>
</ul>

<p>其中 Pull 模式可以控制消费速度，不必担心自己处理不了消息，只需要维护队列中偏移量 Offset。所以对于消费量有限并且推送到队列的生产者不均匀的情况下，采用 Pull 模式比较合适。</p>

<p>Push 比较适合实时性要求比较高的情况，只要生产者消息发送到消息队列中，队列就会主动 Push 消息到消费者，不过这种模式对消费者的能力要求就提高很多，如果出现队列给消费者推送一些不能处理的消息，消费者出现 Exception 情况下，就会再次入队列，造成消费堵塞的情况。</p>

<p>不过互联网业界比较成熟的队列主要以采用 Pull 模式为主，像 Kafka、RabbitMQ（两种方式都支持）、RocketMQ 等</p>

<h2 id="toc_5">幂等</h2>

<blockquote>
<p>什么是幂等设计呢？</p>
</blockquote>

<p>其实很简单，就是一次请求和多个请求的作用是一样的。用数学上的术语，即是 f(x) = f(f(x))。</p>

<p>那我们为什么要做幂等性的设计呢？主要是因为现在的系统都是采用分布式的方式设计系统，在分布式系统中调用一般分为 3 个状态：成功、失败、超时。</p>

<p>如果调用是成功或者失败都不要紧，因为状态是明确和清晰，但是如果出现超时的情况，就不知道请求是成功还是失败的。</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdla3ufL6yQt2W78GGKeq8M06eN6dmOWia6bSn1BwHwtdrw0gZf33g66wA/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<p>如果出现这种情况，我们该怎么办呢？一般采取重试的操作，重新请求对应接口。如果请求接口是 Get 操作的话，那到还好，因为请求多次的效果是一样的。但是如果是 Post 、Put 操作的话，就会造成数据不一致，甚至数据覆盖等问题。</p>

<p>举个例子：在支付收银台页面进行支付的时候，因为网络超时的问题导致支付失败，这个时候我们都会再进行一次支付操作，但是当支付成功后，发现你的账户余额被减了 2 次，这个时候心里肯定很不爽，心里都要开始骂娘了…</p>

<p>造成这个问题的关键是：网络超时后，不知道支付是什么状态？成功还是失败呢？所以说幂等性设计是必须的，尤其在电商、金融、银行等对数据要求比较高的行业中。</p>

<p>一般在这种场景下我们该怎么解决呢？</p>

<ol>
<li><p>请求方一般会生产一个唯一性 ID 标识，这个标识可以具有业务一样，比如订单号或者支付流水号，在发起请求时候带上唯一性 ID。</p></li>
<li><p>接收者在收到请求后，第一步通过获取唯一性 ID 来查询接收端是否有对应的记录，如果有的话，就直接将上次请求的结果返回，如果没有的话，就进行操作，并在操作完成后记录到对应的表里</p></li>
</ol>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdlCPmk4LiaX77Zhuuv3dgZDc6PR9elSx8CEGQGRfXaeuoWPMlScROZGfA/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<h2 id="toc_6">服务降级</h2>

<p>服务降级主要解决资源不足和访问量过大的问题，比如电商平台在双十一、618 等高峰时候采用部分服务不提供访问，减少对系统的影响。</p>

<p>那降级的方式有哪些呢？</p>

<ul>
<li><p>延迟服务：比如春晚，微信发红包就出现抢到红包，但是账号余额并没有增加，要过几天才能加上去。其实这是微信内部采用延迟服务的方式来保证服务的稳定，通过队列实现记录流水账单</p></li>
<li><p>功能降级：停止不重要的功能是非常有用的方式，把相对不重要的功能暂停掉，让系统释放更多的资源。比如关闭相关文章的推荐、用户的评论功能等等，等高峰过去之后，在把服务恢复回来。</p></li>
<li><p>降低数据一致性：在大促的时候，我们发现页面上不显示真实库存的数据，只显示到底有还是没有库存这两种状态。</p></li>
</ul>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/PgqYrEEtEnpebqWGB1ZhMvUTOhUfsMdl8ZZdFwaAib5B4yibFvXffcW1I3libeD2icxf1AaNmfYuyT85pTke7wBU3Q/640?wx_fmt=png" alt=""/>电商平台备战促销季的运维秘诀——高可用服务层</p>

<p>刚刚说了降级的方式，那我们操作降级的时候有哪些注意点呢？</p>

<ol>
<li><p>清晰定义降级级别： 比如出现吞吐量超过 X，单位时间内响应时间超过 Y 秒、失败次数超过 Z 次等，这些阈值需要在准备的时候，通过压测的方式来确定。</p></li>
<li><p>梳理业务级别：降级之前，首先需要确定哪些业务是必须有，哪些业务是可以有的，哪些业务是可有可无的。</p></li>
<li><p>降级开关：可以通过接入配置中心（比如携程 Apollo、百度 Disconf ）的方式直接后台降级。但是如果公司没有配置中心的话，可以封装一个 API 接口来切分，不过该 API 接口要做成幂等的方式，同时需要做一些简单的签名，来保证其一定的安全性。</p></li>
</ol>

<h2 id="toc_7">总结</h2>

<p>总结一下今天分享的主要内容</p>

<ul>
<li><p>整体架构：根据业务属性进行垂直拆分，减少项目依赖，单独开发、上线、运维</p></li>
<li><p>无状态设计：应用服务中不能保存用户状态数据，如果有状态就会出现难以扩容、单点等问题</p></li>
<li><p>超时设置：当某个服务不可用时，不至于整个系统发生连锁反应</p></li>
<li><p>异步调用：同步调用改成异步调用，解决远程调用故障或调用超时对系统的影响</p></li>
<li><p>服务降级：牺牲非核心业务，保证核心业务的高可用</p></li>
</ul>

<p>所有好的架构设计首要的原则并不是追求先进，而是合理性，要与公司的业务规模和发展趋势相匹配，任何一个公司，哪怕是现在看来规模非常大的公司，比如 BAT 之类，在一开始，其系统架构也应简单和清晰的。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[面试-线程池的成长之路]]></title>
    <link href="http://panlw.github.io/15274396845490.html"/>
    <updated>2018-05-28T00:48:04+08:00</updated>
    <id>http://panlw.github.io/15274396845490.html</id>
    <content type="html"><![CDATA[
<p>尹吉欢 投稿 纯洁的微笑  3天前</p>

<blockquote>
<p><a href="https://mp.weixin.qq.com/s/5dexEENTqJWXN_17c6Lz6A">原文地址</a></p>
</blockquote>

<h1 id="toc_0"><strong>背景</strong></h1>

<p>相信大家在面试过程中遇到面试官问线程的很多，线程过后就是线程池了。从易到难，都是这么个过程，还有就是确实很多人在工作中接触线程池比较少，最多的也就是创建一个然后往里面提交线程，对于一些经验很丰富的面试官来说，一下就可以问出很多线程池相关的问题，与其被问的晕头转向，还不如好好学习。此时不努力更待何时。</p>

<h1 id="toc_1">什么是线程池？</h1>

<p>线程池是一种多线程处理形式，处理过程中将任务提交到线程池，任务的执行交由线程池来管理。</p>

<p>如果每个请求都创建一个线程去处理，那么服务器的资源很快就会被耗尽，使用线程池可以减少创建和销毁线程的次数，每个工作线程都可以被重复利用，可执行多个任务。</p>

<p>如果用生活中的列子来说明，我们可以把线程池当做一个客服团队，如果同时有 1000 个人打电话进行咨询，按照正常的逻辑那就是需要 1000 个客服接听电话，服务客户。现实往往需要考虑到很多层面的东西，比如：资源够不够，招这么多人需要费用比较多。正常的做法就是招 100 个人成立一个客服中心，当有电话进来后分配没有接听的客服进行服务，如果超出了 100 个人同时咨询的话，提示客户等待，稍后处理，等有客服空出来就可以继续服务下一个客户，这样才能达到一个资源的合理利用，实现效益的最大化。</p>

<h1 id="toc_2">Java 中的线程池种类</h1>

<p><strong>1. newSingleThreadExecutor</strong></p>

<p>创建方式：</p>

<pre><code>ExecutorService pool = Executors.newSingleThreadExecutor();
</code></pre>

<p>一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。</p>

<p>使用方式：</p>

<pre><code>import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class ThreadPool {    public static void main(String[] args) {        ExecutorService pool = Executors.newSingleThreadExecutor();        for (int i = 0; i &lt; 10; i++) {            pool.execute(() -&gt; {                System.out.println(Thread.currentThread().getName() + &quot;\t开始发车啦....&quot;);            });        }    }}
</code></pre>

<p>输出结果如下：</p>

<pre><code>pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....pool-1-thread-1    开始发车啦....
</code></pre>

<p>从输出的结果我们可以看出，一直只有一个线程在运行。</p>

<p>2.<strong>newFixedThreadPool</strong></p>

<p>创建方式：</p>

<pre><code>ExecutorService pool = Executors.newFixedThreadPool(10);
</code></pre>

<p>创建固定大小的线程池。每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。</p>

<p>使用方式：</p>

<pre><code>import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class ThreadPool {    public static void main(String[] args) {        ExecutorService pool = Executors.newFixedThreadPool(10);        for (int i = 0; i &lt; 10; i++) {            pool.execute(() -&gt; {                System.out.println(Thread.currentThread().getName() + &quot;\t开始发车啦....&quot;);            });        }    }}
</code></pre>

<p>输出结果如下：</p>

<pre><code>pool-1-thread-1    开始发车啦....pool-1-thread-4    开始发车啦....pool-1-thread-3    开始发车啦....pool-1-thread-2    开始发车啦....pool-1-thread-6    开始发车啦....pool-1-thread-7    开始发车啦....pool-1-thread-5    开始发车啦....pool-1-thread-8    开始发车啦....pool-1-thread-9    开始发车啦....pool-1-thread-10 开始发车啦....
</code></pre>

<p><strong>3. newCachedThreadPool</strong></p>

<p>创建方式：</p>

<pre><code>ExecutorService pool = Executors.newCachedThreadPool();
</code></pre>

<p>创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲的线程，当任务数增加时，此线程池又添加新线程来处理任务。</p>

<p>使用方式如上 2 所示。</p>

<p>4.<strong>newScheduledThreadPool</strong></p>

<p>创建方式：</p>

<pre><code>ScheduledExecutorService pool = Executors.newScheduledThreadPool(10);
</code></pre>

<p>此线程池支持定时以及周期性执行任务的需求。</p>

<p>使用方式：</p>

<pre><code>import java.util.concurrent.Executors;import java.util.concurrent.ScheduledExecutorService;import java.util.concurrent.TimeUnit;public class ThreadPool {    public static void main(String[] args) {        ScheduledExecutorService pool = Executors.newScheduledThreadPool(10);        for (int i = 0; i &lt; 10; i++) {            pool.schedule(() -&gt; {                System.out.println(Thread.currentThread().getName() + &quot;\t开始发车啦....&quot;);            }, 10, TimeUnit.SECONDS);        }    }}
</code></pre>

<p>上面演示的是延迟 10 秒执行任务, 如果想要执行周期性的任务可以用下面的方式，每秒执行一次</p>

<pre><code>//pool.scheduleWithFixedDelay也可以pool.scheduleAtFixedRate(() -&gt; {                System.out.println(Thread.currentThread().getName() + &quot;\t开始发车啦....&quot;);}, 1, 1, TimeUnit.SECONDS);
</code></pre>

<p>5.<strong>newWorkStealingPool</strong></p>

<p>newWorkStealingPool 是 jdk1.8 才有的，会根据所需的并行层次来动态创建和关闭线程，通过使用多个队列减少竞争，底层用的 ForkJoinPool 来实现的。ForkJoinPool 的优势在于，可以充分利用多 cpu，多核 cpu 的优势，把一个任务拆分成多个 “小任务”，把多个“小任务” 放到多个处理器核心上并行执行；当多个 “小任务” 执行完成之后，再将这些执行结果合并起来即可。</p>

<h1 id="toc_3">说说线程池的拒绝策略</h1>

<p>当请求任务不断的过来，而系统此时又处理不过来的时候，我们需要采取的策略是拒绝服务。RejectedExecutionHandler 接口提供了拒绝任务处理的自定义方法的机会。在 ThreadPoolExecutor 中已经包含四种处理策略。</p>

<ul>
<li>  AbortPolicy 策略：该策略会直接抛出异常，阻止系统正常工作。</li>
</ul>

<pre><code>public static class AbortPolicy implements RejectedExecutionHandler {    public AbortPolicy() { }    public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {        throw new RejectedExecutionException(&quot;Task &quot; + r.toString() +                                                 &quot; rejected from &quot; +                                                 e.toString());    }}
</code></pre>

<ul>
<li>  CallerRunsPolicy 策略：只要线程池未关闭，该策略直接在调用者线程中，运行当前的被丢弃的任务。</li>
</ul>

<pre><code>public static class CallerRunsPolicy implements RejectedExecutionHandler {    public CallerRunsPolicy() { }    public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {        if (!e.isShutdown()) {                r.run();        }    }}
</code></pre>

<ul>
<li>  DiscardOleddestPolicy 策略： 该策略将丢弃最老的一个请求，也就是即将被执行的任务，并尝试再次提交当前任务。</li>
</ul>

<pre><code>public static class DiscardOldestPolicy implements RejectedExecutionHandler {    public DiscardOldestPolicy() { }    public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {        if (!e.isShutdown()) {            e.getQueue().poll();            e.execute(r);        }    }}
</code></pre>

<ul>
<li>  DiscardPolicy 策略：该策略默默的丢弃无法处理的任务，不予任何处理。</li>
</ul>

<pre><code>public static class DiscardPolicy implements RejectedExecutionHandler {    public DiscardPolicy() { }    public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {    }}
</code></pre>

<p>除了 JDK 默认为什么提供的四种拒绝策略，我们可以根据自己的业务需求去自定义拒绝策略，自定义的方式很简单，直接实现 RejectedExecutionHandler 接口即可</p>

<p>比如 Spring integration 中就有一个自定义的拒绝策略 CallerBlocksPolicy，将任务插入到队列中，直到队列中有空闲并插入成功的时候，否则将根据最大等待时间一直阻塞，直到超时。</p>

<pre><code>package org.springframework.integration.util;import java.util.concurrent.BlockingQueue;import java.util.concurrent.RejectedExecutionException;import java.util.concurrent.RejectedExecutionHandler;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;import org.apache.commons.logging.Log;import org.apache.commons.logging.LogFactory;public class CallerBlocksPolicy implements RejectedExecutionHandler {    private static final Log logger = LogFactory.getLog(CallerBlocksPolicy.class);    private final long maxWait;    /**     * @param maxWait The maximum time to wait for a queue slot to be     * available, in milliseconds.     */    public CallerBlocksPolicy(long maxWait) {        this.maxWait = maxWait;    }    @Override    public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) {        if (!executor.isShutdown()) {            try {                BlockingQueue&lt;Runnable&gt; queue = executor.getQueue();                if (logger.isDebugEnabled()) {                    logger.debug(&quot;Attempting to queue task execution for &quot; + this.maxWait + &quot; milliseconds&quot;);                }                if (!queue.offer(r, this.maxWait, TimeUnit.MILLISECONDS)) {                    throw new RejectedExecutionException(&quot;Max wait time expired to queue task&quot;);                }                if (logger.isDebugEnabled()) {                    logger.debug(&quot;Task execution queued&quot;);                }            }            catch (InterruptedException e) {                Thread.currentThread().interrupt();                throw new RejectedExecutionException(&quot;Interrupted&quot;, e);            }        }        else {            throw new RejectedExecutionException(&quot;Executor has been shut down&quot;);        }    }}
</code></pre>

<p>定义好之后如何使用呢？光定义没用的呀，一定要用到线程池中呀，可以通过下面的方式自定义线程池，指定拒绝策略。</p>

<pre><code>BlockingQueue&lt;Runnable&gt; workQueue = new ArrayBlockingQueue&lt;&gt;(100);ThreadPoolExecutor executor = new ThreadPoolExecutor(    10, 100, 10, TimeUnit.SECONDS, workQueue, new CallerBlocksPolicy());
</code></pre>

<h1 id="toc_4">execute 和 submit 的区别？</h1>

<p>在前面的讲解中，我们执行任务是用的 execute 方法，除了 execute 方法，还有一个 submit 方法也可以执行我们提交的任务。</p>

<p>这两个方法有什么区别呢？分别适用于在什么场景下呢？我们来做一个简单的分析。</p>

<p>execute 适用于不需要关注返回值的场景，只需要将线程丢到线程池中去执行就可以了</p>

<pre><code>public class ThreadPool {    public static void main(String[] args) {        ExecutorService pool = Executors.newFixedThreadPool(10);        pool.execute(() -&gt; {            System.out.println(Thread.currentThread().getName() + &quot;\t开始发车啦....&quot;);        });    }}
</code></pre>

<p>submit 方法适用于需要关注返回值的场景，submit 方法的定义如下：</p>

<pre><code>public interface ExecutorService extends Executor {　　...　　&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task);　　&lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result);　　Future&lt;?&gt; submit(Runnable task);　　...}
</code></pre>

<p>其子类 AbstractExecutorService 实现了 submit 方法, 可以看到无论参数是 Callable 还是 Runnable，最终都会被封装成 RunnableFuture，然后再调用 execute 执行。</p>

<pre><code>    /**     * @throws RejectedExecutionException {@inheritDoc}     * @throws NullPointerException       {@inheritDoc}     */    public Future&lt;?&gt; submit(Runnable task) {        if (task == null) throw new NullPointerException();        RunnableFuture&lt;Void&gt; ftask = newTaskFor(task, null);        execute(ftask);        return ftask;    }    /**     * @throws RejectedExecutionException {@inheritDoc}     * @throws NullPointerException       {@inheritDoc}     */    public &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result) {        if (task == null) throw new NullPointerException();        RunnableFuture&lt;T&gt; ftask = newTaskFor(task, result);        execute(ftask);        return ftask;    }    /**     * @throws RejectedExecutionException {@inheritDoc}     * @throws NullPointerException       {@inheritDoc}     */    public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) {        if (task == null) throw new NullPointerException();        RunnableFuture&lt;T&gt; ftask = newTaskFor(task);        execute(ftask);        return ftask;    }
</code></pre>

<p>下面我们来看看这三个方法分别如何去使用：</p>

<p><strong>submit(Callable<t style="max-width: 100%;box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: break-word !important;"> task);</t></strong></p>

<pre><code>public class ThreadPool {    public static void main(String[] args) throws Exception {        ExecutorService pool = Executors.newFixedThreadPool(10);        Future&lt;String&gt; future = pool.submit(new Callable&lt;String&gt;() {            @Override            public String call() throws Exception {                return &quot;Hello&quot;;            }        });        String result = future.get();        System.out.println(result);    }}
</code></pre>

<p><strong>submit(Runnable task, T result);</strong></p>

<pre><code>public class ThreadPool {    public static void main(String[] args) throws Exception {        ExecutorService pool = Executors.newFixedThreadPool(10);        Data data = new Data();        Future&lt;Data&gt; future = pool.submit(new MyRunnable(data), data);        String result = future.get().getName();        System.out.println(result);    }}class Data {    String name;    public String getName() {        return name;    }    public void setName(String name) {        this.name = name;    }}class MyRunnable implements Runnable {    private Data data;    public MyRunnable(Data data) {        this.data = data;    }    @Override    public void run() {        data.setName(&quot;yinjihuan&quot;);    }}
</code></pre>

<p><strong>Future submit(Runnable task);</strong><br/>
直接 submit 一个 Runnable 是拿不到返回值的，返回值就是 null.</p>

<h1 id="toc_5">五种线程池的使用场景</h1>

<ul>
<li><p>newSingleThreadExecutor：一个单线程的线程池，可以用于需要保证顺序执行的场景，并且只有一个线程在执行。</p></li>
<li><p>newFixedThreadPool：一个固定大小的线程池，可以用于已知并发压力的情况下，对线程数做限制。</p></li>
<li><p>newCachedThreadPool：一个可以无限扩大的线程池，比较适合处理执行时间比较小的任务。</p></li>
<li><p>newScheduledThreadPool：可以延时启动，定时启动的线程池，适用于需要多个后台线程执行周期任务的场景。</p></li>
<li><p>newWorkStealingPool：一个拥有多个任务队列的线程池，可以减少连接数，创建当前可用 cpu 数量的线程来并行执行。</p></li>
</ul>

<h1 id="toc_6">线程池的关闭</h1>

<p>关闭线程池可以调用 shutdownNow 和 shutdown 两个方法来实现</p>

<p><strong>shutdownNow：对正在执行的任务全部发出 interrupt()，停止执行，对还未开始执行的任务全部取消，并且返回还没开始的任务列表</strong></p>

<pre><code>public class ThreadPool {    public static void main(String[] args) throws Exception {        ExecutorService pool = Executors.newFixedThreadPool(1);        for (int i = 0; i &lt; 5; i++) {            System.err.println(i);            pool.execute(() -&gt; {                try {                    Thread.sleep(30000);                    System.out.println(&quot;--&quot;);                } catch (Exception e) {                    e.printStackTrace();                }            });        }        Thread.sleep(1000);        List&lt;Runnable&gt; runs = pool.shutdownNow();    }}
</code></pre>

<p>上面的代码模拟了立即取消的场景，往线程池里添加 5 个线程任务，然后 sleep 一段时间，线程池只有一个线程，如果此时调用 shutdownNow 后应该需要中断一个正在执行的任务和返回 4 个还未执行的任务，控制台输出下面的内容：</p>

<pre><code>01234[fs.ThreadPool$$Lambda$1/990368553@682a0b20, fs.ThreadPool$$Lambda$1/990368553@682a0b20, fs.ThreadPool$$Lambda$1/990368553@682a0b20, fs.ThreadPool$$Lambda$1/990368553@682a0b20]java.lang.InterruptedException: sleep interrupted    at java.lang.Thread.sleep(Native Method)    at fs.ThreadPool.lambda$0(ThreadPool.java:15)    at fs.ThreadPool$$Lambda$1/990368553.run(Unknown Source)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    at java.lang.Thread.run(Thread.java:745)
</code></pre>

<p><strong>shutdown：当我们调用 shutdown 后，线程池将不再接受新的任务，但也不会去强制终止已经提交或者正在执行中的任务</strong></p>

<pre><code>public class ThreadPool {    public static void main(String[] args) throws Exception {        ExecutorService pool = Executors.newFixedThreadPool(1);        for (int i = 0; i &lt; 5; i++) {            System.err.println(i);            pool.execute(() -&gt; {                try {                    Thread.sleep(30000);                    System.out.println(&quot;--&quot;);                } catch (Exception e) {                    e.printStackTrace();                }            });        }        Thread.sleep(1000);        pool.shutdown();        pool.execute(() -&gt; {            try {                Thread.sleep(30000);                System.out.println(&quot;--&quot;);            } catch (Exception e) {                e.printStackTrace();            }        });    }}
</code></pre>

<p>上面的代码模拟了正在运行的状态，然后调用 shutdown，接着再往里面添加任务，肯定是拒绝添加的，请看输出结果：</p>

<pre><code>01234Exception in thread &quot;main&quot; java.util.concurrent.RejectedExecutionException: Task fs.ThreadPool$$Lambda$2/1747585824@3d075dc0 rejected from java.util.concurrent.ThreadPoolExecutor@214c265e[Shutting down, pool size = 1, active threads = 1, queued tasks = 4, completed tasks = 0]    at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)    at fs.ThreadPool.main(ThreadPool.java:24)
</code></pre>

<p>还有一些业务场景下需要知道线程池中的任务是否全部执行完成，当我们关闭线程池之后，可以用 isTerminated 来判断所有的线程是否执行完成，千万不要用 isShutdown，isShutdown 只是返回你是否调用过 shutdown 的结果。</p>

<pre><code>public class ThreadPool {    public static void main(String[] args) throws Exception {        ExecutorService pool = Executors.newFixedThreadPool(1);        for (int i = 0; i &lt; 5; i++) {            System.err.println(i);            pool.execute(() -&gt; {                try {                    Thread.sleep(3000);                    System.out.println(&quot;--&quot;);                } catch (Exception e) {                    e.printStackTrace();                }            });        }        Thread.sleep(1000);        pool.shutdown();        while(true){              if(pool.isTerminated()){                  System.out.println(&quot;所有的子线程都结束了！&quot;);                  break;              }              Thread.sleep(1000);            }      }}
</code></pre>

<h1 id="toc_7">自定义线程池</h1>

<p>在实际的使用过程中，大部分我们都是用 Executors 去创建线程池直接使用，如果有一些其他的需求，比如指定线程池的拒绝策略，阻塞队列的类型，线程名称的前缀等等，我们可以采用自定义线程池的方式来解决。</p>

<p>如果只是简单的想要改变线程名称的前缀的话可以自定义 ThreadFactory 来实现，在 Executors.new… 中有一个 ThreadFactory 的参数，如果没有指定则用的是 DefaultThreadFactory。</p>

<p>自定义线程池核心在于创建一个 ThreadPoolExecutor 对象，指定参数</p>

<p>下面我们看下 ThreadPoolExecutor 构造函数的定义：</p>

<pre><code>public ThreadPoolExecutor(int corePoolSize,                              int maximumPoolSize,                              long keepAliveTime,                              TimeUnit unit,                              BlockingQueue&lt;Runnable&gt; workQueue,                              ThreadFactory threadFactory,                              RejectedExecutionHandler handler) ;
</code></pre>

<ul>
<li><p>corePoolSize<br/>
线程池大小，决定着新提交的任务是新开线程去执行还是放到任务队列中，也是线程池的最最核心的参数。一般线程池开始时是没有线程的，只有当任务来了并且线程数量小于 corePoolSize 才会创建线程。</p></li>
<li><p>maximumPoolSize<br/>
最大线程数，线程池能创建的最大线程数量。</p></li>
<li><p>keepAliveTime<br/>
在线程数量超过 corePoolSize 后，多余空闲线程的最大存活时间。</p></li>
<li><p>unit<br/>
时间单位</p></li>
<li><p>workQueue<br/>
存放来不及处理的任务的队列，是一个 BlockingQueue。</p></li>
<li><p>threadFactory<br/>
生产线程的工厂类，可以定义线程名，优先级等。</p></li>
<li><p>handler<br/>
拒绝策略，当任务来不及处理的时候，如何处理, 前面有讲解。</p></li>
</ul>

<p>了解上面的参数信息后我们就可以定义自己的线程池了，我这边用 ArrayBlockingQueue 替换了 LinkedBlockingQueue，指定了队列的大小，当任务超出队列大小之后使用 CallerRunsPolicy 拒绝策略处理。</p>

<p>这样做的好处是严格控制了队列的大小，不会出现一直往里面添加任务的情况，有的时候任务处理的比较慢，任务数量过多会占用大量内存，导致内存溢出。</p>

<p>当然你也可以在提交到线程池的入口进行控制，比如用 CountDownLatch, Semaphore 等。</p>

<pre><code>/** * 自定义线程池&lt;br&gt; * 默认的newFixedThreadPool里的LinkedBlockingQueue是一个无边界队列，如果不断的往里加任务，最终会导致内存的不可控&lt;br&gt; * 增加了有边界的队列，使用了CallerRunsPolicy拒绝策略 * @author yinjihuan * */public class FangjiaThreadPoolExecutor {    private static ExecutorService executorService = newFixedThreadPool(50);    private static ExecutorService newFixedThreadPool(int nThreads) {        return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS,                new ArrayBlockingQueue&lt;Runnable&gt;(10000), new DefaultThreadFactory(), new CallerRunsPolicy());    }    public static void execute(Runnable command) {        executorService.execute(command);    }    public static void shutdown() {        executorService.shutdown();    }    static class DefaultThreadFactory implements ThreadFactory {        private static final AtomicInteger poolNumber = new AtomicInteger(1);        private final ThreadGroup group;        private final AtomicInteger threadNumber = new AtomicInteger(1);        private final String namePrefix;        DefaultThreadFactory() {            SecurityManager s = System.getSecurityManager();            group = (s != null) ? s.getThreadGroup() :                                  Thread.currentThread().getThreadGroup();            namePrefix = &quot;FSH-pool-&quot; +                          poolNumber.getAndIncrement() +                         &quot;-thread-&quot;;        }        public Thread newThread(Runnable r) {            Thread t = new Thread(group, r,                                  namePrefix + threadNumber.getAndIncrement(),                                  0);            if (t.isDaemon())                t.setDaemon(false);            if (t.getPriority() != Thread.NORM_PRIORITY)                t.setPriority(Thread.NORM_PRIORITY);            return t;        }    }}
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[不理解Zookeeper一致性原理，谈何异地多活改造]]></title>
    <link href="http://panlw.github.io/15274395979385.html"/>
    <updated>2018-05-28T00:46:37+08:00</updated>
    <id>http://panlw.github.io/15274395979385.html</id>
    <content type="html"><![CDATA[
<p>原创： 陈东明 DBAplus社群 <em>1周前</em></p>

<p><strong>作者介绍</strong><br/>
*<strong><em>陈东明</em></strong>*，饿了么北京技术中心架构组负责人，负责饿了么的产品线架构设计及基础架构研发工作。曾任百度架构师，负责百度即时通讯产品的架构设计。具有丰富的大规模系统构建和基础架构的研发经验，善于复杂业务需求下的大并发、分布式系统设计和持续优化。<br/>
在2017年饿了么做异地多活建设之时，我的团队承担了Zookeeper的异地多活改造。在此期间，我听到了关于Zookeeper一致性的两种不同说法：</p>

<ul>
<li>一种说法是Zookeeper是最终一致性，由于多副本，以及保证大多数成功的Zab协议，当一个客户端进程写入一个新值，另一个客户端进程不能保证马上就会读到，但能保证最终会读到这个值；</li>
<li>另一种说法是Zookeeper的Zab协议类似于Paxos协议，并且提供了强一致性。</li>
</ul>

<p>每当听到这两种说法，我都想纠正一下——不对，Zookeeper是顺序一致性（sequential consistency）。但解释起来比较复杂，需要一篇长文来说明，于是就有了本文，下面就和大家一起讨论下我的看法。</p>

<p>*<strong><em>什么是sequetial consistency</em></strong>*</p>

<p>*<strong><em>从Zookeeper的文档中我们可以看到，里面明确写出它的一致性是sequential consistency。</em></strong>*（详细参见Zookeeper官方文档[1]）</p>

<p>那么，什么是sequential consistency呢？</p>

<p>sequential consistency是Lamport在1979年首次提出的。（详细参见他的这篇论文：<em>How to make a multiprocessor computer that correctly executes multiprocess programs</em>）</p>

<p>论文中定义，当满足下面这个条件时就是sequential consistency：</p>

<p>“<br/>
<em>the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.</em></p>

<p>这段英文定义很晦涩（这是Lamport大神的一贯风格，严谨但晦涩，Paxos协议也是如此），我第一次看到时的感觉是：“这是什么鬼？”为啥每个英文单词我都认识，但就是不知道他在说什么？第一次看到这句话和我有同感的小伙伴留个言吧~</p>

<p>后文我再把这段英文定义翻译成中文，现在我们先看看这篇论文的标题和定义中出现的一个关键词，来说明一下sequential consistency的应用范围。</p>

<p>论文的标题和定义中包含<strong>“multiprocessor</strong>”****这个词，multiprocessor是多核处理器的意思。从这个关键字来看，sequential consistency是用来定义多核处理器和跑在多核处理器上的程序的一个特性。Lamport这篇论文的标题可以翻译成：“如何让具有多核处理器的计算机正确执行多进程程序”，也就是说如果一个多核处理器具有sequential consistency的特性，这个多核处理器就可以正确运行，后面我会解释这个正确运行是什么意思（也就是本文后面讲到的sequential consistency的作用）。</p>

<p>从这个标题我们还可以看出，sequential consistency<strong>应该是个并发编程（concurrent programming）领域的概念</strong>。但我们现在常常在分布式系统领域讨论sequential consistency，比如本文主要讨论的Zookeeper（Zookeeper很明显是一个分布式系统）的一致性。实际上，多核处理器上运行的多个程序，其实也是一种分布式系统（Lamport在他的这篇 <em>Time, Clocks, and the Ordering of Events in a Distributed System</em> 分布式系统的开山之作中也阐述了这个观点）。所以虽然sequential consistency最早在并发编程中提出，但是它可以<strong>应用在分布式系统中</strong>，比如本文讨论的Zookeeper这种分布式存储系统。另外一个比较重要的Linearizability（线性一致性），也是在并发编程中最早提出的，目前也被广泛应用在分布式系统领域中。</p>

<p>接下来我们尝试翻译一下上文那段晦涩的定义。做这段翻译让我找到了上学时做阅读理解的感觉，我先不直接翻译，因为就算我把它翻译成中文，估计很多人还是会有“为啥每个中文字我都懂，还是不知道在说什么”的感觉。</p>

<p>首先，我来解释一下个别的词。首先，<strong>“any execution”</strong>是什么意思？你有多个程序（program）在多核处理器上运行，例如你有两个程序，第一个程序叫P1，它的代码如下： <br/>
P1_write(x);<br/>
P1_read(y);</p>

<p>第二个程序叫P2，代码如下：<br/>
P2_write(u);<br/>
P2_read(v);</p>

<p>从理论上来讲，两个程序运行在两个独立处理器的核上，有多少种执行的可能？我列举其中几种来举例说明。</p>

<p>第一种：<br/>
P1---write(x)--------read(y)--------<br/>
P2-----------write(u)-------read(v)-</p>

<p>第二种：</p>

<p>P1----------write(x)-read(y)--------<br/>
P2--write(u)----------------read(v)-</p>

<p>第三种：<br/>
P1---read(y)----------write(x)------<br/>
P2-----------write(u)---------read(v)-</p>

<p>我们有24种可能的执行顺序，也就是这四个操作任意的排列组合，也就是4!=24。类似第一种和第二种这样的可能性很好理解。为什么会出现像第三种这样的可能的执行呢？那是因为就算在同一个程序中，由于处理会有多级的缓存，以及处理器中coherence的存在，虽然你的程序中是先write后read，在内存中真正生效的顺序，也有可能是先read后write。</p>

<p>其实还会出现类似下面这样的执行，两个操作在两个处理器上同时执行。<br/>
P1--write(x)-read(y)--------<br/>
P2--write(u)--------read(v)-</p>

<p>如果加上同时运行的这种情况，那就有更多种可能性。我的算数不太好，这里就不继续算下去了，因为到底有多少个不重要，重要的是要知道有很多种可能性。那么定义中的“any execution”，就是指任意一种可能的执行，在定义中也可以理解为所有的这些可能的执行。</p>

<p>接着我们再来解释一个词——<strong>“sequential order”</strong>。什么叫sequential order？我们来翻一下英语词典（感觉更像在做阅读理解了）。<br/>
“<br/>
<em>sequential：连续的；相继的；有顺序的</em><br/>
<em>order：命令；顺序；规则；[贸易] 定单</em></p>

<p>sequential order——有顺序的顺序，这又是什么鬼？</p>

<p>其实sequential是有一个接一个的意思，在处理器的这种上下文中，sequential就是指操作（operartion）一个接一个地执行，也就是顺序执行，并且没有重叠。order是指经过一定的调整，让某样东西按照一定的规则变得有序。比如，在算法中的排序算法就是ordering，就是让数组这个东西按照从大到小或从小到大的规则变得有序。<strong>那么sequential order就是指让操作（operation）按照一个接一个这样的规则排列，并且没有重叠。</strong></p>

<p>再说回上文的例子，如果把四个操作，按一个接一个的规则排列，这时就可以得到4！的排列组合个可能的排列（order），同样的，有多少个不重要。</p>

<p>比如：<br/>
P1_write(x);P1_read(y);P2_write(u);P2_read(v);<br/>
P1_read(y);P1_write(x);P2_write(u);P2:read(v);<br/>
P2_write(u);P2_read(v);P1_read(y);P1:write(x);</p>

<p>我这里只列举其中三个，其他的大家可以自己排一下。</p>

<p>重点来了，其实sequential order就是让这四个操作按照一个接一个的顺序执行，并且没有重叠。注意这个排列不是真实的执行，真实的执行是any execution，这里说的是逻辑上的假设，也就是为什么定义有一个as if。</p>

<p>做了这么多的铺垫，下面我们开始翻译定义中的第一句话：</p>

<p>“<br/>
*任意一种可能的执行效果，与所有的处理器上的某一种操作按照顺序排列执行的效果是一样的。 *</p>

<p>注意，some在这里是指“某一种”的意思，不是一些，因为order是单数（真的是在做阅读理解）。</p>

<p>*<strong><em>这句话的意思就是说，一个处理器要满足这个条件，就只能允许满足这个条件的那些可能的执行存在，其他不满足的可能的执行都不会出现。</em></strong>*</p>

<p>从第一句话中我们可以看出，一种多核处理器要想满足sequential consistency，那么多个程序在多个核运行效果“等同”于在一个核上顺序执行所有操作的效果是差不多的。如果这样的话，其实多核的威力基本就消失了。所以无论是从Lamport写这篇论文的1979年，还是现在，没有任何一个现实的多核处理器实现了sequential consistency。</p>

<p>那么，为什么Lamport大神提出这样一个不现实的概念呢？（要注意Lamport写这篇论文时，并没有把它引申到分布式系统领域，就是针对多核处理器、并发编程领域提出的）我们稍后再论述。</p>

<p>这里还要注意的一点是，在我的翻译里用了“效果”一词，但实际上英文原文中用的是“result（结果）”一词。那效果和结果有什么区别吗？我们解释一下什么叫执行结果。</p>

<p>不管是任何真实的执行，还是某种经过顺序排序后的假设执行，程序会产生一定的结果，比如print出来的结果（result）。实际上定义中说的是结果一样。如果定义中用“效果”的话，那这个定义就只是一个定性的定义，如果用“结果”的话，那这个定义就是一个定量的定义。定量的，也就是说可以通过数学证明的。从这点我们可以看出，大神就是不一样，任何理论都是可以通过数学证明为正确的。本文后面还会提到证明的事情，我们这里再卖个关子。</p>

<p>到这里，关于定义的第一句，更准确的翻译就是： </p>

<p>“<br/>
*任意一种可能的执行的结果，与某一种所有的处理器上的操作按照顺序排列执行的结果是一样的。 *</p>

<p>这里我们还要注意一点，结果一样就意味着，如果有人真的要实现一种sequential consistency的多核处理器的话，因为要保证结果一样，所以他是有一定的空间来优化，而不会完全是一个核顺序执行的效果。但是估计这种优化也是非常有限的。</p>

<p>好了，终于把最难的第一句话解释完了，大家可以松口气，第二句就非常简单了。我们还是先解释第二句种出现的一个词——<strong>“sequence”</strong>。刚刚解释过的sequential order是顺序排序（就是按一个接一个排序），其实这是一个动作，动作会产生结果，它的结果产生了一个操作（operation）的队列。第二句中出现的sequence就是指这个操作（operation）的队列。</p>

<p>好，那第二句的翻译就是：</p>

<p>“<br/>
*并且每个独立的处理器的操作，都会按照程序指定的顺序出现在操作队列中。 *</p>

<p>也就是说如果程序里是先write(x);后read(y);，那么只有满足这个顺序的操作队列是符合条件的。这样，我们刚刚说的很多可能的执行就少了很多，这里我也就不计算少了多少，还是那句话，数量不重要，反正是有，而且变少了。</p>

<p>那么第二句话意味着什么？意味着如果一个多核处理器实现了sequential consistency，那这种多核处理器基本上就告别自（缓）行（存）车了。这里我还要继续卖关子，连缓存这种最有效提高处理器性能的优化都没了，大神为什么要提出这个概念？</p>

<p>好了，到这里我们可以把两句翻译合起来，完整看一下：</p>

<p>“<br/>
*任意一种可能的执行的结果，与某一种所有的处理器上的操作按照顺序排列执行的结果是一样的，并且每个独立的处理器的操作，都会按照程序指定的顺序出现在操作队列中。 *</p>

<p>从这个定义中，我们可以看出，此概念的核心就是sequential order，这也就是为什么Lamport老爷子把这种一致性模型称之为sequential consistency。可以说这个命名是非常贴切的，不知道这种贴切对于以英语为母语的人来说是不是更好理解一些，应该不会出现“顺序的顺序是什么鬼”这种情况。如果你看完本文，也觉得sequential很贴切的话，那就说明我讲清楚了。</p>

<p>接下来我们举个具体的例子，再来说明一下： <br/>
execution A<br/>
P0 writex=1-------------------------------<br/>
P1 -------write x=2----------------------<br/>
P2 -----------------read x<mark>1--read x</mark>2<br/>
P3 -----------------read x<mark>1--read x</mark>2</p>

<p>sequetial order: P0_write x=1,P3_read x<mark>1,P4_read x</mark>1,P1_write x=2,P3_read x<mark>2,P4_read x</mark>2</p>

<p>execution B<br/>
P0 write=1-------------------------------<br/>
P1 -------write x=2----------------------<br/>
P2 -----------------read x<mark>2--read x</mark>1<br/>
P3 -----------------read x<mark>2--read x</mark>1</p>

<p>sequetial order: P1_write x=2,P3_read x<mark>2,P4_read x</mark>2,P0_write x=1,P3_read x<mark>1,P4_read x</mark>1</p>

<p>execution C<br/>
P0 write=1-------------------------------<br/>
P1 -------write x=2----------------------<br/>
P2 -----------------read x<mark>1--read x</mark>2<br/>
P3 -----------------read x<mark>2--read x</mark>1</p>

<p>sequetial order: 你找不出一个符合定义中2个条件的一种order。</p>

<p>所以说如果一个多核处理器只允许execution A和B出现，不允许C出现，那么这个多核处理器就是sequetial consistency的。如果它允许C出现，那它就不是sequetial consistency。</p>

<p>到这里，我们已经完整地解释完什么是sequetial consistency。但是，细心的朋友可能会问，如果你的program是多线程的程序怎么办？那我们再把定义中最后的一个细节解释一下：program这个词。</p>

<p>*<strong><em>program</em></strong>*是指可以直接运行在处理器上的指令序列。这个并不是program的严格定义，但是我要指出的是这个program是在操作系统都没有的远古时代就存在的概念，在上文的定义中prgram就是指那个时代的program。</p>

<p>这个program里没有进程、线程的概念，这些概念都是在有了操作系统之后才出现的。因为没有操作系统，也没有内存空间的概念。不像我们现在所说的程序（program），不同的程序有自己独立的内存地址空间。我们这里，内存（memory）对于不同的program来说是shared。另外，需要注意的是program可以用来说明各种程序，不管你是操作系统内核，还是应用程序，都适用。</p>

<p>*<strong><em>sequential consistency</em></strong>*<br/>
*<strong><em>是分布式领域的概念</em></strong>*</p>

<p>刚刚我们说了，sequential consistency虽然是针对并发编程领域提出的，但实际上它是分布式领域的概念，特别是分布式存储系统。在 <em>Distributed system: Principles and Paradigms</em> （作者Andrew S.Tanenbaum, Maarten Van Steen），作者稍微修改了一下Lamport的定义，让这个定义更贴近分布式领域中的概念，我们来看一下作者是怎么改的： </p>

<p>“<br/>
*The result of any execution is the same as if the (read and write) operations by all processes on the data store were executed in some sequential order and the operations of-each individual process appear in this sequence in the order specified by its program. *</p>

<p>作者把processor换成了process，并且加了on the data store这个限定，在Lamport的定义里没有这个限定，其实默认指的是memory（内存）。process就是指进程，以Zookeeper为例，就是指访问Zookeeper的应用进程。program也不是那么底层概念，也是基于操作系统的应用程序了。</p>

<p>*<strong><em>sequential consistency的作用</em></strong>*</p>

<p>好了，下面该揭晓我上面卖的两个关子了。在Lamport的论文中，给出了一个小例子，如下： <br/>
process 1<br/>
 a := 1;<br/>
 if b = 0 then critical section:<br/>
 a := 0<br/>
 else ... fi</p>

<p>process 2<br/>
 b := 1;<br/>
 if a = 0 then critical section:<br/>
 b := 0<br/>
 else ... fi</p>

<p>Lamport在论文中说，如果一种多核处理满足sequential consistency的条件，<strong>那么最多只有一个程序能够进入critical section</strong>。在论文中，Lamport老爷子并没有解释为什么最多只有一个程序能够进入critical section。而是把这个证明留给了论文的读者，就像我们常见的教科书中的课后习题一样。</p>

<p>Lamport老爷子应该是认为这个证明太简单了，不需要花费笔墨来证明它。sequential consistency这篇论文只有不到两页A4纸，是我见过的最短的论文。这是Lamport一贯的做事风格，他的Paxos论文中，有很多细节都是一笔带过的，给读者留下无尽的遐想（瞎想）。</p>

<p>假设现在我们已经证明这个是正确的（虽然我也没去证明，论文给出了两个参考文献用于证明），那这个例子说明了什么呢？</p>

<p>大家也许注意到了，这个例子没有用到任何锁，但它实现了critical section，critical section是一种多线程synchronization 机制。如果多核处理器是sequential consistency的，那么你写的并发程序“天然就是正确的”。</p>

<p>但是处理器的设计者为了追求性能，将保证程序正确的任务丢给程序开发者。只在硬件级别提供了一些fence、cas等指令，基于这些指令操作内核和语言基础库实现了各种synchronization机制，用来保证操作系统的正确性和应用程序的正确性。程序员必须小心谨慎地使用线程和这些synchronization机制，否则就会出各种意想不到的问题。如果你没有debug一个多线程bug连续加班两天，那说明你是大神。</p>

<p>这些指令都是具有更高一致性级别，也就是linearizability，（关于linearizability可以参见我的另外一篇文章《线性一致性（Linearizability）是并发控制的基础》[2]），虽然一致性级别高，但只是个别指令的，处理器整体只是实现了比sequential consistency低很多的一致性级别。所以实现难度大大降低了。</p>

<p>虽然Lamport老爷子的sequential consistency概念在concurrent programming领域中还没有实际意义，但却给我们指出了程序员的天堂在哪里。在程序员的天堂里，没有多（车）线（来）程（车）编（往）程，只要写程序就行。你面试的时候不会再有人问你多线程编程，不会再问你各种锁。</p>

<p>在分布式领域中，sequential consistency更实际一些。Zookeeper就实现了sequential consistency。同理，这应该也是可以证明的，但目前还没发现有Zookeeper社区有任何论文来证明这个。如果你已经明白上面解释的定义，就可以想清楚Zookeeper是sequential consistency。欢迎大家一起来探讨。</p>

<p>*<strong><em>为何Zookeeper要实现</em></strong>*<br/>
*<strong><em>sequential consistency</em></strong>*</p>

<p>实际上，Zookeeper的一致性更复杂一些，Zookeeper的读操作是sequential consistency的，Zookeeper的写操作是linearizability的。关于这个说法，Zookeeper的官方文档中没有写出来，但在社区的邮件组有详细的讨论（邮件组的讨论参见[3]）。</p>

<p>另外，在 <em>Modular Composition of Coordination Services</em> 这篇关于Zookeeper的论文中也有提到这个观点（这篇论文不是Zookeeper的主流论文，但全面分析了Zookeeper的特性，以及Zookeeper跨机房方案，饿了么的Zookeeper异地多活改造也参考了这篇论文中的一些观点），我们可以这么理解Zookeeper，从整体（read操作+write操作）上来说是sequential consistency，写操作实现了linearizability。</p>

<p>通过简单的推理，我们可以得出Lamport论文中的小例子，在Zookeeper中也是成立的。我们可以这样实现分布式锁。但Zookeeper官方推荐的分布式实现方法并没有采用这个方式，而是利用了Zookeeper的linearizability特性实现了分布式锁（关于Zookeeper官方是如何实现分布式锁的，请参见我这篇文章《Zookeeper实现分布式锁和选主》[4]）。</p>

<p>为什么Zookeeper要实现sequential consistency？Zookeeper最核心的功能是用来做coordination service，也就是用来做分布式锁服务，在分布式的环境下，Zookeeper本身怎么做到“天然正确”？没有其他的synchronization机制保证Zookeeper是正确的，所以只要Zookeeper实现了sequential consistency，那它自身就可以保证正确性，从而对外提供锁服务。 </p>

<p>*<strong><em>参考文章：</em></strong>*<br/>
[1]<a href="http://zookeeper.apache.org/doc/r3.4.9/zookeeperProgrammers.html#ch_zkGuarantees">http://zookeeper.apache.org/doc/r3.4.9/zookeeperProgrammers.html#ch_zkGuarantees</a><br/>
[2]<a href="https://blog.csdn.net/cadem/article/details/79932574">https://blog.csdn.net/cadem/article/details/79932574</a><br/>
[3]<a href="http://comments.gmane.org/gmane.comp.java.hadoop.zookeeper.user/5221">http://comments.gmane.org/gmane.comp.java.hadoop.zookeeper.user/5221</a><br/>
[4]<a href="https://blog.csdn.net/cadem/article/details/56289825">https://blog.csdn.net/cadem/article/details/56289825</a></p>

<p><strong>近期热文</strong><br/>
<a href="http://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650767616&amp;idx=1&amp;sn=5ef24be557292923c5ce8c0daa24f2cf&amp;chksm=f3f93495c48ebd83f9a0a70c67abcb5785362606ccdde8751fccd2ba1cf62b43913f2d5026df&amp;scene=21#wechat_redirect">_深入浅出分布式缓存的通用方法_</a><br/>
<a href="http://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650767604&amp;idx=1&amp;sn=c3e41c1f9f645b1db91e90d87cbd6953&amp;chksm=f3f93561c48ebc77bf04cf44c22710d487f16a3e43bb75e4c2d0fe380a80606a3e97f5948b79&amp;scene=21#wechat_redirect">_一文详解消息队列的常见功能场景与使用精髓_</a><br/>
<a href="http://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650767596&amp;idx=1&amp;sn=4c438f4bd2e65d0b9cd92150380d8b18&amp;chksm=f3f93579c48ebc6f7a306c84bdaa9e2b038ed82ccb37440eafb2f5ff99aa1db06bf97f4e71a5&amp;scene=21#wechat_redirect">_MySQL上云后引发的雪崩_</a><br/>
<a href="http://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650767592&amp;idx=1&amp;sn=d2c79676981c63d70d7275486c88abdf&amp;chksm=f3f9357dc48ebc6b50f01ee6c4329df0ea1c4b0d416969911f52805dc3b44ec408f75d250b37&amp;scene=21#wechat_redirect">_方法论与技术栈双管齐下的运维可用性能力建设_</a><br/>
<a href="http://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650767557&amp;idx=1&amp;sn=9a1935096cbe2fbd658dc28c8f52a402&amp;chksm=f3f93550c48ebc462c16e23d3e6941b28fd71ce50c790d8e39188e20f57f4a34b3e01ef22409&amp;scene=21#wechat_redirect">_中小型企业大数据体系建设的核心技术选型_</a></p>

<p><strong>近期活动</strong><br/>
<a href="http://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650767615&amp;idx=1&amp;sn=243dd45953cad2716f389bf25f99a01d&amp;chksm=f3f9356ac48ebc7c4ed927bfa23def9a14f927d08e4e255c026c9587de83a69062cfc4b057ad&amp;scene=21#wechat_redirect">_2018 DAMS中国数据资产管理峰会_</a><br/>
微信扫一扫<br/>
关注该公众号</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[消息队列之 RabbitMQ]]></title>
    <link href="http://panlw.github.io/15274392082175.html"/>
    <updated>2018-05-28T00:40:08+08:00</updated>
    <id>http://panlw.github.io/15274392082175.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://juejin.im/post/5a67f7836fb9a01cb74e8931">原文地址</a></p>
</blockquote>

<p>关于消息队列，从前年开始断断续续看了些资料，想写很久了，但一直没腾出空，近来分别碰到几个朋友聊这块的技术选型，是时候把这块的知识整理记录一下了。</p>

<p>市面上的消息队列产品有很多，比如老牌的 ActiveMQ、RabbitMQ ，目前我看最火的 Kafka ，还有 ZeroMQ ，去年底阿里巴巴捐赠给 Apache 的 RocketMQ ，连 redis 这样的 NoSQL 数据库也支持 MQ 功能。总之这块知名的产品就有十几种，就我自己的使用经验和兴趣只打算谈谈 RabbitMQ、Kafka 和 ActiveMQ ，本文先讲 RabbitMQ ，在此之前先看下消息队列的相关概念。</p>

<h1 id="toc_0">什么叫消息队列</h1>

<p>消息（Message）是指在应用间传送的数据。消息可以非常简单，比如只包含文本字符串，也可以更复杂，可能包含嵌入对象。</p>

<p>消息队列（Message Queue）是一种应用间的通信方式，消息发送后可以立即返回，由消息系统来确保消息的可靠传递。消息发布者只管把消息发布到 MQ 中而不用管谁来取，消息使用者只管从 MQ 中取消息而不管是谁发布的。这样发布者和使用者都不用知道对方的存在。</p>

<h1 id="toc_1">为何用消息队列</h1>

<p>从上面的描述中可以看出消息队列是一种应用间的异步协作机制，那什么时候需要使用 MQ 呢？</p>

<p>以常见的订单系统为例，用户点击【下单】按钮之后的业务逻辑可能包括：扣减库存、生成相应单据、发红包、发短信通知。在业务发展初期这些逻辑可能放在一起同步执行，随着业务的发展订单量增长，需要提升系统服务的性能，这时可以将一些不需要立即生效的操作拆分出来异步执行，比如发放红包、发短信通知等。这种场景下就可以用 MQ ，在下单的主流程（比如扣减库存、生成相应单据）完成之后发送一条消息到 MQ 让主流程快速完结，而由另外的单独线程拉取 MQ 的消息（或者由 MQ 推送消息），当发现 MQ 中有发红包或发短信之类的消息时，执行相应的业务逻辑。</p>

<p>以上是用于业务解耦的情况，其它常见场景包括最终一致性、广播、错峰流控等等。</p>

<h1 id="toc_2">RabbitMQ 特点</h1>

<p>RabbitMQ 是一个由 Erlang 语言开发的 AMQP 的开源实现。</p>

<p>AMQP ：Advanced Message Queue，高级消息队列协议。它是应用层协议的一个开放标准，为面向消息的中间件设计，基于此协议的客户端与消息中间件可传递消息，并不受产品、开发语言等条件的限制。</p>

<p>RabbitMQ 最初起源于金融系统，用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。具体特点包括：</p>

<ol>
<li><p>可靠性（Reliability） RabbitMQ 使用一些机制来保证可靠性，如持久化、传输确认、发布确认。</p></li>
<li><p>灵活的路由（Flexible Routing） 在消息进入队列之前，通过 Exchange 来路由消息的。对于典型的路由功能，RabbitMQ 已经提供了一些内置的 Exchange 来实现。针对更复杂的路由功能，可以将多个 Exchange 绑定在一起，也通过插件机制实现自己的 Exchange 。</p></li>
<li><p>消息集群（Clustering） 多个 RabbitMQ 服务器可以组成一个集群，形成一个逻辑 Broker 。</p></li>
<li><p>高可用（Highly Available Queues） 队列可以在集群中的机器上进行镜像，使得在部分节点出问题的情况下队列仍然可用。</p></li>
<li><p>多种协议（Multi-protocol） RabbitMQ 支持多种消息队列协议，比如 STOMP、MQTT 等等。</p></li>
<li><p>多语言客户端（Many Clients） RabbitMQ 几乎支持所有常用语言，比如 Java、.NET、Ruby 等等。</p></li>
<li><p>管理界面（Management UI） RabbitMQ 提供了一个易用的用户界面，使得用户可以监控和管理消息 Broker 的许多方面。</p></li>
<li><p>跟踪机制（Tracing） 如果消息异常，RabbitMQ 提供了消息跟踪机制，使用者可以找出发生了什么。</p></li>
<li><p>插件机制（Plugin System） RabbitMQ 提供了许多插件，来从多方面进行扩展，也可以编写自己的插件。</p></li>
</ol>

<h1 id="toc_3">RabbitMQ 中的概念</h1>

<h3 id="toc_4">消息模型</h3>

<p>所有 MQ 产品从模型抽象上来说都是一样的过程： 消费者（consumer）订阅某个队列。生产者（producer）创建消息，然后发布到队列（queue）中，最后将消息发送到监听的消费者。</p>

<p><img src="https://user-gold-cdn.xitu.io/2018/1/24/161260568dd200d6?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/></p>

<h3 id="toc_5">RabbitMQ 基本概念</h3>

<p>上面只是最简单抽象的描述，具体到 RabbitMQ 则有更详细的概念需要解释。上面介绍过 RabbitMQ 是 AMQP 协议的一个开源实现，所以其内部实际上也是 AMQP 中的基本概念：</p>

<p><img src="https://user-gold-cdn.xitu.io/2018/1/24/161260568dd66584?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/></p>

<ol>
<li> Message 消息，消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括 routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。</li>
<li> Publisher 消息的生产者，也是一个向交换器发布消息的客户端应用程序。</li>
<li> Exchange 交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。</li>
<li> Binding 绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。</li>
<li> Queue 消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。</li>
<li> Connection 网络连接，比如一个 TCP 连接。</li>
<li> Channel 信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的 TCP 连接内地虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。</li>
<li> Consumer 消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。</li>
<li> Virtual Host 虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。</li>
<li> Broker 表示消息队列服务器实体。</li>
</ol>

<h4 id="toc_6">AMQP 中的消息路由</h4>

<p>AMQP 中消息的路由过程和 Java 开发者熟悉的 JMS 存在一些差别，AMQP 中增加了 Exchange 和 Binding 的角色。生产者把消息发布到 Exchange 上，消息最终到达队列并被消费者接收，而 Binding 决定交换器的消息应该发送到那个队列。</p>

<p><img src="https://user-gold-cdn.xitu.io/2018/1/24/161260568e434217?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/></p>

<h4 id="toc_7">Exchange 类型</h4>

<p>Exchange 分发消息时根据类型的不同分发策略有区别，目前共四种类型：direct、fanout、topic、headers 。headers 匹配 AMQP 消息的 header 而不是路由键，此外 headers 交换器和 direct 交换器完全一致，但性能差很多，目前几乎用不到了，所以直接看另外三种类型：</p>

<ol>
<li> direct <img src="https://user-gold-cdn.xitu.io/2018/1/24/161260568dc498b6?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/> 消息中的路由键（routing key）如果和 Binding 中的 binding key 一致， 交换器就将消息发到对应的队列中。路由键与队列名完全匹配，如果一个队列绑定到交换机要求路由键为 “dog”，则只转发 routing key 标记为“dog” 的消息，不会转发 “dog.puppy”，也不会转发“dog.guard” 等等。它是完全匹配、单播的模式。</li>
<li> fanout <img src="https://user-gold-cdn.xitu.io/2018/1/24/161260568fe5ce35?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/> 每个发到 fanout 类型交换器的消息都会分到所有绑定的队列上去。fanout 交换器不处理路由键，只是简单的将队列绑定到交换器上，每个发送到交换器的消息都会被转发到与该交换器绑定的所有队列上。很像子网广播，每台子网内的主机都获得了一份复制的消息。fanout 类型转发消息是最快的。</li>
<li> topic <img src="https://user-gold-cdn.xitu.io/2018/1/24/161260569051565f?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/> topic 交换器通过模式匹配分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。它将路由键和绑定键的字符串切分成单词，这些单词之间用点隔开。它同样也会识别两个通配符：符号 “#” 和符号“<em>”。# 匹配 0 个或多个单词，</em> 匹配不多不少一个单词。</li>
</ol>

<h1 id="toc_8">RabbitMQ 安装</h1>

<p>一般来说安装 RabbitMQ 之前要安装 Erlang ，可以去 <a href="https://link.juejin.im?target=http%3A%2F%2Fwww.erlang.org%2Fdownloads">Erlang 官网</a>下载。接着去 <a href="https://link.juejin.im?target=https%3A%2F%2Fwww.rabbitmq.com%2Fdownload.html">RabbitMQ 官网</a>下载安装包，之后解压缩即可。根据操作系统不同官网提供了相应的安装说明：<a href="https://link.juejin.im?target=http%3A%2F%2Fwww.rabbitmq.com%2Finstall-windows.html">Windows</a>、<a href="https://link.juejin.im?target=http%3A%2F%2Fwww.rabbitmq.com%2Finstall-debian.html">Debian / Ubuntu</a>、<a href="https://link.juejin.im?target=http%3A%2F%2Fwww.rabbitmq.com%2Finstall-rpm.html">RPM-based Linux</a>、<a href="https://link.juejin.im?target=http%3A%2F%2Fwww.rabbitmq.com%2Finstall-standalone-mac.html">Mac</a></p>

<p>如果是 Mac 用户，个人推荐使用 HomeBrew 来安装，安装前要先更新 brew：</p>

<pre><code>brew update

</code></pre>

<p>接着安装 rabbitmq 服务器：</p>

<pre><code>brew install rabbitmq

</code></pre>

<p>这样 RabbitMQ 就安装好了，安装过程中会自动其所依赖的 Erlang 。</p>

<h1 id="toc_9">RabbitMQ 运行和管理</h1>

<ol>
<li> 启动 启动很简单，找到安装后的 RabbitMQ 所在目录下的 sbin 目录，可以看到该目录下有 6 个以 rabbitmq 开头的可执行文件，直接执行 rabbitmq-server 即可，下面将 RabbitMQ 的安装位置以 . 代替，启动命令就是：</li>
</ol>

<pre><code>./sbin/rabbitmq-server

</code></pre>

<p>启动正常的话会看到一些启动过程信息和最后的 completed with 7 plugins，这也说明启动的时候默认加载了 7 个插件。</p>

<p><img src="https://user-gold-cdn.xitu.io/2018/1/24/16126056ba03d9f0?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/></p>

<ol>
<li> 后台启动 如果想让 RabbitMQ 以守护程序的方式在后台运行，可以在启动的时候加上 -detached 参数：</li>
</ol>

<pre><code>./sbin/rabbitmq-server -detached

</code></pre>

<ol>
<li> 查询服务器状态 sbin 目录下有个特别重要的文件叫 rabbitmqctl ，它提供了 RabbitMQ 管理需要的几乎一站式解决方案，绝大部分的运维命令它都可以提供。 查询 RabbitMQ 服务器的状态信息可以用参数 status ：</li>
</ol>

<pre><code>./sbin/rabbitmqctl status

</code></pre>

<p>该命令将输出服务器的很多信息，比如 RabbitMQ 和 Erlang 的版本、OS 名称、内存等等</p>

<ol>
<li> 关闭 RabbitMQ 节点 我们知道 RabbitMQ 是用 Erlang 语言写的，在 Erlang 中有两个概念：节点和应用程序。节点就是 Erlang 虚拟机的每个实例，而多个 Erlang 应用程序可以运行在同一个节点之上。节点之间可以进行本地通信（不管他们是不是运行在同一台服务器之上）。比如一个运行在节点 A 上的应用程序可以调用节点 B 上应用程序的方法，就好像调用本地函数一样。如果应用程序由于某些原因奔溃，Erlang 节点会自动尝试重启应用程序。 如果要关闭整个 RabbitMQ 节点可以用参数 stop ：</li>
</ol>

<pre><code>./sbin/rabbitmqctl stop

</code></pre>

<p>它会和本地节点通信并指示其干净的关闭，也可以指定关闭不同的节点，包括远程节点，只需要传入参数 -n ：</p>

<pre><code>./sbin/rabbitmqctl -n rabbit@server.example.com stop 

</code></pre>

<p>-n node 默认 node 名称是 rabbit@server ，如果你的主机名是 server.example.com ，那么 node 名称就是 <a href="mailto:rabbit@server.example.com">rabbit@server.example.com</a> 。</p>

<ol>
<li> 关闭 RabbitMQ 应用程序 如果只想关闭应用程序，同时保持 Erlang 节点运行则可以用 stop_app：</li>
</ol>

<pre><code>./sbin/rabbitmqctl stop_app

</code></pre>

<p>这个命令在后面要讲的集群模式中将会很有用。</p>

<ol>
<li> 启动 RabbitMQ 应用程序</li>
</ol>

<pre><code>./sbin/rabbitmqctl start_app

</code></pre>

<ol>
<li> 重置 RabbitMQ 节点</li>
</ol>

<pre><code>./sbin/rabbitmqctl reset

</code></pre>

<p>该命令将清除所有的队列。</p>

<ol>
<li> 查看已声明的队列</li>
</ol>

<pre><code>./sbin/rabbitmqctl list_queues

</code></pre>

<ol>
<li> 查看交换器</li>
</ol>

<pre><code>./sbin/rabbitmqctl list_exchanges

</code></pre>

<p>该命令还可以附加参数，比如列出交换器的名称、类型、是否持久化、是否自动删除：</p>

<pre><code>./sbin/rabbitmqctl list_exchanges name type durable auto_delete

</code></pre>

<ol>
<li> 查看绑定</li>
</ol>

<pre><code>./sbin/rabbitmqctl list_bindings

</code></pre>

<h1 id="toc_10">Java 客户端访问</h1>

<p>RabbitMQ 支持多种语言访问，以 Java 为例看下一般使用 RabbitMQ 的步骤。</p>

<ol>
<li> maven 工程的 pom 文件中添加依赖</li>
</ol>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt;
    &lt;artifactId&gt;amqp-client&lt;/artifactId&gt;
    &lt;version&gt;4.1.0&lt;/version&gt;
&lt;/dependency&gt;

</code></pre>

<ol>
<li> 消息生产者</li>
</ol>

<pre><code>package org.study.rabbitmq;
import com.rabbitmq.client.Channel;
import com.rabbitmq.client.Connection;
import com.rabbitmq.client.ConnectionFactory;
import java.io.IOException;
import java.util.concurrent.TimeoutException;
public class Producer {

    public static void main(String[] args) throws IOException, TimeoutException {
        //创建连接工厂
        ConnectionFactory factory = new ConnectionFactory();
        factory.setUsername(&quot;guest&quot;);
        factory.setPassword(&quot;guest&quot;);
        //设置 RabbitMQ 地址
        factory.setHost(&quot;localhost&quot;);
        //建立到代理服务器到连接
        Connection conn = factory.newConnection();
        //获得信道
        Channel channel = conn.createChannel();
        //声明交换器
        String exchangeName = &quot;hello-exchange&quot;;
        channel.exchangeDeclare(exchangeName, &quot;direct&quot;, true);

        String routingKey = &quot;hola&quot;;
        //发布消息
        byte[] messageBodyBytes = &quot;quit&quot;.getBytes();
        channel.basicPublish(exchangeName, routingKey, null, messageBodyBytes);

        channel.close();
        conn.close();
    }
}

</code></pre>

<ol>
<li> 消息消费者</li>
</ol>

<pre><code>package org.study.rabbitmq;
import com.rabbitmq.client.*;
import java.io.IOException;
import java.util.concurrent.TimeoutException;
public class Consumer {

    public static void main(String[] args) throws IOException, TimeoutException {
        ConnectionFactory factory = new ConnectionFactory();
        factory.setUsername(&quot;guest&quot;);
        factory.setPassword(&quot;guest&quot;);
        factory.setHost(&quot;localhost&quot;);
        //建立到代理服务器到连接
        Connection conn = factory.newConnection();
        //获得信道
        final Channel channel = conn.createChannel();
        //声明交换器
        String exchangeName = &quot;hello-exchange&quot;;
        channel.exchangeDeclare(exchangeName, &quot;direct&quot;, true);
        //声明队列
        String queueName = channel.queueDeclare().getQueue();
        String routingKey = &quot;hola&quot;;
        //绑定队列，通过键 hola 将队列和交换器绑定起来
        channel.queueBind(queueName, exchangeName, routingKey);

        while(true) {
            //消费消息
            boolean autoAck = false;
            String consumerTag = &quot;&quot;;
            channel.basicConsume(queueName, autoAck, consumerTag, new DefaultConsumer(channel) {
                @Override
                public void handleDelivery(String consumerTag,
                                           Envelope envelope,
                                           AMQP.BasicProperties properties,
                                           byte[] body) throws IOException {
                    String routingKey = envelope.getRoutingKey();
                    String contentType = properties.getContentType();
                    System.out.println(&quot;消费的路由键：&quot; + routingKey);
                    System.out.println(&quot;消费的内容类型：&quot; + contentType);
                    long deliveryTag = envelope.getDeliveryTag();
                    //确认消息
                    channel.basicAck(deliveryTag, false);
                    System.out.println(&quot;消费的消息体内容：&quot;);
                    String bodyStr = new String(body, &quot;UTF-8&quot;);
                    System.out.println(bodyStr);

                }
            });
        }
    }
}

</code></pre>

<ol>
<li> 启动 RabbitMQ 服务器</li>
</ol>

<pre><code>./sbin/rabbitmq-server

</code></pre>

<ol>
<li> 运行 Consumer 先运行 Consumer ，这样当生产者发送消息的时候能在消费者后端看到消息记录。</li>
<li> 运行 Producer 接着运行 Producer , 发布一条消息，在 Consumer 的控制台能看到接收的消息： <img src="https://user-gold-cdn.xitu.io/2018/1/24/16126056b9f84c7d?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt=""/></li>
</ol>

<h1 id="toc_11">RabbitMQ 集群</h1>

<p>RabbitMQ 最优秀的功能之一就是内建集群，这个功能设计的目的是允许消费者和生产者在节点崩溃的情况下继续运行，以及通过添加更多的节点来线性扩展消息通信吞吐量。RabbitMQ 内部利用 Erlang 提供的分布式通信框架 OTP 来满足上述需求，使客户端在失去一个 RabbitMQ 节点连接的情况下，还是能够重新连接到集群中的任何其他节点继续生产、消费消息。</p>

<h3 id="toc_12">RabbitMQ 集群中的一些概念</h3>

<p>RabbitMQ 会始终记录以下四种类型的内部元数据：</p>

<ol>
<li> 队列元数据 包括队列名称和它们的属性，比如是否可持久化，是否自动删除</li>
<li> 交换器元数据 交换器名称、类型、属性</li>
<li> 绑定元数据 内部是一张表格记录如何将消息路由到队列</li>
<li> vhost 元数据 为 vhost 内部的队列、交换器、绑定提供命名空间和安全属性</li>
</ol>

<p>在单一节点中，RabbitMQ 会将所有这些信息存储在内存中，同时将标记为可持久化的队列、交换器、绑定存储到硬盘上。存到硬盘上可以确保队列和交换器在节点重启后能够重建。而在集群模式下同样也提供两种选择：存到硬盘上（独立节点的默认设置），存在内存中。</p>

<p>如果在集群中创建队列，集群只会在单个节点而不是所有节点上创建完整的队列信息（元数据、状态、内容）。结果是只有队列的所有者节点知道有关队列的所有信息，因此当集群节点崩溃时，该节点的队列和绑定就消失了，并且任何匹配该队列的绑定的新消息也丢失了。还好 RabbitMQ 2.6.0 之后提供了镜像队列以避免集群节点故障导致的队列内容不可用。</p>

<p>RabbitMQ 集群中可以共享 user、vhost、exchange 等，所有的数据和状态都是必须在所有节点上复制的，例外就是上面所说的消息队列。RabbitMQ 节点可以动态的加入到集群中。</p>

<p>当在集群中声明队列、交换器、绑定的时候，这些操作会直到所有集群节点都成功提交元数据变更后才返回。集群中有内存节点和磁盘节点两种类型，内存节点虽然不写入磁盘，但是它的执行比磁盘节点要好。内存节点可以提供出色的性能，磁盘节点能保障配置信息在节点重启后仍然可用，那集群中如何平衡这两者呢？</p>

<p>RabbitMQ 只要求集群中至少有一个磁盘节点，所有其他节点可以是内存节点，当节点加入或离开集群时，它们必须要将该变更通知到至少一个磁盘节点。如果只有一个磁盘节点，刚好又是该节点崩溃了，那么集群可以继续路由消息，但不能创建队列、创建交换器、创建绑定、添加用户、更改权限、添加或删除集群节点。换句话说集群中的唯一磁盘节点崩溃的话，集群仍然可以运行，但直到该节点恢复，否则无法更改任何东西。</p>

<h3 id="toc_13">RabbitMQ 集群配置和启动</h3>

<p>如果是在一台机器上同时启动多个 RabbitMQ 节点来组建集群的话，只用上面介绍的方式启动第二、第三个节点将会因为节点名称和端口冲突导致启动失败。所以在每次调用 rabbitmq-server 命令前，设置环境变量 RABBITMQ_NODENAME 和 RABBITMQ_NODE_PORT 来明确指定唯一的节点名称和端口。下面的例子端口号从 5672 开始，每个新启动的节点都加 1，节点也分别命名为 test_rabbit_1、test_rabbit_2、test_rabbit_3。</p>

<p>启动第 1 个节点：</p>

<pre><code>RABBITMQ_NODENAME=test_rabbit_1 RABBITMQ_NODE_PORT=5672 ./sbin/rabbitmq-server -detached

</code></pre>

<p>启动第 2 个节点：</p>

<pre><code>RABBITMQ_NODENAME=test_rabbit_2 RABBITMQ_NODE_PORT=5673 ./sbin/rabbitmq-server -detached

</code></pre>

<p>启动第 2 个节点前建议将 RabbitMQ 默认激活的插件关掉，否则会存在使用了某个插件的端口号冲突，导致节点启动不成功。</p>

<p>现在第 2 个节点和第 1 个节点都是独立节点，它们并不知道其他节点的存在。集群中除第一个节点外后加入的节点需要获取集群中的元数据，所以要先停止 Erlang 节点上运行的 RabbitMQ 应用程序，并重置该节点元数据，再加入并且获取集群的元数据，最后重新启动 RabbitMQ 应用程序。</p>

<p>停止第 2 个节点的应用程序：</p>

<pre><code>./sbin/rabbitmqctl -n test_rabbit_2 stop_app

</code></pre>

<p>重置第 2 个节点元数据：</p>

<pre><code>./sbin/rabbitmqctl -n test_rabbit_2 reset

</code></pre>

<p>第 2 节点加入第 1 个节点组成的集群：</p>

<pre><code>./sbin/rabbitmqctl -n test_rabbit_2 join_cluster test_rabbit_1@localhost

</code></pre>

<p>启动第 2 个节点的应用程序</p>

<pre><code>./sbin/rabbitmqctl -n test_rabbit_2 start_app

</code></pre>

<p>第 3 个节点的配置过程和第 2 个节点类似：</p>

<pre><code>RABBITMQ_NODENAME=test_rabbit_3 RABBITMQ_NODE_PORT=5674 ./sbin/rabbitmq-server -detached

./sbin/rabbitmqctl -n test_rabbit_3 stop_app

./sbin/rabbitmqctl -n test_rabbit_3 reset

./sbin/rabbitmqctl -n test_rabbit_3 join_cluster test_rabbit_1@localhost

./sbin/rabbitmqctl -n test_rabbit_3 start_app

</code></pre>

<h3 id="toc_14">RabbitMQ 集群运维</h3>

<p>停止某个指定的节点，比如停止第 2 个节点：</p>

<pre><code>RABBITMQ_NODENAME=test_rabbit_2 ./sbin/rabbitmqctl stop

</code></pre>

<p>查看节点 3 的集群状态：</p>

<pre><code>./sbin/rabbitmqctl -n test_rabbit_3 cluster_status

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RPC 原理及 RPC 实例分析 - God is a Coder..]]></title>
    <link href="http://panlw.github.io/15274391142573.html"/>
    <updated>2018-05-28T00:38:34+08:00</updated>
    <id>http://panlw.github.io/15274391142573.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://my.oschina.net/hosee/blog/711632">原文地址</a></p>
</blockquote>

<p>在学校期间大家都写过不少程序，比如写个 hello world 服务类，然后本地调用下，如下所示。这些程序的特点是服务消费方和服务提供方是本地调用关系。</p>

<pre><code>public class Test {
     public static void main(String[] args) {
         HelloWorldService helloWorldService = new HelloWorldServiceImpl();
         helloWorldService.sayHello(&quot;test&quot;);
     }
}
</code></pre>

<p>而一旦踏入公司尤其是大型互联网公司就会发现，公司的系统都由成千上万大大小小的服务组成，各服务部署在不同的机器上，由不同的团队负责。</p>

<p>这时就会遇到两个问题：</p>

<ol>
<li> 要搭建一个新服务，免不了需要依赖他人的服务，而现在他人的服务都在远端，怎么调用？</li>
<li> 其它团队要使用我们的新服务，我们的服务该怎么发布以便他人调用？下文将对这两个问题展开探讨。</li>
</ol>

<h2 id="toc_0">1.  如何调用他人的远程服务？</h2>

<p>由于各服务部署在不同机器，服务间的调用免不了网络通信过程，服务消费方每调用一个服务都要写一坨网络通信相关的代码，不仅复杂而且极易出错。</p>

<p>如果有一种方式能让我们像调用本地服务一样调用远程服务，而让调用者对网络通信这些细节透明，那么将大大提高生产力，比如服务消费方在执行 helloWorldService.sayHello(&quot;test&quot;) 时，实质上调用的是远端的服务。这种方式其实就是 RPC（Remote Procedure Call Protocol），在各大互联网公司中被广泛使用，如阿里巴巴的 hsf、dubbo（开源）、Facebook 的 thrift（开源）、Google grpc（开源）、Twitter 的 finagle（开源）等。</p>

<p>要让网络通信细节对使用者透明，我们需要对通信细节进行封装，我们先看下一个 RPC 调用的流程涉及到哪些通信细节：</p>

<p><img src="http://static.oschina.net/uploads/space/2016/0714/102634_AAIe_2243330.png" alt=""/></p>

<ol>
<li> 服务消费方（client）调用以本地调用方式调用服务；</li>
<li> client stub 接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体；</li>
<li> client stub 找到服务地址，并将消息发送到服务端；</li>
<li> server stub 收到消息后进行解码；</li>
<li> server stub 根据解码结果调用本地的服务；</li>
<li> 本地服务执行并将结果返回给 server stub；</li>
<li> server stub 将返回结果打包成消息并发送至消费方；</li>
<li> client stub 接收到消息，并进行解码；</li>
<li> 服务消费方得到最终结果。</li>
</ol>

<p>RPC 的目标就是要 2~8 这些步骤都封装起来，让用户对这些细节透明。</p>

<h3 id="toc_1">1.1 怎么做到透明化远程服务调用？</h3>

<p>怎么封装通信细节才能让用户像以本地调用方式调用远程服务呢？对 java 来说就是使用代理！java 代理有两种方式：</p>

<ol>
<li> jdk 动态代理</li>
<li> 字节码生成</li>
</ol>

<p>尽管字节码生成方式实现的代理更为强大和高效，但代码维护不易，大部分公司实现 RPC 框架时还是选择动态代理方式。</p>

<p>下面简单介绍下动态代理怎么实现我们的需求。我们需要实现 RPCProxyClient 代理类，代理类的 invoke 方法中封装了与远端服务通信的细节，消费方首先从 RPCProxyClient 获得服务提供方的接口，当执行 helloWorldService.sayHello(&quot;test&quot;) 方法时就会调用 invoke 方法。</p>

<pre><code>public class RPCProxyClient implements java.lang.reflect.InvocationHandler{
    private Object obj;

    public RPCProxyClient(Object obj){
        this.obj=obj;
    }

    /**
     * 得到被代理对象;
     */
    public static Object getProxy(Object obj){
        return java.lang.reflect.Proxy.newProxyInstance(obj.getClass().getClassLoader(),
                obj.getClass().getInterfaces(), new RPCProxyClient(obj));
    }

    /**
     * 调用此方法执行
     */
    public Object invoke(Object proxy, Method method, Object[] args)
            throws Throwable {
        //结果参数;
        Object result = new Object();
        // ...执行通信相关逻辑
        // ...
        return result;
    }
}
</code></pre>

<pre><code>public class Test {
     public static void main(String[] args) {
         HelloWorldService helloWorldService = (HelloWorldService)RPCProxyClient.getProxy(HelloWorldService.class);
         helloWorldService.sayHello(&quot;test&quot;);
     }
 }
</code></pre>

<h3 id="toc_2">1.2  怎么对消息进行编码和解码？</h3>

<h4 id="toc_3">1.2.1 确定消息数据结构</h4>

<p>　　上节讲了 invoke 里需要封装通信细节（通信细节再后面几章详细探讨），而通信的第一步就是要确定客户端和服务端相互通信的消息结构。客户端的请求消息结构一般需要包括以下内容：</p>

<p>1）接口名称</p>

<p>　　在我们的例子里接口名是 “HelloWorldService”，如果不传，服务端就不知道调用哪个接口了；</p>

<p>2）方法名</p>

<p>　　一个接口内可能有很多方法，如果不传方法名服务端也就不知道调用哪个方法；</p>

<p>3）参数类型 &amp; 参数值</p>

<p>　　参数类型有很多，比如有 bool、int、long、double、string、map、list，甚至如 struct（class）；以及相应的参数值；</p>

<p>4）超时时间</p>

<p>5）requestID，标识唯一请求 id，在下面一节会详细描述 requestID 的用处。</p>

<p>　　同理服务端返回的消息结构一般包括以下内容。</p>

<p>1）返回值</p>

<p>2）状态 code</p>

<p>3）requestID </p>

<h4 id="toc_4">1.2.2 序列化</h4>

<p>一旦确定了消息的数据结构后，下一步就是要考虑序列化与反序列化了。</p>

<p>什么是序列化？序列化就是将数据结构或对象转换成二进制串的过程，也就是编码的过程。</p>

<p>什么是反序列化？将在序列化过程中所生成的二进制串转换成数据结构或者对象的过程。</p>

<p>为什么需要序列化？转换为二进制串后才好进行网络传输嘛！</p>

<p>为什么需要反序列化？将二进制转换为对象才好进行后续处理！</p>

<p>现如今序列化的方案越来越多，每种序列化方案都有优点和缺点，它们在设计之初有自己独特的应用场景，那到底选择哪种呢？从 RPC 的角度上看，主要看三点：</p>

<ol>
<li> 通用性，比如是否能支持 Map 等复杂的数据结构；</li>
<li> 性能，包括时间复杂度和空间复杂度，由于 RPC 框架将会被公司几乎所有服务使用，如果序列化上能节约一点时间，对整个公司的收益都将非常可观，同理如果序列化上能节约一点内存，网络带宽也能省下不少；</li>
<li> 可扩展性，对互联网公司而言，业务变化飞快，如果序列化协议具有良好的可扩展性，支持自动增加新的业务字段，而不影响老的服务，这将大大提供系统的灵活度。</li>
</ol>

<p>目前互联网公司广泛使用 Protobuf、Thrift、Avro 等成熟的序列化解决方案来搭建 RPC 框架，这些都是久经考验的解决方案。</p>

<h3 id="toc_5">1.3  通信</h3>

<p>消息数据结构被序列化为二进制串后，下一步就要进行网络通信了。目前有两种常用 IO 通信模型：1）BIO；2）<a href="http://my.oschina.net/hosee/blog/615269">NIO</a>。一般 RPC 框架需要支持这两种 IO 模型。</p>

<p>如何实现 RPC 的 IO 通信框架呢？</p>

<ol>
<li> 使用 java nio 方式自研，这种方式较为复杂，而且很有可能出现隐藏 bug，但也见过一些互联网公司使用这种方式；</li>
<li> 基于 mina，mina 在早几年比较火热，不过这些年版本更新缓慢；</li>
<li> 基于 netty，现在很多 RPC 框架都直接基于 netty 这一 IO 通信框架，省力又省心，比如阿里巴巴的 HSF、dubbo，Twitter 的 finagle 等。</li>
</ol>

<h3 id="toc_6">1.4  <strong>消息里为什么要有 requestID？</strong></h3>

<p>如果使用 netty 的话，一般会用 channel.writeAndFlush()方法来发送消息二进制串，这个方法调用后对于整个远程调用 (从发出请求到接收到结果) 来说是一个异步的，即对于当前线程来说，将请求发送出来后，线程就可以往后执行了，至于服务端的结果，是服务端处理完成后，再以消息的形式发送给客户端的。于是这里出现以下两个问题：</p>

<ol>
<li> 怎么让当前线程 “暂停”，等结果回来后，再向后执行？</li>
<li> 如果有多个线程同时进行远程方法调用，这时建立在 client server 之间的 socket 连接上会有很多双方发送的消息传递，前后顺序也可能是随机的，server 处理完结果后，将结果消息发送给 client，client 收到很多消息，怎么知道哪个消息结果是原先哪个线程调用的？</li>
</ol>

<p>如下图所示，线程 A 和线程 B 同时向 client socket 发送请求 requestA 和 requestB，socket 先后将 requestB 和 requestA 发送至 server，而 server 可能将 responseA 先返回，尽管 requestA 请求到达时间更晚。我们需要一种机制保证 responseA 丢给 ThreadA，responseB 丢给 ThreadB。</p>

<p><img src="http://static.oschina.net/uploads/space/2016/0714/104316_FAgB_2243330.png" alt=""/></p>

<p>怎么解决呢？</p>

<ol>
<li> client 线程每次通过 socket 调用一次远程接口前，生成一个唯一的 ID，即 requestID（requestID 必需保证在一个 Socket 连接里面是唯一的），一般常常使用 AtomicLong 从 0 开始累计数字生成唯一 ID；</li>
<li> 将处理结果的回调对象 callback，存放到全局 ConcurrentHashMap 里面 put(requestID, callback)；</li>
<li> 当线程调用 channel.writeAndFlush() 发送消息后，紧接着执行 callback 的 get() 方法试图获取远程返回的结果。在 get() 内部，则使用 synchronized 获取回调对象 callback 的锁，再先检测是否已经获取到结果，如果没有，然后调用 callback 的 wait() 方法，释放 callback 上的锁，让当前线程处于等待状态。</li>
<li> 服务端接收到请求并处理后，将 response 结果（此结果中包含了前面的 requestID）发送给客户端，客户端 socket 连接上专门监听消息的线程收到消息，分析结果，取到 requestID，再从前面的 ConcurrentHashMap 里面 get(requestID)，从而找到 callback 对象，再用 synchronized 获取 callback 上的锁，将方法调用结果设置到 callback 对象里，再调用 callback.notifyAll() 唤醒前面处于等待状态的线程。</li>
</ol>

<pre><code>public Object get() {
        synchronized (this) { // 旋锁
            while (!isDone) { // 是否有结果了
                wait(); //没结果是释放锁，让当前线程处于等待状态
            }
        }
}
</code></pre>

<pre><code>private void setDone(Response res) {
        this.res = res;
        isDone = true;
        synchronized (this) { //获取锁，因为前面wait()已经释放了callback的锁了
            notifyAll(); // 唤醒处于等待的线程
        }
    }
</code></pre>

<h2 id="toc_7">2 如何发布自己的服务？</h2>

<p>如何让别人使用我们的服务呢？有同学说很简单嘛，告诉使用者服务的 IP 以及端口就可以了啊。确实是这样，这里问题的关键在于是自动告知还是人肉告知。</p>

<p>人肉告知的方式：如果你发现你的服务一台机器不够，要再添加一台，这个时候就要告诉调用者我现在有两个 ip 了，你们要轮询调用来实现负载均衡；调用者咬咬牙改了，结果某天一台机器挂了，调用者发现服务有一半不可用，他又只能手动修改代码来删除挂掉那台机器的 ip。现实生产环境当然不会使用人肉方式。</p>

<p>有没有一种方法能实现自动告知，即机器的增添、剔除对调用方透明，调用者不再需要写死服务提供方地址？当然可以，现如今 zookeeper 被广泛用于实现服务自动注册与发现功能！</p>

<p>简单来讲，zookeeper 可以充当一个<code>服务注册表</code>（Service Registry），让多个<code>服务提供者</code>形成一个集群，让<code>服务消费者</code>通过服务注册表获取具体的服务访问地址（ip + 端口）去访问具体的服务提供者。如下图所示：</p>

<p><img src="http://static.oschina.net/uploads/space/2016/0714/105148_gSi2_2243330.png" alt=""/></p>

<p>具体来说，zookeeper 就是个分布式文件系统，每当一个服务提供者部署后都要将自己的服务注册到 zookeeper 的某一路径上: /{service}/{version}/{ip:port}, 比如我们的 HelloWorldService 部署到两台机器，那么 zookeeper 上就会创建两条目录：分别为 / HelloWorldService/1.0.0/100.19.20.01:16888  /HelloWorldService/1.0.0/100.19.20.02:16888。</p>

<p>zookeeper 提供了 “心跳检测” 功能，它会定时向各个服务提供者发送一个请求（实际上建立的是一个 Socket 长连接），如果长期没有响应，服务中心就认为该服务提供者已经“挂了”，并将其剔除，比如 100.19.20.02 这台机器如果宕机了，那么 zookeeper 上的路径就会只剩 / HelloWorldService/1.0.0/100.19.20.01:16888。</p>

<p>服务消费者会去监听相应路径（/HelloWorldService/1.0.0），一旦路径上的数据有任务变化（增加或减少），zookeeper 都会通知服务消费方服务提供者地址列表已经发生改变，从而进行更新。</p>

<p>更为重要的是 zookeeper 与生俱来的容错容灾能力（比如 leader 选举），可以确保服务注册表的高可用性。</p>

<h2 id="toc_8">3.Hadoop 中 <strong>RPC</strong> 实例分析</h2>

<p>ipc.RPC 类中有一些内部类，为了大家对 RPC 类有个初步的印象，就先罗列几个我们感兴趣的分析一下吧：</p>

<p><strong>Invocation</strong> ：用于封装方法名和参数，作为数据传输层。<br/>
<strong>ClientCache</strong> ：用于存储 client 对象，用 socket factory 作为 hash key, 存储结构为 hashMap <SocketFactory, Client>。<br/>
<strong>Invoker</strong> ：是动态代理中的调用实现类，继承了 InvocationHandler.<br/>
<strong>Server</strong> ：是 ipc.Server 的实现类。</p>

<pre><code>    public Object invoke(Object proxy, Method method, Object[] args)
      throws Throwable {
      •••
      ObjectWritable value = (ObjectWritable)
        client.call(new Invocation(method, args), remoteId);
      •••
      return value.get();
    }

</code></pre>

<p>如果你发现这个 invoke() 方法实现的有些奇怪的话，那你就对了。一般我们看到的<a href="http://my.oschina.net/hosee/blog/656945">动态代理</a>的 invoke() 方法中总会有 method.invoke(ac, arg);  这句代码。而上面代码中却没有，这是为什么呢？其实使用 method.invoke(ac, arg); 是在本地 JVM 中调用；而在 hadoop 中，是将数据发送给服务端，服务端将处理的结果再返回给客户端，所以这里的 invoke() 方法必然需要进行网络通信。而网络通信就是下面的这段代码实现的：</p>

<pre><code>ObjectWritable value = (ObjectWritable)
client.call(new Invocation(method, args), remoteId);

</code></pre>

<p>Invocation 类在这里封装了方法名和参数。其实这里网络通信只是调用了 Client 类的 call() 方法。那我们接下来分析一下 ipc.Client 源码吧。和第一章一样，同样是 3 个问题</p>

<ol>
<li> 客户端和服务端的连接是怎样建立的？</li>
<li> 客户端是怎样给服务端发送数据的？</li>
<li> 客户端是怎样获取服务端的返回数据的？</li>
</ol>

<h3 id="toc_9">3.1 客户端和服务端的连接是怎样建立的？</h3>

<pre><code>public Writable call(Writable param, ConnectionId remoteId)  
                       throws InterruptedException, IOException {
    Call call = new Call(param);       //将传入的数据封装成call对象
    Connection connection = getConnection(remoteId, call);   //获得一个连接
    connection.sendParam(call);     // 向服务端发送call对象
    boolean interrupted = false;
    synchronized (call) {
      while (!call.done) {
        try {
          call.wait(); // 等待结果的返回，在Call类的callComplete()方法里有notify()方法用于唤醒线程
        } catch (InterruptedException ie) {
          // 因中断异常而终止，设置标志interrupted为true
          interrupted = true;
        }
      }
      if (interrupted) {
        Thread.currentThread().interrupt();
      }

      if (call.error != null) {
        if (call.error instanceof RemoteException) {
          call.error.fillInStackTrace();
          throw call.error;
        } else { // 本地异常
          throw wrapException(remoteId.getAddress(), call.error);
        }
      } else {
        return call.value; //返回结果数据
      }
    }
  }

</code></pre>

<p>具体代码的作用我已做了注释，所以这里不再赘述。但到目前为止，你依然不知道 RPC 机制底层的网络连接是怎么建立的。分析代码后，我们会发现和网络通信有关的代码只会是下面的两句了：</p>

<pre><code>  Connection connection = getConnection(remoteId, call);   //获得一个连接
  connection.sendParam(call);      // 向服务端发送call对象

</code></pre>

<p>先看看是怎么获得一个到服务端的连接吧，下面贴出 ipc.Client 类中的 getConnection() 方法。</p>

<pre><code>private Connection getConnection(ConnectionId remoteId,
                                   Call call)
                                   throws IOException, InterruptedException {
    if (!running.get()) {
      // 如果client关闭了
      throw new IOException(&quot;The client is stopped&quot;);
    }
    Connection connection;
//如果connections连接池中有对应的连接对象，就不需重新创建了；如果没有就需重新创建一个连接对象。
//但请注意，该//连接对象只是存储了remoteId的信息，其实还并没有和服务端建立连接。
    do {
      synchronized (connections) {
        connection = connections.get(remoteId);
        if (connection == null) {
          connection = new Connection(remoteId);
          connections.put(remoteId, connection);
        }
      }
    } while (!connection.addCall(call)); //将call对象放入对应连接中的calls池，就不贴出源码了
   //这句代码才是真正的完成了和服务端建立连接哦~
    connection.setupIOstreams();
    return connection;
  }

</code></pre>

<p>下面贴出 Client.Connection 类中的 setupIOstreams() 方法：</p>

<pre><code>  private synchronized void setupIOstreams() throws InterruptedException {
   •••
      try {
       •••
        while (true) {
          setupConnection();  //建立连接
          InputStream inStream = NetUtils.getInputStream(socket);     //获得输入流
          OutputStream outStream = NetUtils.getOutputStream(socket);  //获得输出流
          writeRpcHeader(outStream);
          •••
          this.in = new DataInputStream(new BufferedInputStream
              (new PingInputStream(inStream)));   //将输入流装饰成DataInputStream
          this.out = new DataOutputStream
          (new BufferedOutputStream(outStream));   //将输出流装饰成DataOutputStream
          writeHeader();
          // 跟新活动时间
          touch();
          //当连接建立时，启动接受线程等待服务端传回数据，注意：Connection继承了Tread
          start();
          return;
        }
      } catch (IOException e) {
        markClosed(e);
        close();
      }
    }

</code></pre>

<p>再有一步我们就知道客户端的连接是怎么建立的啦，下面贴出 Client.Connection 类中的 setupConnection() 方法：</p>

<pre><code>  private synchronized void setupConnection() throws IOException {
      short ioFailures = 0;
      short timeoutFailures = 0;
      while (true) {
        try {
          this.socket = socketFactory.createSocket(); //终于看到创建socket的方法了
          this.socket.setTcpNoDelay(tcpNoDelay);
         •••
          // 设置连接超时为20s
          NetUtils.connect(this.socket, remoteId.getAddress(), 20000);
          this.socket.setSoTimeout(pingInterval);
          return;
        } catch (SocketTimeoutException toe) {
          /* 设置最多连接重试为45次。
           * 总共有20s*45 = 15 分钟的重试时间。
           */
          handleConnectionFailure(timeoutFailures++, 45, toe);
        } catch (IOException ie) {
          handleConnectionFailure(ioFailures++, maxRetries, ie);
        }
      }
    }

</code></pre>

<p>终于，我们知道了客户端的连接是怎样建立的了，其实就是创建一个普通的 socket 进行通信。</p>

<h3 id="toc_10">3.2 客户端是怎样给服务端发送数据的？ </h3>

<p>下面贴出 Client.Connection 类的 sendParam() 方法吧：</p>

<pre><code>public void sendParam(Call call) {
      if (shouldCloseConnection.get()) {
        return;
      }
      DataOutputBuffer d=null;
      try {
        synchronized (this.out) {
          if (LOG.isDebugEnabled())
            LOG.debug(getName() + &quot; sending #&quot; + call.id);
          //创建一个缓冲区
          d = new DataOutputBuffer();
          d.writeInt(call.id);
          call.param.write(d);
          byte[] data = d.getData();
          int dataLength = d.getLength();
          out.writeInt(dataLength);        //首先写出数据的长度
          out.write(data, 0, dataLength); //向服务端写数据
          out.flush();
        }
      } catch(IOException e) {
        markClosed(e);
      } finally {
        IOUtils.closeStream(d);
      }
    }  

</code></pre>

<h3 id="toc_11">3.3 客户端是怎样获取服务端的返回数据的？ </h3>

<p>下面贴出 Client.Connection 类和 Client.Call 类中的相关方法：</p>

<pre><code>方法一：  
  public void run() {
      •••
      while (waitForWork()) {
        receiveResponse();  //具体的处理方法
      }
      close();
     •••
}

方法二：
private void receiveResponse() {
      if (shouldCloseConnection.get()) {
        return;
      }
      touch();
      try {
        int id = in.readInt();                    // 阻塞读取id
        if (LOG.isDebugEnabled())
          LOG.debug(getName() + &quot; got value #&quot; + id);
          Call call = calls.get(id);    //在calls池中找到发送时的那个对象
        int state = in.readInt();     // 阻塞读取call对象的状态
        if (state == Status.SUCCESS.state) {
          Writable value = ReflectionUtils.newInstance(valueClass, conf);
          value.readFields(in);           // 读取数据
        //将读取到的值赋给call对象，同时唤醒Client等待线程，贴出setValue()代码方法三
          call.setValue(value);              
          calls.remove(id);               //删除已处理的call    
        } else if (state == Status.ERROR.state) {
        •••
        } else if (state == Status.FATAL.state) {
        •••
        }
      } catch (IOException e) {
        markClosed(e);
      }
}

方法三：
public synchronized void setValue(Writable value) {
      this.value = value;
      callComplete();   //具体实现
}
protected synchronized void callComplete() {
      this.done = true;
      notify();         // 唤醒client等待线程
    }

</code></pre>

<p>完成的功能主要是：启动一个处理线程，读取从服务端传来的 call 对象，将 call 对象读取完毕后，唤醒 client 处理线程。就这么简单，客户端就获取了服务端返回的数据了哦~。客户端的源码分析就到这里了哦，下面我们来分析 Server 端的源码吧。</p>

<h3 id="toc_12">3.4 ipc.Server 源码分析</h3>

<p>为了让大家对 ipc.Server 有个初步的了解，我们先分析一下它的几个内部类吧：</p>

<p><strong>Call</strong> ：用于存储客户端发来的请求<br/>
<strong>Listener</strong> ： 监听类，用于监听客户端发来的请求，同时 Listener 内部还有一个静态类，Listener.Reader，当监听器监听到用户请求，便让 Reader 读取用户请求。<br/>
<strong>Responder</strong> ：响应 RPC 请求类，请求处理完毕，由 Responder 发送给请求客户端。<br/>
<strong>Connection</strong> ：连接类，真正的客户端请求读取逻辑在这个类中。<br/>
<strong>Handler</strong> ：请求处理类，会循环阻塞读取 callQueue 中的 call 对象，并对其进行操作。</p>

<pre><code>private void initialize(Configuration conf) throws IOException {
   •••
    // 创建 rpc server
    InetSocketAddress dnSocketAddr = getServiceRpcServerAddress(conf);
    if (dnSocketAddr != null) {
      int serviceHandlerCount =
        conf.getInt(DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_KEY,
                    DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_DEFAULT);
      //获得serviceRpcServer
      this.serviceRpcServer = RPC.getServer(this, dnSocketAddr.getHostName(), 
          dnSocketAddr.getPort(), serviceHandlerCount,
          false, conf, namesystem.getDelegationTokenSecretManager());
      this.serviceRPCAddress = this.serviceRpcServer.getListenerAddress();
      setRpcServiceServerAddress(conf);
}
//获得server
    this.server = RPC.getServer(this, socAddr.getHostName(),
        socAddr.getPort(), handlerCount, false, conf, namesystem
        .getDelegationTokenSecretManager());

   •••
    this.server.start();  //启动 RPC server   Clients只允许连接该server
    if (serviceRpcServer != null) {
      serviceRpcServer.start();  //启动 RPC serviceRpcServer 为HDFS服务的server
    }
    startTrashEmptier(conf);
  }

</code></pre>

<p>查看 Namenode 初始化源码得知：RPC 的 server 对象是通过 ipc.RPC 类的 getServer() 方法获得的。下面咱们去看看 ipc.RPC 类中的 getServer() 源码吧：</p>

<pre><code>public static Server getServer(final Object instance, final String bindAddress, final int port,
                                 final int numHandlers,
                                 final boolean verbose, Configuration conf,
                                 SecretManager&lt;? extends TokenIdentifier&gt; secretManager) 
    throws IOException {
    return new Server(instance, conf, bindAddress, port, numHandlers, verbose, secretManager);
  }

</code></pre>

<p>这时我们发现 getServer()是一个创建 Server 对象的工厂方法，但创建的却是 RPC.Server 类的对象。哈哈，现在你明白了我前面说的 “RPC.Server 是 ipc.Server 的实现类” 了吧。不过 RPC.Server 的构造函数还是调用了 ipc.Server 类的构造函数的，因篇幅所限，就不贴出相关源码了。</p>

<p>初始化 Server 后，Server 端就运行起来了，看看 ipc.Server 的 start() 源码吧：</p>

<pre><code>  /** 启动服务 */
  public synchronized void start() {
    responder.start();  //启动responder
    listener.start();   //启动listener
    handlers = new Handler[handlerCount];

    for (int i = 0; i &lt; handlerCount; i++) {
      handlers[i] = new Handler(i);
      handlers[i].start();   //逐个启动Handler
    }
  }

</code></pre>

<p>分析过 ipc.Client 源码后，我们知道 Client 端的底层通信直接采用了阻塞式 IO 编程，当时我们曾做出猜测：Server 端是不是也采用了阻塞式 IO。现在我们仔细地分析一下吧，如果 Server 端也采用阻塞式 IO，当连接进来的 Client 端很多时，势必会影响 Server 端的性能。hadoop 的实现者们考虑到了这点，所以他们采用了 java  NIO 来实现 Server 端，那 Server 端采用 java NIO 是怎么建立连接的呢？分析源码得知，Server 端采用 Listener 监听客户端的连接，下面先分析一下 Listener 的构造函数吧：</p>

<pre><code>    public Listener() throws IOException {
      address = new InetSocketAddress(bindAddress, port);
      // 创建ServerSocketChannel,并设置成非阻塞式
      acceptChannel = ServerSocketChannel.open();
      acceptChannel.configureBlocking(false);

      // 将server socket绑定到本地端口
      bind(acceptChannel.socket(), address, backlogLength);
      port = acceptChannel.socket().getLocalPort(); 
      // 获得一个selector
      selector= Selector.open();
      readers = new Reader[readThreads];
      readPool = Executors.newFixedThreadPool(readThreads);
      //启动多个reader线程，为了防止请求多时服务端响应延时的问题
      for (int i = 0; i &lt; readThreads; i++) {       
        Selector readSelector = Selector.open();
        Reader reader = new Reader(readSelector);
        readers[i] = reader;
        readPool.execute(reader);
      }
      // 注册连接事件
      acceptChannel.register(selector, SelectionKey.OP_ACCEPT);
      this.setName(&quot;IPC Server listener on &quot; + port);
      this.setDaemon(true);
    }

</code></pre>

<p>在启动 Listener 线程时，服务端会一直等待客户端的连接，下面贴出 Server.Listener 类的 run() 方法：</p>

<pre><code>  public void run() {
     •••
      while (running) {
        SelectionKey key = null;
        try {
          selector.select();
          Iterator&lt;SelectionKey&gt; iter = selector.selectedKeys().iterator();
          while (iter.hasNext()) {
            key = iter.next();
            iter.remove();
            try {
              if (key.isValid()) {
                if (key.isAcceptable())
                  doAccept(key);     //具体的连接方法
              }
            } catch (IOException e) {
            }
            key = null;
          }
        } catch (OutOfMemoryError e) {
       •••         
    }

</code></pre>

<p>下面贴出 Server.Listener 类中 doAccept() 方法中的关键源码吧：</p>

<pre><code>    void doAccept(SelectionKey key) throws IOException,  OutOfMemoryError {
      Connection c = null;
      ServerSocketChannel server = (ServerSocketChannel) key.channel();
      SocketChannel channel;
      while ((channel = server.accept()) != null) { //建立连接
        channel.configureBlocking(false);
        channel.socket().setTcpNoDelay(tcpNoDelay);
        Reader reader = getReader();  //从readers池中获得一个reader
        try {
          reader.startAdd(); // 激活readSelector，设置adding为true
          SelectionKey readKey = reader.registerChannel(channel);//将读事件设置成兴趣事件
          c = new Connection(readKey, channel, System.currentTimeMillis());//创建一个连接对象
          readKey.attach(c);   //将connection对象注入readKey
          synchronized (connectionList) {
            connectionList.add(numConnections, c);
            numConnections++;
          }
        ••• 
        } finally {
//设置adding为false，采用notify()唤醒一个reader,其实代码十三中启动的每个reader都使
//用了wait()方法等待。因篇幅有限，就不贴出源码了。
          reader.finishAdd();
        }
      }
    }

</code></pre>

<p>当 reader 被唤醒，reader 接着执行 doRead() 方法。</p>

<p>下面贴出 Server.Listener.Reader 类中的 doRead() 方法和 Server.Connection 类中的 readAndProcess() 方法源码：</p>

<pre><code>方法一：   
 void doRead(SelectionKey key) throws InterruptedException {
      int count = 0;
      Connection c = (Connection)key.attachment();  //获得connection对象
      if (c == null) {
        return;  
      }
      c.setLastContact(System.currentTimeMillis());
      try {
        count = c.readAndProcess();    // 接受并处理请求  
      } catch (InterruptedException ieo) {
       •••
      }
     •••    
}

方法二：
public int readAndProcess() throws IOException, InterruptedException {
      while (true) {
        •••
        if (!rpcHeaderRead) {
          if (rpcHeaderBuffer == null) {
            rpcHeaderBuffer = ByteBuffer.allocate(2);
          }
         //读取请求头
          count = channelRead(channel, rpcHeaderBuffer);
          if (count &lt; 0 || rpcHeaderBuffer.remaining() &gt; 0) {
            return count;
          }
        // 读取请求版本号  
          int version = rpcHeaderBuffer.get(0);
          byte[] method = new byte[] {rpcHeaderBuffer.get(1)};
        •••  

          data = ByteBuffer.allocate(dataLength);
        }
        // 读取请求  
        count = channelRead(channel, data);

        if (data.remaining() == 0) {
         •••
          if (useSasl) {
         •••
          } else {
            processOneRpc(data.array());//处理请求
          }
        •••
          }
        } 
        return count;
      }
    }

</code></pre>

<p>下面贴出 Server.Connection 类中的 processOneRpc() 方法和 processData() 方法的源码。</p>

<pre><code>方法一：   
 private void processOneRpc(byte[] buf) throws IOException,
        InterruptedException {
      if (headerRead) {
        processData(buf);
      } else {
        processHeader(buf);
        headerRead = true;
        if (!authorizeConnection()) {
          throw new AccessControlException(&quot;Connection from &quot; + this
              + &quot; for protocol &quot; + header.getProtocol()
              + &quot; is unauthorized for user &quot; + user);
        }
      }
}
方法二：
    private void processData(byte[] buf) throws  IOException, InterruptedException {
      DataInputStream dis =
        new DataInputStream(new ByteArrayInputStream(buf));
      int id = dis.readInt();      // 尝试读取id
      Writable param = ReflectionUtils.newInstance(paramClass, conf);//读取参数
      param.readFields(dis);        

      Call call = new Call(id, param, this);  //封装成call
      callQueue.put(call);   // 将call存入callQueue
      incRpcCount();  // 增加rpc请求的计数
    }

</code></pre>

<h2 id="toc_13">4. RPC 与 web service</h2>

<p>RPC：</p>

<p><img src="http://static.oschina.net/uploads/space/2016/0714/114103_HQGm_2243330.png" alt=""/></p>

<p>Web service<img src="http://static.oschina.net/uploads/space/2016/0714/114022_sKwT_2243330.png" alt=""/></p>

<p>web service 接口就是 RPC 中的 stub 组件，规定了 server 能够提供的服务（web service），这在 server 和 client 上是一致的，但是也是跨语言跨平台的。同时，由于 web service 规范中的 WSDL 文件的存在，现在各平台的 web service 框架，都可以基于 WSDL 文件，自动生成 web service 接口 。</p>

<p>其实两者差不多，只是传输的协议不同。</p>

<h2 id="toc_14">Reference：</h2>

<p>1. <a href="http://www.cnblogs.com/LBSer/p/4853234.html">http://www.cnblogs.com/LBSer/p/4853234.html</a><br/>
2. <a href="http://weixiaolu.iteye.com/blog/1504898">http://weixiaolu.iteye.com/blog/1504898</a><br/>
3. <a href="http://kyfxbl.iteye.com/blog/1745550">http://kyfxbl.iteye.com/blog/1745550</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kubernetes API 使用文档]]></title>
    <link href="http://panlw.github.io/15274389377310.html"/>
    <updated>2018-05-28T00:35:37+08:00</updated>
    <id>http://panlw.github.io/15274389377310.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/">原文地址</a></p>
</blockquote>

<h2 id="toc_0">API 概述</h2>

<h4 id="toc_1">※ 资源分类</h4>

<p>这是对 Kubernetes API 及其主要功能提供的基本资源类型的高级概述。</p>

<ul>
<li>  <strong>Workloads</strong>： 用于在集群中管理和运行容器；</li>
<li>  <strong>Discovery &amp; LB</strong>：用于将 Workloads “缝合” 到一个可从外部访问的负载均衡的服务中；</li>
<li>  <strong>Config &amp; Storage</strong>：用于将初始化数据注入到应用程序中，并保留容器外部的数据；</li>
<li>  <strong>Cluster</strong>：用于定义如何配置集群，这些通常仅由集群运营商使用；</li>
<li>  <strong>Metadata</strong>：用于配置群集内其他资源的行为，例如用于扩展 Workloads 的 HorizontalPodAutoscaler</li>
</ul>

<h4 id="toc_2">※ 资源对象</h4>

<p>资源对象通常包含 3 个组件：</p>

<ul>
<li>  <strong>ResourceSpec</strong>：这由用户定义并描述系统所期望的状态，在创建或更新对象时填写；</li>
<li>  <strong>ResourceStatus</strong>：这由服务器填写并报告系统的当前状态，只有 Kubernetes 组件才能填写此内容；</li>
<li>  <strong>Resource ObjectMeta</strong>：这是关于资源的元数据，例如 name / type / api version / annotations / labels，包含可能由后端用户和系统更新的字段（如 annotations）</li>
</ul>

<h4 id="toc_3">※ 资源操作</h4>

<p>大部分资源提供如下操作：</p>

<p><strong>▶ Create</strong></p>

<p>Create 操作会在存储后端创建资源。资源创建后，系统将应用所期望的状态。</p>

<p><strong>▶ Update</strong></p>

<p>更新有 2 种形式：<strong>Replace</strong> 和 <strong>Patch</strong></p>

<p><strong>Replace</strong>：替换资源对象将通过提供的 spec 替换现有的 spec 来更新资源。对于读写操作，这是安全的，因为如果资源在读和写之间被修改，则会发生乐观锁定失败。注意：ResourceStatus 将被系统忽略，并且不会被更新，要更新状态，必须调用特定的状态更新操作。</p>

<p>注意：替换资源对象可能不会立即传播到下游对象。例如，替换 ConfigMap 或 Secret 资源不会导致所有 Pod 看到更改，除非 Pod 在带外重新启动。</p>

<p><strong>Patch</strong>：Patch 会更改特定字段，如何合并更改是每个字段定义的。列表可以被替换或合并，合并列表不会保留排序。</p>

<p><strong>Patch 操作永远不会导致乐观锁定失败，并且最后一次写入会获胜。</strong>如果在更新之前未读取完整状态，或者乐观锁定失败不可取，则建议使用 patch 操作。修补复杂 types、arrays 和 maps 时，如何应用修补程序是以每个字段为基础定义的，并可以替换字段的当前值，也可以将内容合并到当前值中。</p>

<p><strong>▶ Read</strong></p>

<p>读取有 3 种方式：<strong>Get</strong> 、 <strong>List</strong> 和 <strong>Watch</strong></p>

<p><strong>Get</strong>： 按名称检索特定的资源对象；</p>

<p><strong>List</strong>：检索命名空间内特定类型的所有资源对象，并且结果可以限制为与选择器查询结果匹配的资源；</p>

<p><strong>List All Namespaces</strong>：像 List 一样，但是跨所有命名空间检索资源；</p>

<p><strong>Watch</strong>：Watch 将在对象更新时汇出结果，类似于回调，watch 用于响应资源的更改。</p>

<p><strong>▶ Delete</strong></p>

<p>Delete 将删除资源。根据特定的资源，子对象可能会或可能不会被服务器当做垃圾收集，详情请参阅特定资源对象的注释。</p>

<p><strong>▶ 额外操作</strong></p>

<p>资源可以定义特定于该资源类型的附加操作。</p>

<p><strong>Rollback</strong>：将 PodTemplate 回滚到以前的版本，仅适用于某些资源类型；</p>

<p><strong>Read / Write Scale</strong>：读取或更新给定资源的副本数量，仅适用于某些资源类型；</p>

<p><strong>Read / Write Status</strong>：读取或更新资源对象的状态，状态只能通过这些更新操作进行更改。</p>

<h2 id="toc_4">WORKLOADS</h2>

<p>Workloads 资源负责管理和运行集群上的容器。容器（Containers）由控制器（Controllers）通过 Pod 创建，Pods 运行容器并提供环境依赖，如注入到容器中的共享或永久存储卷、配置或加密数据。</p>

<p>最常见的控制器是：</p>

<ul>
<li>  Deployments 无状态持久应用（如 http servers）</li>
<li>  StatefulSets 有状态持久应用（如 database）</li>
<li>  Jobs 运行至完成的应用 （如 batch jobs）</li>
</ul>

<h2 id="toc_5">DISCOVERY &amp; LOAD BALANCING</h2>

<p>Discovery and Load Balancing 负责将 Workloads “缝合” 到一个可从外部访问的负载均衡的服务中。默认情况下，Workloads 只能在群集内访问，它们必须通过 LoadBalancer 或 NodePort Service 暴露到外部。对于开发，可以使用 <code>kubectl proxy</code>命令通过代理 api 主机访问内部可访问的 Workloads 。</p>

<p>常用的资源类型：</p>

<ul>
<li>  Services 提供跨多个 Workload 副本的负载均衡的单个 IP 端点。</li>
<li>  Ingress 提供路由到一个或多个服务的 https(s) 端点</li>
</ul>

<h2 id="toc_6">CONFIG &amp; STORAGE</h2>

<p>Config and Storage 资源负责将数据注入到应用程序中，并保留容器外部的数据。</p>

<p>常用的资源类型：</p>

<ul>
<li>  ConfigMaps 通过环境变量、命令行参数或文件提供注入应用的 K-V 键值对</li>
<li>  Secrets 通过文件提供注入应用的二进制数据</li>
<li>  Volumes 提供容器外部的文件系统。 可能在同一个 Pod 中跨 Container 容器共享，并且其寿命持续超出 Container 或 Pod。</li>
</ul>

<h2 id="toc_7">METADATA</h2>

<p>Metadata resources 负责集群内其他资源的行为。</p>

<p>常用的资源类型：</p>

<ul>
<li>  HorizontalPodAutoscaler 自动缩放 workloads 的副本数量以响应加载</li>
<li>  PodDisruptionBudget 在执行维护时，可以配置给定 workloads 中的多少副本可能同时不可用</li>
<li>  ThirdPartyResource 使用自己的类型扩展 Kubernetes API</li>
<li>  Event 群集中资源生命周期事件的通知</li>
</ul>

<h2 id="toc_8">API 调用方式</h2>

<p><strong>▶ kubectl</strong></p>

<p>示例：创建 Deployment</p>

<pre><code>1.  `$ echo &#39;apiVersion: apps/v1`

2.  `kind: Deployment`

3.  `metadata:`

4.  `name: deployment-example`

5.  `spec:`

6.  `replicas: 3`

7.  `revisionHistoryLimit: 10`

8.  `template:`

9.  `metadata:`

10.  `labels:`

11.  `app: nginx`

12.  `spec:`

13.  `containers:`

14.  `- name: nginx`

15.  `image: nginx:1.10`

16.  `ports:`

17.  `- containerPort: 80`

18.  `&#39; | kubectl create -f -`

</code></pre>

<p>yaml 文件编写的规则拆解如下图：</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/wbiax4xEAl5xuUnZt7eHIfEe51sP9P3DjdKKVzb6BmPcZAQzsLM5ZM6SsD46djF4siaLHADeaQnrT6qOJDGSCkibw/640?wx_fmt=png" alt=""/></p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/wbiax4xEAl5xuUnZt7eHIfEe51sP9P3DjrdW9EOgJy7BickNuoCKaX7R5t1TZHqDF8RN8qSCRzse8nRDO23IA66w/640?wx_fmt=png" alt=""/></p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/wbiax4xEAl5xuUnZt7eHIfEe51sP9P3DjEr3QHuRt71DVQHpNZHibDhFnDibRvT6ghUdMvCOLqSuUIZYO7YXWZyLg/640?wx_fmt=png" alt=""/></p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/wbiax4xEAl5xuUnZt7eHIfEe51sP9P3Djl2MJd16ycvpxFCy7ibcQmaqMVo0ndFic3xlVqvyzwFtl4VTw6y3WuRdg/640?wx_fmt=png" alt=""/></p>

<p>其他资源的创建或其他操作，都可以按照这种方式来操作。</p>

<p><strong>▶ curl</strong></p>

<p>需要使用 <code>kubectl proxy</code></p>

<pre data-initialized="true" data-gclp-id="6" style="box-sizing: border-box;margin-top: 0px;margin-bottom: 0px;padding: 8px 0px 6px;background-color: rgb(241, 239, 238);border-radius: 0px;overflow-y: auto;color: rgb(80, 97, 109);font-size: 10px;line-height: 12px;">

1.  `$ kubectl proxy`

2.  `$ curl -X POST -H 'Content-Type: application/yaml' --data '`

3.  `apiVersion: apps/v1beta1`

4.  `kind: Deployment`

5.  `metadata:`

6.  `name: deployment-example`

7.  `spec:`

8.  `replicas: 3`

9.  `revisionHistoryLimit: 10`

10.  `template:`

11.  `metadata:`

12.  `labels:`

13.  `app: nginx`

14.  `spec:`

15.  `containers:`

16.  `- name: nginx`

17.  `image: nginx:1.10`

18.  `ports:`

19.  `- containerPort: 80`

20.  `' http://127.0.0.1:8001/apis/apps/v1/namespaces/default/deployments` 

</pre>

<p><section class="" powered-by="xiumi.us" style="white-space: normal;box-sizing: border-box;"></p>

<p><section class="" style="box-sizing: border-box;"></p>

<p><section class="" style="font-size: 14px;box-sizing: border-box;"></p>

<p>推荐：<a href="http://mp.weixin.qq.com/s?__biz=MzU0MDEwMjgwNA==&amp;mid=2247484509&amp;idx=1&amp;sn=e8b7f12fb3660d15379e2087c2e0e5c6&amp;chksm=fb3f1da6cc4894b0421d62ebecd16c9de11a3abf919d2c506643d3b864d0357ffc4eec1bfd29&amp;scene=21#wechat_redirect">译：基于注解的控制器：Spring Web/WebFlux 和 测试</a></p>

<p>上一篇：<a href="http://mp.weixin.qq.com/s?__biz=MzU0MDEwMjgwNA==&amp;mid=2247484512&amp;idx=1&amp;sn=4130871a4e6360b4f1b8c1b4dac4b101&amp;chksm=fb3f1d9bcc48948d5430ce9e1cef224c40421241d3a925a891bb1cf2c1753d90db040529848a&amp;scene=21#wechat_redirect">Spring-5-webflux 和阻塞与非阻塞 JDBC</a></p>

<p></section></p>

<p></section></p>

<p></section></p>

<p><section class="" powered-by="xiumi.us" style="white-space: normal;box-sizing: border-box;"></p>

<p><section class="" style="box-sizing: border-box;"></p>

<p><section class="" style="display: inline-block;vertical-align: top;width: 279px;box-sizing: border-box;"></p>

<p><section class="" powered-by="xiumi.us" style="box-sizing: border-box;"></p>

<p><section class="" style="box-sizing: border-box;"></p>

<p><section class="" style="text-align: center;color: rgb(160, 160, 160);font-size: 14px;box-sizing: border-box;"></p>

<p>最好的赞赏</p>

<p>就是你的关注</p>

<p></section></p>

<p></section></p>

<p></section></p>

<p></section></p>

<p><section class="" style="display: inline-block;vertical-align: top;width: 279px;box-sizing: border-box;"></p>

<p><section class="" powered-by="xiumi.us" style="box-sizing: border-box;"></p>

<p><section class="" style="margin-top: 10px;margin-bottom: 10px;text-align: center;box-sizing: border-box;"></p>

<p><section class="" style="max-width: 100%;vertical-align: middle;display: inline-block;box-sizing: border-box;overflow: hidden !important;"><img src="https://mmbiz.qpic.cn/mmbiz_jpg/wbiax4xEAl5zQkzvFqgk7DUAem1u05eybPdhEythAoe3O0FxUHy0tmzgytKI7tJaiaDsEWib43ZFSYmEROK4MNNAQ/640?wx_fmt=jpeg" alt=""/></section></p>

<p></section></p>

<p></section></p>

<p></section></p>

<p></section></p>

<p></section></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[基于 Docker 搭建 MySQL 主从复制 - 秋田君]]></title>
    <link href="http://panlw.github.io/15274384746652.html"/>
    <updated>2018-05-28T00:27:54+08:00</updated>
    <id>http://panlw.github.io/15274384746652.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://my.oschina.net/u/3773384/blog/1810111">原文地址</a></p>
</blockquote>

<pre><code>本篇博文相对简单，因为是初次使用 Docker，MySQL 的主从复制之前也在 Centos 环境下搭建过，但是也忘的也差不多了，因此本次尝试在 Docker 中搭建。根据网上教程走还是踩了一些坑，不过所幸最终搭建成功，因此记录下来，避免以后踩了重复的坑。
</code></pre>

<h2 id="toc_0">搭建环境</h2>

<p>Centos 7.2 64 位</p>

<p>MySQL 5.7.13</p>

<p>Docker 1.13.1</p>

<p>接下来，我们将会在一台服务器上安装 docker，并使用 docker 运行三个 MySQL 容器，分别为一主两从。</p>

<h2 id="toc_1">安装 docker</h2>

<p>执行命令</p>

<pre><code>[root@VM_0_17_centos ~]# yum install docker
</code></pre>

<p>如果有提示，一路 y 下去</p>

<p>安装成功启动 Docker 后，查看版本</p>

<pre><code>[root@VM_0_17_centos ~]# docker version
Client:
 Version:         1.13.1
 API version:     1.26
 Package version: &lt;unknown&gt;
 Go version:      go1.8.3
 Git commit:      774336d/1.13.1
 Built:           Wed Mar  7 17:06:16 2018
 OS/Arch:         linux/amd64

Server:
 Version:         1.13.1
 API version:     1.26 (minimum version 1.12)
 Package version: &lt;unknown&gt;
 Go version:      go1.8.3
 Git commit:      774336d/1.13.1
 Built:           Wed Mar  7 17:06:16 2018
 OS/Arch:         linux/amd64
 Experimental:    false
</code></pre>

<p>出现版本信息，则安装成功</p>

<h2 id="toc_2">启动 Docker</h2>

<p>启动 Docker 并设置为开机自启动</p>

<pre><code>[root@VM_0_17_centos ~]# systemctl  start docker.service
[root@VM_0_17_centos ~]# systemctl  enable docker.service
</code></pre>

<h2 id="toc_3">安装 MySQL</h2>

<p>使用 Docker 拉取 MySQL 镜像</p>

<pre><code>[root@VM_0_17_centos ~]# docker pull mysql:5.7.13
</code></pre>

<h2 id="toc_4">运行主容器</h2>

<pre><code>[root@VM_0_17_centos ~]# docker run --name master -p 3306:3306 -e MYSQL_ROOT_PASSWORD=root -d mysql:5.7.13

</code></pre>

<p>--name 为容器指定名称，这里是 master</p>

<p>-p 将容器的指定端口映射到主机的指定端口，这里是将容器的 3306 端口映射到主机的 3306 端口</p>

<p>-e 设置环境变量，这里是指定 root 账号的密码为 root</p>

<p>-d 后台运行容器，并返回容器 ID</p>

<p>mysql:5.7.13 指定运行的 mysql 版本</p>

<h2 id="toc_5">检验是否启动成功</h2>

<p>docker ps -a 显示所有的容器，包括未运行的</p>

<pre><code>[root@VM_0_17_centos ~]# docker ps -a
ee86c19336f8        mysql:5.7.13        &quot;docker-entrypoint...&quot;   About an hour ago   Up About an hour    0.0.0.0:3306-&gt;3306/tcp   master

</code></pre>

<p>注意，是 UP 状态，表示正在运行中</p>

<p>开放 3306 端口</p>

<pre><code>[root@VM_0_17_centos ~]# firewall-cmd --zone=public --add-port=3306/tcp --permanent
[root@VM_0_17_centos ~]# firewall-cmd --reload

</code></pre>

<p>--permanent 永久开启，避免下次开机需要再次手动开启端口</p>

<p>使用 Navicat 连接测试</p>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/114916_0h3I_3773384.png" alt=""/></p>

<p>MySQL 主容器已经启动成功</p>

<h2 id="toc_6">创建主容器的复制账号</h2>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/120249_0ZQx_3773384.png" alt=""/></p>

<p>使用 Navicat 友好的图像化界面执行 SQL</p>

<pre><code>GRANT REPLICATION SLAVE ON *.* to &#39;backup&#39;@&#39;%&#39; identified by &#39;backup&#39;;
show grants for &#39;backup&#39;@&#39;%&#39;;
</code></pre>

<p>出现如下信息表示授权成功</p>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/121018_Vtxn_3773384.png" alt=""/></p>

<h2 id="toc_7">修改 MySQL 配置环境</h2>

<p>创建配置文件目录</p>

<p>目录结构如下</p>

<p>/usr/local/mysql/master</p>

<p>/usr/local/mysql/slave1</p>

<p>/usr/local/mysql/slave2</p>

<p>拷贝一份 MySQL 配置文件</p>

<pre><code>[root@VM_0_17_centos local]# docker cp master:/etc/mysql/my.cnf /usr/local/mysql/master/my.cnf

</code></pre>

<p>进到 master 目录下，已存在拷贝的 my.cnf</p>

<pre><code>[root@VM_0_17_centos master]# ll
total 4
-rw-r--r-- 1 root root 1801 May 10 10:27 my.cnf

</code></pre>

<p>修改 my.cnf，在 [mysqld] 节点最后加上后保存</p>

<pre><code>log-bin=mysql-bin
server-id=1
</code></pre>

<p>log-bin=mysql-bin 使用 binary logging，mysql-bin 是 log 文件名的前缀</p>

<p>server-id=1 唯一服务器 ID，非 0 整数，不能和其他服务器的 server-id 重复</p>

<p>将修改后的文件覆盖 Docker 中 MySQL 中的配置文件</p>

<pre><code>[root@VM_0_17_centos master]# docker cp /usr/local/mysql/master/my.cnf master:/etc/mysql/my.cnf

</code></pre>

<p>重启 mysql 的 docker , 让配置生效</p>

<pre><code>[root@VM_0_17_centos master]# docker restart master
</code></pre>

<p>启动后，重新测试连接，连接成功表示主容器配置成功</p>

<h2 id="toc_8">运行 MySQL 从容器</h2>

<p>首先运行从容器</p>

<pre><code>[root@VM_0_17_centos ~]# docker run --name slave1 -p 3307:3306 -e MYSQL_ROOT_PASSWORD=root -d mysql:5.7.13

</code></pre>

<p>与主容器相似，拷贝配置文件至 slave1 目录修改后覆盖回 Docker 中</p>

<pre><code>log-bin=mysql-bin
server-id=2
</code></pre>

<p>别忘记，重启 slave1 容器，使配置生效</p>

<h2 id="toc_9">配置主从复制</h2>

<p>使用 Navicat 连接 slave1 后新建查询，执行以下 SQL</p>

<pre><code>CHANGE MASTER TO 
MASTER_HOST=&#39;ip&#39;,
MASTER_PORT=3306,
MASTER_USER=&#39;backup&#39;,
MASTER_PASSWORD=&#39;backup&#39;;

START SLAVE;
</code></pre>

<p>MASTER_HOST 填 Navicat 连接配置中的 ip 应该就可以</p>

<p>MASTER_PORT 主容器的端口</p>

<p>MASTER_USER 同步账号的用户名</p>

<p>MASTER_PASSWORD 同步账号的密码</p>

<h2 id="toc_10">检查是否配置成功</h2>

<pre><code>show slave status;
</code></pre>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/123902_gnvI_3773384.png" alt=""/></p>

<p>Slave_IO_State 如果是 Waiting for master to send event，那么就成功一半了，如果是 Connecting to master，基本就是配置失败了，建议重新检查下配置，具体失败的原因可以查看日志追踪</p>

<pre><code>[root@VM_0_17_centos master]# docker logs slave -f
</code></pre>

<p>我遇到的是 MASTER_USER 和 MASTER_PASSWORD 是否手打输错了，贴出错误日志</p>

<pre><code>2018-05-10T02:57:00.688887Z 11 [ERROR] Slave I/O for channel &#39;&#39;: error connecting to master &#39;bakcup@ip:3306&#39; - retry-time: 60  retries: 2, Error_code: 1045
2018-05-10T02:58:00.690476Z 11 [ERROR] Slave I/O for channel &#39;&#39;: error connecting to master &#39;bakcup@ip:3306&#39; - retry-time: 60  retries: 3, Error_code: 1045
</code></pre>

<p>注意看日志中的 bakcup，解决方法如下</p>

<pre><code>STOP SLAVE;

CHANGE MASTER TO 
MASTER_HOST=&#39;连接Navicat的ip&#39;,
MASTER_PORT=正确的端口,
MASTER_USER=&#39;正确的用户名&#39;,
MASTER_PASSWORD=&#39;正确的密码&#39;;

START SLAVE;
</code></pre>

<p>接着上文，我们说成功一半，并没有说成功了，那么另一半在于 Slave_IO_Running 与 Slave_SQL_Running</p>

<p>如果都是 Yes，那么恭喜你，可以测试主从复制的效果了，如果有一个不是 Yes，一半是重启从容器后，事务回滚引起的，那么给出解决方法如下</p>

<pre><code>stop slave ;
set GLOBAL SQL_SLAVE_SKIP_COUNTER=1;
start slave ;
</code></pre>

<p>执行后，再次观察三个关键字段应该就都没问题了</p>

<p>至此，一主一从已经搭建完成，再添加从实例的方式与上文一致，这里就不在赘述了。</p>

<h2 id="toc_11">测试主从复制</h2>

<p>首先，在主实例中创建一个测试数据库</p>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/130207_7Ty1_3773384.png" alt=""/></p>

<p>打开（刷新）从实例，可见 test 库已存在</p>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/130133_ITNO_3773384.png" alt=""/></p>

<p>在 test 库中创建一个表 t_test，添加一个 id 测试字段</p>

<p>向表中添加几个数据</p>

<p><img src="https://static.oschina.net/uploads/space/2018/0510/130357_IEZg_3773384.png" alt=""/></p>

<p>刷新从库，可见 t_test 表及其中 1、2、3、4 数据已存在</p>

<p>至此，一个具备主从复制的一主两从的 MySQL 就已搭建完成。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Never touch your local /etc/hosts file in OS X again]]></title>
    <link href="http://panlw.github.io/15274377762013.html"/>
    <updated>2018-05-28T00:16:16+08:00</updated>
    <id>http://panlw.github.io/15274377762013.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://gist.github.com/ogrrd/5831371">原文地址</a></p>

<p>To setup your computer to work with *.dev domains, e.g. project.dev, awesome.dev and so on, without having to add to your hosts file each time.</p>
</blockquote>

<h2 id="toc_0">Requirements</h2>

<ul>
<li><a href="http://mxcl.github.io/homebrew/">Homebrew</a></li>
<li>Mountain Lion</li>
</ul>

<h2 id="toc_1">Install</h2>

<pre><code>brew install dnsmasq
</code></pre>

<h2 id="toc_2">Setup</h2>

<h3 id="toc_3">Create config directory</h3>

<pre><code>mkdir -pv $(brew --prefix)/etc/
</code></pre>

<h3 id="toc_4">Setup *.dev</h3>

<pre><code>echo &#39;address=/.dev/127.0.0.1&#39; &gt; $(brew --prefix)/etc/dnsmasq.conf
</code></pre>

<p>You should probably add <code>strict-order</code> to <code>dnsmasq.conf</code> to keep nameserver order of <code>resolv.conf</code> (<a href="https://gist.github.com/drye/5387341">see here</a>).</p>

<h2 id="toc_5">Autostart</h2>

<h3 id="toc_6">Work after reboot</h3>

<pre><code>sudo cp -v $(brew --prefix dnsmasq)/homebrew.mxcl.dnsmasq.plist /Library/LaunchDaemons
</code></pre>

<h3 id="toc_7">Get it going right now</h3>

<pre><code>sudo launchctl load -w /Library/LaunchDaemons/homebrew.mxcl.dnsmasq.plist
</code></pre>

<h2 id="toc_8">Add to resolvers</h2>

<h3 id="toc_9">Create resolver directory</h3>

<pre><code>sudo mkdir -v /etc/resolver
</code></pre>

<h3 id="toc_10">Add your nameserver to resolvers</h3>

<pre><code>sudo bash -c &#39;echo &quot;nameserver 127.0.0.1&quot; &gt; /etc/resolver/dev&#39;
</code></pre>

<h2 id="toc_11">Add local DNS to search order in System Preferences</h2>

<p>System Preferences &gt; Network &gt; Wi-Fi (or whatever you use) &gt; Advanced... &gt; DNS &gt; add 127.0.0.1 to top of the list.</p>

<h2 id="toc_12">Finished</h2>

<p>That&#39;s it! You can run scutil --dns to show all of your current resolvers, and you should see that all requests for a domain ending in .dev will go to the DNS server at 127.0.0.1</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install NGINX with PHP7-FPM on Mac OS X with Homebrew]]></title>
    <link href="http://panlw.github.io/15274361469556.html"/>
    <updated>2018-05-27T23:49:06+08:00</updated>
    <id>http://panlw.github.io/15274361469556.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://gist.github.com/dtomasi/ab76d14338db82ec24a1fc137caff75b">原文地址</a></p>
</blockquote>

<h2 id="toc_0">Install Commandline Tools</h2>

<p><code>xcode-select --install</code></p>

<h2 id="toc_1">Install Homebrew</h2>

<p><code>ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</code></p>

<h4 id="toc_2">Check Installation</h4>

<p><code>brew doctor</code></p>

<h4 id="toc_3">Install brew services</h4>

<p><code>brew tap homebrew/services</code></p>

<h4 id="toc_4">Install bash completion (Optional)</h4>

<p><code>brew install bash-completion</code></p>

<h4 id="toc_5">Update Brew and Packages if allready installed</h4>

<p><code>brew update &amp;&amp; brew upgrade</code></p>

<h4 id="toc_6">Setup Environment</h4>

<p><code>sudo nano ~/.bash_profile</code></p>

<p>Add following lines</p>

<pre><code>  ##
  # Homebrew
  ##
  export PATH=&quot;/usr/local/bin:$PATH&quot;
  export PATH=&quot;/usr/local/sbin:$PATH&quot;
  
  ##
  # Homebrew bash completion
  ##
  if [ -f $(brew --prefix)/etc/bash_completion ]; then
    source $(brew --prefix)/etc/bash_completion
  fi
</code></pre>

<h2 id="toc_7">DNSMasq</h2>

<p>DNSMasq is used to resolve all domains that end with .dev to 127.0.0.1. So you don´t need to touch hosts-File anymore.</p>

<h4 id="toc_8">Install</h4>

<pre><code>brew install dnsmasq
</code></pre>

<h4 id="toc_9">Configure</h4>

<pre><code>curl -L https://gist.githubusercontent.com/dtomasi/ab76d14338db82ec24a1fc137caff75b/raw/550c84393c4c1eef8a3e68bb720df561b5d3f175/dnsmasq.conf -o /usr/local/etc/dnsmasq.conf

sudo curl -L https://gist.githubusercontent.com/dtomasi/ab76d14338db82ec24a1fc137caff75b/raw/550c84393c4c1eef8a3e68bb720df561b5d3f175/dev -o /etc/resolver/dev
</code></pre>

<h4 id="toc_10">Start, Stop and Restart</h4>

<pre><code># Start
sudo brew services start dnsmasq

# Stop
sudo brew services stop dnsmasq

# Restart
sudo brew services restart dnsmasq
</code></pre>

<h4 id="toc_11">Test</h4>

<pre><code>dig testing.a.domain.that.should.point.to.localhost.dev @127.0.0.1
</code></pre>

<h2 id="toc_12">PHP-FPM</h2>

<h4 id="toc_13">Install php70</h4>

<pre><code>  brew tap homebrew/dupes &amp;&amp; \
  brew tap homebrew/php &amp;&amp; \
  brew install --without-apache --with-fpm --with-mysql php70
</code></pre>

<h4 id="toc_14">Configure</h4>

<p><code>sudo nano /usr/local/etc/php/7.0/php-fpm.d/www.conf</code></p>

<pre><code>  user = YOUR_USERNAME
  group = YOUR_GROUP || staff
</code></pre>

<h4 id="toc_15">Testing</h4>

<p>start php-fpm</p>

<p><code>sudo brew services start php70</code></p>

<p>show running processes</p>

<p><code>lsof -Pni4 | grep LISTEN | grep php</code></p>

<h2 id="toc_16">NGINX</h2>

<h4 id="toc_17">Install NGINX</h4>

<pre><code>brew tap homebrew/nginx &amp;&amp; \
brew install nginx
</code></pre>

<h4 id="toc_18">Test Installation</h4>

<pre><code>  ## Start Nginx
  sudo brew services start nginx
  
  ## Check if Nginx is running on default port
  curl -IL http://127.0.0.1:8080
</code></pre>

<p>Output should look like this</p>

<pre><code>HTTP/1.1 200 OK
Server: nginx/1.10.0
Date: Sat, 07 May 2016 07:36:32 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 26 Apr 2016 13:31:24 GMT
Connection: keep-alive
ETag: &quot;571f6dac-264&quot;
Accept-Ranges: bytes
</code></pre>

<h4 id="toc_19">Stop Nginx</h4>

<p><code>sudo brew services stop nginx</code></p>

<h4 id="toc_20">Configure</h4>

<p>Create missing directories</p>

<pre><code>  mkdir -p /usr/local/etc/nginx/sites-available &amp;&amp; \
  mkdir -p /usr/local/etc/nginx/sites-enabled &amp;&amp; \
  mkdir -p /usr/local/etc/nginx/conf.d &amp;&amp; \
  mkdir -p /usr/local/etc/nginx/ssl
</code></pre>

<p>Configure nginx.conf</p>

<pre><code># Remove default
rm /usr/local/etc/nginx/nginx.conf
# Copy mine
curl -L https://gist.githubusercontent.com/dtomasi/ab76d14338db82ec24a1fc137caff75b/raw/c7c99476e6d8bd5b23e814c5593861adb9b54765/nginx.conf -o /usr/local/etc/nginx/nginx.conf
</code></pre>

<p>Start and Test Nginx</p>

<pre><code>  ## Start Nginx
  sudo brew services start nginx
  
  ## Check if Nginx is running on default port
  curl -IL http://localhost

  ## Output should look like this
  HTTP/1.1 200 OK
  Server: nginx/1.10.0
  Date: Sat, 07 May 2016 08:35:57 GMT
  Content-Type: text/html
  Content-Length: 612
  Last-Modified: Tue, 26 Apr 2016 13:31:24 GMT
  Connection: keep-alive
  ETag: &quot;571f6dac-264&quot;
  Accept-Ranges: bytes
</code></pre>

<h2 id="toc_21">Setup SSL</h2>

<p>Create a folder for our SSL certificates and private keys:</p>

<p><code>mkdir -p /usr/local/etc/nginx/ssl</code></p>

<p>Generate 4096 bit RSA keys and the self-sign the certificates in one command:</p>

<p><code>openssl req -new -newkey rsa:4096 -days 365 -nodes -x509 -subj &quot;/C=US/ST=State/L=Town/O=Office/CN=localhost&quot; -keyout /usr/local/etc/nginx/ssl/localhost.key -out /usr/local/etc/nginx/ssl/localhost.crt</code></p>

<h2 id="toc_22">Setup example virtual hosts</h2>

<p>These are working presets. But you need to edit Document-Root</p>

<pre><code>curl -L https://gist.githubusercontent.com/dtomasi/ab76d14338db82ec24a1fc137caff75b/raw/c7c99476e6d8bd5b23e814c5593861adb9b54765/default -o /usr/local/etc/nginx/sites-available/default &amp;&amp; \
curl -L https://gist.githubusercontent.com/dtomasi/ab76d14338db82ec24a1fc137caff75b/raw/c7c99476e6d8bd5b23e814c5593861adb9b54765/default-ssl -o /usr/local/etc/nginx/sites-available/default-ssl
</code></pre>

<p>Activate Virtual Hosts</p>

<pre><code>ln -sfv /usr/local/etc/nginx/sites-available/default /usr/local/etc/nginx/sites-enabled/default
ln -sfv /usr/local/etc/nginx/sites-available/default-ssl /usr/local/etc/nginx/sites-enabled/default-ssl
</code></pre>

<p>Create info.php for testing</p>

<p><code>echo &quot;&lt;?php phpinfo();&quot; &gt; /path/to/your/document/root</code></p>

<p>Test</p>

<pre><code>sudo brew services restart nginx

curl -IL http://localhost/info.php

# Output should look like this
HTTP/1.1 200 OK
Server: nginx/1.10.0
Date: Sat, 07 May 2016 08:40:36 GMT
Content-Type: text/html; charset=UTF-8
Connection: keep-alive
X-Powered-By: PHP/7.0.6
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Alpine Docker Image]]></title>
    <link href="http://panlw.github.io/15273973619516.html"/>
    <updated>2018-05-27T13:02:41+08:00</updated>
    <id>http://panlw.github.io/15273973619516.html</id>
    <content type="html"><![CDATA[
<pre><code class="language-Dockerfile">FROM alpine:3.5
# Install base packages
RUN apk update &amp;&amp; apk add curl bash tree tzdata \
    &amp;&amp; cp -r -f /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \
    &amp;&amp; echo -ne &quot;Alpine Linux 3.4 image. (`uname -rsv`)\n&quot; &gt;&gt; /root/.built
# Define bash as default command
RUN apk --no-cache add openjdk8-jre ttf-droid
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[分布式之消息队列复习精讲]]></title>
    <link href="http://panlw.github.io/15273831746106.html"/>
    <updated>2018-05-27T09:06:14+08:00</updated>
    <id>http://panlw.github.io/15273831746106.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>原文地址 <a href="http://www.cnblogs.com/rjzheng/p/8994962.html">http://www.cnblogs.com/rjzheng/p/8994962.html</a></p>
</blockquote>

<h2 id="toc_0">引言</h2>

<h3 id="toc_1">为什么写这篇文章?</h3>

<p>博主有两位朋友分别是小 A 和小 B:</p>

<ol>
<li> 小 A，工作于传统软件行业 (某社保局的软件外包公司)，每天工作内容就是和产品聊聊需求，改改业务逻辑。再不然就是和运营聊聊天，写几个 SQL，生成下报表。又或者接到客服的通知，某某功能故障了，改改数据，然后下班部署上线。每天过的都是这种生活，技术零成长。</li>
<li> 小 B，工作于某国企，虽然能接触到一些中间件技术。然而，他只会订阅 / 发布消息。通俗点说，就是调调 API。对为什么使用这些中间件啊？如何保证高可用啊？没有充分的认识。</li>
</ol>

<p>庆幸的是两位朋友都很有上进心，于是博主写这篇文章，帮助他们复习一下关于消息队列中间件这块的要点</p>

<h3 id="toc_2">复习要点</h3>

<p>本文大概围绕如下几点进行阐述:</p>

<ol>
<li> 为什么使用消息队列？</li>
<li> 使用消息队列有什么缺点?</li>
<li> 消息队列如何选型?</li>
<li> 如何保证消息队列是高可用的？</li>
<li> 如何保证消息不被重复消费?</li>
<li> 如何保证消费的可靠性传输?</li>
<li> 如何保证消息的顺序性？</li>
</ol>

<p>我们围绕以上七点进行阐述。需要说明一下，本文不是《消息队列从入门到精通》这种课程，因此只是提供一个复习思路，而不是去教你们怎么调用消息队列的 API。建议对消息队列不了解的人，去找点消息队列的博客看看，再看本文，收获更大</p>

<h2 id="toc_3">正文</h2>

<h3 id="toc_4">1、为什么要使用消息队列?</h3>

<p><strong>分析</strong>: 一个用消息队列的人，不知道为啥用，这就有点尴尬。没有复习这点，很容易被问蒙，然后就开始胡扯了。<br/>
<strong>回答</strong>: 这个问题, 咱只答三个最主要的应用场景 (不可否认还有其他的，但是只答三个主要的), 即以下六个字: <strong>解耦、异步、削峰</strong></p>

<h4 id="toc_5">(1) 解耦</h4>

<p><strong>传统模式:</strong><br/>
<img src="media/15273831746106/15273914396502.png" alt=""/></p>

<p>传统模式的缺点：</p>

<ul>
<li>  系统间耦合性太强，如上图所示，系统 A 在代码中直接调用系统 B 和系统 C 的代码，如果将来 D 系统接入，系统 A 还需要修改代码，过于麻烦！</li>
</ul>

<p><strong>中间件模式:</strong><br/>
<img src="media/15273831746106/15273918339281.png" alt=""/></p>

<p>中间件模式的的优点：</p>

<ul>
<li>  将消息写入消息队列，需要消息的系统自己从消息队列中订阅，从而系统 A 不需要做任何修改。</li>
</ul>

<h4 id="toc_6">(2) 异步</h4>

<p><strong>传统模式:</strong></p>

<p><img src="media/15273831746106/15273918558340.png" alt=""/></p>

<p>传统模式的缺点：</p>

<ul>
<li>  一些非必要的业务逻辑以同步的方式运行，太耗费时间。</li>
</ul>

<p><strong>中间件模式:</strong><br/>
<img src="media/15273831746106/15273918690655.png" alt=""/></p>

<p>中间件模式的的优点：</p>

<ul>
<li>  将消息写入消息队列，非必要的业务逻辑以异步的方式运行，加快响应速度</li>
</ul>

<h4 id="toc_7">(3) 削峰</h4>

<p><strong>传统模式</strong><br/>
<img src="media/15273831746106/15273918819389.png" alt=""/></p>

<p>传统模式的缺点：</p>

<ul>
<li>  并发量大的时候，所有的请求直接怼到数据库，造成数据库连接异常</li>
</ul>

<p><strong>中间件模式:</strong><br/>
<img src="media/15273831746106/15273924798559.png" alt=""/></p>

<p>中间件模式的的优点：</p>

<ul>
<li>  系统 A 慢慢的按照数据库能处理的并发量，从消息队列中慢慢拉取消息。在生产中，这个短暂的高峰期积压是允许的。</li>
</ul>

<h3 id="toc_8">2、使用了消息队列会有什么缺点?</h3>

<p><strong>分析</strong>: 一个使用了 MQ 的项目，如果连这个问题都没有考虑过，就把 MQ 引进去了，那就给自己的项目带来了风险。我们引入一个技术，要对这个技术的弊端有充分的认识，才能做好预防。<strong>要记住，不要给公司挖坑！</strong><br/>
<strong>回答</strong>: 回答也很容易，从以下两个个角度来答</p>

<ul>
<li>  <strong>系统可用性降低</strong>: 你想啊，本来其他系统只要运行好好的，那你的系统就是正常的。现在你非要加个消息队列进去，那消息队列挂了，你的系统不是呵呵了。因此，系统可用性降低</li>
<li>  <strong>系统复杂性增加</strong>: 要多考虑很多方面的问题，比如一致性问题、如何保证消息不被重复消费，如何保证保证消息可靠传输。因此，需要考虑的东西更多，系统复杂性增大。</li>
</ul>

<p>但是，我们该用还是要用的。</p>

<h3 id="toc_9">3、消息队列如何选型?</h3>

<p>先说一下，博主只会 ActiveMQ,RabbitMQ,RocketMQ,Kafka，对什么 ZeroMQ 等其他 MQ 没啥理解，因此只能基于这四种 MQ 给出回答。<br/>
<strong>分析</strong>: 既然在项目中用了 MQ，肯定事先要对业界流行的 MQ 进行调研，如果连每种 MQ 的优缺点都没了解清楚，就拍脑袋依据喜好，用了某种 MQ，还是给项目挖坑。如果面试官问:&quot;你为什么用这种 MQ？。&quot; 你直接回答 &quot;领导决定的。&quot; 这种回答就很 LOW 了。<strong>还是那句话，不要给公司挖坑。</strong><br/>
<strong>回答</strong>: 首先，咱先上 <a href="http://activemq.apache.org/">ActiveMQ 的社区</a>，看看该 MQ 的更新频率:</p>

<pre><code>Apache ActiveMQ 5.15.3 Release
Christopher L. Shannon posted on Feb 12, 2018
Apache ActiveMQ 5.15.2 Released
Christopher L. Shannon posted on Oct 23, 2017
Apache ActiveMQ 5.15.0 Released
Christopher L. Shannon posted on Jul 06, 2017
省略以下记录
...
</code></pre>

<p>我们可以看出，ActiveMq 几个月才发一次版本，据说研究重心在他们的下一代产品 Apollo。<br/>
接下来，我们再去 <a href="http://www.rabbitmq.com/">RabbitMQ 的社区</a>去看一下, RabbitMQ 的更新频率</p>

<pre><code>RabbitMQ 3.7.3 release  30 January 2018
RabbitMQ 3.6.15 release  17 January 2018
RabbitMQ 3.7.2 release23 December 2017
RabbitMQ 3.7.1 release21 December 2017
省略以下记录
...
</code></pre>

<p>我们可以看出，RabbitMQ 版本发布比 ActiveMq 频繁很多。至于 RocketMQ 和 kafka 就不带大家看了，总之也比 ActiveMQ 活跃的多。详情，可自行查阅。<br/>
再来一个性能对比表</p>

<table>
<thead>
<tr>
<th>特性</th>
<th>ActiveMQ</th>
<th>RabbitMQ</th>
<th>RocketMQ</th>
<th>kafka</th>
</tr>
</thead>

<tbody>
<tr>
<td>开发语言</td>
<td>java</td>
<td>erlang</td>
<td>java</td>
<td>scala</td>
</tr>
<tr>
<td>单机吞吐量</td>
<td>万级</td>
<td>万级</td>
<td>10 万级</td>
<td>10 万级</td>
</tr>
<tr>
<td>时效性</td>
<td>ms 级</td>
<td>us 级</td>
<td>ms 级</td>
<td>ms 级以内</td>
</tr>
<tr>
<td>可用性</td>
<td>高 (主从架构)</td>
<td>高 (主从架构)</td>
<td>非常高 (分布式架构)</td>
<td>非常高 (分布式架构)</td>
</tr>
<tr>
<td>功能特性</td>
<td>成熟的产品，在很多公司得到应用；有较多的文档；各种协议支持较好</td>
<td>基于 erlang 开发，所以并发能力很强，性能极其好，延时很低; 管理界面较丰富</td>
<td>MQ 功能比较完备，扩展性佳</td>
<td>只支持主要的 MQ 功能，像一些消息查询，消息回溯等功能没有提供，毕竟是为大数据准备的，在大数据领域应用广。</td>
</tr>
</tbody>
</table>

<p>综合上面的材料得出以下两点:<br/>
(1) 中小型软件公司，建议选 RabbitMQ. 一方面，erlang 语言天生具备高并发的特性，而且他的管理界面用起来十分方便。正所谓，成也萧何，败也萧何！他的弊端也在这里，虽然 RabbitMQ 是开源的，然而国内有几个能定制化开发 erlang 的程序员呢？所幸，RabbitMQ 的社区十分活跃，可以解决开发过程中遇到的 bug，这点对于中小型公司来说十分重要。不考虑 rocketmq 和 kafka 的原因是，一方面中小型软件公司不如互联网公司，数据量没那么大，选消息中间件，应首选功能比较完备的，所以 kafka 排除。不考虑 rocketmq 的原因是，rocketmq 是阿里出品，如果阿里放弃维护 rocketmq，中小型公司一般抽不出人来进行 rocketmq 的定制化开发，因此不推荐。<br/>
(2) 大型软件公司，根据具体使用在 rocketMq 和 kafka 之间二选一。一方面，大型软件公司，具备足够的资金搭建分布式环境，也具备足够大的数据量。针对 rocketMQ, 大型软件公司也可以抽出人手对 rocketMQ 进行定制化开发，毕竟国内有能力改 JAVA 源码的人，还是相当多的。至于 kafka，根据业务场景选择，如果有日志采集功能，肯定是首选 kafka 了。具体该选哪个，看使用场景。</p>

<h3 id="toc_10">4、如何保证消息队列是高可用的？</h3>

<p><strong>分析</strong>: 在第二点说过了，引入消息队列后，系统的可用性下降。在生产中，没人使用单机模式的消息队列。因此，作为一个合格的程序员，应该对消息队列的高可用有很深刻的了解。如果面试的时候，面试官问，你们的消息中间件如何保证高可用的？你的回答只是表明自己只会订阅和发布消息，面试官就会怀疑你是不是只是自己搭着玩，压根没在生产用过。<strong>请做一个爱思考，会思考，懂思考的程序员。</strong><br/>
<strong>回答</strong>: 这问题，其实要对消息队列的集群模式要有深刻了解，才好回答。<br/>
以 rcoketMQ 为例，他的集群就有多 master 模式、多 master 多 slave 异步复制模式、多 master 多 slave 同步双写模式。多 master 多 slave 模式部署架构图 (网上找的, 偷个懒，懒得画):<br/>
<img src="https://images.cnblogs.com/cnblogs_com/rjzheng/1202350/o_rocketcluster.png" alt=""/><br/>
其实博主第一眼看到这个图，就觉得和 kafka 好像，只是 NameServer 集群，在 kafka 中是用 zookeeper 代替，都是用来保存和发现 master 和 slave 用的。通信过程如下:<br/>
Producer 与 NameServer 集群中的其中一个节点（随机选择）建立长连接，定期从 NameServer 获取 Topic 路由信息，并向提供 Topic 服务的 Broker Master 建立长连接，且定时向 Broker 发送心跳。Producer 只能将消息发送到 Broker master，但是 Consumer 则不一样，它同时和提供 Topic 服务的 Master 和 Slave 建立长连接，既可以从 Broker Master 订阅消息，也可以从 Broker Slave 订阅消息。<br/>
那么 kafka 呢, 为了对比说明直接上 kafka 的拓补架构图 (也是找的，懒得画)<br/>
<img src="https://images.cnblogs.com/cnblogs_com/rjzheng/1202350/o_kafka.png" alt=""/><br/>
如上图所示，一个典型的 Kafka 集群中包含若干 Producer（可以是 web 前端产生的 Page View，或者是服务器日志，系统 CPU、Memory 等），若干 broker（Kafka 支持水平扩展，一般 broker 数量越多，集群吞吐率越高），若干 Consumer Group，以及一个 Zookeeper 集群。Kafka 通过 Zookeeper 管理集群配置，选举 leader，以及在 Consumer Group 发生变化时进行 rebalance。Producer 使用 push 模式将消息发布到 broker，Consumer 使用 pull 模式从 broker 订阅并消费消息。<br/>
至于 rabbitMQ, 也有普通集群和镜像集群模式，自行去了解，比较简单，两小时即懂。<br/>
要求，在回答高可用的问题时，应该能逻辑清晰的画出自己的 MQ 集群架构或清晰的叙述出来。</p>

<h3 id="toc_11">5、如何保证消息不被重复消费？</h3>

<p><strong>分析</strong>: 这个问题其实换一种问法就是，如何保证消息队列的幂等性? 这个问题可以认为是消息队列领域的基本问题。换句话来说，是在考察你的设计能力，这个问题的回答可以根据具体的业务场景来答，没有固定的答案。<br/>
<strong>回答</strong>: 先来说一下为什么会造成重复消费?<br/>
  其实无论是那种消息队列，造成重复消费原因其实都是类似的。正常情况下，消费者在消费消息时候，消费完毕后，会发送一个确认信息给消息队列，消息队列就知道该消息被消费了，就会将该消息从消息队列中删除。只是不同的消息队列发送的确认信息形式不同, 例如 RabbitMQ 是发送一个 ACK 确认消息，RocketMQ 是返回一个 CONSUME_SUCCESS 成功标志，kafka 实际上有个 offset 的概念，简单说一下 (如果还不懂，出门找一个 kafka 入门到精通教程), 就是每一个消息都有一个 offset，kafka 消费过消息后，需要提交 offset，让消息队列知道自己已经消费过了。那造成重复消费的原因?，就是因为网络传输等等故障，确认信息没有传送到消息队列，导致消息队列不知道自己已经消费过该消息了，再次将该消息分发给其他的消费者。<br/>
  如何解决? 这个问题针对业务场景来答分以下几点<br/>
  (1) 比如，你拿到这个消息做数据库的 insert 操作。那就容易了，给这个消息做一个唯一主键，那么就算出现重复消费的情况，就会导致主键冲突，避免数据库出现脏数据。<br/>
  (2) 再比如，你拿到这个消息做 redis 的 set 的操作，那就容易了，不用解决，因为你无论 set 几次结果都是一样的，set 操作本来就算幂等操作。<br/>
  (3) 如果上面两种情况还不行，上大招。准备一个第三方介质, 来做消费记录。以 redis 为例，给消息分配一个全局 id，只要消费过该消息，将 &lt; id,message &gt; 以 K-V 形式写入 redis。那消费者开始消费前，先去 redis 中查询有没消费记录即可。</p>

<h3 id="toc_12">6、如何保证消费的可靠性传输?</h3>

<p><strong>分析</strong>: 我们在使用消息队列的过程中，应该做到消息不能多消费，也不能少消费。如果无法做到可靠性传输，可能给公司带来千万级别的财产损失。同样的，如果可靠性传输在使用过程中，没有考虑到，这不是给公司挖坑么，你可以拍拍屁股走了，公司损失的钱，谁承担。还是那句话，<strong>认真对待每一个项目，不要给公司挖坑。</strong><br/>
<strong>回答</strong>: 其实这个可靠性传输，每种 MQ 都要从三个角度来分析: 生产者弄丢数据、消息队列弄丢数据、消费者弄丢数据</p>

<h4 id="toc_13">RabbitMQ</h4>

<p><strong>(1) 生产者丢数据</strong><br/>
从生产者弄丢数据这个角度来看，RabbitMQ 提供 transaction 和 confirm 模式来确保生产者不丢消息。<br/>
transaction 机制就是说，发送消息前，开启事物 (channel.txSelect())，然后发送消息，如果发送过程中出现什么异常，事物就会回滚 (channel.txRollback())，如果发送成功则提交事物 (channel.txCommit())。<br/>
然而缺点就是吞吐量下降了。因此，按照博主的经验，生产上用 confirm 模式的居多。一旦 channel 进入 confirm 模式，所有在该信道上面发布的消息都将会被指派一个唯一的 ID(从 1 开始)，一旦消息被投递到所有匹配的队列之后，rabbitMQ 就会发送一个 Ack 给生产者 (包含消息的唯一 ID)，这就使得生产者知道消息已经正确到达目的队列了. 如果 rabiitMQ 没能处理该消息，则会发送一个 Nack 消息给你，你可以进行重试操作。处理 Ack 和 Nack 的代码如下所示（说好不上代码的，偷偷上了）:</p>

<pre><code>channel.addConfirmListener(new ConfirmListener() {  
                @Override  
                public void handleNack(long deliveryTag, boolean multiple) throws IOException {  
                    System.out.println(&quot;nack: deliveryTag = &quot;+deliveryTag+&quot; multiple: &quot;+multiple);  
                }  
                @Override  
                public void handleAck(long deliveryTag, boolean multiple) throws IOException {  
                    System.out.println(&quot;ack: deliveryTag = &quot;+deliveryTag+&quot; multiple: &quot;+multiple);  
                }  
            });  
</code></pre>

<p><strong>(2) 消息队列丢数据</strong><br/>
处理消息队列丢数据的情况，一般是开启持久化磁盘的配置。这个持久化配置可以和 confirm 机制配合使用，你可以在消息持久化磁盘后，再给生产者发送一个 Ack 信号。这样，如果消息持久化磁盘之前，rabbitMQ 阵亡了，那么生产者收不到 Ack 信号，生产者会自动重发。<br/>
那么如何持久化呢，这里顺便说一下吧，其实也很容易，就下面两步<br/>
1、将 queue 的持久化标识 durable 设置为 true, 则代表是一个持久的队列<br/>
2、发送消息的时候将 deliveryMode=2<br/>
这样设置以后，rabbitMQ 就算挂了，重启后也能恢复数据<br/>
<strong>(3) 消费者丢数据</strong><br/>
消费者丢数据一般是因为采用了自动确认消息模式。这种模式下，消费者会自动确认收到信息。这时 rahbitMQ 会立即将消息删除，这种情况下如果消费者出现异常而没能处理该消息，就会丢失该消息。<br/>
至于解决方案，采用手动确认消息即可。</p>

<h4 id="toc_14">kafka</h4>

<p>这里先引一张 kafka Replication 的<a href="http://www.infoq.com/cn/articles/kafka-analysis-part-2/">数据流向图</a><br/>
<img src="https://images.cnblogs.com/cnblogs_com/rjzheng/1202350/o_kafka1png.png" alt=""/><br/>
Producer 在发布消息到某个 Partition 时，先通过 ZooKeeper 找到该 Partition 的 Leader，然后无论该 Topic 的 Replication Factor 为多少（也即该 Partition 有多少个 Replica），Producer 只将该消息发送到该 Partition 的 Leader。Leader 会将该消息写入其本地 Log。每个 Follower 都从 Leader 中 pull 数据。<br/>
针对上述情况，得出如下分析<br/>
<strong>(1) 生产者丢数据</strong><br/>
在 kafka 生产中，基本都有一个 leader 和多个 follwer。follwer 会去同步 leader 的信息。因此，为了避免生产者丢数据，做如下两点配置</p>

<ol>
<li> 第一个配置要在 producer 端设置 acks=all。这个配置保证了，follwer 同步完成后，才认为消息发送成功。</li>
<li> 在 producer 端设置 retries=MAX，一旦写入失败，这无限重试</li>
</ol>

<p><strong>(2) 消息队列丢数据</strong><br/>
针对消息队列丢数据的情况，无外乎就是，数据还没同步，leader 就挂了，这时 zookpeer 会将其他的 follwer 切换为 leader, 那数据就丢失了。针对这种情况，应该做两个配置。</p>

<ol>
<li> replication.factor 参数，这个值必须大于 1，即要求每个 partition 必须有至少 2 个副本</li>
<li> min.insync.replicas 参数，这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系</li>
</ol>

<p>这两个配置加上上面生产者的配置联合起来用，基本可确保 kafka 不丢数据</p>

<p><strong>(3) 消费者丢数据</strong><br/>
这种情况一般是自动提交了 offset，然后你处理程序过程中挂了。kafka 以为你处理好了。再强调一次 offset 是干嘛的<br/>
<strong>offset</strong>：指的是 kafka 的 topic 中的每个消费组消费的下标。简单的来说就是一条消息对应一个 offset 下标，每次消费数据的时候如果提交 offset，那么下次消费就会从提交的 offset 加一那里开始消费。<br/>
比如一个 topic 中有 100 条数据，我消费了 50 条并且提交了，那么此时的 kafka 服务端记录提交的 offset 就是 49(offset 从 0 开始)，那么下次消费的时候 offset 就从 50 开始消费。<br/>
解决方案也很简单，改成手动提交即可。</p>

<h4 id="toc_15">ActiveMQ 和 RocketMQ</h4>

<p>大家自行查阅吧</p>

<h3 id="toc_16">7、如何保证消息的顺序性？</h3>

<p><strong>分析</strong>: 其实并非所有的公司都有这种业务需求，但是还是对这个问题要有所复习。<br/>
<strong>回答</strong>: 针对这个问题，通过某种算法，将需要保持先后顺序的消息放到同一个消息队列中 (kafka 中就是 partition,rabbitMq 中就是 queue)。然后只用一个消费者去消费该队列。<br/>
有的人会问: 那如果为了吞吐量，有多个消费者去消费怎么办？<br/>
这个问题，没有固定回答的套路。比如我们有一个微博的操作，发微博、写评论、删除微博，这三个异步操作。如果是这样一个业务场景，那只要重试就行。比如你一个消费者先执行了写评论的操作，但是这时候，微博都还没发，写评论一定是失败的，等一段时间。等另一个消费者，先执行写评论的操作后，再执行，就可以成功。<br/>
总之，针对这个问题，我的观点是保证入队有序就行，出队以后的顺序交给消费者自己去保证，没有固定套路。</p>

<h2 id="toc_17">总结</h2>

<p>写到这里，希望读者把本文提出的这几个问题，经过深刻的准备后，一般来说，能囊括大部分的消息队列的知识点。如果面试官不问这几个问题怎么办，简单，自己把几个问题讲清楚，突出以下自己考虑的全面性。<br/>
最后，其实我不太提倡这样突击复习，希望大家打好基本功，<strong>做一个爱思考，懂思考，会思考的程序员</strong>。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[权限系统设计模型分析（DAC，MAC，RBAC，ABAC）]]></title>
    <link href="http://panlw.github.io/15264401346282.html"/>
    <updated>2018-05-16T11:08:54+08:00</updated>
    <id>http://panlw.github.io/15264401346282.html</id>
    <content type="html"><![CDATA[
<pre><code>作者：https://www.jianshu.com/u/11c0ebe856b8
发表：2017.12.10 23:16
原文：https://www.jianshu.com/p/ce0944b4a903
</code></pre>

<blockquote>
<p>好久没有更新文章了…… 这一年过得太忙。<br/>
准备一篇个人认为值得拿出来分享的文章真的需要很多时间，如果你喜欢，请评论、点赞让我知道，我会抽更多的时间来更新一些分享给大家，谢谢！</p>
</blockquote>

<p>此篇文章主要尝试将世面上现有的一些权限系统设计做一下简单的总结分析，个人水平有限，如有错误请不吝指出。</p>

<h2 id="toc_0">术语</h2>

<p>这里对后面会用到的词汇做一个说明，老司机请直接翻到<strong>常见设计模式</strong>。</p>

<h3 id="toc_1">用户</h3>

<p>发起操作的主体。</p>

<h3 id="toc_2">对象（Subject）</h3>

<p>指操作所针对的客体对象，比如订单数据或图片文件。</p>

<h3 id="toc_3">权限控制表 (ACL: Access Control List)</h3>

<p>用来描述权限规则或用户和权限之间关系的数据表。</p>

<h3 id="toc_4">权限 (Permission)</h3>

<p>用来指代对某种对象的某一种操作，例如 “添加文章的操作”。</p>

<h3 id="toc_5">权限标识</h3>

<p>权限的代号，例如用 “ARTICLE_ADD” 来指代 “添加文章的操作” 权限。</p>

<h2 id="toc_6">常见设计模式</h2>

<h3 id="toc_7">自主访问控制（DAC: Discretionary Access Control）</h3>

<p>系统会识别用户，然后根据被操作对象（Subject）的权限控制列表（ACL: Access Control List）或者权限控制矩阵（ACL: Access Control Matrix）的信息来决定用户的是否能对其进行哪些操作，例如读取或修改。</p>

<p>而拥有对象权限的用户，又可以将该对象的权限分配给其他用户，所以称之为 “自主（Discretionary）” 控制。</p>

<p>这种设计最常见的应用就是文件系统的权限设计，如微软的 NTFS。</p>

<p>DAC 最大缺陷就是对权限控制比较分散，不便于管理，比如无法简单地将一组文件设置统一的权限开放给指定的一群用户。</p>

<p><img src="https://upload-images.jianshu.io/upload_images/594774-6d77f13cc568797f.png" alt=""/></p>

<h3 id="toc_8">强制访问控制（MAC: Mandatory Access Control）</h3>

<p>MAC 是为了弥补 DAC 权限控制过于分散的问题而诞生的。在 MAC 的设计中，每一个对象都都有一些权限标识，每个用户同样也会有一些权限标识，而用户能否对该对象进行操作取决于双方的权限标识的关系，这个限制判断通常是由系统硬性限制的。比如在影视作品中我们经常能看到特工在查询机密文件时，屏幕提示需要 “无法访问，需要一级安全许可”，这个例子中，文件上就有“一级安全许可” 的权限标识，而用户并不具有。</p>

<p>MAC 非常适合机密机构或者其他等级观念强烈的行业，但对于类似商业服务系统，则因为不够灵活而不能适用。</p>

<p><img src="https://upload-images.jianshu.io/upload_images/594774-f9ce520635dc31b2.png" alt=""/></p>

<blockquote>
<p><a href="https://link.jianshu.com?t=https://www.centos.org/docs/5/html/Deployment_Guide-en-US/sec-mls-ov.html">Red Hat: MLS</a></p>
</blockquote>

<h3 id="toc_9">基于角色的访问控制（RBAC: Role-Based Access Control)</h3>

<p>因为 DAC 和 MAC 的诸多限制，于是诞生了 RBAC，并且成为了迄今为止最为普及的权限设计模型。</p>

<p>RBAC 在用户和权限之间引入了 “角色（Role）” 的概念（暂时忽略 Session 这个概念）：</p>

<p><img src="https://upload-images.jianshu.io/upload_images/594774-b220fc093138a2c7.png" alt=""/></p>

<blockquote>
<p>图片来自 <a href="https://link.jianshu.com?t=http://directory.apache.org/fortress/user-guide/1.3-what-rbac-is.html">Apache Directory</a></p>
</blockquote>

<p>如图所示，每个用户关联一个或多个角色，每个角色关联一个或多个权限，从而可以实现了非常灵活的权限管理。角色可以根据实际业务需求灵活创建，这样就省去了每新增一个用户就要关联一遍所有权限的麻烦。简单来说 RBAC 就是：用户关联角色，角色关联权限。另外，RBAC 是可以模拟出 DAC 和 MAC 的效果的。</p>

<p>例如数据库软件 MongoDB 便是采用 RBAC 模型，对数据库的操作都划分成了权限（<a href="https://link.jianshu.com?t=https://docs.mongodb.com/manual/reference/privilege-actions/">MongoDB 权限文档</a>）：</p>

<table>
<thead>
<tr>
<th>权限标识</th>
<th>说明</th>
</tr>
</thead>

<tbody>
<tr>
<td>find</td>
<td>具有此权限的用户可以运行所有和查询有关的命令，如：aggregate、checkShardingIndex、count 等。</td>
</tr>
<tr>
<td>insert</td>
<td>具有此权限的用户可以运行所有和新建数据有关的命令：insert 和 create 等。</td>
</tr>
<tr>
<td>collStats</td>
<td>具有此权限的用户可以对指定 database 或 collection 执行 collStats 命令。</td>
</tr>
<tr>
<td>viewRole</td>
<td>具有此权限的用户可以查看指定 database 的角色信息。</td>
</tr>
<tr>
<td>…</td>
<td></td>
</tr>
</tbody>
</table>

<p>基于这些权限，MongoDB 提供了一些预定义的角色（<a href="https://link.jianshu.com?t=https://docs.mongodb.com/manual/reference/built-in-roles/">MongoDB 预定义角色文档</a>，用户也可以自己定义角色）：</p>

<table>
<thead>
<tr>
<th>角色</th>
<th>find</th>
<th>insert</th>
<th>collStats</th>
<th>viewRole</th>
<th>…</th>
</tr>
</thead>

<tbody>
<tr>
<td>read</td>
<td>✔</td>
<td></td>
<td>✔</td>
<td></td>
<td>…</td>
</tr>
<tr>
<td>readWrite</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td></td>
<td>…</td>
</tr>
<tr>
<td>dbAdmin</td>
<td>✔</td>
<td></td>
<td>✔</td>
<td></td>
<td>…</td>
</tr>
<tr>
<td>userAdmin</td>
<td></td>
<td></td>
<td></td>
<td>✔</td>
<td>…</td>
</tr>
</tbody>
</table>

<p>最后授予用户不同的角色，就可以实现不同粒度的权限分配了。</p>

<p>目前市面上绝大部分系统在设计权限系统时都采用 RBAC 模型。然而也有的系统错误地实现了 RBAC，他们采用的是判断用户是否具有某个角色而不是判断权限，例如以下代码：</p>

<pre><code>&lt;?php

if ($user-&gt;hasRole(&#39;hr&#39;)) {
    // 执行某种只有“HR”角色才能做的功能，例如给员工涨薪…
    // ...
}

</code></pre>

<p>如果后期公司规定部门经理也可以给员工涨薪，这时就不得不修改代码了。</p>

<p>以上基本就是 RBAC 的核心设计（RBAC Core）。而基于核心概念之上，RBAC 规范还提供了扩展模式。</p>

<h4 id="toc_10">角色继承 (Hierarchical Role)</h4>

<p><img src="https://upload-images.jianshu.io/upload_images/594774-386977fee82f6152.png" alt=""/></p>

<blockquote>
<p>带有角色继承的 RBAC。图片来自 <a href="https://link.jianshu.com?t=http://directory.apache.org/fortress/user-guide/1.3-what-rbac-is.html">Apache Directory</a></p>
</blockquote>

<p>顾名思义，角色继承就是指角色可以继承于其他角色，在拥有其他角色权限的同时，自己还可以关联额外的权限。这种设计可以给角色分组和分层，一定程度简化了权限管理工作。</p>

<h4 id="toc_11">职责分离 (Separation of Duty)</h4>

<p>为了避免用户拥有过多权限而产生利益冲突，例如一个篮球运动员同时拥有裁判的权限（看一眼就给你判犯规狠不狠？），另一种职责分离扩展版的 RBAC 被提出。</p>

<p>职责分离有两种模式：</p>

<ul>
<li>  静态职责分离 (Static Separation of Duty)：用户无法同时被赋予有冲突的角色。</li>
<li>  动态职责分离 (Dynamic Separation of Duty)：用户在一次会话（Session）中不能同时激活自身所拥有的、互相有冲突的角色，只能选择其一。</li>
</ul>

<p><img src="https://upload-images.jianshu.io/upload_images/594774-feb7c1074d151113.png" alt=""/></p>

<blockquote>
<p>静态职责分离。图片来自 <a href="https://link.jianshu.com?t=http://directory.apache.org/fortress/user-guide/1.3-what-rbac-is.html">Apache Directory</a></p>
</blockquote>

<p><img src="https://upload-images.jianshu.io/upload_images/594774-059b93e4209e8fa6.png" alt=""/></p>

<blockquote>
<p>动态职责分离。图片来自 <a href="https://link.jianshu.com?t=http://directory.apache.org/fortress/user-guide/1.3-what-rbac-is.html">Apache Directory</a></p>
</blockquote>

<p>讲了这么多 RBAC，都还只是在用户和权限之间进行设计，并没有涉及到用户和对象之间的权限判断，而在实际业务系统中限制用户能够使用的对象是很常见的需求。例如华中区域的销售没有权限查询华南区域的客户数据，虽然他们都具有销售的角色，而销售的角色拥有查询客户信息的权限。</p>

<p>那么我们应该怎么办呢？</p>

<h4 id="toc_12">用户和对象的权限控制</h4>

<p>在 RBAC 标准中并没有涉及到这个内容（RBAC 基本只能做到对一类对象的控制），但是这里讲几种基于 RBAC 的实现方式。</p>

<p>首先我们看看 PHP 框架 <a href="https://link.jianshu.com?t=http://www.yiiframework.com/wiki/136/getting-to-understand-hierarchical-rbac-scheme/">Yii 1.X 的解决方案</a>（2.X 中代码更为优雅，但 1.X 的示例代码更容易看明白）：</p>

<pre><code>&lt;?php
$auth=Yii::app()-&gt;authManager;

// command-permission
$auth-&gt;createOperation(&#39;listPosts&#39;,&#39;list posts&#39;);     // define a perm: listPosts
$auth-&gt;createOperation(&#39;createPost&#39;,&#39;create a post&#39;); // define a perm: createPost
$auth-&gt;createOperation(&#39;readPost&#39;,&#39;read a post&#39;);     // define a perm: readPost
$auth-&gt;createOperation(&#39;updatePost&#39;,&#39;update a post&#39;); // define a perm: updatePost
$auth-&gt;createOperation(&#39;deletePost&#39;,&#39;delete a post&#39;); // define a perm: deletePost

// qualifier-permission
// define a qualifier on domain(user)
$bizRule=&#39;return Yii::app()-&gt;user-&gt;id==$params[&quot;post&quot;]-&gt;authID;&#39;;

// define a perm with qualifier: listOwnPosts
$perm=$auth-&gt;createTask(&#39;listOwnPosts&#39;, &#39;list posts by author himself&#39;, $bizRule);
$perm-&gt;addChild(&#39;listPosts&#39;); // sub perm

// define a perm with qualifier: updateOwnPost
$perm=$auth-&gt;createTask(&#39;updateOwnPost&#39;,&#39;update a post by author himself&#39;,$bizRule);
$perm-&gt;addChild(&#39;updatePost&#39;); // sub perm

// define a role: reader
$role=$auth-&gt;createRole(&#39;reader&#39;);
$role-&gt;addChild(&#39;readPost&#39;); // sub perm

// define a role: author
$role=$auth-&gt;createRole(&#39;author&#39;);
$role-&gt;addChild(&#39;reader&#39;); // sub role
$role-&gt;addChild(&#39;createPost&#39;); // sub perm
$role-&gt;addChild(&#39;updateOwnPost&#39;); // sub perm

// define a role: editor
$role=$auth-&gt;createRole(&#39;editor&#39;);
$role-&gt;addChild(&#39;reader&#39;); // sub role
$role-&gt;addChild(&#39;updatePost&#39;); // sub perm

// define a role: admin
$role=$auth-&gt;createRole(&#39;admin&#39;);
$role-&gt;addChild(&#39;editor&#39;); // sub role
$role-&gt;addChild(&#39;author&#39;); // sub role
$role-&gt;addChild(&#39;deletePost&#39;); // sub perm
</code></pre>

<p>实现效果：</p>

<p><img src="https://upload-images.jianshu.io/upload_images/594774-fb67d571497fb0b2.gif" alt=""/></p>

<blockquote>
<p>图片来自 <a href="https://link.jianshu.com?t=http://www.yiiframework.com/wiki/136/getting-to-understand-hierarchical-rbac-scheme/">Yii 官方 WiKi</a></p>
</blockquote>

<p>在这个 Yii 的官方例子中，<code>updateOwnPost</code>在判断用户是否具有<code>updatePost</code>权限的基础上更进一步判断了用户是否有权限操作这个特定的对象，并且这个判断逻辑是通过代码设置的，非常灵活。</p>

<p>不过大部分时候我们并不需要这样的灵活程度，会带来额外的开发和维护成本，而另一种基于模式匹配规则的对象权限控制可能更适合。例如判断用户是否对 Id 为 123 的文章具有编辑的权限，代码可能是这样的：</p>

<pre><code>&lt;?php

// 假设articleId是动态获取的
$articleId = 123;

if ($user-&gt;can(&quot;article:edit:{$articleId}&quot;)) {
    // ...
}

</code></pre>

<p>而给用户授权则有多种方式可以选择：</p>

<pre><code>&lt;?php

// 允许用户编辑Id为123的文章
$user-&gt;grant(&#39;article:edit:123&#39;);

// 使用通配符，允许用户编辑所有文章
$user-&gt;grant(&#39;article:edit:*&#39;);

</code></pre>

<p>虽然不及 Yii 方案的灵活，但某些场景下这样就够用了。</p>

<p>如果大家还有更好的方案，欢迎在评论中提出。</p>

<h3 id="toc_13">基于属性的权限验证（ABAC: Attribute-Based Access Control）</h3>

<p>ABAC 被一些人称为是权限系统设计的未来。</p>

<p>不同于常见的将用户通过某种方式关联到权限的方式，ABAC 则是通过动态计算一个或一组属性来是否满足某种条件来进行授权判断（可以编写简单的逻辑）。属性通常来说分为四类：用户属性（如用户年龄），环境属性（如当前时间），操作属性（如读取）和对象属性（如一篇文章，又称资源属性），所以理论上能够实现非常灵活的权限控制，几乎能满足所有类型的需求。</p>

<p>例如规则：“允许所有班主任在上课时间自由进出校门”这条规则，其中，“班主任”是用户的角色属性，“上课时间”是环境属性，“进出”是操作属性，而 “校门” 就是对象属性了。为了实现便捷的规则设置和规则判断执行，ABAC 通常有配置文件（XML、YAML 等）或 DSL 配合规则解析引擎使用。XACML（eXtensible Access Control Markup Language）是 ABAC 的一个实现，但是该设计过于复杂，我还没有完全理解，故不做介绍。</p>

<p>总结一下，ABAC 有如下特点：</p>

<ol>
<li> 集中化管理</li>
<li> 可以按需实现不同颗粒度的权限控制</li>
<li> 不需要预定义判断逻辑，减轻了权限系统的维护成本，特别是在需求经常变化的系统中</li>
<li> 定义权限时，不能直观看出用户和对象间的关系</li>
<li> 规则如果稍微复杂一点，或者设计混乱，会给管理者维护和追查带来麻烦</li>
<li> 权限判断需要实时执行，规则过多会导致性能问题</li>
</ol>

<p>既然 ABAC 这么好，那最流行的为什么还是 RBAC 呢？</p>

<p>我认为主要还是因为大部分系统对权限控制并没有过多的需求，而且 ABAC 的管理相对来说太复杂了。<a href="https://link.jianshu.com?t=http://blog.kubernetes.io/2017/04/rbac-support-in-kubernetes.html">Kubernetes 便因为 ABAC 太难用，在<code>1.8</code>版本里引入了 RBAC 的方案</a>。</p>

<blockquote>
<p>ABAC 有时也被称为 PBAC（Policy-Based Access Control）或 CBAC（Claims-Based Access Control）。</p>
</blockquote>

<h2 id="toc_14">结语</h2>

<p>权限系统设计可谓博大精深，这篇文章只是介绍了一点皮毛。</p>

<p>随着人类在信息化道路上越走越远，权限系统的设计也在不断创新，但目前好像处在了平台期。</p>

<p>可能因为在 RBAC 到 ABAC 之间有着巨大的鸿沟，无法轻易跨越，也可能是一些基于 RBAC 的微创新方案还不够规范化从而做到普及。不过在服务化架构的浪潮下，未来这一块必然有极高的需求，也许巨头们已经开始布局了。</p>

<h2 id="toc_15">参考文档</h2>

<p><a href="https://link.jianshu.com?t=https://support.microsoft.com/en-us/help/949608/changes-to-the-default-ntfs-discretionary-access-control-list-dacl-set">NTFS 文件系统权限</a></p>

<p><a href="https://link.jianshu.com?t=https://docs.oracle.com/cd/E56344_01/html/E53956/rbac-28.html#scrolltoc">Solaris 权限模型</a></p>

<p><a href="https://link.jianshu.com?t=https://baike.baidu.com/item/%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/8545517">百度百科：访问控制</a></p>

<p><a href="https://link.jianshu.com?t=https://www.centos.org/docs/5/html/Deployment_Guide-en-US/sec-mls-ov.html">Red Hat: Multi-Level Security (MLS)</a></p>

<p><a href="https://link.jianshu.com?t=http://files.cnblogs.com/files/Wenzy/an_introduction_to_rbac.pdf">冰云：An Introduction To Role-Based Access Control</a></p>

<p><a href="https://link.jianshu.com?t=https://csrc.nist.gov/projects/role-based-access-control">NIST: Role-Based Access Control</a></p>

<p><a href="https://link.jianshu.com?t=https://docs.mongodb.com/manual/core/authorization/">MongoDB RBAC</a></p>

<p><a href="https://link.jianshu.com?t=https://stackoverflow.com/questions/7770728/group-vs-role-any-real-difference">Stackoverflow: Group vs role Any real difference?</a></p>

<p><a href="https://link.jianshu.com?t=http://www.yiiframework.com/wiki/136/getting-to-understand-hierarchical-rbac-scheme/">Yii: Getting to Understand Hierarchical RBAC Scheme</a></p>

<p><a href="https://link.jianshu.com?t=http://www.informit.com/articles/article.aspx?p=782116">Role-Based Access Control in Computer Security</a></p>

<p><a href="https://link.jianshu.com?t=http://www.cis.syr.edu/%7Ewedu/Teaching/cis643/LectureNotes_New/RBAC.pdf">(Syracuse University: Role-Based Access Control (RBAC)</a></p>

<p><a href="https://link.jianshu.com?t=http://www.yiiframework.com/doc-2.0/guide-security-authorization.html">Yii 2.0 Guide</a></p>

<p><a href="https://link.jianshu.com?t=https://en.wikipedia.org/wiki/Computer_access_control">WIKIPEDIA: Computer access control</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[“Exit Trap” 让你的 Bash 脚本更稳固可靠]]></title>
    <link href="http://panlw.github.io/15263920307765.html"/>
    <updated>2018-05-15T21:47:10+08:00</updated>
    <id>http://panlw.github.io/15263920307765.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><a href="https://linux.cn/article-9639-1.html">https://linux.cn/article-9639-1.html</a> 2017-02-17<br/>
作者：<a href="http://redsymbol.net/">aaron maxwell</a> 译者：<a href="https://github.com/Dotcra">Dotcra</a> 校对：<a href="https://github.com/wxy">wxy</a><br/>
作者简介：美国加利福尼亚旧金山的作家，软件工程师，企业家。<a href="https://www.amazon.com/d/0692878971">Powerful Python</a> 的作者，他的 <a href="https://powerfulpython.com/blog/">blog</a>。</p>
</blockquote>

<p>有个简单实用的技巧可以让你的 bash 脚本更稳健 -- 确保总是执行必要的收尾工作，哪怕是在发生异常的时候。要做到这一点，秘诀就是 bash 提供的一个叫做 EXIT 的伪信号，你可以 <a href="http://www.gnu.org/software/bash/manual/bashref.html#index-trap">trap</a> 它，当脚本因为任何原因退出时，相应的命令或函数就会执行。我们来看看它是如何工作的。</p>

<p>基本的代码结构看起来像这样：</p>

<pre><code class="language-sh">#!/bin/bash
function finish {
  # 你的收尾代码
}
trap finish EXIT
</code></pre>

<p>你可以把任何你觉得务必要运行的代码放在这个 <code>finish</code> 函数里。一个很好的例子是：创建一个临时目录，事后再删除它。</p>

<pre><code class="language-sh">#!/bin/bash
scratch=$(mktemp -d -t tmp.XXXXXXXXXX)
function finish {
  rm -rf &quot;$scratch&quot;
}
trap finish EXIT
</code></pre>

<p>这样，在你的核心代码中，你就可以在这个 <code>$scratch</code> 目录里下载、生成、操作中间或临时数据了。<sup><a href="http://redsymbol.net/articles/bash-exit-traps/#footnote-1">注 1</a></sup></p>

<pre><code class="language-sh"># 下载所有版本的 linux 内核…… 为了科学研究！
for major in {1..4}; do
  for minor in {0..99}; do
    for patchlevel in {0..99}; do
      tarball=&quot;linux-${major}-${minor}-${patchlevel}.tar.bz2&quot;
      curl -q &quot;http://kernel.org/path/to/$tarball&quot; -o &quot;$scratch/$tarball&quot; || true
      if [ -f &quot;$scratch/$tarball&quot; ]; then
        tar jxf &quot;$scratch/$tarball&quot;
      fi
    done
  done
done
# 整合成单个文件
# 复制到目标位置
cp &quot;$scratch/frankenstein-linux.tar.bz2&quot; &quot;$1&quot;
# 脚本结束， scratch 目录自动被删除
</code></pre>

<p>比较一下如果不用 <code>trap</code> ，你是怎么删除 <code>scratch</code> 目录的：</p>

<pre><code class="language-sh">#!/bin/bash
# 别这样做！

scratch=$(mktemp -d -t tmp.XXXXXXXXXX)

# 在这里插入你的几十上百行代码

# 都搞定了，退出之前把目录删除
rm -rf &quot;$scratch&quot;
</code></pre>

<p>这有什么问题么？很多：</p>

<ul>
<li>  如果运行出错导致脚本提前退出， <code>scratch</code> 目录及里面的内容不会被删除。这会导致资料泄漏，可能引发安全问题。</li>
<li><p>如果这个脚本的设计初衷就是在脚本末尾以前退出，那么你必须手动复制粘贴 <code>rm</code> 命令到每一个出口。</p></li>
<li><p>这也给维护带来了麻烦。如果今后在脚本某处添加了一个 <code>exit</code> ，你很可能就忘了加上删除操作 -- 从而制造潜在的安全漏洞。</p></li>
</ul>

<h3 id="toc_0">无论如何，服务要在线</h3>

<p>另外一个场景： 想象一下你正在运行一些自动化系统运维任务，要临时关闭一项服务，最后这项服务需要重启，而且要万无一失，即使脚本运行出错。那么你可以这样做：</p>

<pre><code class="language-sh">function finish {
  # 重启服务
  sudo /etc/init.d/something start
}
trap finish EXIT
sudo /etc/init.d/something stop
# 主要任务代码

# 脚本结束，执行 finish 函数重启服务
</code></pre>

<p>一个具体的实例：比如 Ubuntu 服务器上运行着 MongoDB ，你要为 crond 写一个脚本来临时关闭服务并做一些日常维护工作。你应该这样写：</p>

<pre><code class="language-sh">function finish {
  # 重启服务
  sudo service mongdb start
}
trap finish EXIT
# 关闭 mongod 服务
sudo service mongdb stop
# （如果 mongod 配置了 fork ，比如 replica set ，你可能需要执行 “sudo killall --wait /usr/bin/mongod”）
</code></pre>

<h3 id="toc_1">控制开销</h3>

<p>有一种情况特别能体现 EXIT <code>trap</code> 的价值：如果你的脚本运行过程中需要初始化一下成本高昂的资源，结束时要确保把它们释放掉。比如你在 AWS (Amazon Web Services) 上工作，要在脚本中创建一个镜像。</p>

<p>（名词解释: 在亚马逊云上的运行的服务器叫 “<a href="http://aws.amazon.com/ec2/">实例</a>”。实例从<ruby>亚马逊机器镜像 <rt>Amazon Machine Image</rt></ruby> 创建而来，通常被称为 “AMI” 或 “镜像” 。AMI 相当于某个特殊时间点的服务器快照。）</p>

<p>我们可以这样创建一个自定义的 AMI ：</p>

<ol>
<li>基于一个基准 AMI 运行一个实例（例如，启动一个服务器）。</li>
<li>在实例中手动或运行脚本来做一些修改。</li>
<li>用修改后的实例创建一个镜像。</li>
<li>如果不再需要这个实例，可以将其删除。</li>
</ol>

<p>最后一步<strong>相当重要</strong>。如果你的脚本没有把实例删除掉，它会一直运行并计费。（到月底你的账单让你大跌眼镜时，恐怕哭都来不及了！）</p>

<p>如果把 AMI 的创建封装在脚本里，我们就可以利用 <code>trap</code> EXIT 来删除实例了。我们还可以用上 EC2 的命令行工具：</p>

<pre><code class="language-sh">#!/bin/bash
# 定义基准 AMI 的 ID
ami=$1
# 保存临时实例的 ID
instance=&#39;&#39;
# 作为 IT 人，让我们看看 scratch 目录的另类用法
scratch=$(mktemp -d -t tmp.XXXXXXXXXX)
function finish {
  if [ -n &quot;$instance&quot; ]; then
    ec2-terminate-instances &quot;$instance&quot;
  fi
  rm -rf &quot;$scratch&quot;
}
trap finish EXIT
# 创建实例，将输出(包含实例 ID )保存到 scratch 目录下的文件里
ec2-run-instances &quot;$ami&quot; &gt; &quot;$scratch/run-instance&quot;
# 提取实例 ID
instance=$(grep &#39;^INSTANCE&#39; &quot;$scratch/run-instance&quot; | cut -f 2)
</code></pre>

<p>脚本执行到这里，实例（EC2 服务器）已经开始运行 <sup><a href="http://redsymbol.net/articles/bash-exit-traps/#footnote-2">注 2</a>。接下来你可以做任何事情：在实例中安装软件，修改配置文件等，然后为最终版本创建一个镜像。实例会在脚本结束时被删除</sup> -- 即使脚本因错误而提前退出。（请确保实例创建成功后再运行业务代码。）</p>

<h3 id="toc_2">更多应用</h3>

<p>这篇文章只讲了些皮毛。我已经使用这个 bash 技巧很多年了，现在还能不时发现一些有趣的用法。你也可以把这个方法应用到你自己的场景中，从而提升你的 bash 脚本的可靠性。</p>

<h3 id="toc_3">尾注</h3>

<ul>
<li>  注 1. <code>mktemp</code> 的选项 <code>-t</code> 在 Linux 上是可选的，在 OS X 上是必需的。带上此选项可以让你的脚本有更好的可移植性。</li>
<li>  注 2. 如果只是为了获取实例 ID ，我们不用创建文件，直接写成 <code>instance=$(ec2-run-instances &quot;$ami&quot; | grep &#39;^INSTANCE&#39; | cut -f 2)</code> 就可以。但把输出写入文件可以记录更多有用信息，便于调试 ，代码可读性也更强。</li>
</ul>

]]></content>
  </entry>
  
</feed>
